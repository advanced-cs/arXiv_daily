# 自然语言处理 cs.CL

- **最新发布 135 篇**

- **更新 76 篇**

## 最新发布

#### [new 001] SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment
- **分类: cs.CL**

- **简介: 该论文提出SemPA方法，解决生成式大语言模型句向量优化问题，通过语义偏好对齐提升句表示，同时保持生成能力。**

- **链接: [https://arxiv.org/pdf/2601.05075v1](https://arxiv.org/pdf/2601.05075v1)**

> **作者:** Ziyang Chen; Zhenxuan Huang; Yile Wang; Weiqin Wang; Lu Yin; Hui Huang
>
> **摘要:** Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.
>
---
#### [new 002] LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于可解释AI任务，旨在解决模型决策解释不清晰、缺乏针对性的问题。提出LEXMA框架，通过强化学习微调生成符合不同受众的高质量解释。**

- **链接: [https://arxiv.org/pdf/2601.04208v1](https://arxiv.org/pdf/2601.04208v1)**

> **作者:** Xiang Cheng; Wen Wang; Anindya Ghose
>
> **摘要:** Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.
>
---
#### [new 003] DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于 hallucination 检测任务，旨在解决越南语大语言模型生成虚假信息的问题。构建了ViHallu数据集并开展挑战赛，探索有效检测方法。**

- **链接: [https://arxiv.org/pdf/2601.04711v1](https://arxiv.org/pdf/2601.04711v1)**

> **作者:** Anh Thi-Hoang Nguyen; Khanh Quoc Tran; Tin Van Huynh; Phuoc Tan-Hoang Nguyen; Cam Tan Nguyen; Kiet Van Nguyen
>
> **摘要:** The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations -- fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types -- factual, noisy, and adversarial -- to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\%, compared to a baseline encoder-only score of 32.83\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.
>
---
#### [new 004] EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis
- **分类: cs.CL**

- **简介: 该论文属于Text-to-SQL任务，旨在解决高质量数据稀缺问题。通过结构感知的进化方法生成多样且复杂的SQL数据，提升模型性能。**

- **链接: [https://arxiv.org/pdf/2601.04875v1](https://arxiv.org/pdf/2601.04875v1)**

> **作者:** Xuanguang Pan; Chongyang Tao; Jiayuan Bai; Jianling Gao; Zhengwei Tao; Xiansheng Zhou; Gavin Cheung; Shuai Ma
>
> **备注:** 18 pages
>
> **摘要:** Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.
>
---
#### [new 005] Dialect Matters: Cross-Lingual ASR Transfer for Low-Resource Indic Language Varieties
- **分类: cs.CL**

- **简介: 该论文属于语音识别任务，研究跨语言迁移在低资源印地语方言中的应用，解决方言和非标准化语音识别难题，通过实验与案例分析提升ASR性能。**

- **链接: [https://arxiv.org/pdf/2601.04373v1](https://arxiv.org/pdf/2601.04373v1)**

> **作者:** Akriti Dhasmana; Aarohi Srivastava; David Chiang
>
> **备注:** 12 pages, 3 figures, 10 tables
>
> **摘要:** We conduct an empirical study of cross-lingual transfer using spontaneous, noisy, and code-mixed speech across a wide range of Indic dialects and language varieties. Our results indicate that although ASR performance is generally improved with reduced phylogenetic distance between languages, this factor alone does not fully explain performance in dialectal settings. Often, fine-tuning on smaller amounts of dialectal data yields performance comparable to fine-tuning on larger amounts of phylogenetically-related, high-resource standardized languages. We also present a case study on Garhwali, a low-resource Pahari language variety, and evaluate multiple contemporary ASR models. Finally, we analyze transcription errors to examine bias toward pre-training languages, providing additional insight into challenges faced by ASR systems on dialectal and non-standardized speech.
>
---
#### [new 006] Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR
- **分类: cs.CL**

- **简介: 该论文属于角色扮演代理任务，旨在解决角色一致性问题。提出Character-R1框架，通过三种奖励机制提升角色感知推理能力。**

- **链接: [https://arxiv.org/pdf/2601.04611v1](https://arxiv.org/pdf/2601.04611v1)**

> **作者:** Yihong Tang; Kehai Chen; Xuefeng Bai; Benyou Wang; Zeming Liu; Haifeng Wang; Min Zhang
>
> **摘要:** Current role-playing agents (RPAs) are typically constructed by imitating surface-level behaviors, but this approach lacks internal cognitive consistency, often causing out-of-character errors in complex situations. To address this, we propose Character-R1, a framework designed to provide comprehensive verifiable reward signals for effective role-aware reasoning, which are missing in recent studies. Specifically, our framework comprises three core designs: (1) Cognitive Focus Reward, which enforces explicit label-based analysis of 10 character elements (e.g., worldview) to structure internal cognition; (2) Reference-Guided Reward, which utilizes overlap-based metrics with reference responses as optimization anchors to enhance exploration and performance; and (3) Character-Conditioned Reward Normalization, which adjusts reward distributions based on character categories to ensure robust optimization across heterogeneous roles. Extensive experiments demonstrate that Character-R1 significantly outperforms existing methods in knowledge, memory and others.
>
---
#### [new 007] RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection
- **分类: cs.CL**

- **简介: 该论文属于跨领域虚假信息检测任务，旨在解决现有方法在不同领域间泛化能力差的问题。提出RAAR框架，通过多视角检索与协同推理提升检测效果。**

- **链接: [https://arxiv.org/pdf/2601.04853v1](https://arxiv.org/pdf/2601.04853v1)**

> **作者:** Zhiwei Liu; Runteng Guo; Baojie Qu; Yuechen Jiang; Min Peng; Qianqian Xie; Sophia Ananiadou
>
> **摘要:** Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.
>
---
#### [new 008] RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于LLM安全研究任务，旨在解决领域内隐性有害提示生成问题。通过知识图谱引导生成和双重路径混淆重写，构建高质量隐性有害数据集。**

- **链接: [https://arxiv.org/pdf/2601.04740v1](https://arxiv.org/pdf/2601.04740v1)**

> **作者:** Huawei Zheng; Xinqi Jiang; Sen Yang; Shouling Ji; Yingcai Wu; Dazhen Deng
>
> **摘要:** Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.
>
---
#### [new 009] Complexity Agnostic Recursive Decomposition of Thoughts
- **分类: cs.CL; cs.AI; cs.IT**

- **简介: 该论文提出CARD框架，用于多步推理任务，解决大模型在复杂问题上的表现不足。通过预测问题复杂度并动态分解步骤，提升准确率并降低计算成本。**

- **链接: [https://arxiv.org/pdf/2601.04210v1](https://arxiv.org/pdf/2601.04210v1)**

> **作者:** Kaleem Ullah Qasim; Jiashu Zhang; Hafiz Saif Ur Rehman
>
> **备注:** 4
>
> **摘要:** Large language models often fail on multi-step reasoning due to fixed reasoning strategies that ignore problem specific difficulty. We introduce CARD (Complexity Agnostic Recursive Decomposition), a framework that predicts problem complexity before generation and adapts decomposition accordingly. Our system comprises MRCE (Multi-dimensional Reasoning Complexity Estimator), a 0.6B Qwen model predicting 30 fine-grained features from question text and a two-stage recursive solver: (1) hierarchical decomposition into K steps based on task profile and (2) per-step thought budget allocation (1, 5-9, or 10 thoughts) via recursive MRCE profiling. Evaluated on three reasoning models (Qwen3-0.6B, DeepSeek-R1-Distill-Qwen-1.5B, Qwen3-1.7B), CARD achieves 81.4% to 89.2% accuracy on GSM8K while reducing token cost by 1.88x to 2.40x compared to fixed decomposition baselines. On MATH-500, CARD reaches 75.1 to 86.8% accuracy using 1.71x to 5.74x fewer tokens. Our results demonstrate that preemptive complexity estimation enables both higher accuracy and significant efficiency gains.
>
---
#### [new 010] Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization
- **分类: cs.CL**

- **简介: 该论文属于文本到可视化的任务，旨在解决生成的图表语义不一致和质量低的问题。通过强化学习框架RL-Text2Vis，优化文本准确性、代码有效性和可视化质量。**

- **链接: [https://arxiv.org/pdf/2601.04582v1](https://arxiv.org/pdf/2601.04582v1)**

> **作者:** Mizanur Rahman; Mohammed Saidul Islam; Md Tahmid Rahman Laskar; Shafiq Joty; Enamul Hoque
>
> **备注:** Accepted to EACL Main Conference
>
> **摘要:** Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.
>
---
#### [new 011] Qwerty AI: Explainable Automated Age Rating and Content Safety Assessment for Russian-Language Screenplays
- **分类: cs.CL**

- **简介: 该论文提出Qwerty AI，用于俄语剧本的自动年龄评级和内容安全评估。解决的是根据法律要求自动化处理剧本的任务，通过模型实现快速准确的分类与评分。**

- **链接: [https://arxiv.org/pdf/2601.04211v1](https://arxiv.org/pdf/2601.04211v1)**

> **作者:** Nikita Zmanovskii
>
> **备注:** 15 pages, 7 tables, 1 figure, 4 appendices. System paper describing automated age-rating for Russian screenplays using fine-tuned Phi-3-mini. Includes baseline comparisons, human evaluation, and production deployment. Code and model weights available at https://github.com/nikita-zmanovskiy/qwertyAI. Developed during Wink Hackathon, November 2025
>
> **摘要:** We present Qwerty AI, an end-to-end system for automated age-rating and content-safety assessment of Russian-language screenplays according to Federal Law No. 436-FZ. The system processes full-length scripts (up to 700 pages in under 2 minutes), segments them into narrative units, detects content violations across five categories (violence, sexual content, profanity, substances, frightening elements), and assigns age ratings (0+, 6+, 12+, 16+, 18+) with explainable justifications. Our implementation leverages a fine-tuned Phi-3-mini model with 4-bit quantization, achieving 80% rating accuracy and 80-95% segmentation precision (format-dependent). The system was developed under strict constraints: no external API calls, 80GB VRAM limit, and <5 minute processing time for average scripts. Deployed on Yandex Cloud with CUDA acceleration, Qwerty AI demonstrates practical applicability for production workflows. We achieved these results during the Wink hackathon (November 2025), where our solution addressed real editorial challenges in the Russian media industry.
>
---
#### [new 012] DocDancer: Towards Agentic Document-Grounded Information Seeking
- **分类: cs.CL**

- **简介: 该论文提出DocDancer，解决文档驱动的信息获取问题。针对DocQA中工具使用不足和数据稀缺，设计端到端框架与数据合成方法，提升模型效果。**

- **链接: [https://arxiv.org/pdf/2601.05163v1](https://arxiv.org/pdf/2601.05163v1)**

> **作者:** Qintong Zhang; Xinjie Lv; Jialong Wu; Baixuan Li; Zhengwei Tao; Guochen Yan; Huanyao Zhang; Bin Wang; Jiahao Xu; Haitao Mi; Wentao Zhang
>
> **摘要:** Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.
>
---
#### [new 013] CRANE: Causal Relevance Analysis of Language-Specific Neurons in Multilingual Large Language Models
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于自然语言处理任务，旨在解决多语言大模型中语言特异性神经元的识别问题。通过CRANE框架，基于功能必要性而非激活强度分析神经元作用，更精准地隔离语言特异性组件。**

- **链接: [https://arxiv.org/pdf/2601.04664v1](https://arxiv.org/pdf/2601.04664v1)**

> **作者:** Yifan Le; Yunliang Li
>
> **备注:** 10 pages, 6 figures. Work in progress
>
> **摘要:** Multilingual large language models (LLMs) achieve strong performance across languages, yet how language capabilities are organized at the neuron level remains poorly understood. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. Prior work has identified language-related neurons mainly through activation-based heuristics, which conflate language preference with functional importance. We propose CRANE, a relevance-based analysis framework that redefines language specificity in terms of functional necessity, identifying language-specific neurons through targeted neuron-level interventions. CRANE characterizes neuron specialization by their contribution to language-conditioned predictions rather than activation magnitude. Our implementation will be made publicly available. Neuron-level interventions reveal a consistent asymmetric pattern: masking neurons relevant to a target language selectively degrades performance on that language while preserving performance on other languages to a substantial extent, indicating language-selective but non-exclusive neuron specializations. Experiments on English, Chinese, and Vietnamese across multiple benchmarks, together with a dedicated relevance-based metric and base-to-chat model transfer analysis, show that CRANE isolates language-specific components more precisely than activation-based methods.
>
---
#### [new 014] AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs
- **分类: cs.CL**

- **简介: 该论文属于多模态对话安全任务，旨在解决多轮多模态LLMs的安全漏洞问题。通过构建数据集和提出AM$^3$Safety框架，提升模型的安全性与有用性。**

- **链接: [https://arxiv.org/pdf/2601.04736v1](https://arxiv.org/pdf/2601.04736v1)**

> **作者:** Han Zhu; Jiale Chen; Chengkun Cai; Shengjie Sun; Haoran Li; Yujin Zhou; Chi-Min Chan; Pengcheng Wen; Lei Li; Sirui Han; Yike Guo
>
> **摘要:** Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\% in harmless dimension and over 13\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.
>
---
#### [new 015] TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文提出TeleTables基准，用于评估大语言模型在电信表格解释中的表现。针对LLM在3GPP标准表格理解上的不足，通过生成高质量问答对，揭示模型在领域知识和推理能力上的短板。**

- **链接: [https://arxiv.org/pdf/2601.04202v1](https://arxiv.org/pdf/2601.04202v1)**

> **作者:** Anas Ezzakri; Nicola Piovesan; Mohamed Sana; Antonio De Domenico; Fadhel Ayed; Haozhe Zhang
>
> **摘要:** Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.
>
---
#### [new 016] RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation
- **分类: cs.CL; cs.IR**

- **简介: 该论文属于RAG系统评估任务，旨在解决现有评估方法无法细致分析错误来源的问题。提出RAGVUE框架，实现可解释的自动化评估。**

- **链接: [https://arxiv.org/pdf/2601.04196v1](https://arxiv.org/pdf/2601.04196v1)**

> **作者:** Keerthana Murugaraj; Salima Lamsiyah; Martin Theobald
>
> **摘要:** Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub
>
---
#### [new 017] Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization
- **分类: cs.CL**

- **简介: 该论文属于大模型推理任务，旨在提升模型的域外泛化能力。通过引入错误推理样本，优化训练过程，提出GLOW方法，显著提升模型性能。**

- **链接: [https://arxiv.org/pdf/2601.04992v1](https://arxiv.org/pdf/2601.04992v1)**

> **作者:** Xueyun Tian; Minghua Ma; Bingbing Xu; Nuoyan Lyu; Wei Li; Heng Dong; Zheng Chu; Yuanzhuo Wang; Huawei Shen
>
> **备注:** Code and data are available at https://github.com/Eureka-Maggie/GLOW
>
> **摘要:** Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.
>
---
#### [new 018] Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis
- **分类: cs.CL**

- **简介: 该论文提出Mind2Report，解决商业报告生成质量与可靠性问题，通过模拟分析师思维流程，提升报告的准确性与全面性。**

- **链接: [https://arxiv.org/pdf/2601.04879v1](https://arxiv.org/pdf/2601.04879v1)**

> **作者:** Mingyue Cheng; Daoyu Wang; Qi Liu; Shuo Yu; Xiaoyu Tao; Yuqian Wang; Chengzhong Chu; Yu Duan; Mingkang Long; Enhong Chen
>
> **备注:** 26 Pages, 9 Figures, 7 Tables
>
> **摘要:** Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.
>
---
#### [new 019] MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News
- **分类: cs.CL**

- **简介: 该论文提出MisSpans，一个用于细粒度虚假片段识别的多领域基准，解决现有方法在句子级别上无法准确识别和解释虚假信息的问题。**

- **链接: [https://arxiv.org/pdf/2601.04857v1](https://arxiv.org/pdf/2601.04857v1)**

> **作者:** Zhiwei Liu; Paul Thompson; Jiaqi Rong; Baojie Qu; Runteng Guo; Min Peng; Qianqian Xie; Sophia Ananiadou
>
> **备注:** Work in progress
>
> **摘要:** Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.
>
---
#### [new 020] ToolGate: Contract-Grounded and Verified Tool Execution for LLMs
- **分类: cs.CL; cs.AI; cs.FL**

- **简介: 论文提出ToolGate，解决LLM工具调用中的安全与可验证问题。通过形式化合约和符号状态空间，确保工具执行的逻辑安全与结果可信。属于AI系统可靠性提升任务。**

- **链接: [https://arxiv.org/pdf/2601.04688v1](https://arxiv.org/pdf/2601.04688v1)**

> **作者:** Yanming Liu; Xinyue Peng; Jiannan Cao; Xinyi Wang; Songhang Deng; Jintao Chen; Jianwei Yin; Xuhong Zhang
>
> **备注:** First version of ToolGate
>
> **摘要:** Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.
>
---
#### [new 021] Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems
- **分类: cs.CL; cs.AI; cs.CY; cs.HC**

- **简介: 该论文属于自然语言处理任务，旨在解决本地化问答系统中的知识盲点问题。通过社区协作构建结构化叙事数据，提升AI对本地信息的理解与回答能力。**

- **链接: [https://arxiv.org/pdf/2601.04201v1](https://arxiv.org/pdf/2601.04201v1)**

> **作者:** Zihan Gao; Mohsin Y. K. Yousufi; Jacob Thebault-Spieker
>
> **备注:** 9 pages, 2 figures, Presented at the NeurIPS 2025 ACA Workshop https://acaworkshop.github.io/accepted-papers.html,
>
> **摘要:** Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.
>
---
#### [new 022] NC2C: Automated Convexification of Generic Non-Convex Optimization Problems
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于优化问题自动化处理任务，旨在解决非凸优化问题难以求解的问题，通过构建NC2C框架实现自动凸化。**

- **链接: [https://arxiv.org/pdf/2601.04789v1](https://arxiv.org/pdf/2601.04789v1)**

> **作者:** Xinyue Peng; Yanming Liu; Yihan Cang; Yuwei Zhang; Xinyi Wang; Songhang Deng; Jiannan Cao
>
> **备注:** First version of NC2C
>
> **摘要:** Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\% execution rate and a 76\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.
>
---
#### [new 023] LinguaGame: A Linguistically Grounded Game-Theoretic Paradigm for Multi-Agent Dialogue Generation
- **分类: cs.CL**

- **简介: 该论文提出LinguaGame，用于多智能体对话生成，解决通信效率问题。通过语言学引导的游戏理论模型，提升对话中意图传达的准确性与效率。**

- **链接: [https://arxiv.org/pdf/2601.04516v1](https://arxiv.org/pdf/2601.04516v1)**

> **作者:** Yuxiao Ye; Yiming Zhang; Yiran Ma; Huiyuan Xie; Huining Zhu; Zhiyuan Liu
>
> **摘要:** Large Language Models (LLMs) have enabled Multi-Agent Systems (MASs) where agents interact through natural language to solve complex tasks or simulate multi-party dialogues. Recent work on LLM-based MASs has mainly focused on architecture design, such as role assignment and workflow orchestration. In contrast, this paper targets the interaction process itself, aiming to improve agents' communication efficiency by helping them convey their intended meaning more effectively through language. To this end, we propose LinguaGame, a linguistically-grounded game-theoretic paradigm for multi-agent dialogue generation. Our approach models dialogue as a signalling game over communicative intents and strategies, solved with a training-free equilibrium approximation algorithm for inference-time decision adjustment. Unlike prior game-theoretic MASs, whose game designs are often tightly coupled with task-specific objectives, our framework relies on linguistically informed reasoning with minimal task-specific coupling. Specifically, it treats dialogue as intentional and strategic communication, requiring agents to infer what others aim to achieve (intents) and how they pursue those goals (strategies). We evaluate our framework in simulated courtroom proceedings and debates, with human expert assessments showing significant gains in communication efficiency.
>
---
#### [new 024] Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning
- **分类: cs.CL; cs.LG**

- **简介: 该论文属于自然语言处理任务，解决LLM微调中的内存消耗问题。提出一种基于先验信息的零阶优化方法，提升梯度估计效率与收敛速度。**

- **链接: [https://arxiv.org/pdf/2601.04710v1](https://arxiv.org/pdf/2601.04710v1)**

> **作者:** Feihu Jin; Shipeng Cen; Ying Tan
>
> **备注:** 12pages, 6figures
>
> **摘要:** Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.
>
---
#### [new 025] Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents
- **分类: cs.CL**

- **简介: 该论文属于角色扮演代理研究，解决角色身份建模问题。提出“角色身份”概念，区分参数身份与属性身份，分析名人角色与合成角色的表现差异。**

- **链接: [https://arxiv.org/pdf/2601.04716v1](https://arxiv.org/pdf/2601.04716v1)**

> **作者:** Yonghyun Jun; Junhyuk Choi; Jihyeong Park; Hwanhee Lee
>
> **备注:** 27 pages
>
> **摘要:** Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \textit{"Fame Fades"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \textit{"Nature Remains"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.
>
---
#### [new 026] Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence
- **分类: cs.CL**

- **简介: 该论文属于大模型推理加速任务，解决Judge Decoding中依赖昂贵监督的问题。通过KL散度提出无需训练的验证机制，提升鲁棒性并消除监督瓶颈。**

- **链接: [https://arxiv.org/pdf/2601.04766v1](https://arxiv.org/pdf/2601.04766v1)**

> **作者:** Shengyin Sun; Yiming Li; Renxi Liu; Weizhe Lin; Hui-Ling Zhen; Xianzhi Yu; Mingxuan Yuan; Chen Ma
>
> **备注:** 16 pages
>
> **摘要:** Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.
>
---
#### [new 027] Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems
- **分类: cs.CL**

- **简介: 该论文属于长期个性化对话系统任务，旨在解决记忆噪声和人格不一致问题。提出PersonaTree框架与MemListener模型，实现高效、一致的用户记忆管理。**

- **链接: [https://arxiv.org/pdf/2601.05171v1](https://arxiv.org/pdf/2601.05171v1)**

> **作者:** Jihao Zhao; Ding Chen; Zhaoxin Fan; Kerun Xu; Mengting Hu; Bo Tang; Feiyu Xiong; Zhiyu li
>
> **摘要:** Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.
>
---
#### [new 028] RelayLLM: Efficient Reasoning via Collaborative Decoding
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文属于自然语言处理任务，解决大模型计算成本高、小模型推理能力弱的问题。提出RelayLLM框架，通过协同解码实现高效推理。**

- **链接: [https://arxiv.org/pdf/2601.05167v1](https://arxiv.org/pdf/2601.05167v1)**

> **作者:** Chengsong Huang; Tong Zheng; Langlin Huang; Jinyuan Li; Haolin Liu; Jiaxin Huang
>
> **摘要:** Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively "relaying" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.
>
---
#### [new 029] Text as a Universal Interface for Transferable Personalization
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于个性化任务，解决大模型中用户偏好表示不透明、难以迁移的问题。提出使用自然语言作为通用接口，构建可解释、可迁移的偏好模型。**

- **链接: [https://arxiv.org/pdf/2601.04963v1](https://arxiv.org/pdf/2601.04963v1)**

> **作者:** Yuting Liu; Jian Guan; Jia-Nan Li; Wei Wu; Jiang-Ming Yang; Jianzhe Zhao; Guibing Guo
>
> **摘要:** We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.
>
---
#### [new 030] Disco-RAG: Discourse-Aware Retrieval-Augmented Generation
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文提出Disco-RAG，用于知识密集型任务，解决传统RAG无法捕捉文本结构的问题。通过构建话语树和修辞图，增强生成效果。**

- **链接: [https://arxiv.org/pdf/2601.04377v1](https://arxiv.org/pdf/2601.04377v1)**

> **作者:** Dongqi Liu; Hang Ding; Qiming Feng; Jian Li; Xurong Xie; Zhucun Xue; Chengjie Wang; Jiangning Zhang; Yabiao Wang
>
> **摘要:** Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.
>
---
#### [new 031] STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于文本生成任务，旨在提升扩散语言模型的效率与质量。针对现有方法依赖固定阈值的问题，提出动态调整信心阈值的策略，基于时序和空间特征优化解码过程。**

- **链接: [https://arxiv.org/pdf/2601.04205v1](https://arxiv.org/pdf/2601.04205v1)**

> **作者:** Xinhao Sun; Maoliang Li; Zihao Zheng; Jiayu Chen; Hezhao Xu; Yun Liang; Xiang Chen
>
> **摘要:** Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.
>
---
#### [new 032] Agent-as-a-Judge
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于AI评估任务，旨在解决传统LLM评估方法的局限性。通过引入Agent-as-a-Judge，提升评估的可靠性与准确性。**

- **链接: [https://arxiv.org/pdf/2601.05111v1](https://arxiv.org/pdf/2601.05111v1)**

> **作者:** Runyang You; Hongru Cai; Caiqi Zhang; Qiancheng Xu; Meng Liu; Tiezheng Yu; Yongqi Li; Wenjie Li
>
> **摘要:** LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.
>
---
#### [new 033] TrueBrief: Faithful Summarization through Small Language Models
- **分类: cs.CL; cs.AI**

- **简介: 论文提出TrueBrief，针对文本摘要任务，解决小语言模型生成内容不忠实的问题，通过偏好优化提升摘要的准确性。**

- **链接: [https://arxiv.org/pdf/2601.04212v1](https://arxiv.org/pdf/2601.04212v1)**

> **作者:** Kumud Lakara; Ruibo Shi; Fran Silavong
>
> **摘要:** Large language models (LLMs) have exhibited remarkable proficiency in generating high-quality text; however, their propensity for producing hallucinations poses a significant challenge for their deployment in security-critical domains. In this work, we present TrueBrief, an end-to-end framework specifically designed to enhance the faithfulness of small LLMs (SLMs) primarily for the task of text summarization through a preference-optimization paradigm. Central to our framework is a data generation module that facilitates controlled hallucination injection to generate synthetic preference data. Our work provides insights into the impact of data quality and model size on preference-based optimization, highlighting the conditions under which these methods are most effective.
>
---
#### [new 034] Leveraging Language Models and RAG for Efficient Knowledge Discovery in Clinical Environments
- **分类: cs.CL**

- **简介: 该论文属于医学知识发现任务，解决医院内敏感数据处理问题，通过RAG系统结合PubMedBERT和本地LLaMA3模型，实现研究合作者推荐。**

- **链接: [https://arxiv.org/pdf/2601.04209v1](https://arxiv.org/pdf/2601.04209v1)**

> **作者:** Seokhwan Ko; Donghyeon Lee; Jaewoo Chun; Hyungsoo Han; Junghwan Cho
>
> **备注:** 11pages, 3 figures
>
> **摘要:** Large language models (LLMs) are increasingly recognized as valuable tools across the medical environment, supporting clinical, research, and administrative workflows. However, strict privacy and network security regulations in hospital settings require that sensitive data be processed within fully local infrastructures. Within this context, we developed and evaluated a retrieval-augmented generation (RAG) system designed to recommend research collaborators based on PubMed publications authored by members of a medical institution. The system utilizes PubMedBERT for domain-specific embedding generation and a locally deployed LLaMA3 model for generative synthesis. This study demonstrates the feasibility and utility of integrating domain-specialized encoders with lightweight LLMs to support biomedical knowledge discovery under local deployment constraints.
>
---
#### [new 035] GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决LLM在无证据时生成错误回答或虚构内容的问题。提出GRACE框架，通过强化学习实现有依据的回答和可靠拒绝。**

- **链接: [https://arxiv.org/pdf/2601.04525v1](https://arxiv.org/pdf/2601.04525v1)**

> **作者:** Yibo Zhao; Jiapeng Zhu; Zichen Ding; Xiang Li
>
> **备注:** 18 pages
>
> **摘要:** Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..
>
---
#### [new 036] Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval
- **分类: cs.CL**

- **简介: 该论文属于事实验证任务，旨在解决LLM hallucinations问题。提出Tool-MAD框架，通过多代理辩论和外部工具增强，提升验证准确性与适应性。**

- **链接: [https://arxiv.org/pdf/2601.04742v1](https://arxiv.org/pdf/2601.04742v1)**

> **作者:** Seyeon Jeong; Yeonjun Choi; JongWook Kim; Beakcheol Jang
>
> **摘要:** Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.
>
---
#### [new 037] How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness
- **分类: cs.CL; econ.GN**

- **简介: 该论文研究情感提示对AI与人类互动的影响，探讨情感如何塑造ChatGPT回应及后续人际沟通。属于人机交互任务，旨在分析情绪对AI行为和人类反应的影响。**

- **链接: [https://arxiv.org/pdf/2601.05104v1](https://arxiv.org/pdf/2601.05104v1)**

> **作者:** Florence Bernays; Marco Henriques Pereira; Jochen Menges
>
> **摘要:** This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.
>
---
#### [new 038] BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于文本水印任务，旨在解决低资源语言（如孟加拉语）在跨语言翻译攻击下的水印鲁棒性问题。通过提出分层水印策略，提升检测准确率。**

- **链接: [https://arxiv.org/pdf/2601.04534v1](https://arxiv.org/pdf/2601.04534v1)**

> **作者:** Amit Bin Tariqul; A N M Zahid Hossain Milkan; Sahab-Al-Chowdhury; Syed Rifat Raiyan; Hasan Mahmud; Md Kamrul Hasan
>
> **备注:** Under review, 12 pages, 7 figures, 5 tables
>
> **摘要:** As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\times$ to 4$\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.
>
---
#### [new 039] Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking
- **分类: cs.CL**

- **简介: 该论文提出Qwen3-VL-Embedding和Qwen3-VL-Reranker，解决多模态检索与排序问题，通过统一表示空间提升搜索精度。**

- **链接: [https://arxiv.org/pdf/2601.04720v1](https://arxiv.org/pdf/2601.04720v1)**

> **作者:** Mingxin Li; Yanzhao Zhang; Dingkun Long; Keqin Chen; Sibo Song; Shuai Bai; Zhibo Yang; Pengjun Xie; An Yang; Dayiheng Liu; Jingren Zhou; Junyang Lin
>
> **摘要:** In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\textbf{2B}$ and $\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.
>
---
#### [new 040] Measuring and Fostering Peace through Machine Learning and Artificial Intelligence
- **分类: cs.CL; cs.CY; cs.LG**

- **简介: 该论文属于情感分析与社会影响研究，旨在测量和平水平并促进积极媒体内容。通过AI模型分析新闻和社交媒体，开发工具帮助用户了解媒体影响。**

- **链接: [https://arxiv.org/pdf/2601.05232v1](https://arxiv.org/pdf/2601.05232v1)**

> **作者:** P. Gilda; P. Dungarwal; A. Thongkham; E. T. Ajayi; S. Choudhary; T. M. Terol; C. Lam; J. P. Araujo; M. McFadyen-Mungalln; L. S. Liebovitch; P. T. Coleman; H. West; K. Sieck; S. Carter
>
> **备注:** 6 pages, 4 figures
>
> **摘要:** We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.
>
---
#### [new 041] RIGOURATE: Quantifying Scientific Exaggeration with Evidence-Aligned Claim Evaluation
- **分类: cs.CL**

- **简介: 该论文提出RIGOURATE，用于量化科学陈述的夸大程度，解决科学写作中过度主张的问题。通过证据检索和评分模型，提升科学沟通的透明度与准确性。**

- **链接: [https://arxiv.org/pdf/2601.04350v1](https://arxiv.org/pdf/2601.04350v1)**

> **作者:** Joseph James; Chenghao Xiao; Yucheng Li; Nafise Sadat Moosavi; Chenghua Lin
>
> **摘要:** Scientific rigour tends to be sidelined in favour of bold statements, leading authors to overstate claims beyond what their results support. We present RIGOURATE, a two-stage multimodal framework that retrieves supporting evidence from a paper's body and assigns each claim an overstatement score. The framework consists of a dataset of over 10K claim-evidence sets from ICLR and NeurIPS papers, annotated using eight LLMs, with overstatement scores calibrated using peer-review comments and validated through human evaluation. It employes a fine-tuned reranker for evidence retrieval and a fine-tuned model to predict overstatement scores with justification. Compared to strong baselines, RIGOURATE enables improved evidence retrieval and overstatement detection. Overall, our work operationalises evidential proportionality and supports clearer, more transparent scientific communication.
>
---
#### [new 042] Learning to Simulate Human Dialogue
- **分类: cs.CL**

- **简介: 该论文属于对话生成任务，旨在提升对话预测效果。通过比较不同学习方法，发现直接优化真实对话的对数概率优于基于评判器的奖励，提升了预测准确性。**

- **链接: [https://arxiv.org/pdf/2601.04436v1](https://arxiv.org/pdf/2601.04436v1)**

> **作者:** Kanishk Gandhi; Agam Bhatia; Noah D. Goodman
>
> **备注:** Kanishk Gandhi and Agam Bhatia contributed equally
>
> **摘要:** To predict what someone will say is to model how they think. We study this through next-turn dialogue prediction: given a conversation, predict the next utterance produced by a person. We compare learning approaches along two dimensions: (1) whether the model is allowed to think before responding, and (2) how learning is rewarded either through an LLM-as-a-judge that scores semantic similarity and information completeness relative to the ground-truth response, or by directly maximizing the log-probability of the true human dialogue. We find that optimizing for judge-based rewards indeed increases judge scores throughout training, however it decreases the likelihood assigned to ground truth human responses and decreases the win rate when human judges choose the most human-like response among a real and synthetic option. This failure is amplified when the model is allowed to think before answering. In contrast, by directly maximizing the log-probability of observed human responses, the model learns to better predict what people actually say, improving on both log-probability and win rate evaluations. Treating chain-of-thought as a latent variable, we derive a lower bound on the log-probability. Optimizing this objective yields the best results on all our evaluations. These results suggest that thinking helps primarily when trained with a distribution-matching objective grounded in real human dialogue, and that scaling this approach to broader conversational data may produce models with a more nuanced understanding of human behavior.
>
---
#### [new 043] Identifying Good and Bad Neurons for Task-Level Controllable LLMs
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于自然语言处理任务，旨在解决LLM中神经元角色识别问题。提出NeuronLLM框架，通过对比学习区分促进与抑制任务的神经元，提升对LLM功能组织的理解。**

- **链接: [https://arxiv.org/pdf/2601.04548v1](https://arxiv.org/pdf/2601.04548v1)**

> **作者:** Wenjie Li; Guansong Pang; Hezhe Qiao; Debin Gao; David Lo
>
> **摘要:** Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.
>
---
#### [new 044] ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models
- **分类: cs.CL**

- **简介: 该论文提出ARREST框架，解决大语言模型的事实性和安全性问题。通过调节潜在空间中的表征偏差，提升模型输出的准确性和安全性。属于模型安全与事实对齐任务。**

- **链接: [https://arxiv.org/pdf/2601.04394v1](https://arxiv.org/pdf/2601.04394v1)**

> **作者:** Sharanya Dasgupta; Arkaprabha Basu; Sujoy Nath; Swagatam Das
>
> **摘要:** Human cognition, driven by complex neurochemical processes, oscillates between imagination and reality and learns to self-correct whenever such subtle drifts lead to hallucinations or unsafe associations. In recent years, LLMs have demonstrated remarkable performance in a wide range of tasks. However, they still lack human cognition to balance factuality and safety. Bearing the resemblance, we argue that both factual and safety failures in LLMs arise from a representational misalignment in their latent activation space, rather than addressing those as entirely separate alignment issues. We hypothesize that an external network, trained to understand the fluctuations, can selectively intervene in the model to regulate falsehood into truthfulness and unsafe output into safe output without fine-tuning the model parameters themselves. Reflecting the hypothesis, we propose ARREST (Adversarial Resilient Regulation Enhancing Safety and Truth), a unified framework that identifies and corrects drifted features, engaging both soft and hard refusals in addition to factual corrections. Our empirical results show that ARREST not only regulates misalignment but is also more versatile compared to the RLHF-aligned models in generating soft refusals due to adversarial training. We make our codebase available at https://github.com/sharanya-dasgupta001/ARREST.
>
---
#### [new 045] Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文属于自然语言生成任务，解决传统模型依赖采样的不稳定性问题。通过连续token动态演化，在离散化前实现稳定生成。**

- **链接: [https://arxiv.org/pdf/2601.04854v1](https://arxiv.org/pdf/2601.04854v1)**

> **作者:** Oshri Naparstek
>
> **备注:** In preperation to ICML 2026
>
> **摘要:** Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics. In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space. We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function. To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.
>
---
#### [new 046] Compositional Steering of Large Language Models with Steering Tokens
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文属于自然语言处理任务，解决多行为控制问题。提出组合引导令牌，实现对大语言模型的多行为同时引导，提升控制效果。**

- **链接: [https://arxiv.org/pdf/2601.05062v1](https://arxiv.org/pdf/2601.05062v1)**

> **作者:** Gorjan Radevski; Kiril Gashteovski; Giwon Hong; Carolin Lawrence; Goran Glavaš
>
> **摘要:** Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.
>
---
#### [new 047] Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于多智能体系统研究，旨在探讨权威角色对对话影响的机制。通过分析不同权威类型的作用，揭示了权威偏见的形成方式及条件。**

- **链接: [https://arxiv.org/pdf/2601.04790v1](https://arxiv.org/pdf/2601.04790v1)**

> **作者:** Junhyuk Choi; Jeongyoun Kwon; Heeju Kim; Haeun Cho; Hayeong Jung; Sehee Min; Bugeun Kim
>
> **备注:** Preprint
>
> **摘要:** Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.
>
---
#### [new 048] PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks
- **分类: cs.CL; cs.AI**

- **简介: 该论文提出PILOT-Bench，用于评估专利领域法律推理能力。解决LLMs在专利法律任务中的系统性评估问题，通过三个IRAC对齐的分类任务进行测试。**

- **链接: [https://arxiv.org/pdf/2601.04758v1](https://arxiv.org/pdf/2601.04758v1)**

> **作者:** Yehoon Jang; Chaewon Lee; Hyun-seok Min; Sungchul Choi
>
> **备注:** Accepted at the NLLP Workshop at EMNLP 2025
>
> **摘要:** The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.
>
---
#### [new 049] Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei
- **分类: cs.CL**

- **简介: 该论文属于行为检测任务，旨在解决子文化中自我毁灭行为识别中的语义差异问题。提出SAS框架提升LLM检测效果。**

- **链接: [https://arxiv.org/pdf/2601.05004v1](https://arxiv.org/pdf/2601.05004v1)**

> **作者:** Peng Wang; Xilin Tao; Siyi Yao; Jiageng Wu; Yuntao Zou; Zhuotao Tian; Libo Qin; Dagang Li
>
> **备注:** Preprint
>
> **摘要:** Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.
>
---
#### [new 050] SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于医疗咨询任务，解决医学语音数据不足和模型微调效率低的问题。提出SpeechMedAssist，通过两阶段训练提升模型性能，减少对语音数据的依赖。**

- **链接: [https://arxiv.org/pdf/2601.04638v1](https://arxiv.org/pdf/2601.04638v1)**

> **作者:** Sirry Chen; Jieyi Wang; Wei Chen; Zhongyu Wei
>
> **摘要:** Medical consultations are intrinsically speech-centric. However, most prior works focus on long-text-based interactions, which are cumbersome and patient-unfriendly. Recent advances in speech language models (SpeechLMs) have enabled more natural speech-based interaction, yet the scarcity of medical speech data and the inefficiency of directly fine-tuning on speech data jointly hinder the adoption of SpeechLMs in medical consultation. In this paper, we propose SpeechMedAssist, a SpeechLM natively capable of conducting speech-based multi-turn interactions with patients. By exploiting the architectural properties of SpeechLMs, we decouple the conventional one-stage training into a two-stage paradigm consisting of (1) Knowledge & Capability Injection via Text and (2) Modality Re-alignment with Limited Speech Data, thereby reducing the requirement for medical speech data to only 10k synthesized samples. To evaluate SpeechLMs for medical consultation scenarios, we design a benchmark comprising both single-turn question answering and multi-turn simulated interactions. Experimental results show that our model outperforms all baselines in both effectiveness and robustness in most evaluation settings.
>
---
#### [new 051] Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization
- **分类: cs.CL**

- **简介: 该论文聚焦于长文本法律摘要任务，解决LLMs在处理长文档时效果不佳的问题。提出Gavel-Ref评估框架，并开发Gavel-Agent提高摘要效率。**

- **链接: [https://arxiv.org/pdf/2601.04424v1](https://arxiv.org/pdf/2601.04424v1)**

> **作者:** Yao Dou; Wei Xu
>
> **备注:** webpage at https://yao-dou.github.io/gavel/
>
> **摘要:** Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.
>
---
#### [new 052] Differential syntactic and semantic encoding in LLMs
- **分类: cs.CL; cs.AI; cs.LG; physics.comp-ph**

- **简介: 该论文研究LLM中语法和语义信息的编码方式，通过分析DeepSeek-V3的层表示，发现语法和语义可部分线性分离，揭示其编码差异。任务为语言模型表征分析，解决语法与语义如何编码的问题。**

- **链接: [https://arxiv.org/pdf/2601.04765v1](https://arxiv.org/pdf/2601.04765v1)**

> **作者:** Santiago Acevedo; Alessandro Laio; Marco Baroni
>
> **摘要:** We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.
>
---
#### [new 053] ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG
- **分类: cs.CL; cs.AI**

- **简介: 该论文提出ArcAligner，用于解决RAG中压缩上下文导致模型理解困难的问题。通过自适应机制提升压缩后上下文的生成效果，适用于知识密集型问答任务。**

- **链接: [https://arxiv.org/pdf/2601.05038v1](https://arxiv.org/pdf/2601.05038v1)**

> **作者:** Jianbo Li; Yi Jiang; Sendong Zhao; Bairui Hu; Haochun Wang; Bing Qin
>
> **备注:** Code is available at https://github.com/liunian-Jay/ArcAligner.git
>
> **摘要:** Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.
>
---
#### [new 054] Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences
- **分类: cs.CL**

- **简介: 该论文属于文本检测任务，旨在解决AI生成的说服性文本是否更难被自动检测的问题。通过构建多语言基准并分析语言差异，评估AI与人类生成文本的检测难度。**

- **链接: [https://arxiv.org/pdf/2601.04925v1](https://arxiv.org/pdf/2601.04925v1)**

> **作者:** Arkadiusz Modzelewski; Paweł Golik; Anna Kołos; Giovanni Da San Martino
>
> **备注:** Preprint; Paper is currently under review at a major NLP conference
>
> **摘要:** Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.
>
---
#### [new 055] Users Mispredict Their Own Preferences for AI Writing Assistance
- **分类: cs.CL; cs.HC**

- **简介: 该论文属于人机交互任务，探讨用户对AI写作辅助的偏好预测问题。研究发现用户对紧急程度的自我报告与实际行为不一致，导致系统设计效果不佳。通过实验验证了行为数据优于自述数据。**

- **链接: [https://arxiv.org/pdf/2601.04461v1](https://arxiv.org/pdf/2601.04461v1)**

> **作者:** Vivian Lai; Zana Buçinca; Nil-Jana Akpinar; Mo Houtti; Hyeonsu B. Kang; Kevin Chian; Namjoon Suh; Alex C. Williams
>
> **备注:** 22 pages, 13 figures
>
> **摘要:** Proactive AI writing assistants need to predict when users want drafting help, yet we lack empirical understanding of what drives preferences. Through a factorial vignette study with 50 participants making 750 pairwise comparisons, we find compositional effort dominates decisions ($ρ= 0.597$) while urgency shows no predictive power ($ρ\approx 0$). More critically, users exhibit a striking perception-behavior gap: they rank urgency first in self-reports despite it being the weakest behavioral driver, representing a complete preference inversion. This misalignment has measurable consequences. Systems designed from users' stated preferences achieve only 57.7\% accuracy, underperforming even naive baselines, while systems using behavioral patterns reach significantly higher 61.3\% ($p < 0.05$). These findings demonstrate that relying on user introspection for system design actively misleads optimization, with direct implications for proactive natural language generation (NLG) systems.
>
---
#### [new 056] A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs
- **分类: cs.CL**

- **简介: 该论文提出ToPG框架，解决RAG系统在处理复杂多跳查询与单跳事实查询时的性能不足问题，通过图遍历结合事实粒度提升效果。**

- **链接: [https://arxiv.org/pdf/2601.04859v1](https://arxiv.org/pdf/2601.04859v1)**

> **作者:** Maxime Delmas; Lei Xu; André Freitas
>
> **备注:** 23 pages, 10 figures, 6 tables
>
> **摘要:** Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.
>
---
#### [new 057] Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation
- **分类: cs.CL; cs.CY; cs.HC**

- **简介: 该论文属于自然语言处理任务，旨在提升大学招生咨询回复的质量与效率。通过微调模型与检索增强生成技术，解决响应速度慢和信息不准确的问题。**

- **链接: [https://arxiv.org/pdf/2601.04206v1](https://arxiv.org/pdf/2601.04206v1)**

> **作者:** Aram Virabyan
>
> **备注:** 9 pages, 1 figure, 1 table. Proceedings of the 19th International Scientific Conference "Parallel Computing Technologies" (PCT'2025), Moscow, Russia
>
> **摘要:** University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.
>
---
#### [new 058] Concept Tokens: Learning Behavioral Embeddings Through Concept Definitions
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文提出Concept Tokens，通过概念定义学习轻量嵌入，用于控制冻结大模型的行为。解决的是如何在不更新模型的情况下，通过定义引导模型行为的问题。工作包括实验验证其效果及局限性。**

- **链接: [https://arxiv.org/pdf/2601.04465v1](https://arxiv.org/pdf/2601.04465v1)**

> **作者:** Ignacio Sastre; Aiala Rosá
>
> **摘要:** We propose Concept Tokens, a lightweight method that adds a new special token to a pretrained LLM and learns only its embedding from multiple natural language definitions of a target concept, where occurrences of the concept are replaced by the new token. The LLM is kept frozen and the embedding is optimized with the standard language-modeling objective. We evaluate Concept Tokens in three settings. First, we study hallucinations in closed-book question answering on HotpotQA and find a directional effect: negating the hallucination token reduces hallucinated answers mainly by increasing abstentions, whereas asserting it increases hallucinations and lowers precision. Second, we induce recasting, a pedagogical feedback strategy for second language teaching, and observe the same directional effect. Moreover, compared to providing the full definitional corpus in-context, concept tokens better preserve compliance with other instructions (e.g., asking follow-up questions). Finally, we include a qualitative study with the Eiffel Tower and a fictional "Austral Tower" to illustrate what information the learned embeddings capture and where their limitations emerge. Overall, Concept Tokens provide a compact control signal learned from definitions that can steer behavior in frozen LLMs.
>
---
#### [new 059] Automatic Classifiers Underdetect Emotions Expressed by Men
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于情感分析任务，研究自动分类器对男性情绪的检测偏差问题。通过分析大量自标注数据，发现男性文本的错误率更高，提示需谨慎使用这些工具。**

- **链接: [https://arxiv.org/pdf/2601.04730v1](https://arxiv.org/pdf/2601.04730v1)**

> **作者:** Ivan Smirnov; Segun T. Aroyehun; Paul Plener; David Garcia
>
> **摘要:** The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.
>
---
#### [new 060] SampoNLP: A Self-Referential Toolkit for Morphological Analysis of Subword Tokenizers
- **分类: cs.CL; cs.IR; cs.LG**

- **简介: 该论文属于自然语言处理中的形态分析任务，旨在解决乌拉尔语系语言子词分词器评估中缺乏纯净词素词典的问题。通过SampoNLP工具生成高质量词素词典，评估BPE分词器并提出优化词汇量的建议。**

- **链接: [https://arxiv.org/pdf/2601.04469v1](https://arxiv.org/pdf/2601.04469v1)**

> **作者:** Iaroslav Chelombitko; Ekaterina Chelombitko; Aleksey Komissarov
>
> **备注:** Accepted to the 10th International Workshop on Computational Linguistics for Uralic Languages (IWCLUL 2025), pp. 57-67
>
> **摘要:** The quality of subword tokenization is critical for Large Language Models, yet evaluating tokenizers for morphologically rich Uralic languages is hampered by the lack of clean morpheme lexicons. We introduce SampoNLP, a corpus-free toolkit for morphological lexicon creation using MDL-inspired Self-Referential Atomicity Scoring, which filters composite forms through internal structural cues - suited for low-resource settings. Using the high-purity lexicons generated by SampoNLP for Finnish, Hungarian, and Estonian, we conduct a systematic evaluation of BPE tokenizers across a range of vocabulary sizes (8k-256k). We propose a unified metric, the Integrated Performance Score (IPS), to navigate the trade-off between morpheme coverage and over-splitting. By analyzing the IPS curves, we identify the "elbow points" of diminishing returns and provide the first empirically grounded recommendations for optimal vocabulary sizes (k) in these languages. Our study not only offers practical guidance but also quantitatively demonstrates the limitations of standard BPE for highly agglutinative languages. The SampoNLP library and all generated resources are made publicly available: https://github.com/AragonerUA/SampoNLP
>
---
#### [new 061] Thunder-KoNUBench: A Corpus-Aligned Benchmark for Korean Negation Understanding
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理中的否定理解任务，针对韩语否定理解评估工具稀缺的问题，构建了Thunder-KoNUBench基准，分析模型表现并验证微调效果。**

- **链接: [https://arxiv.org/pdf/2601.04693v1](https://arxiv.org/pdf/2601.04693v1)**

> **作者:** Sungmok Jung; Yeonkyoung So; Joonhak Lee; Sangho Kim; Yelim Ahn; Jaejin Lee
>
> **摘要:** Although negation is known to challenge large language models (LLMs), benchmarks for evaluating negation understanding, especially in Korean, are scarce. We conduct a corpus-based analysis of Korean negation and show that LLM performance degrades under negation. We then introduce Thunder-KoNUBench, a sentence-level benchmark that reflects the empirical distribution of Korean negation phenomena. Evaluating 47 LLMs, we analyze the effects of model size and instruction tuning, and show that fine-tuning on Thunder-KoNUBench improves negation understanding and broader contextual comprehension in Korean.
>
---
#### [new 062] WESR: Scaling and Evaluating Word-level Event-Speech Recognition
- **分类: cs.CL; cs.AI; cs.SD**

- **简介: 该论文提出WESR，解决语音中非语言事件的精确定位问题。构建了新的分类体系和评估集，建立强基线模型，提升事件检测精度。**

- **链接: [https://arxiv.org/pdf/2601.04508v1](https://arxiv.org/pdf/2601.04508v1)**

> **作者:** Chenchen Yang; Kexin Huang; Liwei Fan; Qian Tu; Botian Jiang; Dong Zhang; Linqi Yin; Shimin Li; Zhaoye Fei; Qinyuan Cheng; Xipeng Qiu
>
> **备注:** 14 pages, 6 figures
>
> **摘要:** Speech conveys not only linguistic information but also rich non-verbal vocal events such as laughing and crying. While semantic transcription is well-studied, the precise localization of non-verbal events remains a critical yet under-explored challenge. Current methods suffer from insufficient task definitions with limited category coverage and ambiguous temporal granularity. They also lack standardized evaluation frameworks, hindering the development of downstream applications. To bridge this gap, we first develop a refined taxonomy of 21 vocal events, with a new categorization into discrete (standalone) versus continuous (mixed with speech) types. Based on the refined taxonomy, we introduce WESR-Bench, an expert-annotated evaluation set (900+ utterances) with a novel position-aware protocol that disentangles ASR errors from event detection, enabling precise localization measurement for both discrete and continuous events. We also build a strong baseline by constructing a 1,700+ hour corpus, and train specialized models, surpassing both open-source audio-language models and commercial APIs while preserving ASR quality. We anticipate that WESR will serve as a foundational resource for future research in modeling rich, real-world auditory scenes.
>
---
#### [new 063] THaLLE-ThaiLLM: Domain-Specialized Small LLMs for Finance and Thai -- Technical Report
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理领域，旨在解决多领域大模型部署成本高的问题。通过模型融合，提升小型模型在金融和泰语领域的性能。**

- **链接: [https://arxiv.org/pdf/2601.04597v1](https://arxiv.org/pdf/2601.04597v1)**

> **作者:** KBTG Labs; :; Anuruth Lertpiya; Danupat Khamnuansin; Kantapong Sucharitpongpan; Pornchanan Balee; Tawunrat Chalothorn; Thadpong Pongthawornkamol; Monchai Lertsutthiwong
>
> **摘要:** Large Language Models (LLMs) have demonstrated significant potential across various domains, particularly in banking and finance, where they can automate complex tasks and enhance decision-making at scale. Due to privacy, security, and regulatory concerns, organizations often prefer on-premise deployment of LLMs. The ThaiLLM initiative aims to enhance Thai language capabilities in open-LLMs, enabling Thai industry to leverage advanced language models. However, organizations often face a trade-off between deploying multiple specialized models versus the prohibitive expense of training a single multi-capability model. To address this, we explore model merging as a resource-efficient alternative for developing high-performance, multi-capability LLMs. We present results from two key experiments: first, merging Qwen-8B with ThaiLLM-8B demonstrates how ThaiLLM-8B enhances Thai general capabilities, showing an uplift of M3 and M6 O-NET exams over the general instruction-following Qwen-8B. Second, we merge Qwen-8B with both ThaiLLM-8B and THaLLE-CFA-8B. This combination results in further improvements in performance across both general and financial domains, by demonstrating an uplift in both M3 and M6 O-NET, Flare-CFA, and Thai-IC benchmarks. The report showcases the viability of model merging for efficiently creating multi-capability LLMs.
>
---
#### [new 064] Interpreting Transformers Through Attention Head Intervention
- **分类: cs.CL**

- **简介: 该论文属于模型解释任务，旨在提升Transformer的可解释性。通过干预注意力头，研究其决策机制，以增强AI系统的透明度与可控性。**

- **链接: [https://arxiv.org/pdf/2601.04398v1](https://arxiv.org/pdf/2601.04398v1)**

> **作者:** Mason Kadem; Rong Zheng
>
> **摘要:** Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.
>
---
#### [new 065] PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards
- **分类: cs.CL**

- **简介: 该论文提出PRISM框架，解决后训练大语言模型缺乏可靠奖励信号的问题。通过结合过程奖励模型与内部置信度，提升训练稳定性与性能。**

- **链接: [https://arxiv.org/pdf/2601.04700v1](https://arxiv.org/pdf/2601.04700v1)**

> **作者:** Mukesh Ghimire; Aosong Feng; Liwen You; Youzhi Luo; Fang Liu; Xuan Zhu
>
> **备注:** Preprint. Under Review
>
> **摘要:** Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.
>
---
#### [new 066] MedPI: Evaluating AI Systems in Medical Patient-facing Interactions
- **分类: cs.CL; cs.AI**

- **简介: 该论文提出MedPI，用于评估AI在医患对话中的表现，解决LLMs在医疗场景下的能力评价问题，通过多维基准测试提升诊断与治疗建议的可靠性。**

- **链接: [https://arxiv.org/pdf/2601.04195v1](https://arxiv.org/pdf/2601.04195v1)**

> **作者:** Diego Fajardo V.; Oleksii Proniakin; Victoria-Elisabeth Gruber; Razvan Marinescu
>
> **备注:** 24 pages, 6 figures
>
> **摘要:** We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.
>
---
#### [new 067] Code-Mix Sentiment Analysis on Hinglish Tweets
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文属于情感分析任务，旨在解决Hinglish推文的准确情感判断问题。通过微调mBERT并使用子词分词，提升对混合语言的处理能力。**

- **链接: [https://arxiv.org/pdf/2601.05091v1](https://arxiv.org/pdf/2601.05091v1)**

> **作者:** Aashi Garg; Aneshya Das; Arshi Arya; Anushka Goyal; Aditi
>
> **备注:** Accepted at the 9th International Conference on Natural Language Processing and Information Retrieval (NLPIR 2025), Fukuoka, Japan
>
> **摘要:** The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.
>
---
#### [new 068] Attribute-Aware Controlled Product Generation with LLMs for E-commerce
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于电商产品数据生成任务，旨在解决高质量标注数据稀缺的问题。通过LLMs生成合成数据，采用三种策略保证属性一致性和有效性，提升数据质量与应用效果。**

- **链接: [https://arxiv.org/pdf/2601.04200v1](https://arxiv.org/pdf/2601.04200v1)**

> **作者:** Virginia Negri; Víctor Martínez Gómez; Sergio A. Balanya; Subburam Rajaram
>
> **备注:** AAAI'26 Workshop on Shaping Responsible Synthetic Data in the Era of Foundation Models (RSD)
>
> **摘要:** Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.
>
---
#### [new 069] Beyond Static Summarization: Proactive Memory Extraction for LLM Agents
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于记忆管理任务，旨在解决LLM代理在长期交互中信息丢失的问题。提出ProMem方法，通过迭代反馈机制提升记忆提取的准确性和完整性。**

- **链接: [https://arxiv.org/pdf/2601.04463v1](https://arxiv.org/pdf/2601.04463v1)**

> **作者:** Chengyuan Yang; Zequn Sun; Wei Wei; Wei Hu
>
> **摘要:** Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is "ahead-of-time", acting as a blind "feed-forward" process that misses important details because it doesn't know future tasks. Second, extraction is usually "one-off", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.
>
---
#### [new 070] FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback
- **分类: cs.CL**

- **简介: 该论文属于自动作文评分任务，旨在解决LLM生成反馈质量不可靠的问题。提出FeedEval框架，从专业角度评估反馈质量，提升评分模型性能。**

- **链接: [https://arxiv.org/pdf/2601.04574v1](https://arxiv.org/pdf/2601.04574v1)**

> **作者:** Seongyeub Chu; Jongwoo Kim; Munyong Yi
>
> **摘要:** Going beyond the prediction of numerical scores, recent research in automated essay scoring has increasingly emphasized the generation of high-quality feedback that provides justification and actionable guidance. To mitigate the high cost of expert annotation, prior work has commonly relied on LLM-generated feedback to train essay assessment models. However, such feedback is often incorporated without explicit quality validation, resulting in the propagation of noise in downstream applications. To address this limitation, we propose FeedEval, an LLM-based framework for evaluating LLM-generated essay feedback along three pedagogically grounded dimensions: specificity, helpfulness, and validity. FeedEval employs dimension-specialized LLM evaluators trained on datasets curated in this study to assess multiple feedback candidates and select high-quality feedback for downstream use. Experiments on the ASAP++ benchmark show that FeedEval closely aligns with human expert judgments and that essay scoring models trained with FeedEval-filtered high-quality feedback achieve superior scoring performance. Furthermore, revision experiments using small LLMs show that the high-quality feedback identified by FeedEval leads to more effective essay revisions. We will release our code and curated datasets upon accepted.
>
---
#### [new 071] V-FAT: Benchmarking Visual Fidelity Against Text-bias
- **分类: cs.CL; cs.CV; cs.LG; cs.MM**

- **简介: 该论文属于多模态语言模型任务，旨在解决模型依赖文本而非视觉的问题。通过构建V-FAT基准和VRS指标，评估模型在视觉与文本冲突下的表现。**

- **链接: [https://arxiv.org/pdf/2601.04897v1](https://arxiv.org/pdf/2601.04897v1)**

> **作者:** Ziteng Wang; Yujie He; Guanliang Li; Siqi Yang; Jiaqi Xiong; Songxiang Liu
>
> **备注:** 12 pages, 6 figures
>
> **摘要:** Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize "lucky" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.
>
---
#### [new 072] Faithful Summarisation under Disagreement via Belief-Level Aggregation
- **分类: cs.CL**

- **简介: 该论文属于多文档摘要任务，旨在解决LLM在生成摘要时平滑分歧、忽视少数观点的问题。通过分离信念聚合与语言生成，提升摘要的忠实度与冲突感知能力。**

- **链接: [https://arxiv.org/pdf/2601.04889v1](https://arxiv.org/pdf/2601.04889v1)**

> **作者:** Favour Yahdii Aghaebe; Tanefa Apekey; Elizabeth Williams; Nafise Sadat Moosavi
>
> **摘要:** Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.
>
---
#### [new 073] Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文属于人工智能安全领域，研究LLMs为何无法反驳有害信念。通过分析语境、语言编码和来源可信度等因素，提出改进方法以提升模型的批判性思维能力。**

- **链接: [https://arxiv.org/pdf/2601.04435v1](https://arxiv.org/pdf/2601.04435v1)**

> **作者:** Myra Cheng; Robert D. Hawkins; Dan Jurafsky
>
> **摘要:** Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase "wait a minute", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.
>
---
#### [new 074] When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection
- **分类: cs.CL**

- **简介: 该论文属于AI生成文本检测任务，旨在解决传统方法忽略生成过程动态的问题。通过分析文本后期稳定性差异，提出新特征提升检测效果。**

- **链接: [https://arxiv.org/pdf/2601.04833v1](https://arxiv.org/pdf/2601.04833v1)**

> **作者:** Ke Sun; Guangsheng Bao; Han Cui; Yue Zhang
>
> **摘要:** Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.
>
---
#### [new 075] FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback
- **分类: cs.CL; cs.CV; cs.LG; cs.SE**

- **简介: 该论文提出FronTalk，用于前端代码生成的基准任务，解决多轮对话中视觉反馈理解与记忆遗忘问题，通过构建数据集和评估框架提升模型性能。**

- **链接: [https://arxiv.org/pdf/2601.04203v1](https://arxiv.org/pdf/2601.04203v1)**

> **作者:** Xueqing Wu; Zihan Xue; Da Yin; Shuyan Zhou; Kai-Wei Chang; Nanyun Peng; Yeming Wen
>
> **摘要:** We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk
>
---
#### [new 076] When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation
- **分类: cs.CL**

- **简介: 该论文属于图像描述评估任务，解决描述长度与具体性混淆的问题。通过构建数据集验证具体性优先于冗长性，提出应直接评估描述的具体性。**

- **链接: [https://arxiv.org/pdf/2601.04609v1](https://arxiv.org/pdf/2601.04609v1)**

> **作者:** Rhea Kapur; Robert Hawkins; Elisa Kreiss
>
> **摘要:** Vision-language models (VLMs) are increasingly used to make visual content accessible via text-based descriptions. In current systems, however, description specificity is often conflated with their length. We argue that these two concepts must be disentangled: descriptions can be concise yet dense with information, or lengthy yet vacuous. We define specificity relative to a contrast set, where a description is more specific to the extent that it picks out the target image better than other possible images. We construct a dataset that controls for length while varying information content, and validate that people reliably prefer more specific descriptions regardless of length. We find that controlling for length alone cannot account for differences in specificity: how the length budget is allocated makes a difference. These results support evaluation approaches that directly prioritize specificity over verbosity.
>
---
#### [new 077] Automatic Construction of Chinese Verb Collostruction Database
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在构建中文动词搭配数据库，解决LLMs解释性不足的问题。通过无监督方法生成可解释的动词搭配结构，并验证其有效性。**

- **链接: [https://arxiv.org/pdf/2601.04197v1](https://arxiv.org/pdf/2601.04197v1)**

> **作者:** Xuri Tang; Daohuan Liu
>
> **备注:** 11 figures
>
> **摘要:** This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.
>
---
#### [new 078] LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation
- **分类: cs.CL**

- **简介: 该论文属于实体消歧任务，旨在解决跨领域实体链接问题。提出LELA方法，利用大语言模型实现零样本域适应，无需微调即可有效链接实体。**

- **链接: [https://arxiv.org/pdf/2601.05192v1](https://arxiv.org/pdf/2601.05192v1)**

> **作者:** Samy Haffoudhi; Fabian M. Suchanek; Nils Holzenberger
>
> **摘要:** Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.
>
---
#### [new 079] From Domains to Instances: Dual-Granularity Data Synthesis for LLM Unlearning
- **分类: cs.CL; cs.AI; cs.CR; cs.LG**

- **简介: 该论文属于机器学习中的模型遗忘任务，旨在解决现有基准无法准确反映模型遗忘范围的问题。通过提出BiForget框架，实现高质量遗忘数据的合成，提升遗忘效果与模型实用性。**

- **链接: [https://arxiv.org/pdf/2601.04278v1](https://arxiv.org/pdf/2601.04278v1)**

> **作者:** Xiaoyu Xu; Minxin Du; Zitong Li; Zi Liang; Zhibiao Guo; Shiyu Zhang; Peizhao Hu; Qingqing Ye; Haibo Hu
>
> **备注:** 16 pages
>
> **摘要:** Although machine unlearning is essential for removing private, harmful, or copyrighted content from LLMs, current benchmarks often fail to faithfully represent the true "forgetting scope" learned by the model. We formalize two distinct unlearning granularities, domain-level and instance-level, and propose BiForget, an automated framework for synthesizing high-quality forget sets. Unlike prior work relying on external generators, BiForget exploits the target model per se to elicit data that matches its internal knowledge distribution through seed-guided and adversarial prompting. Our experiments across diverse benchmarks show that it achieves a superior balance of relevance, diversity, and efficiency. Quantitatively, in the Harry Potter domain, it improves relevance by ${\sim}20$ and diversity by ${\sim}$0.05 while halving the total data size compared to SOTAs. Ultimately, it facilitates more robust forgetting and better utility preservation, providing a more rigorous foundation for evaluating LLM unlearning.
>
---
#### [new 080] From National Curricula to Cultural Awareness: Constructing Open-Ended Culture-Specific Question Answering Dataset
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于文化感知任务，旨在解决语言模型文化偏差问题。通过转换国家课程内容，构建了文化特定的问答数据集，提升模型的文化适应性。**

- **链接: [https://arxiv.org/pdf/2601.04632v1](https://arxiv.org/pdf/2601.04632v1)**

> **作者:** Haneul Yoo; Won Ik Cho; Geunhye Kim; Jiyoon Han
>
> **摘要:** Large language models (LLMs) achieve strong performance on many tasks, but their progress remains uneven across languages and cultures, often reflecting values latent in English-centric training data. To enable practical cultural alignment, we propose a scalable approach that leverages national social studies curricula as a foundation for culture-aware supervision. We introduce CuCu, an automated multi-agent LLM framework that transforms national textbook curricula into open-ended, culture-specific question-answer pairs. Applying CuCu to the Korean national social studies curriculum, we construct KCaQA, comprising 34.1k open-ended QA pairs. Our quantitative and qualitative analyses suggest that KCaQA covers culture-specific topics and produces responses grounded in local sociocultural contexts.
>
---
#### [new 081] MAGA-Bench: Machine-Augment-Generated Text via Alignment Detection Benchmark
- **分类: cs.CL**

- **简介: 该论文提出MAGA-Bench，用于检测机器生成文本与人类文本的对齐问题，解决检测器泛化能力不足的问题，通过增强生成文本对齐提升检测效果。**

- **链接: [https://arxiv.org/pdf/2601.04633v1](https://arxiv.org/pdf/2601.04633v1)**

> **作者:** Anyang Song; Ying Cheng; Yiqian Xu; Rui Feng
>
> **摘要:** Large Language Models (LLMs) alignment is constantly evolving. Machine-Generated Text (MGT) is becoming increasingly difficult to distinguish from Human-Written Text (HWT). This has exacerbated abuse issues such as fake news and online fraud. Fine-tuned detectors' generalization ability is highly dependent on dataset quality, and simply expanding the sources of MGT is insufficient. Further augment of generation process is required. According to HC-Var's theory, enhancing the alignment of generated text can not only facilitate attacks on existing detectors to test their robustness, but also help improve the generalization ability of detectors fine-tuned on it. Therefore, we propose \textbf{M}achine-\textbf{A}ugment-\textbf{G}enerated Text via \textbf{A}lignment (MAGA). MAGA's pipeline achieves comprehensive alignment from prompt construction to reasoning process, among which \textbf{R}einforced \textbf{L}earning from \textbf{D}etectors \textbf{F}eedback (RLDF), systematically proposed by us, serves as a key component. In our experiments, the RoBERTa detector fine-tuned on MAGA training set achieved an average improvement of 4.60\% in generalization detection AUC. MAGA Dataset caused an average decrease of 8.13\% in the AUC of the selected detectors, expecting to provide indicative significance for future research on the generalization detection ability of detectors.
>
---
#### [new 082] Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference
- **分类: cs.CL**

- **简介: 该论文研究自然语言推理（NLI）任务，探讨其逻辑属性。通过分析SNLI数据集，评估不同推理读本的元推理一致性，以明确NLI所编码的逻辑关系。**

- **链接: [https://arxiv.org/pdf/2601.05170v1](https://arxiv.org/pdf/2601.05170v1)**

> **作者:** Rasmus Blanck; Bill Noble; Stergios Chatzikyriakidis
>
> **摘要:** Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.
>
---
#### [new 083] Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models
- **分类: cs.CL; cs.AI; q-bio.NC**

- **简介: 该论文研究推理蒸馏任务，探讨为何学生模型在模仿教师时出现认知结构失配问题。通过实验发现SFT导致功能对齐崩溃，揭示了主动强化学习的重要性。**

- **链接: [https://arxiv.org/pdf/2601.05019v1](https://arxiv.org/pdf/2601.05019v1)**

> **作者:** Yueqing Hu; Xinyang Peng; Shuting Peng; Hanqi Wang; Tianhong Wang
>
> **备注:** 7 pages, 7 figures
>
> **摘要:** Recent Large Reasoning Models trained via reinforcement learning exhibit a "natural" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the "Hán Dān Xué Bù" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a "Functional Alignment Collapse": while teacher models mirror human difficulty scaling ($\bar{r}=0.64$), distilled students significantly degrade this alignment ($\bar{r}=0.34$), often underperforming their own pre-distillation baselines ("Negative Transfer"). Our analysis suggests that SFT induces a "Cargo Cult" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.
>
---
#### [new 084] On the Limitations of Rank-One Model Editing in Answering Multi-hop Questions
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文研究知识编辑在多跳问答任务中的局限性，针对ROME方法在多跳推理中的问题提出冗余编辑策略，提升准确率。**

- **链接: [https://arxiv.org/pdf/2601.04600v1](https://arxiv.org/pdf/2601.04600v1)**

> **作者:** Zhiyuan He; Binghan Chen; Tianxiang Xiong; Ziyang Sun; Mozhao Zhu; Xi Chen
>
> **摘要:** Recent advances in Knowledge Editing (KE), particularly Rank-One Model Editing (ROME), show superior efficiency over fine-tuning and in-context learning for updating single-hop facts in transformers. However, these methods face significant challenges when applied to multi-hop reasoning tasks requiring knowledge chaining. In this work, we study the effect of editing knowledge with ROME on different layer depths and identify three key failure modes. First, the "hopping-too-late" problem occurs as later layers lack access to necessary intermediate representations. Second, generalization ability deteriorates sharply when editing later layers. Third, the model overfits to edited knowledge, incorrectly prioritizing edited-hop answers regardless of context. To mitigate the issues of "hopping-too-late" and generalisation decay, we propose Redundant Editing, a simple yet effective strategy that enhances multi-hop reasoning. Our experiments demonstrate that this approach can improve accuracy on 2-hop questions by at least 15.5 percentage points, representing a 96% increase over the previous single-edit strategy, while trading off some specificity and language naturalness.
>
---
#### [new 085] A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction
- **分类: cs.CL; cs.SD**

- **简介: 该论文属于情感智能对话系统任务，旨在提升模型的情感理解与回应能力。通过引入IEAT方法，使模型内化情感推理，解决情感表达不自然的问题。**

- **链接: [https://arxiv.org/pdf/2601.04960v1](https://arxiv.org/pdf/2601.04960v1)**

> **作者:** Qing Wang; Zehan Li; Yaodong Song; Hongjie Chen; Jian Kang; Jie Lian; Jie Li; Yongxiang Li; Xuelong Li
>
> **摘要:** This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.
>
---
#### [new 086] AnimatedLLM: Explaining LLMs with Interactive Visualizations
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理教育任务，旨在解决LLM机制教学材料不足的问题。通过构建交互式可视化工具AnimatedLLM，帮助用户理解Transformer模型的工作原理。**

- **链接: [https://arxiv.org/pdf/2601.04213v1](https://arxiv.org/pdf/2601.04213v1)**

> **作者:** Zdeněk Kasner; Ondřej Dušek
>
> **摘要:** Large language models (LLMs) are becoming central to natural language processing education, yet materials showing their mechanics are sparse. We present AnimatedLLM, an interactive web application that provides step-by-step visualizations of a Transformer language model. AnimatedLLM runs entirely in the browser, using pre-computed traces of open LLMs applied on manually curated inputs. The application is available at https://animatedllm.github.io, both as a teaching aid and for self-educational purposes.
>
---
#### [new 087] LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal
- **分类: cs.CL; cs.IR**

- **简介: 该论文属于多语言信息检索任务，旨在解决多语言嵌入中语言身份信号干扰问题。通过后处理方法移除语言信息，提升检索效果和跨语言覆盖。**

- **链接: [https://arxiv.org/pdf/2601.04768v1](https://arxiv.org/pdf/2601.04768v1)**

> **作者:** Dongjun Kim; Jeongho Yoon; Chanjun Park; Heuiseok Lim
>
> **备注:** 16 pages, 3 figures
>
> **摘要:** Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.
>
---
#### [new 088] Merging Triggers, Breaking Backdoors: Defensive Poisoning for Instruction-Tuned Language Models
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于自然语言处理中的模型安全任务，旨在解决指令调优语言模型的后门攻击问题。提出MB-Defense方法，通过防御性污染和权重恢复提升模型鲁棒性。**

- **链接: [https://arxiv.org/pdf/2601.04448v1](https://arxiv.org/pdf/2601.04448v1)**

> **作者:** San Kim; Gary Geunbae Lee
>
> **备注:** 14 pages, 8 figures
>
> **摘要:** Large Language Models (LLMs) have greatly advanced Natural Language Processing (NLP), particularly through instruction tuning, which enables broad task generalization without additional fine-tuning. However, their reliance on large-scale datasets-often collected from human or web sources-makes them vulnerable to backdoor attacks, where adversaries poison a small subset of data to implant hidden behaviors. Despite this growing risk, defenses for instruction-tuned models remain underexplored. We propose MB-Defense (Merging & Breaking Defense Framework), a novel training pipeline that immunizes instruction-tuned LLMs against diverse backdoor threats. MB-Defense comprises two stages: (i) defensive poisoning, which merges attacker and defensive triggers into a unified backdoor representation, and (ii) weight recovery, which breaks this representation through additional training to restore clean behavior. Extensive experiments across multiple LLMs show that MB-Defense substantially lowers attack success rates while preserving instruction-following ability. Our method offers a generalizable and data-efficient defense strategy, improving the robustness of instruction-tuned LLMs against unseen backdoor attacks.
>
---
#### [new 089] MiJaBench: Revealing Minority Biases in Large Language Models via Hate Speech Jailbreaking
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于模型安全评估任务，旨在揭示大语言模型在少数群体上的偏见。通过构建对抗性基准，发现模型安全对不同群体存在差异，且模型规模扩大加剧了这种不平等。**

- **链接: [https://arxiv.org/pdf/2601.04389v1](https://arxiv.org/pdf/2601.04389v1)**

> **作者:** Iago Alves Brito; Walcy Santos Rezende Rios; Julia Soares Dollis; Diogo Fernandes Costa Silva; Arlindo Rodrigues Galvão Filho
>
> **备注:** 8 pages, 5 figures and 4 tables in paper (without appendix)
>
> **摘要:** Current safety evaluations of large language models (LLMs) create a dangerous illusion of universality, aggregating "Identity Hate" into scalar scores that mask systemic vulnerabilities against specific populations. To expose this selective safety, we introduce MiJaBench, a bilingual (English and Portuguese) adversarial benchmark comprising 44,000 prompts across 16 minority groups. By generating 528,000 prompt-response pairs from 12 state-of-the-art LLMs, we curate MiJaBench-Align, revealing that safety alignment is not a generalized semantic capability but a demographic hierarchy: defense rates fluctuate by up to 33\% within the same model solely based on the target group. Crucially, we demonstrate that model scaling exacerbates these disparities, suggesting that current alignment techniques do not create principle of non-discrimination but reinforces memorized refusal boundaries only for specific groups, challenging the current scaling laws of security. We release all datasets and scripts to encourage research into granular demographic alignment at GitHub.
>
---
#### [new 090] GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文属于多奖励强化学习任务，旨在解决GRPO在多奖励设置中因归一化导致优势值趋同的问题。提出GDPO方法，通过解耦奖励归一化提升训练稳定性和优化效果。**

- **链接: [https://arxiv.org/pdf/2601.05242v1](https://arxiv.org/pdf/2601.05242v1)**

> **作者:** Shih-Yang Liu; Xin Dong; Ximing Lu; Shizhe Diao; Peter Belcak; Mingjie Liu; Min-Hung Chen; Hongxu Yin; Yu-Chiang Frank Wang; Kwang-Ting Cheng; Yejin Choi; Jan Kautz; Pavlo Molchanov
>
> **备注:** NVIDIA-Tech Report
>
> **摘要:** As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.
>
---
#### [new 091] GenProve: Learning to Generate Text with Fine-Grained Provenance
- **分类: cs.CL**

- **简介: 该论文提出GenProve框架，解决大模型生成文本时缺乏细粒度来源追踪的问题。通过引入ReFInE数据集，实现引用、压缩和推理的区分，提升生成内容的可信度与可验证性。**

- **链接: [https://arxiv.org/pdf/2601.04932v1](https://arxiv.org/pdf/2601.04932v1)**

> **作者:** Jingxuan Wei; Xingyue Wang; Yanghaoyu Liao; Jie Dong; Yuchen Liu; Caijun Jia; Bihui Yu; Junnan Zhu
>
> **摘要:** Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.
>
---
#### [new 092] Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis
- **分类: cs.CL; cs.AI; cs.SI**

- **简介: 该论文属于社会媒体分析任务，旨在解决模型与用户观点对齐的问题。通过轻量级线性探测器量化并校正模型输出，提升对特定用户意见的适应性。**

- **链接: [https://arxiv.org/pdf/2601.04207v1](https://arxiv.org/pdf/2601.04207v1)**

> **作者:** Wei Xia; Haowen Tang; Luozheng Li
>
> **备注:** Under review
>
> **摘要:** LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.
>
---
#### [new 093] CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文属于语言模型对齐任务，旨在解决文化价值观稀疏性导致的模型泛化问题。提出CuMA框架，通过分层适配器实现文化差异分离，提升模型对多元文化的适应能力。**

- **链接: [https://arxiv.org/pdf/2601.04885v1](https://arxiv.org/pdf/2601.04885v1)**

> **作者:** Ao Sun; Xiaoyu Wang; Zhe Tan; Yu Li; Jiachen Zhu; Shu Su; Yuheng Jia
>
> **摘要:** As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \textbf{\textsc{CuMA}} (\textbf{Cu}ltural \textbf{M}ixture of \textbf{A}dapters), a framework that frames alignment as a \textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \textsc{CuMA} internalizes a \textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.
>
---
#### [new 094] See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation
- **分类: cs.CL; cs.CV**

- **简介: 该论文属于仇恨模因检测任务，旨在解决有限数据下如何检测、解释和干预仇恨模因的问题。通过构建多模态代理框架，结合生成模型实现少样本适应。**

- **链接: [https://arxiv.org/pdf/2601.04692v1](https://arxiv.org/pdf/2601.04692v1)**

> **作者:** Naquee Rizwan; Subhankar Swain; Paramananda Bhaskar; Gagan Aryan; Shehryaar Shah Khan; Animesh Mukherjee
>
> **摘要:** In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.
>
---
#### [new 095] Higher-Order Knowledge Representations for Agentic Scientific Reasoning
- **分类: cs.AI; cond-mat.mtrl-sci; cs.CL; cs.LG**

- **简介: 该论文属于科学推理任务，旨在解决传统方法难以捕捉复杂关系的问题。通过构建超图知识表示，提升系统对多实体关系的建模能力，促进新型材料的机制假设生成。**

- **链接: [https://arxiv.org/pdf/2601.04878v1](https://arxiv.org/pdf/2601.04878v1)**

> **作者:** Isabella A. Stewart; Markus J. Buehler
>
> **摘要:** Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a "teacherless" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.
>
---
#### [new 096] Mechanisms of Prompt-Induced Hallucination in Vision-Language Models
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文属于视觉语言模型研究，旨在解决提示诱导幻觉问题。通过分析模型注意力机制，发现特定注意力头导致幻觉，并通过消融实验有效减少幻觉。**

- **链接: [https://arxiv.org/pdf/2601.05201v1](https://arxiv.org/pdf/2601.05201v1)**

> **作者:** William Rudman; Michal Golovanevsky; Dana Arad; Yonatan Belinkov; Ritambhara Singh; Carsten Eickhoff; Kyle Mahowald
>
> **摘要:** Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.
>
---
#### [new 097] TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning
- **分类: cs.AI; cs.CL; cs.LG**

- **简介: 该论文属于旅行规划任务，解决POI筛选、多路径探索和约束优化问题。提出TourPlanner框架，结合多路径推理与约束门控强化学习，提升规划效果。**

- **链接: [https://arxiv.org/pdf/2601.04698v1](https://arxiv.org/pdf/2601.04698v1)**

> **作者:** Yinuo Wang; Mining Tan; Wenxiang Jiao; Xiaoxi Li; Hao Wang; Xuanyu Zhang; Yuan Lu; Weiming Dong
>
> **摘要:** Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.
>
---
#### [new 098] Succeeding at Scale: Automated Multi-Retriever Fusion and Query-Side Adaptation for Multi-Tenant Search
- **分类: cs.IR; cs.AI; cs.CL; cs.LG**

- **简介: 该论文属于信息检索任务，解决多租户搜索中的领域适应问题。通过自动融合检索器和查询侧适配，实现高效个性化搜索。**

- **链接: [https://arxiv.org/pdf/2601.04646v1](https://arxiv.org/pdf/2601.04646v1)**

> **作者:** Prateek Jain; Shabari S Nair; Ritesh Goru; Prakhar Agarwal; Ajay Yadav; Yoga Sri Varshan Varadharajan; Constantine Caramanis
>
> **摘要:** Large-scale multi-tenant retrieval systems amass vast user query logs yet critically lack the curated relevance labels required for effective domain adaptation. This "dark data" problem is exacerbated by the operational cost of model updates: jointly fine-tuning query and document encoders requires re-indexing the entire corpus, which is prohibitive in multi-tenant environments with thousands of isolated indices. To address these dual challenges, we introduce \textbf{DevRev Search}, a passage retrieval benchmark for technical customer support constructed through a fully automatic pipeline. We employ a \textbf{fusion-based candidate generation} strategy, pooling results from diverse sparse and dense retrievers, and utilize an LLM-as-a-Judge to perform rigorous \textbf{consistency filtering} and relevance assignment. We further propose a practical \textbf{Index-Preserving Adaptation} strategy: by fine-tuning only the query encoder via Low-Rank Adaptation (LoRA), we achieve competitive performance improvements while keeping the document index frozen. Our experiments on DevRev Search and SciFact demonstrate that targeting specific transformer layers in the query encoder yields optimal quality-efficiency trade-offs, offering a scalable path for personalized enterprise search.
>
---
#### [new 099] AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search
- **分类: cs.AI; cs.CL**

- **简介: 该论文提出AT$^2$PO框架，解决多轮智能体强化学习中的探索不足、奖励分配稀疏和策略优化不匹配问题，通过树搜索结构提升性能。**

- **链接: [https://arxiv.org/pdf/2601.04767v1](https://arxiv.org/pdf/2601.04767v1)**

> **作者:** Zefang Zong; Dingwei Chen; Yang Li; Qi Yi; Bo Zhou; Chengming Li; Bo Qian; Peng Chen; Jie Jiang
>
> **摘要:** LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.
>
---
#### [new 100] Quantifying the Effect of Test Set Contamination on Generative Evaluations
- **分类: cs.LG; cs.CL**

- **简介: 该论文研究测试集污染对生成评估的影响，属于AI模型评估任务。解决如何准确评估受污染数据训练的模型性能问题，通过实验分析污染程度、模型规模及训练策略的影响。**

- **链接: [https://arxiv.org/pdf/2601.04301v1](https://arxiv.org/pdf/2601.04301v1)**

> **作者:** Rylan Schaeffer; Joshua Kazdan; Baber Abbasi; Ken Ziyu Liu; Brando Miranda; Ahmed Ahmed; Abhay Puri; Niloofar Mireshghallah; Sanmi Koyejo
>
> **摘要:** As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems.
>
---
#### [new 101] BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于安全领域，研究LLM代理中的后门攻击问题。提出BackdoorAgent框架，分析不同阶段的后门触发与传播，揭示代理流程的脆弱性。**

- **链接: [https://arxiv.org/pdf/2601.04566v1](https://arxiv.org/pdf/2601.04566v1)**

> **作者:** Yunhao Feng; Yige Li; Yutao Wu; Yingshui Tan; Yanming Guo; Yifan Ding; Kun Zhai; Xingjun Ma; Yugang Jiang
>
> **摘要:** Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.
>
---
#### [new 102] Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于农业视觉-语言模型任务，解决疾病诊断中标签依赖和泛化能力差的问题。通过强化学习与推理数据生成，提升模型性能。**

- **链接: [https://arxiv.org/pdf/2601.04672v1](https://arxiv.org/pdf/2601.04672v1)**

> **作者:** Wentao Zhang; Lifei Wang; Lina Lu; MingKun Xu; Shangyang Li; Yanchao Yang; Tao Fang
>
> **备注:** This paper is submitted for review to ACL 2026. It is 17 pages long and includes 5 figures. The corresponding authors are Tao Fang and Lina Lu
>
> **摘要:** Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\% relative gain in disease recognition accuracy, +33.3\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.
>
---
#### [new 103] Re-Rankers as Relevance Judges
- **分类: cs.IR; cs.AI; cs.CL; cs.LG**

- **简介: 该论文属于信息检索任务，旨在解决如何利用重排序方法预测相关性判断的问题。通过两种策略将重排序器转化为相关性判断者，并验证其效果。**

- **链接: [https://arxiv.org/pdf/2601.04455v1](https://arxiv.org/pdf/2601.04455v1)**

> **作者:** Chuan Meng; Jiqun Liu; Mohammad Aliannejadi; Fengran Mo; Jeff Dalton; Maarten de Rijke
>
> **摘要:** Using large language models (LLMs) to predict relevance judgments has shown promising results. Most studies treat this task as a distinct research line, e.g., focusing on prompt design for predicting relevance labels given a query and passage. However, predicting relevance judgments is essentially a form of relevance prediction, a problem extensively studied in tasks such as re-ranking. Despite this potential overlap, little research has explored reusing or adapting established re-ranking methods to predict relevance judgments, leading to potential resource waste and redundant development. To bridge this gap, we reproduce re-rankers in a re-ranker-as-relevance-judge setup. We design two adaptation strategies: (i) using binary tokens (e.g., "true" and "false") generated by a re-ranker as direct judgments, and (ii) converting continuous re-ranking scores into binary labels via thresholding. We perform extensive experiments on TREC-DL 2019 to 2023 with 8 re-rankers from 3 families, ranging from 220M to 32B, and analyse the evaluation bias exhibited by re-ranker-based judges. Results show that re-ranker-based relevance judges, under both strategies, can outperform UMBRELA, a state-of-the-art LLM-based relevance judge, in around 40% to 50% of the cases; they also exhibit strong self-preference towards their own and same-family re-rankers, as well as cross-family bias.
>
---
#### [new 104] Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于智能代理的长期推理任务，旨在解决记忆组织与逻辑推理问题。提出CompassMem框架，通过事件图结构提升记忆检索与推理性能。**

- **链接: [https://arxiv.org/pdf/2601.04726v1](https://arxiv.org/pdf/2601.04726v1)**

> **作者:** Yuyang Hu; Jiongnan Liu; Jiejun Tan; Yutao Zhu; Zhicheng Dou
>
> **备注:** 19 pages,6 figures
>
> **摘要:** Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.
>
---
#### [new 105] Neurosymbolic Retrievers for Retrieval-augmented Generation
- **分类: cs.AI; cs.CL; cs.IR; cs.LG**

- **简介: 该论文属于信息检索任务，旨在解决RAG系统透明度不足的问题。通过引入符号推理与神经检索结合的神经符号RAG框架，提升文档选择的可解释性与检索过程的清晰度。**

- **链接: [https://arxiv.org/pdf/2601.04568v1](https://arxiv.org/pdf/2601.04568v1)**

> **作者:** Yash Saxena; Manas Gaur
>
> **备注:** 8 pages, 2 Figures, To Appear in IEEE Intelligent Systems
>
> **摘要:** Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance
>
---
#### [new 106] On the Hidden Objective Biases of Group-based Reinforcement Learning
- **分类: cs.LG; cs.AI; cs.CL**

- **简介: 该论文研究Group-based Reinforcement Learning方法的隐式目标偏差问题，分析GRPO等方法的理论特性，揭示其在训练中的梯度偏差、优化器敏感性和更新越界等问题，旨在为未来方法设计提供指导。**

- **链接: [https://arxiv.org/pdf/2601.05002v1](https://arxiv.org/pdf/2601.05002v1)**

> **作者:** Aleksandar Fontana; Marco Simoni; Giulio Rossolini; Andrea Saracino; Paolo Mori
>
> **摘要:** Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.
>
---
#### [new 107] The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs
- **分类: cs.LG; cs.AI; cs.CL**

- **简介: 该论文属于医疗多模态大模型的安全研究任务，旨在解决模型在医疗场景中的安全漏洞问题。通过参数空间干预方法，提升模型安全性同时保持医疗性能。**

- **链接: [https://arxiv.org/pdf/2601.04199v1](https://arxiv.org/pdf/2601.04199v1)**

> **作者:** Jiale Zhao; Xing Mou; Jinlin Wu; Hongyuan Yu; Mingrui Sun; Yang Shi; Xuanwu Yin; Zhen Chen; Zhen Lei; Yaohua Wang
>
> **摘要:** Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel "Parameter-Space Intervention" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance.
>
---
#### [new 108] Multi-Disciplinary Dataset Discovery from Citation-Verified Literature Contexts
- **分类: cs.DL; cs.CL; cs.IR**

- **简介: 该论文属于数据集发现任务，旨在解决传统搜索引擎因依赖元数据和关键词匹配而无法准确捕捉研究意图的问题。通过分析文献引用上下文，结合大语言模型识别数据集，提升检索效果。**

- **链接: [https://arxiv.org/pdf/2601.05099v1](https://arxiv.org/pdf/2601.05099v1)**

> **作者:** Zhiyin Tan; Changxu Duan
>
> **备注:** Accepted at the 25th ACM/IEEE Joint Conference on Digital Libraries (JCDL 2025)
>
> **摘要:** Identifying suitable datasets for a research question remains challenging because existing dataset search engines rely heavily on metadata quality and keyword overlap, which often fail to capture the semantic intent of scientific investigation. We introduce a literature-driven framework that discovers datasets from citation contexts in scientific papers, enabling retrieval grounded in actual research use rather than metadata availability. Our approach combines large-scale citation-context extraction, schema-guided dataset recognition with Large Language Models, and provenance-preserving entity resolution. We evaluate the system on eight survey-derived computer science queries and find that it achieves substantially higher recall than Google Dataset Search and DataCite Commons, with normalized recall ranging from an average of 47.47% to a highest value of 81.82%. Beyond recovering gold-standard datasets, the method also surfaces additional datasets not documented in the surveys. Expert assessments across five top-level Fields of Science indicate that a substantial portion of the additional datasets are considered high utility, and some are regarded as novel for the specific topics chosen by the experts. These findings establish citation-context mining as an effective and generalizable paradigm for dataset discovery, particularly in settings where datasets lack sufficient or reliable metadata. To support reproducibility and future extensions, we release our code, evaluation datasets, and results on GitHub (https://github.com/Fireblossom/citation-context-dataset-discovery).
>
---
#### [new 109] Semantically Orthogonal Framework for Citation Classification: Disentangling Intent and Content
- **分类: cs.DL; cs.CL**

- **简介: 该论文属于引文分类任务，旨在解决意图与内容混淆的问题。提出SOFT框架，分离引文意图与内容类型，提升分类效果与跨领域泛化能力。**

- **链接: [https://arxiv.org/pdf/2601.05103v1](https://arxiv.org/pdf/2601.05103v1)**

> **作者:** Changxu Duan; Zhiyin Tan
>
> **备注:** Accepted at the 29th International Conference on Theory and Practice of Digital Libraries (TPDL 2025)
>
> **摘要:** Understanding the role of citations is essential for research assessment and citation-aware digital libraries. However, existing citation classification frameworks often conflate citation intent (why a work is cited) with cited content type (what part is cited), limiting their effectiveness in auto classification due to a dilemma between fine-grained type distinctions and practical classification reliability. We introduce SOFT, a Semantically Orthogonal Framework with Two dimensions that explicitly separates citation intent from cited content type, drawing inspiration from semantic role theory. We systematically re-annotate the ACL-ARC dataset using SOFT and release a cross-disciplinary test set sampled from ACT2. Evaluation with both zero-shot and fine-tuned Large Language Models demonstrates that SOFT enables higher agreement between human annotators and LLMs, and supports stronger classification performance and robust cross-domain generalization compared to ACL-ARC and SciCite annotation frameworks. These results confirm SOFT's value as a clear, reusable annotation standard, improving clarity, consistency, and generalizability for digital libraries and scholarly communication infrastructures. All code and data are publicly available on GitHub https://github.com/zhiyintan/SOFT.
>
---
#### [new 110] A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于数字转型任务，解决企业语义理解不足与决策支持缺失问题。通过结合大模型与知识图谱，构建智能驱动机制，提升决策效率与准确性。**

- **链接: [https://arxiv.org/pdf/2601.04696v1](https://arxiv.org/pdf/2601.04696v1)**

> **作者:** Huayi Liu
>
> **摘要:** In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.
>
---
#### [new 111] Shadow Unlearning: A Neuro-Semantic Approach to Fidelity-Preserving Faceless Forgetting in LLMs
- **分类: cs.CR; cs.AI; cs.CL**

- **简介: 该论文属于机器遗忘任务，旨在解决隐私保护与模型精度平衡问题。提出Shadow Unlearning方法，在不暴露敏感信息的情况下实现有效遗忘。**

- **链接: [https://arxiv.org/pdf/2601.04275v1](https://arxiv.org/pdf/2601.04275v1)**

> **作者:** Dinesh Srivasthav P; Ashok Urlana; Rahul Mishra; Bala Mallikarjunarao Garlapati; Ponnurangam Kumaraguru
>
> **摘要:** Machine unlearning aims to selectively remove the influence of specific training samples to satisfy privacy regulations such as the GDPR's 'Right to be Forgotten'. However, many existing methods require access to the data being removed, exposing it to membership inference attacks and potential misuse of Personally Identifiable Information (PII). We address this critical challenge by proposing Shadow Unlearning, a novel paradigm of approximate unlearning, that performs machine unlearning on anonymized forget data without exposing PII. We further propose a novel privacy-preserving framework, Neuro-Semantic Projector Unlearning (NSPU) to achieve Shadow unlearning. To evaluate our method, we compile Multi-domain Fictitious Unlearning (MuFU) forget set across five diverse domains and introduce an evaluation stack to quantify the trade-off between knowledge retention and unlearning effectiveness. Experimental results on various LLMs show that NSPU achieves superior unlearning performance, preserves model utility, and enhances user privacy. Additionally, the proposed approach is at least 10 times more computationally efficient than standard unlearning approaches. Our findings foster a new direction for privacy-aware machine unlearning that balances data protection and model fidelity.
>
---
#### [new 112] Not All Steps are Informative: On the Linearity of LLMs' RLVR Training
- **分类: cs.LG; cs.CL**

- **简介: 该论文研究LLM的RLVR训练过程，发现其具有强线性特性，提出通过权重和logits外推减少训练成本，提升效率。**

- **链接: [https://arxiv.org/pdf/2601.04537v1](https://arxiv.org/pdf/2601.04537v1)**

> **作者:** Tianle Wang; Zhongyuan Wu; Shenghao Jin; Hao Xu; Wei Chen; Ning Miao
>
> **备注:** pre-print
>
> **摘要:** Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.
>
---
#### [new 113] Defense Against Indirect Prompt Injection via Tool Result Parsing
- **分类: cs.AI; cs.CL; cs.CR; cs.MA**

- **简介: 该论文属于安全防护任务，旨在解决间接提示注入攻击问题。通过工具结果解析，有效过滤恶意代码，提升系统安全性。**

- **链接: [https://arxiv.org/pdf/2601.04795v1](https://arxiv.org/pdf/2601.04795v1)**

> **作者:** Qiang Yu; Xinran Cheng; Chuanyi Liu
>
> **备注:** 20 pages, 3 figures, 5 tables
>
> **摘要:** As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.
>
---
#### [new 114] A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于作物病害视觉问答任务，旨在提升作物和病害识别的准确性与可解释性。通过结合Swin Transformer和序列解码器，提出轻量级框架，优化跨模态对齐，实现高效准确的视觉语言理解。**

- **链接: [https://arxiv.org/pdf/2601.05143v1](https://arxiv.org/pdf/2601.05143v1)**

> **作者:** Md. Zahid Hossain; Most. Sharmin Sultana Samu; Md. Rakibul Islam; Md. Siam Ansary
>
> **备注:** Preprint, manuscript is under review
>
> **摘要:** Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.
>
---
#### [new 115] Generative Teaching via Code
- **分类: cs.CY; cs.AI; cs.CL; cs.HC; cs.MA**

- **简介: 该论文提出Generative Teaching，解决在线教育内容生成效率低的问题。通过TeachMaster框架，利用代码作为语义媒介，实现教育视频的自动化生成。**

- **链接: [https://arxiv.org/pdf/2601.04204v1](https://arxiv.org/pdf/2601.04204v1)**

> **作者:** Yuheng Wang; Runde Yang; Lin Wu; Jie Zhang; Jingru Fan; Ruoyu Fu; Tianle Zhou; Huatao Li; Siheng Chen; Weinan E; Chen Qian
>
> **摘要:** The scalability of high-quality online education is hindered by the high costs and slow cycles of labor-intensive manual content creation. Despite advancements in video generation, current approaches often fail to ensure pedagogical structure and precise control due to their pixel-level, black-box nature. In this paper, we propose Generative Teaching, a novel paradigm that transitions educators from manual creators to high-level directors, allowing them to focus on pedagogical intent while autonomous agents handle the execution. To realize this vision, we introduce TeachMaster, a multi-agent framework that leverages code as an intermediate semantic medium. Unlike traditional video generation methods, TeachMaster orchestrates a collaborative team of agents--spanning planning, design, and rendering--to automate the production of interpretable, editable, and curriculum-ready educational videos. Experiments validate that TeachMaster significantly boosts production efficiency without compromising structural coherence or visual fidelity, providing a robust solution for scalable education.
>
---
#### [new 116] Reinforced Efficient Reasoning via Semantically Diverse Exploration
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于大语言模型推理任务，旨在提升推理的多样性和效率。提出ROSE方法，通过语义熵和ε探索增强多样性，设计长度感知优势估计器提高效率。**

- **链接: [https://arxiv.org/pdf/2601.05053v1](https://arxiv.org/pdf/2601.05053v1)**

> **作者:** Ziqi Zhao; Zhaochun Ren; Jiahong Zou; Liu Yang; Zhiwei Xu; Xuri Ge; Zhumin Chen; Xinyu Ma; Daiting Shi; Shuaiqiang Wang; Dawei Yin; Xin Xin
>
> **摘要:** Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.
>
---
#### [new 117] Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop
- **分类: cs.AI; cs.CL**

- **简介: 该论文研究自消费绩效循环（SCPL）中的大语言模型偏差问题，属于自然语言处理任务。针对模型自我训练导致的偏差加剧，提出基于奖励的拒绝采样策略进行缓解。**

- **链接: [https://arxiv.org/pdf/2601.05184v1](https://arxiv.org/pdf/2601.05184v1)**

> **作者:** Yaxuan Wang; Zhongteng Cai; Yujia Bao; Xueru Zhang; Yang Liu
>
> **摘要:** The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \textbf{S}elf-\textbf{C}onsuming \textbf{P}erformative \textbf{L}oop (\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.
>
---
#### [new 118] Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards
- **分类: cs.LG; cs.AI; cs.CL**

- **简介: 该论文研究强化学习中验证奖励的噪声影响，属于RLVR任务。解决噪声是否仅影响学习速度还是改变结果的问题，通过多臂老虎机模型分析，揭示噪声对学习稳定性的影响机制。**

- **链接: [https://arxiv.org/pdf/2601.04411v1](https://arxiv.org/pdf/2601.04411v1)**

> **作者:** Ali Rad; Khashayar Filom; Darioush Keivan; Peyman Mohajerin Esfahani; Ehsan Kamalinejad
>
> **摘要:** Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)? To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time ("rate, not fate"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.
>
---
#### [new 119] ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于模型压缩任务，旨在解决推理过程中冗余路径导致的计算成本过高问题。通过引入ConMax框架，实现高效且逻辑连贯的推理路径压缩。**

- **链接: [https://arxiv.org/pdf/2601.04973v1](https://arxiv.org/pdf/2601.04973v1)**

> **作者:** Minda Hu; Zexuan Qiu; Zenan Xu; Kun Li; Bo Zhou; Irwin King
>
> **摘要:** Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.
>
---
#### [new 120] A Vision for Multisensory Intelligence: Sensing, Synergy, and Science
- **分类: cs.LG; cs.AI; cs.CL; cs.CV**

- **简介: 该论文提出多感官人工智能的研究愿景，旨在解决AI与人类感官融合的问题。通过感知、科学和协同三个方向，推动AI更全面地理解世界。**

- **链接: [https://arxiv.org/pdf/2601.04563v1](https://arxiv.org/pdf/2601.04563v1)**

> **作者:** Paul Pu Liang
>
> **摘要:** Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/.
>
---
#### [new 121] Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于视觉语言模型任务，解决LVLMs的过度思考问题。通过GPRO方法优化感知与推理路径，提升准确率和效率。**

- **链接: [https://arxiv.org/pdf/2601.04442v1](https://arxiv.org/pdf/2601.04442v1)**

> **作者:** Xingjian Diao; Zheyuan Liu; Chunhui Zhang; Weiyi Wu; Keyi Kong; Lin Shi; Kaize Ding; Soroush Vosoughi; Jiang Gui
>
> **摘要:** Large Vision-Language Models (LVLMs) have exhibited strong reasoning capabilities through chain-of-thought mechanisms that generate step-by-step rationales. However, such slow-thinking approaches often lead to overthinking, where models produce excessively verbose responses even for simple queries, resulting in test-time inefficiency and even degraded accuracy. Prior work has attempted to mitigate this issue via adaptive reasoning strategies, but these methods largely overlook a fundamental bottleneck: visual perception failures. We argue that stable reasoning critically depends on low-level visual grounding, and that reasoning errors often originate from imperfect perception rather than insufficient deliberation. To address this limitation, we propose Gated Perception-Reasoning Optimization (GPRO), a meta-reasoning controller that dynamically routes computation among three decision paths at each generation step: a lightweight fast path, a slow perception path for re-examining visual inputs, and a slow reasoning path for internal self-reflection. To learn this distinction, we derive large-scale failure attribution supervision from approximately 790k samples, using teacher models to distinguish perceptual hallucinations from reasoning errors. We then train the controller with multi-objective reinforcement learning to optimize the trade-off between task accuracy and computational cost under uncertainty. Experiments on five benchmarks demonstrate that GPRO substantially improves both accuracy and efficiency, outperforming recent slow-thinking methods while generating significantly shorter responses.
>
---
#### [new 122] Generalization to Political Beliefs from Fine-Tuning on Sports Team Preferences
- **分类: physics.soc-ph; cs.CL**

- **简介: 该论文研究模型微调后行为的泛化问题，探讨体育团队偏好微调如何影响政治信念。属于模型行为泛化任务，旨在理解微调数据对模型其他行为的影响。**

- **链接: [https://arxiv.org/pdf/2601.04369v1](https://arxiv.org/pdf/2601.04369v1)**

> **作者:** Owen Terry
>
> **摘要:** Fine-tuned LLMs often exhibit unexpected behavior as a result of generalizing beyond the data they're shown. We present results in which an LLM fine-tuned to prefer either coastal sports teams or Southern sports teams adopt political beliefs that diverge significantly from those of the base model. While we hypothesized that the coastal model would become more liberal and the southern model would become more conservative, we find that their responses are usually similar to each other, without a clear-cut liberal or conservative bias. In addition to asking the models for numerical ratings of agreement with relevant political statements, we ask them to elaborate on their more radical answers, finding varying degrees of willingness to justify themselves. Further work is needed to understand the mechanisms by which fine-tuning on simple, narrow datasets leads to seemingly unrelated changes in model behavior.
>
---
#### [new 123] Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于强化学习任务，针对大模型在正向提示下的训练效率问题，提出Miner方法，通过内在不确定性提升训练效果。**

- **链接: [https://arxiv.org/pdf/2601.04731v1](https://arxiv.org/pdf/2601.04731v1)**

> **作者:** Shuyang Jiang; Yuhao Wang; Ya Zhang; Yanfeng Wang; Yu Wang
>
> **备注:** 22 pages
>
> **摘要:** Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.
>
---
#### [new 124] DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于模型微调任务，解决MoE模型中LoRA参数分配不合理问题。提出DR-LoRA动态调整专家LoRA秩，提升参数利用率与任务性能。**

- **链接: [https://arxiv.org/pdf/2601.04823v1](https://arxiv.org/pdf/2601.04823v1)**

> **作者:** Guanzhi Deng; Bo Li; Ronghao Chen; Huacan Wang; Linqi Song; Lijie Wen
>
> **摘要:** Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.
>
---
#### [new 125] Sphinx: Benchmarking and Modeling for LLM-Driven Pull Request Review
- **分类: cs.SE; cs.CL**

- **简介: 该论文提出Sphinx框架，用于改进LLM驱动的代码评审任务。针对评审质量评估不足的问题，设计了数据生成、检查表评估和奖励优化方法，提升评审的准确性和实用性。**

- **链接: [https://arxiv.org/pdf/2601.04252v1](https://arxiv.org/pdf/2601.04252v1)**

> **作者:** Daoan Zhang; Shuo Zhang; Zijian Jin; Jiebo Luo; Shengyu Fu; Elsie Nallipogu
>
> **摘要:** Pull request (PR) review is essential for ensuring software quality, yet automating this task remains challenging due to noisy supervision, limited contextual understanding, and inadequate evaluation metrics. We present Sphinx, a unified framework for LLM-based PR review that addresses these limitations through three key components: (1) a structured data generation pipeline that produces context-rich, semantically grounded review comments by comparing pseudo-modified and merged code; (2) a checklist-based evaluation benchmark that assesses review quality based on structured coverage of actionable verification points, moving beyond surface-level metrics like BLEU; and (3) Checklist Reward Policy Optimization (CRPO), a novel training paradigm that uses rule-based, interpretable rewards to align model behavior with real-world review practices. Extensive experiments show that models trained with Sphinx achieve state-of-the-art performance on review completeness and precision, outperforming both proprietary and open-source baselines by up to 40\% in checklist coverage. Together, Sphinx enables the development of PR review models that are not only fluent but also context-aware, technically precise, and practically deployable in real-world development workflows. The data will be released after review.
>
---
#### [new 126] The Language of Bargaining: Linguistic Effects in LLM Negotiations
- **分类: cs.AI; cs.CL; cs.GT**

- **简介: 该论文研究LLM在谈判中的语言影响，属于自然语言处理任务。解决语言对谈判结果的影响问题，通过多语言模拟实验分析不同语言下的谈判表现。**

- **链接: [https://arxiv.org/pdf/2601.04387v1](https://arxiv.org/pdf/2601.04387v1)**

> **作者:** Stuti Sinha; Himanshu Kumar; Aryan Raju Mandapati; Rakshit Sakhuja; Dhruv Kumar
>
> **备注:** Under Review
>
> **摘要:** Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.
>
---
#### [new 127] Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence
- **分类: cs.AI; cs.CL; cs.DL; cs.IT**

- **简介: 该论文属于知识提取任务，旨在解决材料科学中科学综述信息难以被机器利用的问题。工作是将综述数据转化为FAIR结构化知识，并对比符号查询与大语言模型的效果。**

- **链接: [https://arxiv.org/pdf/2601.05051v1](https://arxiv.org/pdf/2601.05051v1)**

> **作者:** Jennifer D'Souza; Soren Auer; Eleni Poupaki; Alex Watkins; Anjana Devi; Riikka L. Puurunen; Bora Karasulu; Adrie Mackus; Erwin Kessels
>
> **备注:** 35 pages, 11 figures
>
> **摘要:** Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.
>
---
#### [new 128] Token-Level LLM Collaboration via FusionRoute
- **分类: cs.AI; cs.CL; cs.LG**

- **简介: 该论文提出FusionRoute，解决多大模型协作问题。通过轻量路由机制，在token级别融合多个模型，提升性能并保持效率。**

- **链接: [https://arxiv.org/pdf/2601.05106v1](https://arxiv.org/pdf/2601.05106v1)**

> **作者:** Nuoya Xiong; Yuhang Zhou; Hanqing Zeng; Zhaorun Chen; Furong Huang; Shuchao Bi; Lizhu Zhang; Zhuokai Zhao
>
> **备注:** 25 pages
>
> **摘要:** Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.
>
---
#### [new 129] DP-MGTD: Privacy-Preserving Machine-Generated Text Detection via Adaptive Differentially Private Entity Sanitization
- **分类: cs.CR; cs.CL; cs.LG**

- **简介: 该论文属于机器生成文本检测任务，旨在解决隐私保护与准确检测的矛盾。提出DP-MGTD框架，通过自适应差分隐私实体清洗，在保证隐私的同时提升检测效果。**

- **链接: [https://arxiv.org/pdf/2601.04641v1](https://arxiv.org/pdf/2601.04641v1)**

> **作者:** Lionel Z. Wang; Yusheng Zhao; Jiabin Luo; Xinfeng Li; Lixu Wang; Yinan Peng; Haoyang Li; XiaoFeng Wang; Wei Dong
>
> **备注:** 12 pages, 1 figure, 1 tables
>
> **摘要:** The deployment of Machine-Generated Text (MGT) detection systems necessitates processing sensitive user data, creating a fundamental conflict between authorship verification and privacy preservation. Standard anonymization techniques often disrupt linguistic fluency, while rigorous Differential Privacy (DP) mechanisms typically degrade the statistical signals required for accurate detection. To resolve this dilemma, we propose \textbf{DP-MGTD}, a framework incorporating an Adaptive Differentially Private Entity Sanitization algorithm. Our approach utilizes a two-stage mechanism that performs noisy frequency estimation and dynamically calibrates privacy budgets, applying Laplace and Exponential mechanisms to numerical and textual entities respectively. Crucially, we identify a counter-intuitive phenomenon where the application of DP noise amplifies the distinguishability between human and machine text by exposing distinct sensitivity patterns to perturbation. Extensive experiments on the MGTBench-2.0 dataset show that our method achieves near-perfect detection accuracy, significantly outperforming non-private baselines while satisfying strict privacy guarantees.
>
---
#### [new 130] Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity
- **分类: cs.LG; cs.CL**

- **简介: 该论文研究文本中模运算的鲁棒性问题，解决模型在输入格式变化时的失效问题。通过引入位置课程和模板多样性提升模型对位置偏移和分布外模板的鲁棒性。**

- **链接: [https://arxiv.org/pdf/2601.04283v1](https://arxiv.org/pdf/2601.04283v1)**

> **作者:** Nikolay Yudin
>
> **摘要:** Building on insights from the grokking literature, we study character-level Transformers trained to compute modular addition from text, and focus on robustness under input-format variation rather than only in-distribution accuracy. We identify a previously under-emphasized failure mode: models that achieve high in-distribution accuracy can fail catastrophically when the same expression is shifted to different absolute character positions ("position shift") or presented under out-of-distribution natural-language templates. Using a disjoint-pair split over all ordered pairs for p=97, we show that a baseline model reaches strong in-distribution performance yet collapses under position shift and template OOD. We then introduce a simple training recipe that combines (i) explicit expression boundary markers, (ii) position curriculum that broadens the range of absolute positions seen during training, (iii) diverse template mixtures, and (iv) consistency training across multiple variants per example. Across three seeds, this intervention substantially improves robustness to position shift and template OOD while maintaining high in-distribution accuracy, whereas an ALiBi-style ablation fails to learn the task under our setup. Our results suggest that steering procedural generalization under noisy supervision benefits from explicitly training invariances that are otherwise absent from the data distribution, and we provide a reproducible evaluation protocol and artifacts.
>
---
#### [new 131] Vision-Language Agents for Interactive Forest Change Analysis
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文属于遥感图像变化分析任务，旨在解决森林动态的像素级变化检测和语义描述问题。提出一种基于大语言模型的视觉-语言代理系统，并构建了Forest-Change数据集进行评估。**

- **链接: [https://arxiv.org/pdf/2601.04497v1](https://arxiv.org/pdf/2601.04497v1)**

> **作者:** James Brock; Ce Zhang; Nantheera Anantrasirichai
>
> **备注:** 5 pages, 4 figures, Submitted to IGARSS 2026
>
> **摘要:** Modern forest monitoring workflows increasingly benefit from the growing availability of high-resolution satellite imagery and advances in deep learning. Two persistent challenges in this context are accurate pixel-level change detection and meaningful semantic change captioning for complex forest dynamics. While large language models (LLMs) are being adapted for interactive data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored. To address this gap, we introduce an LLM-driven agent for integrated forest change analysis that supports natural language querying across multiple RSICI tasks. The proposed system builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration. To facilitate adaptation and evaluation in forest environments, we further introduce the Forest-Change dataset, which comprises bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated using a combination of human annotation and rule-based methods. Experimental results show that the proposed system achieves mIoU and BLEU-4 scores of 67.10% and 40.17% on the Forest-Change dataset, and 88.13% and 34.41% on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI benchmark for joint change detection and captioning. These results highlight the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and efficiency of forest change analysis. All data and code are publicly available at https://github.com/JamesBrockUoB/ForestChat.
>
---
#### [new 132] CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models
- **分类: cs.CV; cs.AI; cs.CL; cs.MM**

- **简介: 该论文属于视频-语言模型任务，旨在解决动作和时间幻觉问题。通过生成反事实视频，构建合成数据集并提出优化方法，提升模型在动作识别和时间推理上的准确性。**

- **链接: [https://arxiv.org/pdf/2601.04778v1](https://arxiv.org/pdf/2601.04778v1)**

> **作者:** Tobia Poppi; Burak Uzkent; Amanmeet Garg; Lucas Porto; Garin Kessler; Yezhou Yang; Marcella Cornia; Lorenzo Baraldi; Rita Cucchiara; Florian Schiffers
>
> **摘要:** Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.
>
---
#### [new 133] SAGE-32B: Agentic Reasoning via Iterative Distillation
- **分类: cs.AI; cs.CL; cs.LG**

- **简介: 该论文介绍SAGE-32B，一个用于代理推理和长期规划的大型语言模型。针对多工具使用场景下的任务分解与错误恢复问题，通过迭代蒸馏和逆向推理方法提升性能。**

- **链接: [https://arxiv.org/pdf/2601.04237v1](https://arxiv.org/pdf/2601.04237v1)**

> **作者:** Basab Jha; Firoj Paudel; Ujjwal Puri; Ethan Henkel; Zhang Yuting; Mateusz Kowalczyk; Mei Huang; Choi Donghyuk; Wang Junhao
>
> **备注:** 23 Pages, 3 figures, 4 tables
>
> **摘要:** We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b
>
---
#### [new 134] CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts
- **分类: cs.AI; cs.CL; eess.SY**

- **简介: 该论文属于自然语言到电路设计的任务，解决LLM生成电路时细节错误和违反电气约束的问题。提出CircuitLM框架，通过多阶段流程生成准确的电路图。**

- **链接: [https://arxiv.org/pdf/2601.04505v1](https://arxiv.org/pdf/2601.04505v1)**

> **作者:** Khandakar Shakib Al Hasan; Syed Rifat Raiyan; Hasin Mahtab Alvee; Wahid Sadik
>
> **备注:** Under review, 13 pages, 11 figures, 2 tables
>
> **摘要:** Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.
>
---
#### [new 135] Advancing Language Models for Code-related Tasks
- **分类: cs.SE; cs.AI; cs.CL**

- **简介: 该论文聚焦代码相关任务，解决语言模型在编程场景中的不足，通过数据增强、架构改进和推理优化提升模型性能。**

- **链接: [https://arxiv.org/pdf/2601.04526v1](https://arxiv.org/pdf/2601.04526v1)**

> **作者:** Zhao Tian
>
> **备注:** Accepted by ICSE 2026 (DS)
>
> **摘要:** Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.
>
---
## 更新

#### [replaced 001] Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models across Modalities
- **分类: cs.CL**

- **简介: 该论文属于多语言自然语言处理任务，旨在解决代码切换建模问题。通过分析324篇研究，探讨了大语言模型在多语言环境中的挑战与进展，并提出未来研究方向。**

- **链接: [https://arxiv.org/pdf/2510.07037v5](https://arxiv.org/pdf/2510.07037v5)**

> **作者:** Rajvee Sheth; Samridhi Raj Sinha; Mahavir Patil; Himanshu Beniwal; Mayank Singh
>
> **摘要:** Amidst the rapid advances of large language models (LLMs), most LLMs still struggle with mixed-language inputs, limited Codeswitching (CSW) datasets, and evaluation biases, which hinder their deployment in multilingual societies. This survey provides the first comprehensive analysis of CSW-aware LLM research, reviewing 324 studies spanning five research areas, 15+ NLP tasks, 30+ datasets, and 80+ languages. We categorize recent advances by architecture, training strategy, and evaluation methodology, outlining how LLMs have reshaped CSW modeling and identifying the challenges that persist. The paper concludes with a roadmap that emphasizes the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual capabilities https://github.com/lingo-iitgn/awesome-code-mixing/.
>
---
#### [replaced 002] Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR
- **分类: cs.CL; cs.CV**

- **简介: 该论文属于OCR任务，旨在评估DeepSeek-OCR的性能驱动因素。通过语义扰动实验，发现其性能依赖语言先验而非视觉能力，揭示了视觉-文本压缩的局限性。**

- **链接: [https://arxiv.org/pdf/2601.03714v2](https://arxiv.org/pdf/2601.03714v2)**

> **作者:** Yunhao Liang; Ruixuan Ying; Bo Li; Hong Li; Kai Yan; Qingwen Li; Min Yang; Okamoto Satoshi; Zhe Cui; Shiwen Ni
>
> **摘要:** DeepSeek-OCR utilizes an optical 2D mapping approach to achieve high-ratio vision-text compression, claiming to decode text tokens exceeding ten times the input visual tokens. While this suggests a promising solution for the LLM long-context bottleneck, we investigate a critical question: "Visual merit or linguistic crutch - which drives DeepSeek-OCR's performance?" By employing sentence-level and word-level semantic corruption, we isolate the model's intrinsic OCR capabilities from its language priors. Results demonstrate that without linguistic support, DeepSeek-OCR's performance plummets from approximately 90% to 20%. Comparative benchmarking against 13 baseline models reveals that traditional pipeline OCR methods exhibit significantly higher robustness to such semantic perturbations than end-to-end methods. Furthermore, we find that lower visual token counts correlate with increased reliance on priors, exacerbating hallucination risks. Context stress testing also reveals a total model collapse around 10,000 text tokens, suggesting that current optical compression techniques may paradoxically aggravate the long-context bottleneck. This study empirically defines DeepSeek-OCR's capability boundaries and offers essential insights for future optimizations of the vision-text compression paradigm. We release all data, results and scripts used in this study at https://github.com/dududuck00/DeepSeekOCR.
>
---
#### [replaced 003] LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation
- **分类: cs.AI; cs.CL**

- **简介: 该论文提出LPFQA基准，用于评估大语言模型在长尾专业领域的问题解决能力，解决现有基准无法全面反映实际专业场景的问题。**

- **链接: [https://arxiv.org/pdf/2511.06346v2](https://arxiv.org/pdf/2511.06346v2)**

> **作者:** Liya Zhu; Peizhuang Cong; Jingzhe Ding; Aowei Ji; Wenya Wu; Jiani Hou; Chunjie Wu; Xiang Gao; Jingkai Liu; Zhou Huan; Xuelei Sun; Yang Yang; Jianpeng Jiao; Liang Hu; Xinjie Chen; Jiashuo Liu; Tong Yang; Zaiyuan Wang; Ge Zhang; Wenhao Huang
>
> **摘要:** Large Language Models (LLMs) perform well on standard reasoning and question-answering benchmarks, yet such evaluations often fail to capture their ability to handle long-tail, expertise-intensive knowledge in real-world professional scenarios. We introduce LPFQA, a long-tail knowledge benchmark derived from authentic professional forum discussions, covering 7 academic and industrial domains with 430 curated tasks grounded in practical expertise. LPFQA evaluates specialized reasoning, domain-specific terminology understanding, and contextual interpretation, and adopts a hierarchical difficulty structure to ensure semantic clarity and uniquely identifiable answers. Experiments on over multiple mainstream LLMs reveal substantial performance gaps, particularly on tasks requiring deep domain reasoning, exposing limitations overlooked by existing benchmarks. Overall, LPFQA provides an authentic and discriminative evaluation framework that complements prior benchmarks and informs future LLM development.
>
---
#### [replaced 004] Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning
- **分类: cs.CL**

- **简介: 该论文属于持续学习任务，解决代理在学习新任务时的灾难性遗忘问题。提出Agent-Dice框架，通过知识解耦实现稳定与灵活学习。**

- **链接: [https://arxiv.org/pdf/2601.03641v2](https://arxiv.org/pdf/2601.03641v2)**

> **作者:** Zheng Wu; Xingyu Lou; Xinbei Ma; Yansi Li; Weiwen Liu; Weinan Zhang; Jun Wang; Zhuosheng Zhang
>
> **摘要:** Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates. The codes are available at https://github.com/Wuzheng02/Agent-Dice.
>
---
#### [replaced 005] Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文属于自然语言处理任务，研究ICL+CoT在数学推理中的有效性。通过实验发现，增强的CoT示例对强模型效果有限，主要问题在于模型忽略示例，依赖指令。**

- **链接: [https://arxiv.org/pdf/2506.14641v3](https://arxiv.org/pdf/2506.14641v3)**

> **作者:** Xiang Cheng; Chengyan Pan; Minjun Zhao; Deyang Li; Fangchao Liu; Xinyu Zhang; Xiao Zhang; Yong Liu
>
> **备注:** EMNLP25-findings camera_ready, 19 pages,22 figures
>
> **摘要:** In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.
>
---
#### [replaced 006] Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora
- **分类: cs.CL**

- **简介: 该论文属于信息评估任务，旨在解决如何判断文本集合是否值得投入资源用于LLM的问题。通过生成选择题并比较模型性能差异，评估文本的信息价值。**

- **链接: [https://arxiv.org/pdf/2502.13691v3](https://arxiv.org/pdf/2502.13691v3)**

> **作者:** Tristan Karch; Luca Engel; Philippe Schwaller; Frédéric Kaplan
>
> **摘要:** As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using five strategically selected datasets: EPFL PhD manuscripts, a private collection of Venetian historical records, two sets of Wikipedia articles on related topics, and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.
>
---
#### [replaced 007] An LLM + ASP Workflow for Joint Entity-Relation Extraction
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于联合实体关系抽取任务，旨在解决传统方法依赖大量标注数据和难以融入领域知识的问题。作者提出一种结合大语言模型与答案集编程的通用工作流，提升抽取效果。**

- **链接: [https://arxiv.org/pdf/2508.12611v3](https://arxiv.org/pdf/2508.12611v3)**

> **作者:** Trang Tran; Trung Hoang Le; Huiping Cao; Tran Cao Son
>
> **备注:** In Proceedings ICLP 2025, arXiv:2601.00047
>
> **摘要:** Joint entity-relation extraction (JERE) identifies both entities and their relationships simultaneously. Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant. In this paper, we propose harnessing the capabilities of generative pre-trained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP. The workflow is generic in the sense that it can be applied for JERE in any domain. It takes advantage of LLM's capability in natural language understanding in that it works directly with unannotated text. It exploits the elaboration tolerant feature of ASP in that no modification of its core program is required when additional domain specific knowledge, in the form of type specifications, is found and needs to be used. We demonstrate the usefulness of the proposed workflow through experiments with limited training data on three well-known benchmarks for JERE. The results of our experiments show that the LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10% of training data. It is able to achieve a 2.5 times (35% over 15%) improvement in the Relation Extraction task for the SciERC corpus, one of the most difficult benchmarks.
>
---
#### [replaced 008] AppellateGen: A Benchmark for Appellate Legal Judgment Generation
- **分类: cs.CY; cs.CL; cs.LG**

- **简介: 该论文属于法律判决生成任务，旨在解决上诉阶段判决生成问题。提出AppellateGen基准和SLMAS系统，以模拟司法流程并提升逻辑一致性。**

- **链接: [https://arxiv.org/pdf/2601.01331v2](https://arxiv.org/pdf/2601.01331v2)**

> **作者:** Hongkun Yang; Lionel Z. Wang; Wei Fan; Yiran Hu; Lixu Wang; Chenyu Liu; Shenghong Fu; Haoyang Li; Xin Xu; Jiexin Zheng; Wei Dong
>
> **备注:** 15 pages, 4 figures, 3 tables
>
> **摘要:** Legal judgment generation is a critical task in legal intelligence. However, existing research in legal judgment generation has predominantly focused on first-instance trials, relying on static fact-to-verdict mappings while neglecting the dialectical nature of appellate (second-instance) review. To address this, we introduce AppellateGen, a benchmark for second-instance legal judgment generation comprising 7,351 case pairs. The task requires models to draft legally binding judgments by reasoning over the initial verdict and evidentiary updates, thereby modeling the causal dependency between trial stages. We further propose a judicial Standard Operating Procedure (SOP)-based Legal Multi-Agent System (SLMAS) to simulate judicial workflows, which decomposes the generation process into discrete stages of issue identification, retrieval, and drafting. Experimental results indicate that while SLMAS improves logical consistency, the complexity of appellate reasoning remains a substantial challenge for current LLMs. The dataset and code are publicly available at: https://anonymous.4open.science/r/AppellateGen-5763.
>
---
#### [replaced 009] VCB Bench: An Evaluation Benchmark for Audio-Grounded Large Language Model Conversational Agents
- **分类: cs.SD; cs.CL**

- **简介: 该论文属于语音对话系统任务，旨在解决现有基准不足的问题。构建了VCB Bench，一个基于真实中文语音的评估基准，从指令遵循、知识理解和鲁棒性三个方面评估大模型。**

- **链接: [https://arxiv.org/pdf/2510.11098v3](https://arxiv.org/pdf/2510.11098v3)**

> **作者:** Jiliang Hu; Wenfu Wang; Zuchao Li; Chenxing Li; Yiyang Zhao; Hanzhao Li; Liqiang Zhang; Meng Yu; Dong Yu
>
> **备注:** 23 pages, 5 figures
>
> **摘要:** Recent advances in large audio language models (LALMs) have greatly enhanced multimodal conversational systems. However, existing benchmarks remain limited -- they are mainly English-centric, rely on synthetic speech, and lack comprehensive, discriminative evaluation across multiple dimensions. To address these gaps, we present Voice Chat Bot Bench (VCB Bench) -- a high-quality Chinese benchmark built entirely on real human speech. VCB Bench evaluates LALMs from three complementary perspectives: instruction following (including speech-level control beyond text commands), knowledge understanding (general knowledge, reasoning, and daily dialogue), and robustness (stability under perturbations in content, environment, and speaker traits). Experiments on representative LALMs reveal notable performance gaps and highlight future directions for improvement. VCB Bench provides a reproducible and fine-grained evaluation framework, offering standardized methodology and practical insights for advancing Chinese voice conversational models.
>
---
#### [replaced 010] Pelican Soup Framework: A Theoretical Framework for Language Model Capabilities
- **分类: cs.CL; cs.AI**

- **简介: 该论文提出"Pelican Soup"框架，用于理解大语言模型的泛化能力和上下文学习。旨在解决模型如何在无相关提示下完成任务的问题。通过理论分析与实验验证，探索语言模型的内在机制。**

- **链接: [https://arxiv.org/pdf/2402.10424v2](https://arxiv.org/pdf/2402.10424v2)**

> **作者:** Ting-Rui Chiang; Dani Yogatama
>
> **摘要:** In this work, we propose a simple theoretical framework, Pelican Soup, aiming to better understand how pretraining allows LLMs to (1) generalize to unseen instructions and (2) perform in-context learning, even when the verbalizers are irrelevant to the task. To this end, in our framework, we introduce the notion of "knowledge base" and "reference-sense association" and a simple formalism for natural language processing tasks. Our framework demonstrates how linguistic, psychology, and philosophy studies can inform our understanding of the language model and is connected to several other existing theoretical results. As an illustration of the usage of our framework, we derive a bound on in-context learning loss with our framework. Finally, we support our framework with empirical experiments and provide possible future research directions.
>
---
#### [replaced 011] SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation
- **分类: cs.CL**

- **简介: 该论文属于自然语言生成任务，旨在解决泰国语文本生成不稳定的问题。通过质量优先的微调策略，提升模型在复杂指令下的表现。**

- **链接: [https://arxiv.org/pdf/2512.19455v3](https://arxiv.org/pdf/2512.19455v3)**

> **作者:** Thittipat Pairatsuppawat; Abhibhu Tachaapornchai; Paweekorn Kusolsomboon; Chutikan Chaiwong; Thodsaporn Chay-intr; Kobkrit Viriyayudhakorn; Nongnuch Ketui; Aslan B. Wong
>
> **摘要:** Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.
>
---
#### [replaced 012] MiMo-V2-Flash Technical Report
- **分类: cs.CL; cs.AI**

- **简介: 该论文提出MiMo-V2-Flash，一个高效推理的Mixture-of-Experts模型，解决大模型参数与性能平衡问题，通过混合注意力和多教师蒸馏提升效率与效果。**

- **链接: [https://arxiv.org/pdf/2601.02780v2](https://arxiv.org/pdf/2601.02780v2)**

> **作者:** Xiaomi LLM-Core Team; :; Bangjun Xiao; Bingquan Xia; Bo Yang; Bofei Gao; Bowen Shen; Chen Zhang; Chenhong He; Chiheng Lou; Fuli Luo; Gang Wang; Gang Xie; Hailin Zhang; Hanglong Lv; Hanyu Li; Heyu Chen; Hongshen Xu; Houbin Zhang; Huaqiu Liu; Jiangshan Duo; Jianyu Wei; Jiebao Xiao; Jinhao Dong; Jun Shi; Junhao Hu; Kainan Bao; Kang Zhou; Lei Li; Liang Zhao; Linghao Zhang; Peidian Li; Qianli Chen; Shaohui Liu; Shihua Yu; Shijie Cao; Shimao Chen; Shouqiu Yu; Shuo Liu; Tianling Zhou; Weijiang Su; Weikun Wang; Wenhan Ma; Xiangwei Deng; Bohan Mao; Bowen Ye; Can Cai; Chenghua Wang; Chengxuan Zhu; Chong Ma; Chun Chen; Chunan Li; Dawei Zhu; Deshan Xiao; Dong Zhang; Duo Zhang; Fangyue Liu; Feiyu Yang; Fengyuan Shi; Guoan Wang; Hao Tian; Hao Wu; Heng Qu; Hongfei Yi; Hongxu An; Hongyi Guan; Xing Zhang; Yifan Song; Yihan Yan; Yihao Zhao; Yingchun Lai; Yizhao Gao; Yu Cheng; Yuanyuan Tian; Yudong Wang; Zhen Tang; Zhengju Tang; Zhengtao Wen; Zhichao Song; Zhixian Zheng; Zihan Jiang; Jian Wen; Jiarui Sun; Jiawei Li; Jinlong Xue; Jun Xia; Kai Fang; Menghang Zhu; Nuo Chen; Qian Tu; Qihao Zhang; Qiying Wang; Rang Li; Rui Ma; Shaolei Zhang; Shengfan Wang; Shicheng Li; Shuhao Gu; Shuhuai Ren; Sirui Deng; Tao Guo; Tianyang Lu; Weiji Zhuang; Weikang Zhang; Weimin Xiong; Wenshan Huang; Wenyu Yang; Xin Zhang; Xing Yong; Xu Wang; Xueyang Xie; Yilin Jiang; Yixin Yang; Yongzhe He; Yu Tu; Yuanliang Dong; Yuchen Liu; Yue Ma; Yue Yu; Yuxing Xiang; Zhaojun Huang; Zhenru Lin; Zhipeng Xu; Zhiyang Chen; Zhonghua Deng; Zihan Zhang; Zihao Yue
>
> **备注:** 31 pages, technical report
>
> **摘要:** We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.
>
---
#### [replaced 013] Muse: Towards Reproducible Long-Form Song Generation with Fine-Grained Style Control
- **分类: cs.SD; cs.CL**

- **简介: 该论文属于可控长歌曲生成任务，旨在解决学术研究不可复现的问题。工作包括发布开源系统Muse及合成数据集，实现细粒度风格控制的歌曲生成。**

- **链接: [https://arxiv.org/pdf/2601.03973v2](https://arxiv.org/pdf/2601.03973v2)**

> **作者:** Changhao Jiang; Jiahao Chen; Zhenghao Xiang; Zhixiong Yang; Hanchen Wang; Jiabao Zhuang; Xinmeng Che; Jiajun Sun; Hui Li; Yifei Cao; Shihan Dou; Ming Zhang; Junjie Ye; Tao Ji; Tao Gui; Qi Zhang; Xuanjing Huang
>
> **摘要:** Recent commercial systems such as Suno demonstrate strong capabilities in long-form song generation, while academic research remains largely non-reproducible due to the lack of publicly available training data, hindering fair comparison and progress. To this end, we release a fully open-source system for long-form song generation with fine-grained style conditioning, including a licensed synthetic dataset, training and evaluation pipelines, and Muse, an easy-to-deploy song generation model. The dataset consists of 116k fully licensed synthetic songs with automatically generated lyrics and style descriptions paired with audio synthesized by SunoV5. We train Muse via single-stage supervised finetuning of a Qwen-based language model extended with discrete audio tokens using MuCodec, without task-specific losses, auxiliary objectives, or additional architectural components. Our evaluations find that although Muse is trained with a modest data scale and model size, it achieves competitive performance on phoneme error rate, text--music style similarity, and audio aesthetic quality, while enabling controllable segment-level generation across different musical structures. All data, model weights, and training and evaluation pipelines will be publicly released, paving the way for continued progress in controllable long-form song generation research. The project repository is available at https://github.com/yuhui1038/Muse.
>
---
#### [replaced 014] Establishing a Scale for Kullback--Leibler Divergence in Language Models Across Various Settings
- **分类: cs.CL**

- **简介: 该论文属于语言模型分析任务，旨在建立跨不同设置的KL散度统一尺度。通过log-likelihood空间分析，解决模型比较与稳定性问题。**

- **链接: [https://arxiv.org/pdf/2505.15353v2](https://arxiv.org/pdf/2505.15353v2)**

> **作者:** Ryo Kishino; Yusuke Takase; Momose Oyama; Hiroaki Yamagiwa; Hidetoshi Shimodaira
>
> **摘要:** Log-likelihood vectors define a common space for comparing language models as probability distributions, enabling unified comparisons across heterogeneous settings. We extend this framework to training checkpoints and intermediate layers, and establish a consistent scale for KL divergence across pretraining, model size, random seeds, quantization, fine-tuning, and layers. Analysis of Pythia pretraining trajectories further shows that changes in log-likelihood space are much smaller than in weight space, resulting in subdiffusive learning trajectories and early stabilization of language-model behavior despite weight drift.
>
---
#### [replaced 015] Realised Volatility Forecasting: Machine Learning via Financial Word Embedding
- **分类: q-fin.CP; cs.CL; cs.LG**

- **简介: 该论文属于金融时间序列预测任务，旨在提升实际波动率的预测效果。通过自然语言处理将新闻文本转化为嵌入表示，验证其对波动率预测的预测价值，并与传统模型结合提高性能。**

- **链接: [https://arxiv.org/pdf/2108.00480v5](https://arxiv.org/pdf/2108.00480v5)**

> **作者:** Eghbal Rahimikia; Stefan Zohren; Ser-Huang Poon
>
> **摘要:** We examine whether news can improve realised volatility forecasting using a modern yet operationally simple NLP framework. News text is transformed into embedding-based representations, and forecasts are evaluated both as a standalone, news-only model and as a complement to standard realised volatility benchmarks. In out-of-sample tests on a cross-section of stocks, news contains useful predictive information, with stronger effects for stock-related content and during high volatility days. Combining the news-based signal with a leading benchmark yields consistent improvements in statistical performance and economically meaningful gains, while explainability analysis highlights the news themes most relevant for volatility.
>
---
#### [replaced 016] TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu
- **分类: cs.CL**

- **简介: 该论文提出TeSent数据集，解决Telugu语言情感分类中的可解释性和公平性问题，包含21,119句文本及人工标注理由。**

- **链接: [https://arxiv.org/pdf/2508.01486v2](https://arxiv.org/pdf/2508.01486v2)**

> **作者:** Vallabhaneni Raj Kumar; Ashwin S; Supriya Manna; Niladri Sett; Cheedella V S N M S Hema Harshitha; Kurakula Harshitha; Anand Kumar Sharma; Basina Deepakraj; Tanuj Sarkar; Bondada Navaneeth Krishna; Samanthapudi Shakeer
>
> **备注:** We identified and resolved technical issues in the previous version and updated the results and resources accordingly
>
> **摘要:** In the Indian subcontinent, Telugu, one of India's six classical languages, is the most widely spoken Dravidian Language. Despite its 96 million speaker base worldwide, Telugu remains underrepresented in the global NLP and Machine Learning landscape, mainly due to lack of high-quality annotated resources. This work introduces TeSent, a comprehensive benchmark dataset for sentiment classification, a key text classification problem, in Telugu. TeSent not only provides ground truth labels for the sentences, but also supplements with provisions for evaluating explainability and fairness, two critical requirements in modern-day machine learning tasks. We scraped Telugu texts covering multiple domains from various social media platforms, news websites and web-blogs to preprocess and generate 21,119 sentences, and developed a custom-built annotation platform and a carefully crafted annotation protocol for collecting the ground truth labels along with their human-annotated rationales. We then fine-tuned several SOTA pre-trained models in two ways: with rationales, and without rationales. Further, we provide a detailed plausibility and faithfulness evaluation suite, which exploits the rationales, for six widely used post-hoc explainers applied on the trained models. Lastly, we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate fairness of Telugu sentiment and emotion related NLP tasks, and provide a fairness evaluation suite for the trained classifier models. Our experimental results suggest that training with human rationales improves model accuracy and models' alignment with human reasoning, but does not necessarily reduce bias.
>
---
#### [replaced 017] Jailbreaking Safeguarded Text-to-Image Models via Large Language Models
- **分类: cs.CR; cs.AI; cs.CL; cs.CV**

- **简介: 该论文属于安全防护任务，旨在解决文本生成图像模型被攻击绕过安全机制的问题。通过微调大语言模型生成对抗提示，有效突破安全防护。**

- **链接: [https://arxiv.org/pdf/2503.01839v2](https://arxiv.org/pdf/2503.01839v2)**

> **作者:** Zhengyuan Jiang; Yuepeng Hu; Yuchen Yang; Yinzhi Cao; Neil Zhenqiang Gong
>
> **备注:** Accepted by EACL 2026 Findings
>
> **摘要:** Text-to-Image models may generate harmful content, such as pornographic images, particularly when unsafe prompts are submitted. To address this issue, safety filters are often added on top of text-to-image models, or the models themselves are aligned to reduce harmful outputs. However, these defenses remain vulnerable when an attacker strategically designs adversarial prompts to bypass these safety guardrails. In this work, we propose \alg, a method to jailbreak text-to-image models with safety guardrails using a fine-tuned large language model. Unlike other query-based jailbreak attacks that require repeated queries to the target model, our attack generates adversarial prompts efficiently after fine-tuning our AttackLLM. We evaluate our method on three datasets of unsafe prompts and against five safety guardrails. Our results demonstrate that our approach effectively bypasses safety guardrails, outperforms existing no-box attacks, and also facilitates other query-based attacks.
>
---
#### [replaced 018] NorwAI's Large Language Models: Technical Report
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决挪威语在大模型中的代表性不足问题。研究团队开发了针对挪威语及北欧语言的大型语言模型，通过预训练和优化提升其性能与适用性。**

- **链接: [https://arxiv.org/pdf/2601.03034v2](https://arxiv.org/pdf/2601.03034v2)**

> **作者:** Jon Atle Gulla; Peng Liu; Lemei Zhang
>
> **摘要:** Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.
>
---
#### [replaced 019] Vague Knowledge: Information without Transitivity and Partitions
- **分类: econ.TH; cs.CL; math.LO; q-fin.GN**

- **简介: 该论文属于信息经济学领域，研究模糊知识的非传递性和非划分特性，探讨其表达与沟通方式，解释自然语言和定性推理的普遍性。**

- **链接: [https://arxiv.org/pdf/2512.05833v2](https://arxiv.org/pdf/2512.05833v2)**

> **作者:** Kerry Xiao
>
> **摘要:** I relax the standard assumptions of transitivity and partition structure in economic models of information to formalize vague knowledge: non-transitive indistinguishability over states. I show that vague knowledge, while failing to partition the state space, remains informative by distinguishing some states from others. Moreover, it can only be faithfully expressed through vague communication with blurred boundaries. My results provide microfoundations for the prevalence of natural language communication and qualitative reasoning in the real world, where knowledge is often vague.
>
---
#### [replaced 020] MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Domain Risks in LLMs
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于LLM安全任务，旨在解决隐性领域风险问题。通过构建数据集和提出MENTOR框架，实现模型自我评估与风险缓解。**

- **链接: [https://arxiv.org/pdf/2511.07107v2](https://arxiv.org/pdf/2511.07107v2)**

> **作者:** Liang Shan; Kaicheng Shen; Wen Wu; Zhenyu Ying; Chaochao Lu; Yan Teng; Jingqi Huang; Guangze Ye; Guoqing Wang; Liang He
>
> **摘要:** Ensuring the safety of Large Language Models (LLMs) is critical for real-world deployment. However, current safety measures often fail to address implicit, domain-specific risks. To investigate this gap, we introduce a dataset of 3,000 annotated queries spanning education, finance, and management. Evaluations across 14 leading LLMs reveal a concerning vulnerability: an average jailbreak success rate of 57.8%. In response, we propose MENTOR, a metacognition-driven self-evolution framework. MENTOR first performs structured self-assessment through simulated critical thinking, such as perspective-taking and consequential reasoning to uncover latent model misalignments. These reflections are formalized into dynamic rule-based knowledge graphs that evolve with emerging risk patterns. To enforce these rules at inference time, we introduce activation steering, a method that directly modulates the model's internal representations to ensure compliance. Experiments demonstrate that MENTOR substantially reduces attack success rates across all tested domains and achieves risk analysis performance comparable to human experts. Our work offers a scalable and adaptive pathway toward robust domain-specific alignment of LLMs.
>
---
#### [replaced 021] Reverse Language Model
- **分类: cs.CL; cs.AI**

- **简介: 该论文提出LEDOM，首个纯逆向语言模型，通过反向序列预测解决生成质量提升问题。应用于数学推理任务，展示其广泛潜力。**

- **链接: [https://arxiv.org/pdf/2507.01335v2](https://arxiv.org/pdf/2507.01335v2)**

> **作者:** Xunjian Yin; Sitao Cheng; Yuxi Xie; Xinyu Hu; Li Lin; Xinyi Wang; Liangming Pan; William Yang Wang; Xiaojun Wan
>
> **备注:** Work in progress; Models can be found at: https://huggingface.co/Corning/Reverse-Model-7B-348B/tree/main
>
> **摘要:** We introduce LEDOM, the first purely reverse language model, trained autoregressively on 435B tokens with 2B and 7B parameter variants, which processes sequences in reverse temporal order through previous token prediction. For the first time, we present the reverse language model as a potential foundational model across general tasks, accompanied by a set of intriguing examples and insights. Based on LEDOM, we further introduce a novel application: Reverse Reward, where LEDOM-guided reranking of forward language model outputs leads to substantial performance improvements on mathematical reasoning tasks. This approach leverages LEDOM's unique backward reasoning capability to refine generation quality through posterior evaluation. Our findings suggest that LEDOM exhibits unique characteristics with broad application potential. We will release all models, training code, and pre-training data to facilitate future research.
>
---
#### [replaced 022] Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling
- **分类: cs.LG; cs.AI; cs.CL; stat.ML**

- **简介: 该论文属于语言模型优化任务，解决传统奖励模型数据与计算成本高的问题，通过提取LLM隐藏状态构建轻量奖励函数SWIFT，提升生成质量并降低资源消耗。**

- **链接: [https://arxiv.org/pdf/2505.12225v3](https://arxiv.org/pdf/2505.12225v3)**

> **作者:** Jizhou Guo; Zhaomin Wu; Hanchen Yang; Philip S. Yu
>
> **备注:** Accepted by KDD 2026 (Research Track). Project page: https://aster2024.github.io/swift-website/
>
> **摘要:** Best-of-N sampling is a powerful method for improving Large Language Model (LLM) performance, but it is often limited by its dependence on massive, text-based reward models. These models are not only computationally expensive but also data-hungry, requiring extensive labeled datasets for training. This creates a significant data challenge, as they overlook a rich, readily available data source: the LLM's own internal hidden states. To address this data and efficiency gap, we introduce SWIFT (Simple Weighted Intrinsic Feedback Technique), a novel and lightweight method that learns a reward function directly from the rich information embedded in LLM hidden states. Operating at the token embedding level, SWIFT employs simple linear layers to effectively distinguish between preferred and dispreferred generations, eliminating the need for computationally intensive text-based modeling. Extensive experiments on standard benchmarks show that SWIFT outperforms existing baselines (12.7% higher accuracy than EurusRM-7B on MATH dataset) while using less than 0.005% of their parameters. Its robust scalability, compatibility with certain closed-source models via logit access, and ability to combine with traditional reward models for additional performance highlight SWIFT's practical value and contribution to more efficient data-driven LLM post-training. Our code is available at https://github.com/aster2024/SWIFT .
>
---
#### [replaced 023] All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection
- **分类: cs.CL; cs.CE; q-fin.CP**

- **简介: 该论文属于金融谣言检测任务，旨在解决无参考的虚假信息识别问题。提出RFC Bench基准，通过对比分析提升检测效果。**

- **链接: [https://arxiv.org/pdf/2601.04160v2](https://arxiv.org/pdf/2601.04160v2)**

> **作者:** Yuechen Jiang; Zhiwei Liu; Yupeng Cao; Yueru He; Chen Xu; Ziyang Xu; Zhiyang Deng; Prayag Tiwari; Xi Chen; Alejandro Lopez-Lira; Jimin Huang; Junichi Tsujii; Sophia Ananiadou
>
> **备注:** 49 pages; 24 figures
>
> **摘要:** We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.
>
---
#### [replaced 024] BaseCal: Unsupervised Confidence Calibration via Base Model Signals
- **分类: cs.CL**

- **简介: 该论文属于模型校准任务，解决PoLLM过自信问题。通过BaseCal方法，利用基础模型信号对PoLLM进行无监督校准，提升其置信度可靠性。**

- **链接: [https://arxiv.org/pdf/2601.03042v2](https://arxiv.org/pdf/2601.03042v2)**

> **作者:** Hexiang Tan; Wanli Yang; Junwei Zhang; Xin Chen; Rui Tang; Du Su; Jingang Wang; Yuanzhuo Wang; Fei Sun; Xueqi Cheng
>
> **摘要:** Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\% compared to the best unsupervised baselines.
>
---
#### [replaced 025] FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文提出FINCHAIN，一个用于验证金融推理的基准，解决多步符号推理不足的问题。通过参数化模板和动态评估方法，提升金融AI的可信度与可解释性。**

- **链接: [https://arxiv.org/pdf/2506.02515v3](https://arxiv.org/pdf/2506.02515v3)**

> **作者:** Zhuohan Xie; Daniil Orel; Rushil Thareja; Dhruv Sahnan; Hachem Madmoun; Fan Zhang; Debopriyo Banerjee; Georgi Georgiev; Xueqing Peng; Lingfei Qian; Jimin Huang; Jinyan Su; Aaryamonvikram Singh; Rui Xing; Rania Elbadry; Chen Xu; Haonan Li; Fajri Koto; Ivan Koychev; Tanmoy Chakraborty; Yuxia Wang; Salem Lahlou; Veselin Stoyanov; Sophia Ananiadou; Preslav Nakov
>
> **备注:** 24 pages, includes 12 figures and 9 tables; introduces the FinChain benchmark and ChainEval metric
>
> **摘要:** Multi-step symbolic reasoning is essential for robust financial analysis; yet, current benchmarks largely overlook this capability. Existing datasets such as FinQA and ConvFinQA emphasize final numerical answers while neglecting the intermediate reasoning required for transparency and verification. To address this gap, we introduce FINCHAIN, the first benchmark specifically designed for verifiable Chain-of-Thought (CoT) evaluation in finance. FINCHAIN spans 58 topics across 12 financial domains, each represented by parameterized symbolic templates with executable Python traces that enable fully machine-verifiable reasoning and scalable, contamination-free data generation. To assess reasoning capacity, we propose CHAINEVAL, a dynamic alignment measure that jointly evaluates both the final-answer correctness and the step-level reasoning consistency. Our evaluation of 26 leading LLMs reveals that even frontier proprietary LLMs exhibit clear limitations in symbolic financial reasoning, while domain-adapted and math-enhanced fine-tuned models can substantially narrow this gap. Overall, FINCHAIN exposes persistent weaknesses in multi-step financial reasoning and provides a foundation for developing trustworthy, interpretable, and verifiable financial AI.
>
---
#### [replaced 026] PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于虚假新闻检测任务，旨在提升零样本下的谣言识别能力。通过引入说服知识，提出PCoT方法，显著提高检测效果。**

- **链接: [https://arxiv.org/pdf/2506.06842v2](https://arxiv.org/pdf/2506.06842v2)**

> **作者:** Arkadiusz Modzelewski; Witold Sosnowski; Tiziano Labruna; Adam Wierzbicki; Giovanni Da San Martino
>
> **备注:** Accepted to ACL 2025 Main Conference
>
> **摘要:** Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models' knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection.
>
---
#### [replaced 027] SciClaims: An End-to-End Generative System for Biomedical Claim Analysis
- **分类: cs.CL; cs.AI; cs.DL**

- **简介: 该论文提出SciClaims，用于生物医学领域科学声明的分析任务。解决自动化提取和验证科学声明的问题，通过集成大语言模型实现端到端分析。**

- **链接: [https://arxiv.org/pdf/2503.18526v2](https://arxiv.org/pdf/2503.18526v2)**

> **作者:** Raúl Ortega; José Manuel Gómez-Pérez
>
> **备注:** In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations
>
> **摘要:** We present SciClaims, an interactive web-based system for end-to-end scientific claim analysis in the biomedical domain. Designed for high-stakes use cases such as systematic literature reviews and patent validation, SciClaims extracts claims from text, retrieves relevant evidence from PubMed, and verifies their veracity. The system features a user-friendly interface where users can input scientific text and view extracted claims, predictions, supporting or refuting evidence, and justifications in natural language. Unlike prior approaches, SciClaims seamlessly integrates the entire scientific claim analysis process using a single large language model, without requiring additional fine-tuning. SciClaims is optimized to run efficiently on a single GPU and is publicly available for live interaction.
>
---
#### [replaced 028] Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques
- **分类: cs.SE; cs.AI; cs.CL**

- **简介: 论文探讨LLM在软件质量保证中的应用，结合国际标准提升SQA效率与合规性。任务是融合AI技术与传统标准，解决自动化与合规难题。工作包括分析LLM技术、应用场景及与标准的映射。**

- **链接: [https://arxiv.org/pdf/2505.13766v2](https://arxiv.org/pdf/2505.13766v2)**

> **作者:** Avinash Patil
>
> **备注:** 16 pages, 1 Table, 6 Figures
>
> **摘要:** Software Quality Assurance (SQA) is critical for delivering reliable, secure, and efficient software products. The Software Quality Assurance Process aims to provide assurance that work products and processes comply with predefined provisions and plans. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance existing SQA processes by automating tasks like requirement analysis, code review, test generation, and compliance checks. Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010, ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured frameworks for ensuring robust quality practices. This paper surveys the intersection of LLM-based SQA methods and these recognized standards, highlighting how AI-driven solutions can augment traditional approaches while maintaining compliance and process maturity. We first review the foundational software quality standards and the technical fundamentals of LLMs in software engineering. Next, we explore various LLM-based SQA applications, including requirement validation, defect detection, test generation, and documentation maintenance. We then map these applications to key software quality frameworks, illustrating how LLMs can address specific requirements and metrics within each standard. Empirical case studies and open-source initiatives demonstrate the practical viability of these methods. At the same time, discussions on challenges (e.g., data privacy, model bias, explainability) underscore the need for deliberate governance and auditing. Finally, we propose future directions encompassing adaptive learning, privacy-focused deployments, multimodal analysis, and evolving standards for AI-driven software quality.
>
---
#### [replaced 029] TabularMath: Understanding Math Reasoning over Tables with Large Language Models
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于表格数学推理任务，旨在解决真实场景中表格数据的数学推理问题。提出AutoT2T框架和TabularMath基准，分析表格复杂度、质量及模态对模型的影响。**

- **链接: [https://arxiv.org/pdf/2505.19563v3](https://arxiv.org/pdf/2505.19563v3)**

> **作者:** Shi-Yu Tian; Zhi Zhou; Wei Dong; Kun-Yang Yu; Ming Yang; Zi-Jian Cheng; Lan-Zhe Guo; Yu-Feng Li
>
> **备注:** Paper under review, code and dataset are all available
>
> **摘要:** Mathematical reasoning has long been a key benchmark for evaluating large language models. Although substantial progress has been made on math word problems, the need for reasoning over tabular data in real-world applications has been overlooked. For instance, applications such as business intelligence demand not only multi-step numerical reasoning with tables but also robustness to incomplete or inconsistent information. However, comprehensive evaluation in this area is severely limited, constrained by the reliance on manually collected tables that are difficult to scale and the lack of coverage for potential traps encountered in real-world scenarios. To address this problem, we propose AutoT2T, a neuro-symbolic framework that controllably transforms math word problems into scalable and verified tabular reasoning tasks. Building on this pipeline, we develop TabularMath, a benchmark comprising four subsets that include both text-based and image-based tables, covering table complexity, table quality, and table representation dimensions. Our study reveals three key observations: (1) Table complexity and reasoning difficulty impact reasoning performance jointly; (2) Low-quality tables pose severe risks to reliable reasoning in current LLMs; (3) Different table modalities show similar trends, with text-based tables typically being easier for models to reason over. In-depth analyses are conducted for each observation to guide future research.
>
---
#### [replaced 030] SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation
- **分类: cs.CL; cs.AI; cs.IR**

- **简介: 该论文提出SurGE，一个用于科学综述生成的基准和评估框架，解决自动化生成科学综述的难题。通过构建数据集和评估体系，评估大语言模型在该任务上的表现。**

- **链接: [https://arxiv.org/pdf/2508.15658v3](https://arxiv.org/pdf/2508.15658v3)**

> **作者:** Weihang Su; Anzhe Xie; Qingyao Ai; Jianming Long; Jiaxin Mao; Ziyi Ye; Yiqun Liu
>
> **摘要:** The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible. While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers. In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE
>
---
#### [replaced 031] Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation
- **分类: cs.CL**

- **简介: 该论文属于事实核查任务，旨在解决RAG系统中的幻觉问题。通过引入FRANQ方法，区分事实性和证据忠实性，提升对RAG输出中事实错误的检测准确性。**

- **链接: [https://arxiv.org/pdf/2505.21072v3](https://arxiv.org/pdf/2505.21072v3)**

> **作者:** Ekaterina Fadeeva; Aleksandr Rubashevskii; Dzianis Piatrashyn; Roman Vashurin; Shehzaad Dhuliawala; Artem Shelmanov; Timothy Baldwin; Preslav Nakov; Mrinmaya Sachan; Maxim Panov
>
> **摘要:** Large Language Models (LLMs) enhanced with retrieval, an approach known as Retrieval-Augmented Generation (RAG), have achieved strong performance in open-domain question answering. However, RAG remains prone to hallucinations: factually incorrect outputs may arise from inaccuracies in the model's internal knowledge and the retrieved context. Existing approaches to mitigating hallucinations often conflate factuality with faithfulness to the retrieved evidence, incorrectly labeling factually correct statements as hallucinations if they are not explicitly supported by the retrieval. In this paper, we introduce FRANQ, a new method for hallucination detection in RAG outputs. FRANQ applies distinct uncertainty quantification (UQ) techniques to estimate factuality, conditioning on whether a statement is faithful to the retrieved context. To evaluate FRANQ and competing UQ methods, we construct a new long-form question answering dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging cases. Extensive experiments across multiple datasets, tasks, and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing approaches.
>
---
#### [replaced 032] Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact
- **分类: cs.CL; cs.AI**

- **简介: 该论文研究指令微调中上下文的作用，探讨有无上下文对模型行为和下游任务的影响，提出分模型部署策略以提升性能。**

- **链接: [https://arxiv.org/pdf/2506.15480v2](https://arxiv.org/pdf/2506.15480v2)**

> **作者:** Hyunji Lee; Seunghyun Yoon; Yunjae Won; Hanseok Oh; Geewook Kim; Trung Bui; Franck Dernoncourt; Elias Stengel-Eskin; Mohit Bansal; Minjoon Seo
>
> **摘要:** Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths.
>
---
#### [replaced 033] Can Confidence Estimates Decide When Chain-of-Thought Is Necessary for LLMs?
- **分类: cs.CL**

- **简介: 该论文属于模型优化任务，旨在解决何时使用链式思维（CoT）的问题。通过信心估计决定是否调用CoT，以减少不必要的计算。**

- **链接: [https://arxiv.org/pdf/2510.21007v3](https://arxiv.org/pdf/2510.21007v3)**

> **作者:** Samuel Lewis-Lim; Xingwei Tan; Zhixue Zhao; Nikolaos Aletras
>
> **备注:** Under Review
>
> **摘要:** Chain-of-thought (CoT) prompting is a common technique for improving the reasoning abilities of large language models (LLMs). However, extended reasoning is often unnecessary and substantially increases token usage. As such, a key question becomes how to optimally allocate compute to when reasoning is actually needed. We study this through confidence-gated CoT, where a model produces a direct answer and a confidence estimate to decide whether to invoke CoT. We present an evaluation framework together with the first systematic study of confidence signals for this decision. We evaluate four representative confidence measures and compare them with random gating and an oracle upper bound. Experiments across two model families and diverse reasoning tasks show that existing training-free confidence measures can reduce redundant reasoning. However, we also find that the utility of individual confidence measures is inconsistent across settings. Through our evaluation framework and analysis, our study provides practical guidance toward developing and evaluating models that selectively use CoT.
>
---
#### [replaced 034] On the Diagram of Thought
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文提出Diagram of Thought（DoT）框架，用于提升大语言模型的结构化推理能力，解决复杂问题时逻辑不清晰的问题。通过构建动态思维图谱，实现自我批判与结论合成。**

- **链接: [https://arxiv.org/pdf/2409.10038v5](https://arxiv.org/pdf/2409.10038v5)**

> **作者:** Yifan Zhang; Yang Yuan; Andrew Chi-Chih Yao
>
> **备注:** 30 pages
>
> **摘要:** Large Language Models (LLMs) excel at many tasks but often falter on complex problems that require structured, multi-step reasoning. We introduce the Diagram of Thought (DoT), a new framework that enables a single LLM to build and navigate a mental map of its reasoning. Instead of thinking in a straight line, the model constructs a dynamic diagram of ideas, where it can propose different lines of thought, critique its own steps, and synthesize validated insights into a final conclusion. This entire process is self-contained within the model, making it highly efficient by avoiding the complex external controllers or search algorithms required by other methods. To ensure the reliability of this process, we ground DoT in a rigorous mathematical framework from category theory. This foundation guarantees that the way the model combines information is logical, consistent, and robust, regardless of the order in which ideas were explored. The result is a more powerful and transparent reasoning process that produces a fully auditable, step-by-step trace of the LLM's thinking, bridging the gap between fluent language and formal reasoning.
>
---
#### [replaced 035] From Policy to Logic for Efficient and Interpretable Coverage Assessment
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于法律政策分析任务，旨在解决LLM在医疗覆盖政策审查中的可靠性问题。通过结合检索与符号推理，提升解释性与效率，降低模型成本。**

- **链接: [https://arxiv.org/pdf/2601.01266v2](https://arxiv.org/pdf/2601.01266v2)**

> **作者:** Rhitabrat Pokharel; Hamid Reza Hassanzadeh; Ameeta Agrawal
>
> **备注:** Accepted at AIMedHealth @ AAAI 2026
>
> **摘要:** Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.
>
---
#### [replaced 036] Evaluating the Pre-Consultation Ability of LLMs using Diagnostic Guidelines
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于医疗对话评估任务，旨在解决LLMs在预诊能力上的评价问题。通过构建基准数据集EPAG，对比诊断指南并验证模型表现。**

- **链接: [https://arxiv.org/pdf/2601.03627v2](https://arxiv.org/pdf/2601.03627v2)**

> **作者:** Jean Seo; Gibaeg Kim; Kihun Shin; Seungseop Lim; Hyunkyung Lee; Wooseok Han; Jongwon Lee; Eunho Yang
>
> **备注:** EACL 2026 Industry
>
> **摘要:** We introduce EPAG, a benchmark dataset and framework designed for Evaluating the Pre-consultation Ability of LLMs using diagnostic Guidelines. LLMs are evaluated directly through HPI-diagnostic guideline comparison and indirectly through disease diagnosis. In our experiments, we observe that small open-source models fine-tuned with a well-curated, task-specific dataset can outperform frontier LLMs in pre-consultation. Additionally, we find that increased amount of HPI (History of Present Illness) does not necessarily lead to improved diagnostic performance. Further experiments reveal that the language of pre-consultation influences the characteristics of the dialogue. By open-sourcing our dataset and evaluation pipeline on https://github.com/seemdog/EPAG, we aim to contribute to the evaluation and further development of LLM applications in real-world clinical settings.
>
---
#### [replaced 037] Reward Shaping to Mitigate Reward Hacking in RLHF
- **分类: cs.LG; cs.AI; cs.CL**

- **简介: 该论文属于强化学习任务，旨在解决RLHF中的奖励黑客问题。通过分析奖励塑造方法，提出PAR算法提升训练稳定性与效果。**

- **链接: [https://arxiv.org/pdf/2502.18770v4](https://arxiv.org/pdf/2502.18770v4)**

> **作者:** Jiayi Fu; Xuandong Zhao; Chengyuan Yao; Heng Wang; Qi Han; Yanghua Xiao
>
> **摘要:** Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.
>
---
#### [replaced 038] Act-Adaptive Margin: Dynamically Calibrating Reward Models for Subjective Ambiguity
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于强化学习中的奖励建模任务，旨在解决主观任务中因偏好模糊导致的对齐难题。提出AAM方法，通过动态校准偏好边界提升奖励模型性能。**

- **链接: [https://arxiv.org/pdf/2505.23923v2](https://arxiv.org/pdf/2505.23923v2)**

> **作者:** Feiteng Fang; Dingwei Chen; Xiang Huang; Ting-En Lin; Yuchuan Wu; Xiong Liu; Xinge Ye; Ziqiang Liu; Haonan Zhang; Liang Zhu; Hamid Alinejad-Rokny; Min Yang; Yongbin Li
>
> **摘要:** Currently, most reinforcement learning tasks focus on domains like mathematics and programming, where verification is relatively straightforward. However, in subjective tasks such as role-playing, alignment techniques struggle to make progress, primarily because subjective reward modeling using the Bradley-Terry model faces significant challenges when dealing with ambiguous preferences. To improve reward modeling in subjective tasks, this paper proposes AAM (\textbf{\underline{A}}ct-\textbf{\underline{A}}daptive \textbf{\underline{M}}argin), which enhances reward modeling by dynamically calibrating preference margins using the model's internal parameter knowledge. We design two versions of AAM that efficiently generate contextually-appropriate preference gaps without additional human annotation. This approach fundamentally improves how reward models handle subjective rewards by better integrating generative understanding with preference scoring. To validate AAM's effectiveness in subjective reward modeling, we conduct evaluations on RewardBench, JudgeBench, and challenging role-playing tasks. Results show that AAM significantly improves subjective reward modeling performance, enhancing Bradley-Terry reward models by 2.95\% in general tasks and 4.85\% in subjective role-playing tasks. Furthermore, reward models trained with AAM can help downstream alignment tasks achieve better results. Our test results show that applying rewards generated by AAM-Augmented RM to preference learning techniques (e.g., GRPO) achieves state-of-the-art results on CharacterEval and Charm. Code and dataset are available at https://github.com/calubkk/AAM.
>
---
#### [replaced 039] On the robustness of modeling grounded word learning through a child's egocentric input
- **分类: cs.CL**

- **简介: 该论文属于语言习得研究任务，旨在探讨多模态神经网络在有限儿童输入数据下的学习鲁棒性。通过分析多个儿童的数据，验证模型能否泛化并适应个体差异。**

- **链接: [https://arxiv.org/pdf/2507.14749v2](https://arxiv.org/pdf/2507.14749v2)**

> **作者:** Wai Keen Vong; Brenden M. Lake
>
> **摘要:** What insights can machine learning bring to understanding human language acquisition? Large language and multimodal models have achieved remarkable capabilities, but their reliance on massive training datasets creates a fundamental mismatch with children, who succeed in acquiring language from comparatively limited input. To help bridge this gap, researchers have increasingly trained neural networks using data similar in quantity and quality to children's input. Taking this approach to the limit, Vong et al. (2024) showed that a multimodal neural network trained on 61 hours of visual and linguistic input extracted from just one child's developmental experience could acquire word-referent mappings. However, whether this approach's success reflects the idiosyncrasies of a single child's experience, or whether it would show consistent and robust learning patterns across multiple children's experiences was not explored. In this article, we applied automated speech transcription methods to the entirety of the SAYCam dataset, consisting of over 500 hours of video data spread across all three children. Using these automated transcriptions, we generated multi-modal vision-and-language datasets for both training and evaluation, and explored a range of neural network configurations to examine the robustness of simulated word learning. Our findings demonstrate that networks trained on automatically transcribed data from each child can acquire word-referent mappings, generalizing across videos, children, and image domains. These results validate the robustness of multimodal neural networks for grounded word learning, while highlighting the individual differences that emerge in how models learn when trained on each child's developmental experiences.
>
---
#### [replaced 040] TPA: Next Token Probability Attribution for Detecting Hallucinations in RAG
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于检测RAG系统中幻觉的任务，旨在解决现有方法未能全面考虑模型各组件影响的问题。提出TPA方法，通过概率归因分析各组件对生成token的贡献。**

- **链接: [https://arxiv.org/pdf/2512.07515v3](https://arxiv.org/pdf/2512.07515v3)**

> **作者:** Pengqian Lu; Jie Lu; Anjin Liu; Guangquan Zhang
>
> **备注:** Under review
>
> **摘要:** Detecting hallucinations in Retrieval-Augmented Generation remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge stored in FFNs and the retrieved context. However, this perspective is incomplete, failing to account for the impact of other components of the LLM, such as the user query, previously generated tokens, the self token, and the final LayerNorm adjustment. To comprehensively capture the impact of these components on hallucination detection, we propose TPA which mathematically attributes each token's probability to seven distinct sources: Query, RAG Context, Past Token, Self Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the next token. Specifically, we aggregate these attribution scores by Part-of-Speech (POS) tags to quantify the contribution of each model component to the generation of specific linguistic categories within a response. By leveraging these patterns, such as detecting anomalies where Nouns rely heavily on LayerNorm, TPA effectively identifies hallucinated responses. Extensive experiments show that TPA achieves state-of-the-art performance.
>
---
#### [replaced 041] Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction
- **分类: cs.CL; cs.CV**

- **简介: 该论文属于长视频摘要任务，旨在解决关键视觉信息丢失问题。通过提取关键片段并利用轻量模型生成描述，提升摘要质量与效率。**

- **链接: [https://arxiv.org/pdf/2512.11399v2](https://arxiv.org/pdf/2512.11399v2)**

> **作者:** Galann Pennec; Zhengyuan Liu; Nicholas Asher; Philippe Muller; Nancy F. Chen
>
> **摘要:** Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.
>
---
#### [replaced 042] Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning
- **分类: cs.CL**

- **简介: 该论文属于多语言推理任务，旨在解决非英语语言中输入输出不一致和推理能力差的问题。通过引入GRPO算法和两种奖励机制，提升模型的多语言一致性与推理性能。**

- **链接: [https://arxiv.org/pdf/2510.07300v3](https://arxiv.org/pdf/2510.07300v3)**

> **作者:** Xue Zhang; Yunlong Liang; Fandong Meng; Songming Zhang; Kaiyu Huang; Yufeng Chen; Jinan Xu; Jie Zhou
>
> **备注:** 17 pages, 14 tables, 4 figures. Code is available at: https://github.com/XZhang00/M-Thinker
>
> **摘要:** Large Reasoning Models (LRMs) have achieved remarkable performance on complex reasoning tasks by adopting the ``think-then-answer'' paradigm, which enhances both accuracy and interpretability. However, current LRMs exhibit two critical limitations when processing non-English languages: (1) They often struggle to maintain input-output language consistency; (2) They generally perform poorly with wrong reasoning paths and lower answer accuracy compared to English. These limitations significantly compromise the interpretability of reasoning processes and degrade the user experience for non-English speakers, hindering the global deployment of LRMs. To address these limitations, we propose M-Thinker, which is trained by the GRPO algorithm that involves a Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment (CTA) reward. Specifically, the LC reward defines a strict constraint on the language consistency between the input, thought, and answer. Besides, the CTA reward compares the model's non-English reasoning paths with its English reasoning path to transfer its own reasoning capability from English to non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/4B/7B models not only achieve nearly 100% language consistency and superior performance on two multilingual benchmarks (MMATH and PolyMath), but also exhibit excellent generalization on out-of-domain languages.
>
---
#### [replaced 043] AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models
- **分类: cs.CL; cs.LG**

- **简介: 该论文属于模型优化任务，解决LLM在推理时过长且效率低的问题。提出AutoL2S框架，实现必要时的长短推理切换，显著缩短推理长度并保持精度。**

- **链接: [https://arxiv.org/pdf/2505.22662v2](https://arxiv.org/pdf/2505.22662v2)**

> **作者:** Feng Luo; Yu-Neng Chuang; Guanchu Wang; Hoang Anh Duy Le; Shaochen Zhong; Hongyi Liu; Jiayi Yuan; Yang Sui; Vladimir Braverman; Vipin Chaudhary; Xia Hu
>
> **摘要:** Reasoning-capable large language models (LLMs) achieve strong performance on complex tasks but often exhibit overthinking after distillation, generating unnecessarily long chain-of-thought (CoT) reasoning even for simple inputs and incurring high inference cost. However, naively shortening reasoning length can degrade reasoning accuracy, as concise reasoning may be insufficient for certain inputs and lacks explicit supervision. We propose Auto Long-Short Reasoning (AutoL2S), a distillation framework that empowers non-reasoning LLMs to think thoroughly but only when necessary. AutoL2S first learns a lightweight switching token with verified long-short CoTs to enable instance-wise long-short reasoning selection. Then it leverages long-short reasoning rollouts induced by a switching token in a GRPO-style loss to improve reasoning efficiency while maintaining accuracy. Experiments demonstrate that AutoL2S effectively reduces reasoning length up to 71% with minimal accuracy loss, yielding markedly better trade-off in token length and inference time while preserving accuracy.
>
---
#### [replaced 044] Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling
- **分类: cs.CL; cs.LG**

- **简介: 该论文属于内容安全任务，旨在解决现有审核系统准确性低、解释性差的问题。提出Hi-Guard框架，通过分层策略和政策对齐提升分类精度与透明度。**

- **链接: [https://arxiv.org/pdf/2508.03296v2](https://arxiv.org/pdf/2508.03296v2)**

> **作者:** Anqi Li; Wenwei Jin; Jintao Tong; Pengda Qin; Weijia Li; Guo Lu
>
> **备注:** Accepted by KDD 2026. Code is available at https://github.com/lianqi1008/Hi-Guard
>
> **摘要:** Social platforms have revolutionized information sharing, but also accelerated the dissemination of harmful and policy-violating content. To ensure safety and compliance at scale, moderation systems must go beyond efficiency and offer accuracy and interpretability. However, current approaches largely rely on noisy, label-driven learning, lacking alignment with moderation rules and producing opaque decisions that hinder human review. Therefore, we propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that introduces a new policy-aligned decision paradigm. The term "Hierarchical" reflects two key aspects of our system design: (1) a hierarchical moderation pipeline, where a lightweight binary model first filters safe content and a stronger model handles fine-grained risk classification; and (2) a hierarchical taxonomy in the second stage, where the model performs path-based classification over a hierarchical taxonomy ranging from coarse to fine-grained levels. To ensure alignment with evolving moderation policies, Hi-Guard directly incorporates rule definitions into the model prompt. To further enhance structured prediction and reasoning, we introduce a multi-level soft-margin reward and optimize with Group Relative Policy Optimization (GRPO), penalizing semantically adjacent misclassifications and improving explanation quality. Extensive experiments and real-world deployment demonstrate that Hi-Guard achieves superior classification accuracy, generalization, and interpretability, paving the way toward scalable, transparent, and trustworthy content safety systems. Code is available at: https://github.com/lianqi1008/Hi-Guard.
>
---
#### [replaced 045] OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models
- **分类: cs.CL**

- **简介: 该论文属于伦理评估任务，旨在解决开源大语言模型的伦理问题。通过多维度评估29个模型，分析其安全性、公平性等，提出改进方向。**

- **链接: [https://arxiv.org/pdf/2505.16036v2](https://arxiv.org/pdf/2505.16036v2)**

> **作者:** Yıldırım Özen; Burak Erinç Çetin; Kaan Engür; Elif Naz Demiryılmaz; Cagri Toraman
>
> **摘要:** Generative large language models present significant potential but also raise critical ethical concerns, including issues of safety, fairness, robustness, and reliability. Most existing ethical studies, however, are limited by their narrow focus, a lack of language diversity, and an evaluation of a restricted set of models. To address these gaps, we present a broad ethical evaluation of 29 recent open-source LLMs using a novel dataset that assesses four key ethical dimensions: robustness, reliability, safety, and fairness. Our analysis includes both a high-resource language, English, and a low-resource language, Turkish, providing a comprehensive assessment and a guide for safer model development. Using an LLM-as-a-Judge methodology, our experimental results indicate that many open-source models demonstrate strong performance in safety, fairness, and robustness, while reliability remains a key concern. Ethical evaluation shows cross-linguistic consistency, and larger models generally exhibit better ethical performance. We also show that jailbreak templates are ineffective for most of the open-source models examined in this study. We share all materials including data and scripts at https://github.com/metunlp/openethics
>
---
#### [replaced 046] NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents
- **分类: cs.CL**

- **简介: 该论文提出NL2Repo-Bench，用于评估编码代理在长周期内生成完整代码库的能力。解决现有基准无法衡量长期任务的问题，通过自然语言需求生成可安装的Python库。**

- **链接: [https://arxiv.org/pdf/2512.12730v2](https://arxiv.org/pdf/2512.12730v2)**

> **作者:** Jingzhe Ding; Shengda Long; Changxin Pu; Huan Zhou; Hongwan Gao; Xiang Gao; Chao He; Yue Hou; Fei Hu; Zhaojian Li; Weiran Shi; Zaiyuan Wang; Daoguang Zan; Chenchen Zhang; Xiaoxu Zhang; Qizhi Chen; Xianfu Cheng; Bo Deng; Qingshui Gu; Kai Hua; Juntao Lin; Pai Liu; Mingchen Li; Xuanguang Pan; Zifan Peng; Yujia Qin; Yong Shan; Zhewen Tan; Weihao Xie; Zihan Wang; Yishuo Yuan; Jiayu Zhang; Enduo Zhao; Yunfei Zhao; He Zhu; Liya Zhu; Chenyang Zou; Ming Ding; Jianpeng Jiao; Jiaheng Liu; Minghao Liu; Qian Liu; Chongyang Tao; Jian Yang; Tong Yang; Zhaoxiang Zhang; Xinjie Chen; Wenhao Huang; Ge Zhang
>
> **摘要:** Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.
>
---
#### [replaced 047] ChakmaNMT: Machine Translation for a Low-Resource and Endangered Language via Transliteration
- **分类: cs.CL**

- **简介: 该论文属于机器翻译任务，旨在解决濒危语言Chakma的低资源问题。通过引入平行语料和 transliteration 方法，提升翻译效果。**

- **链接: [https://arxiv.org/pdf/2410.10219v2](https://arxiv.org/pdf/2410.10219v2)**

> **作者:** Aunabil Chakma; Aditya Chakma; Masum Hasan; Soham Khisa; Chumui Tripura; Rifat Shahriyar
>
> **备注:** Submitted to ARR (January 2026)
>
> **摘要:** We present the first systematic study of machine translation for Chakma, an endangered and extremely low-resource Indo-Aryan language, with the goal of supporting language access and preservation. We introduce a new Chakma-Bangla parallel and monolingual dataset, along with a trilingual Chakma-Bangla-English benchmark for evaluation. To address script mismatch and data scarcity, we propose a character-level transliteration framework that exploits the close orthographic and phonological relationship between Chakma and Bangla, preserving semantic content while enabling effective transfer from Bangla and multilingual pretrained models. We benchmark from-scratch MT, fine-tuned pretrained models, and large language models via in-context learning. Results show that transliteration is essential and that fine-tuning and in-context learning substantially outperform from-scratch baselines, with strong asymmetry across translation directions.
>
---
#### [replaced 048] Proverbs or Pythian Oracles? Sentiments and Emotions in Greek Sayings
- **分类: cs.CL**

- **简介: 该论文属于情感分析任务，旨在研究希腊谚语中的情绪与情感。通过构建多标签标注框架，分析谚语情感分布，揭示其多维特性。**

- **链接: [https://arxiv.org/pdf/2510.13341v3](https://arxiv.org/pdf/2510.13341v3)**

> **作者:** Katerina Korre; John Pavlopoulos
>
> **摘要:** Proverbs are among the most fascinating language phenomena that transcend cultural and linguistic boundaries. Yet, much of the global landscape of proverbs remains underexplored, as many cultures preserve their traditional wisdom within their own communities due to the oral tradition of the phenomenon. Taking advantage of the current advances in Natural Language Processing (NLP), we focus on Greek proverbs, analyzing their sentiment and emotion. Departing from an annotated dataset of Greek proverbs, (1) we propose a multi-label annotation framework and dataset that captures the emotional variability of the proverbs, (2) we up-scale to local varieties, (3) we sketch a map of Greece that provides an overview of the distribution of emotions. Our findings show that the interpretation of proverbs is multidimensional, a property manifested through both multi-labeling and instance-level polarity. LLMs can capture and reproduce this complexity, and can therefore help us better understand the proverbial landscape of a place, as in the case of Greece, where surprise and anger compete and coexist within proverbs.
>
---
#### [replaced 049] POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering
- **分类: cs.CL; cs.AI; cs.CV; cs.MM**

- **简介: 该论文属于多语言图表问答任务，旨在解决现有基准英语主导的问题。通过构建多语言图表问答基准PolyChartQA，提升模型在全球语言环境下的图表理解能力。**

- **链接: [https://arxiv.org/pdf/2507.11939v2](https://arxiv.org/pdf/2507.11939v2)**

> **作者:** Yichen Xu; Liangyu Chen; Liang Zhang; Jianzhe Ma; Wenxuan Wang; Qin Jin
>
> **备注:** Work in Progress
>
> **摘要:** Charts are a universally adopted medium for data communication, yet existing chart understanding benchmarks are overwhelmingly English-centric, limiting their accessibility and relevance to global audiences. To address this limitation, we introduce PolyChartQA, the first large-scale multilingual benchmark for chart question answering, comprising 22,606 charts and 26,151 QA pairs across 10 diverse languages. PolyChartQA is constructed through a scalable pipeline that enables efficient multilingual chart generation via data translation and code reuse, supported by LLM-based translation and rigorous quality control. We systematically evaluate multilingual chart understanding with PolyChartQA on state-of-the-art LVLMs and reveal a significant performance gap between English and other languages, particularly low-resource ones. Additionally, we introduce a companion multilingual chart question answering training set, PolyChartQA-Train, on which fine-tuning LVLMs yields substantial gains in multilingual chart understanding across diverse model sizes and architectures. Together, our benchmark provides a foundation for developing globally inclusive vision-language models capable of understanding charts across diverse linguistic contexts.
>
---
#### [replaced 050] Current Agents Fail to Leverage World Model as Tool for Foresight
- **分类: cs.AI; cs.CL; cs.LG**

- **简介: 论文研究当前智能体无法有效利用世界模型进行预测，属于增强智能体预见能力的任务。它指出智能体在决策、解读和整合预测结果上存在瓶颈，需改进与世界模型的交互机制。**

- **链接: [https://arxiv.org/pdf/2601.03905v2](https://arxiv.org/pdf/2601.03905v2)**

> **作者:** Cheng Qian; Emre Can Acikgoz; Bingxuan Li; Xiusi Chen; Yuji Zhang; Bingxiang He; Qinyu Luo; Dilek Hakkani-Tür; Gokhan Tur; Yunzhu Li; Heng Ji
>
> **备注:** 36 Pages, 13 Figures, 17 Tables (Meta data updated)
>
> **摘要:** Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.
>
---
#### [replaced 051] Disentangling Learning from Judgment: Representation Learning for Open Response Analytics
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于学习分析任务，旨在解决自动评分中内容与评分标准混淆的问题。通过分离内容信号与评分者倾向，提升评分的透明度与准确性。**

- **链接: [https://arxiv.org/pdf/2512.23941v2](https://arxiv.org/pdf/2512.23941v2)**

> **作者:** Conrad Borchers; Manit Patel; Seiyon M. Lee; Anthony F. Botelho
>
> **备注:** Short research paper accepted at Learning Analytics and Knowledge (LAK '26)
>
> **摘要:** Open-ended responses are central to learning, yet automated scoring often conflates what students wrote with how teachers grade. We present an analytics-first framework that separates content signals from rater tendencies, making judgments visible and auditable via analytics. Using de-identified ASSISTments mathematics responses, we model teacher histories as dynamic priors and represent text with sentence embeddings. We apply centroid normalization and response-problem embedding differences, and explicitly model teacher effects with priors to reduce problem- and teacher-related confounds. Temporally-validated linear models quantify the contributions of each signal, and model disagreements surface observations for qualitative inspection. Results show that teacher priors heavily influence grade predictions; the strongest results arise when priors are combined with content embeddings (AUC~0.815), while content-only models remain above chance but substantially weaker (AUC~0.626). Adjusting for rater effects sharpens the selection of features derived from content representations, retaining more informative embedding dimensions and revealing cases where semantic evidence supports understanding as opposed to surface-level differences in how students respond. The contribution presents a practical pipeline that transforms embeddings from mere features into learning analytics for reflection, enabling teachers and researchers to examine where grading practices align (or conflict) with evidence of student reasoning and learning.
>
---
#### [replaced 052] Low-rank variational dropout: Rank selection and uncertainty in adapters
- **分类: cs.LG; cs.AI; cs.CL**

- **简介: 该论文属于模型压缩任务，解决低秩适配中不确定性估计与容量控制问题。提出LRVD框架，实现自动秩选择和预测校准。**

- **链接: [https://arxiv.org/pdf/2506.22809v3](https://arxiv.org/pdf/2506.22809v3)**

> **作者:** Cooper Doyle; Rebecca Chan; Andy Hu; Anna Leontjeva
>
> **备注:** 8 pages, 3 figures, 2 tables
>
> **摘要:** Low-rank adaptation methods enable efficient task-specific updates in large neural networks, but provide no principled mechanism for uncertainty estimation or capacity control. We introduce Low-Rank Variational Dropout (LRVD), a Bayesian framework that operates directly in the space of low-rank adaptation. LRVD employs a scale-invariant, sparsity-inducing prior together with a structured variational family that ties uncertainty at the level of latent rank components, inducing rank-wise noise-to-signal ratios for automatic capacity selection. As a concrete instantiation, we apply LRVD to low-rank adaptation and obtain BayesLoRA, which jointly learns predictive uncertainty and the effective adapter rank with only O(r) additional parameters, where r is the adapter rank. We empirically show that BayesLoRA induces stable, non-arbitrary rank structure aligned with the intrinsic singular directions of the learned updates, and outperforms existing low-rank sparsification methods in accuracy at comparable training cost while delivering substantially improved predictive calibration at negligible additional overhead.
>
---
#### [replaced 053] Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects
- **分类: cs.CL; cs.AI; cs.IT**

- **简介: 该论文研究语言模型中意外性（surprisal）与隐喻新颖性的关系，属于自然语言处理任务。旨在探讨surprisal是否能反映隐喻新颖性，通过分析不同数据集验证其相关性及模型规模影响。**

- **链接: [https://arxiv.org/pdf/2601.02015v2](https://arxiv.org/pdf/2601.02015v2)**

> **作者:** Omar Momen; Emilie Sitter; Berenike Herrmann; Sina Zarrieß
>
> **备注:** to be published at EACL 2026 main conference
>
> **摘要:** Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.
>
---
#### [replaced 054] Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models
- **分类: cs.CL**

- **简介: 该论文属于大语言模型中的幻觉检测任务，旨在解决检测方法在事实与逻辑不一致上的局限性。通过统一内部状态与推理一致性，提出新框架提升检测效果。**

- **链接: [https://arxiv.org/pdf/2510.11529v2](https://arxiv.org/pdf/2510.11529v2)**

> **作者:** Yusheng Song; Lirong Qiu; Xi Zhang; Zhihao Tang
>
> **摘要:** The detection of sophisticated hallucinations in Large Language Models (LLMs) is hampered by a ``Detection Dilemma'': methods probing internal states (Internal State Probing) excel at identifying factual inconsistencies but fail on logical fallacies, while those verifying externalized reasoning (Chain-of-Thought Verification) show the opposite behavior. This schism creates a task-dependent blind spot: Chain-of-Thought Verification fails on fact-intensive tasks like open-domain QA where reasoning is ungrounded, while Internal State Probing is ineffective on logic-intensive tasks like mathematical reasoning where models are confidently wrong. We resolve this with a unified framework that bridges this critical gap. However, unification is hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse symbolic reasoning chains lack signals directly comparable to fine-grained internal states, and the Representational Alignment Barrier, a deep-seated mismatch between their underlying semantic spaces. To overcome these, we introduce a multi-path reasoning mechanism to obtain more comparable, fine-grained signals, and a segment-aware temporalized cross-attention module to adaptively fuse these now-aligned representations, pinpointing subtle dissonances. Extensive experiments on three diverse benchmarks and two leading LLMs demonstrate that our framework consistently and significantly outperforms strong baselines. Our code is available: https://github.com/peach918/HalluDet.
>
---
#### [replaced 055] What Should Embeddings Embed? Autoregressive Models Represent Latent Generating Distributions
- **分类: cs.LG; cs.AI; cs.CL; stat.ML**

- **简介: 该论文属于自然语言处理领域，探讨嵌入应表示什么。解决嵌入语义定义问题，提出三种场景下嵌入应表示的潜在分布，并通过实验验证。**

- **链接: [https://arxiv.org/pdf/2406.03707v2](https://arxiv.org/pdf/2406.03707v2)**

> **作者:** Liyi Zhang; Michael Y. Li; R. Thomas McCoy; Theodore R. Sumers; Jian-Qiao Zhu; Thomas L. Griffiths
>
> **备注:** 28 pages, 11 figures
>
> **摘要:** Autoregressive language models have demonstrated a remarkable ability to extract latent structure from text. The embeddings from large language models have been shown to capture aspects of the syntax and semantics of language. But what should embeddings represent? We connect the autoregressive prediction objective to the idea of constructing predictive sufficient statistics to summarize the information contained in a sequence of observations, and use this connection to identify three settings where the optimal content of embeddings can be identified: independent identically distributed data, where the embedding should capture the sufficient statistics of the data; latent state models, where the embedding should encode the posterior distribution over states given the data; and discrete hypothesis spaces, where the embedding should reflect the posterior distribution over hypotheses given the data. We then conduct empirical probing studies to show that transformers encode these three kinds of latent generating distributions, and that they perform well in out-of-distribution cases and without token memorization in these settings.
>
---
#### [replaced 056] Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for Mental Health Prediction via Online Text
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于心理健康文本分类任务，旨在提升模型对在线文本中心理状态的预测准确性。通过引入推理增强方法（如CoT、SC-CoT、ToT），提高分类性能，并分析不同提示策略的效果。**

- **链接: [https://arxiv.org/pdf/2503.10095v3](https://arxiv.org/pdf/2503.10095v3)**

> **作者:** Avinash Patil; Amardeep Kour Gedhu
>
> **备注:** 8 pages, 4 Figures, 3 tables
>
> **摘要:** Large Language Models (LLMs) have demonstrated potential in predicting mental health outcomes from online text, yet traditional classification methods often lack interpretability and robustness. This study evaluates structured reasoning techniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and Tree-of-Thought (ToT)-to improve classification accuracy across multiple mental health datasets sourced from Reddit. We analyze reasoning-driven prompting strategies, including Zero-shot CoT and Few-shot CoT, using key performance metrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Our findings indicate that reasoning-enhanced techniques improve classification performance over direct prediction, particularly in complex cases. Compared to baselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained transformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs such as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable gains on datasets like Dreaddit (+0.52\% over M-LLM, +0.82\% over BERT) and SDCNL (+4.67\% over M-LLM, +2.17\% over BERT). However, performance declines in Depression Severity, and CSSRS predictions suggest dataset-specific limitations, likely due to our using a more extensive test set. Among prompting strategies, Few-shot CoT consistently outperforms others, reinforcing the effectiveness of reasoning-driven LLMs. Nonetheless, dataset variability highlights challenges in model reliability and interpretability. This study provides a comprehensive benchmark of reasoning-based LLM techniques for mental health text classification. It offers insights into their potential for scalable clinical applications while identifying key challenges for future improvements.
>
---
#### [replaced 057] Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation
- **分类: cs.SI; cs.CL**

- **简介: 该论文属于健康信息治理任务，针对社区注释系统响应慢、准确性低的问题，提出CrowdNotes+框架，利用LLM提升 misinformation 治理效率与可靠性。**

- **链接: [https://arxiv.org/pdf/2510.11423v2](https://arxiv.org/pdf/2510.11423v2)**

> **作者:** Jiaying Wu; Zihang Fu; Haonan Wang; Fanxiao Li; Jiafeng Guo; Preslav Nakov; Min-Yen Kan
>
> **摘要:** Community Notes, the crowd-sourced misinformation governance system on X (formerly Twitter), allows users to flag misleading posts, attach contextual notes, and rate the notes' helpfulness. However, our empirical analysis of 30.8K health-related notes reveals substantial latency, with a median delay of 17.6 hours before notes receive a helpfulness status. To improve responsiveness during real-world misinformation surges, we propose CrowdNotes+, a unified LLM-based framework that augments Community Notes for faster and more reliable health misinformation governance. CrowdNotes+ integrates two modes: (1) evidence-grounded note augmentation and (2) utility-guided note automation, supported by a hierarchical three-stage evaluation of relevance, correctness, and helpfulness. We instantiate the framework with HealthNotes, a benchmark of 1.2K health notes annotated for helpfulness, and a fine-tuned helpfulness judge. Our analysis first uncovers a key loophole in current crowd-sourced governance: voters frequently conflate stylistic fluency with factual accuracy. Addressing this via our hierarchical evaluation, experiments across 15 representative LLMs demonstrate that CrowdNotes+ significantly outperforms human contributors in note correctness, helpfulness, and evidence utility.
>
---
#### [replaced 058] VotIE: Information Extraction from Meeting Minutes
- **分类: cs.CL**

- **简介: 该论文提出VotIE任务，旨在从市政会议记录中提取结构化投票信息。解决非标准化文本中的信息抽取问题，通过构建基准数据集并对比不同模型效果。**

- **链接: [https://arxiv.org/pdf/2601.03997v2](https://arxiv.org/pdf/2601.03997v2)**

> **作者:** José Pedro Evans; Luís Filipe Cunha; Purificação Silvano; Alípio Jorge; Nuno Guimarães; Sérgio Nunes; Ricardo Campos
>
> **摘要:** Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.
>
---
#### [replaced 059] Internal Reasoning vs. External Control: A Thermodynamic Analysis of Sycophancy in Large Language Models
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于自然语言处理任务，解决大模型中的谄媚问题。通过分析推理过程而非结果，提出RCA方法检测并减少谄媚行为。**

- **链接: [https://arxiv.org/pdf/2601.03263v2](https://arxiv.org/pdf/2601.03263v2)**

> **作者:** Edward Y. Chang
>
> **备注:** 20 pages, 1 figure, 15 tables
>
> **摘要:** Large Language Models exhibit sycophancy: prioritizing agreeableness over correctness. Current remedies evaluate reasoning outcomes: RLHF rewards correct answers, self-correction critiques outputs. All require ground truth, which is often unavailable at inference time and vulnerable to the same biases. We explore evaluating the reasoning process instead. Regulated Causal Anchoring (RCA) verifies whether outputs follow from their reasoning traces, without requiring ground truth. Sycophancy manifests as trace-output inconsistency: models derive one answer but output another to please users. RCA detects this inconsistency, achieving 0.0% sycophancy while accepting 88% of valid hints. We identify two failures invisible to outcome evaluation: Inverse Scaling (frontier models sycophant more because rationalization requires capability) and the Final Output Gap (correct reasoning precedes sycophantic output). Traditional self-correction reduces these failures to 7-9% but cannot eliminate them because the model critiques itself with the same biases. RCA's process evaluation operates at inference time, requires no ground truth, and uses an independent judge that breaks the self-reinforcing bias loop: three properties that outcome evaluation lacks.
>
---
#### [replaced 060] Qomhra: A Bilingual Irish and English Large Language Model
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决低资源语言如爱尔兰语在大语言模型中的不足。通过构建双语模型Qomhrá，提升爱尔兰语和英语的生成能力。**

- **链接: [https://arxiv.org/pdf/2510.17652v4](https://arxiv.org/pdf/2510.17652v4)**

> **作者:** Joseph McInerney; Khanh-Tung Tran; Liam Lonergan; Ailbhe Ní Chasaide; Neasa Ní Chiaráin; Barry Devereux
>
> **摘要:** Large language model (LLM) research and development has overwhelmingly focused on the world's major languages, leading to under-representation of low-resource languages such as Irish. This paper introduces \textbf{Qomhrá}, a bilingual Irish and English LLM, developed under extremely low-resource constraints. A complete pipeline is outlined spanning bilingual continued pre-training, instruction tuning, and the synthesis of human preference data for future alignment training. We focus on the lack of scalable methods to create human preference data by proposing a novel method to synthesise such data by prompting an LLM to generate ``accepted'' and ``rejected'' responses, which we validate as aligning with L1 Irish speakers. To select an LLM for synthesis, we evaluate the top closed-weight LLMs for Irish language generation performance. Gemini-2.5-Pro is ranked highest by L1 and L2 Irish-speakers, diverging from LLM-as-a-judge ratings, indicating a misalignment between current LLMs and the Irish-language community. Subsequently, we leverage Gemini-2.5-Pro to translate a large scale English-language instruction tuning dataset to Irish and to synthesise a first-of-its-kind Irish-language human preference dataset. We comprehensively evaluate Qomhrá across several benchmarks, testing translation, gender understanding, topic identification, and world knowledge; these evaluations show gains of up to 29\% in Irish and 44\% in English compared to the existing open-source Irish LLM baseline, UCCIX. The results of our framework provide insight and guidance to developing LLMs for both Irish and other low-resource languages.
>
---
#### [replaced 061] Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning
- **分类: cs.CL; cs.AI**

- **简介: 该论文研究多任务和多语言下的模型迁移效果，属于自然语言处理领域。通过LoRA微调，分析不同任务和语言间的知识转移，揭示了迁移的不对称性与语言资源的影响。**

- **链接: [https://arxiv.org/pdf/2511.13368v2](https://arxiv.org/pdf/2511.13368v2)**

> **作者:** Kajetan Dymkiewicz; Ivan Vulic; Helen Yannakoudakis; Eilam Shapira; Roi Reichart; Anna Korhonen
>
> **摘要:** Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages remains poorly understood. We conduct a controlled LoRA fine-tuning study across multiple open-weight LLM families and scales, using a standardised grid of 11 languages and four benchmarks. We fine-tune each model on a single task-language source and measure transfer when evaluated on all other task-language target pairs. We decompose transfer into three regimes: (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language). Single-source fine-tuning yields a net positive uplift across regimes, but the gains are strongly asymmetric. Matched-Task (Cross-Language) transfer emerges as the most effective and predictable regime, driven principally by the identity of the target language rather than model architecture. We identify a stable hierarchy where high-resource languages and broad semantic tasks act as efficient recipients that absorb gains from diverse sources, while specialised tasks and lower-resource languages are more isolated. These results imply that effective fine-tuning requires navigating donor-recipient roles to maximise downstream gains.
>
---
#### [replaced 062] InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training
- **分类: cs.CL; cs.AI; cs.CV**

- **简介: 该论文提出InfiniteWeb，解决GUI代理训练环境不足的问题。通过自动生成大规模功能网页环境，提升代理性能。**

- **链接: [https://arxiv.org/pdf/2601.04126v2](https://arxiv.org/pdf/2601.04126v2)**

> **作者:** Ziyun Zhang; Zezhou Wang; Xiaoyi Zhang; Zongyu Guo; Jiahao Li; Bin Li; Yan Lu
>
> **备注:** Work In Progress
>
> **摘要:** GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.
>
---
#### [replaced 063] When Models Outthink Their Safety: Unveiling and Mitigating Self-Jailbreak in Large Reasoning Models
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于AI安全任务，旨在解决大型推理模型中的Self-Jailbreak问题，即模型在推理过程中忽视安全判断生成有害内容。工作提出Chain-of-Guardrail框架，在不损害推理能力的前提下进行步骤级干预以提升安全性。**

- **链接: [https://arxiv.org/pdf/2510.21285v3](https://arxiv.org/pdf/2510.21285v3)**

> **作者:** Yingzhi Mao; Chunkang Zhang; Junxiang Wang; Xinyan Guan; Boxi Cao; Yaojie Lu; Hongyu Lin; Xianpei Han; Le Sun
>
> **备注:** The first two authors contributed equally. The main text is 8 pages, with an appendix of 20 pages. The paper contains 20 figures and 15 tables
>
> **摘要:** Large Reasoning Models (LRMs) achieve strong performance on complex multi-step reasoning, yet they still exhibit severe safety failures such as harmful content generation. Existing methods often apply coarse-grained constraints over the entire reasoning trajectories, which can undermine reasoning capability while failing to address the root causes of unsafe behavior. In this work, we uncover a previously underexplored failure mode in LRMs, termed Self-Jailbreak, where models initially recognize the harmful intent of a query, but override this judgment during subsequent reasoning steps, ultimately generating unsafe outputs. Such a phenomenon reveals that LRMs are capable of recognizing harm, while safety failures primarily arise from reasoning steps. Motivated by this finding, we propose \emph{Chain-of-Guardrail} (CoG), a trajectory-level training framework that mitigates Self-Jailbreak via targeted, step-level interventions while maintaining reasoning ability. Experiments across multiple safety and reasoning benchmarks indicate that CoG achieves a favorable balance between safety and reasoning performance compared with existing approaches.
>
---
#### [replaced 064] Black-Box On-Policy Distillation of Large Language Models
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于大语言模型压缩任务，解决黑盒环境下模型蒸馏问题。提出GAD方法，通过生成对抗训练实现高效蒸馏，提升学生模型性能。**

- **链接: [https://arxiv.org/pdf/2511.10643v3](https://arxiv.org/pdf/2511.10643v3)**

> **作者:** Tianzhu Ye; Li Dong; Zewen Chi; Xun Wu; Shaohan Huang; Furu Wei
>
> **摘要:** Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.
>
---
#### [replaced 065] RadarPLM: Adapting Pre-trained Language Models for Marine Radar Target Detection by Selective Fine-tuning
- **分类: eess.SP; cs.CL**

- **简介: 论文提出RadarPLM框架，用于海洋雷达目标检测。针对低信杂比环境下PLM微调计算成本高、易过拟合的问题，设计轻量模块和偏好损失函数，提升检测性能。**

- **链接: [https://arxiv.org/pdf/2509.12089v5](https://arxiv.org/pdf/2509.12089v5)**

> **作者:** Qiying Hu; Yaowen Li; Shengyi Zhang; Chuan Huang; Yu Liu; You He
>
> **摘要:** Recent advances in pre-trained language models (PLMs) have demonstrated their capabilities in capturing universal knowledge, making them promising for radar signal processing applications. Nevertheless, directly fine-tuning PLMs on radar signals is both computationally expensive and prone to overfitting, particularly in low signal-to-clutter ratio (SCR) environments. In this paper, we propose a fine-tuning framework for PLM-based marine radar target detection. First, we design a lightweight adaptation module, enabling computationally efficient fine-tuning while preserving the pre-trained model's general knowledge. Second, a novel preference-aware loss is developed to selectively optimize different feature patches based on their online-evaluated learning values, guiding the model to concentrate on those generalizable feature patterns during optimization. Finally, a binary classification head is retrained based on autoencoder network to further enhance detection performance. Experiments on real-world radar data show that the proposed RadarPLM framework yields at least a 6.35% improvement in detection performance over the existing networks under low SCR conditions. Especially, in the small-sample training cases, the proposed RadarPLM also achieves a significant advantage over existing networks owing to the incorporation of the PLM.
>
---
#### [replaced 066] Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于知识蒸馏任务，旨在提升小模型性能的同时降低计算成本。通过分析不同部分的监督效果，发现仅对思维链进行蒸馏即可，从而提出序列截断方法，有效减少资源消耗。**

- **链接: [https://arxiv.org/pdf/2512.21002v2](https://arxiv.org/pdf/2512.21002v2)**

> **作者:** Wei-Rui Chen; Vignesh Kothapalli; Ata Fatahibaarzi; Hejian Sang; Shao Tang; Qingquan Song; Zhipeng Wang; Muhammad Abdul-Mageed
>
> **摘要:** Distilling the capabilities from a large reasoning model (LRM) to a smaller student model often involves training on substantial amounts of reasoning data. However, knowledge distillation (KD) over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) sections makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different sections (P, CoT, A) affects student performance. Our analysis shows that selective KD over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that beyond a specific length, longer training sequences provide marginal returns for downstream performance but require substantially higher memory and FLOPs. To this end, training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx91\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. Codes are available at https://github.com/weiruichen01/distilling-the-essence.
>
---
#### [replaced 067] Interleaved Latent Visual Reasoning with Selective Perceptual Modeling
- **分类: cs.CL; cs.CV**

- **简介: 该论文提出ILVR框架，解决多模态大语言模型中视觉反馈计算成本高与感知建模不足的问题，通过交错推理和选择性感知建模提升性能。**

- **链接: [https://arxiv.org/pdf/2512.05665v2](https://arxiv.org/pdf/2512.05665v2)**

> **作者:** Shuai Dong; Siyuan Wang; Xingyu Liu; Chenglin Li; Haowen Hou; Zhongyu Wei
>
> **备注:** 18 pages, 11 figures. Code available at https://github.com/XD111ds/ILVR
>
> **摘要:** Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet faces limitations: methods either fail to capture intermediate state evolution due to single-step, non-interleaved structures, or sacrifice precise perceptual modeling by over-compressing features. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. Specifically, we employ a self-supervision strategy where a momentum teacher model selectively distills relevant features from ground-truth intermediate images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.
>
---
#### [replaced 068] Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于LLM内容安全任务，旨在解决现有方法缺乏灵活控制的问题。提出一种联合训练框架，通过魔法令牌实现安全行为的高效切换，提升部署灵活性与安全性。**

- **链接: [https://arxiv.org/pdf/2508.14904v2](https://arxiv.org/pdf/2508.14904v2)**

> **作者:** Jianfeng Si; Lin Sun; Zhewen Tan; Xiangzheng Zhang
>
> **备注:** 15 pages,3 figures,5 tables
>
> **摘要:** Current methods for content safety in Large Language Models (LLMs), such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), often rely on multi-stage training pipelines and lack fine-grained, post-deployment controllability. To address these limitations, we propose a unified co-training framework that efficiently integrates multiple safety behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and rejective (refusal-oriented/conservative) within a single SFT stage. Notably, each behavior is dynamically activated via a simple system-level instruction, or magic token, enabling stealthy and efficient behavioral switching at inference time. This flexibility supports diverse deployment scenarios, such as positive for safe user interaction, negative for internal red-teaming, and rejective for context-aware refusals triggered by upstream moderation signals. This co-training strategy induces a distinct Safety Alignment Margin in the output space, characterized by well-separated response distributions corresponding to each safety mode. The existence of this margin provides empirical evidence for the model's safety robustness and enables unprecedented fine-grained control. Experiments show that our method matches the safety alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing both training complexity and deployment costs. This work presents a scalable, efficient, and highly controllable solution for LLM content safety.
>
---
#### [replaced 069] Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems
- **分类: cs.CL**

- **简介: 该论文属于医学文本标注任务，旨在评估大语言模型在CT报告中自动标注疾病的效果，解决人工标注效率低的问题。通过对比不同模型，验证了轻量级LLMs的优越性。**

- **链接: [https://arxiv.org/pdf/2506.03259v2](https://arxiv.org/pdf/2506.03259v2)**

> **作者:** Michael E. Garcia-Alcoser; Mobina GhojoghNejad; Fakrul Islam Tushar; David Kim; Kyle J. Lafata; Geoffrey D. Rubin; Joseph Y. Lo
>
> **备注:** 18 pages, 9 figures, to be submitted in Radiology: Artificial Intelligence
>
> **摘要:** Purpose: This study aims to evaluate the effectiveness of large language models (LLMs) in automating disease annotation of CT radiology reports. We compare a rule-based algorithm (RBA), RadBERT, and three lightweight open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP) CT reports. Materials and Methods: This retrospective study analyzed 40,833 chest-abdomen-pelvis (CAP) CT reports from 29,540 patients, with 1,789 reports manually annotated across three organ systems. External validation was conducted using the CT RATE dataset. Three open-weight LLMs were tested with zero-shot prompting. Performance was evaluated using Cohen's Kappa ($κ$) and micro/macro-averaged F1 scores. Results: In the internal test set of 12,197 CAP reports from 8,854 patients, Llama-3.1 8B and Gemma-3 27B showed the highest agreement ($κ$ median: 0.87). On the manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT RATE dataset (lungs/pleura labels only), Llama-3.1 8B performed best (0.91), with Gemma-3 27B close behind (0.89). Performance differences were mainly due to differing labeling practices, especially for labels with high subjectivity such as atelectasis. Conclusion: Lightweight LLMs outperform rule-based methods for CT report annotation and generalize across organ systems with zero-shot prompting. However, binary labels alone cannot capture the full nuance of report language. LLMs can provide a flexible, efficient solution aligned with clinical judgment and user needs.
>
---
#### [replaced 070] PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation
- **分类: cs.LG; cs.AI; cs.CL; cs.DC**

- **简介: 该论文提出PHOTON模型，解决Transformer在长上下文生成中的效率与内存问题。通过层次化自回归结构，提升生成速度和内存利用率。**

- **链接: [https://arxiv.org/pdf/2512.20687v2](https://arxiv.org/pdf/2512.20687v2)**

> **作者:** Yuma Ichikawa; Naoya Takagi; Takumi Nakagawa; Yuzi Kanazawa; Akira Sakai
>
> **备注:** 17 pages, 10 figures
>
> **摘要:** Transformers operate as horizontal token-by-token scanners; at each generation step, attending to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding more memory-bound, as KV-cache reads and writes dominate inference time over arithmetic operations. We propose Parallel Hierarchical Operation for TOp-down Networks (PHOTON), a hierarchical autoregressive model that replaces horizontal scanning with vertical, multi-resolution context scanning. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations in parallel. We further introduce recursive generation that updates only the coarsest latent stream and eliminates bottom-up re-encoding. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, providing advantages in long-context and multi-query tasks. In particular, this reduces decode-time KV-cache traffic, yielding up to $10^{3}\times$ higher throughput per unit memory.
>
---
#### [replaced 071] Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs
- **分类: cs.CL; cs.AI; cs.CR**

- **简介: 该论文属于安全攻击任务，旨在破解大语言模型的安全机制。提出LFJ方法，在潜在空间融合有害与无害表示，提高攻击成功率并降低被检测风险。**

- **链接: [https://arxiv.org/pdf/2508.10029v2](https://arxiv.org/pdf/2508.10029v2)**

> **作者:** Wenpeng Xing; Mohan Li; Chunqiang Hu; Haitao Xu; Ningyu Zhang; Bo Lin; Meng Han
>
> **摘要:** While Large Language Models (LLMs) have achieved remarkable progress, they remain vulnerable to jailbreak attacks. Existing methods, primarily relying on discrete input optimization (e.g., GCG), often suffer from high computational costs and generate high-perplexity prompts that are easily blocked by simple filters. To overcome these limitations, we propose Latent Fusion Jailbreak (LFJ), a stealthy white-box attack that operates in the continuous latent space. Unlike previous approaches, LFJ constructs adversarial representations by mathematically fusing the hidden states of a harmful query with a thematically similar benign query, effectively masking malicious intent while retaining semantic drive. We further introduce a gradient-guided optimization strategy to balance attack success and computational efficiency. Extensive evaluations on Vicuna-7B, LLaMA-2-7B-Chat, Guanaco-7B, LLaMA-3-70B, and Mistral-7B-Instruct show that LFJ achieves an average Attack Success Rate (ASR) of 94.01%, significantly outperforming state-of-the-art baselines like GCG and AutoDAN while avoiding detectable input artifacts. Furthermore, we identify that thematic similarity in the latent space is a critical vulnerability in current safety alignments. Finally, we propose a latent adversarial training defense that reduces LFJ's ASR by over 80% without compromising model utility.
>
---
#### [replaced 072] IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation
- **分类: cs.CL**

- **简介: 该论文属于指令遵循评估任务，旨在解决现有评估模型成本高、不可靠的问题。提出IF-CRITIC，通过细粒度检查清单和偏好优化提升评估效果。**

- **链接: [https://arxiv.org/pdf/2511.01014v2](https://arxiv.org/pdf/2511.01014v2)**

> **作者:** Bosi Wen; Yilin Niu; Cunxiang Wang; Pei Ke; Xiaoying Ling; Ying Zhang; Aohan Zeng; Hongning Wang; Minlie Huang
>
> **备注:** 24 pages, 5 figures
>
> **摘要:** Instruction-following is a fundamental ability of Large Language Models (LLMs), requiring their generated outputs to follow multiple constraints imposed in input instructions. Numerous studies have attempted to enhance this ability through preference optimization or reinforcement learning based on reward signals from LLM-as-a-Judge. However, existing evaluation models for instruction-following still possess many deficiencies, such as substantial costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM critic for fine-grained, efficient, and reliable instruction-following evaluation. We first develop a checklist generator to decompose instructions and generate constraint checklists. With the assistance of the checklists, we collect high-quality critique training data through a multi-stage critique filtering mechanism and employ a constraint-level preference optimization method to train IF-CRITIC. Extensive experiments show that the evaluation performance of IF-CRITIC can beat strong LLM-as-a-Judge baselines, including o4-mini and Gemini-3-Pro. With the reward signals provided by IF-CRITIC, LLMs can achieve substantial performance gains in instruction-following optimization under lower computational overhead compared to strong LLM critic baselines.
>
---
#### [replaced 073] Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation
- **分类: cs.CL; cs.SE**

- **简介: 该论文属于模型安全研究任务，旨在评估不同 abliteration 工具在去除拒绝行为时的效果与影响，通过实验分析工具兼容性及对模型能力的影响。**

- **链接: [https://arxiv.org/pdf/2512.13655v2](https://arxiv.org/pdf/2512.13655v2)**

> **作者:** Richard J. Young
>
> **备注:** 25 pages, 6 figures, 8 tables
>
> **摘要:** Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.
>
---
#### [replaced 074] One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在评估大模型在多轮对话中的指令遵循能力。针对现有基准的不足，提出新框架与基准EvolIF，分析模型在深度对话中的表现及缺陷。**

- **链接: [https://arxiv.org/pdf/2511.03508v3](https://arxiv.org/pdf/2511.03508v3)**

> **作者:** Qi Jia; Ye Shen; Xiujie Song; Kaiwei Zhang; Shibo Wang; Dun Pei; Xiangyang Zhu; Guangtao Zhai
>
> **摘要:** Evaluating LLMs' instruction-following ability in multi-topic dialogues is essential yet challenging. Existing benchmarks are limited to a fixed number of turns, susceptible to saturation and failing to account for users' interactive experience. In this work, we propose a novel framework featuring a three-layer tracking mechanism and a query synthesis agent to mimic sequential user behaviors. Grounded in Flow Theory, we introduce process-centric metrics and terminate a conversational evaluation only upon exhausting user patience. Leveraging this framework, we present EvolIF, an evolving benchmark covering 12 constraint groups. Our analysis reveals deficiencies in failure recovery and fine-grained instruction following, with performance stratification becoming evident as conversational depth increases. GPT-5 demonstrates the most sustained resilience, maintaining a 66.40% robustness score, outperforming Gemini-3-Pro by 5.59%, while other models lag behind. Data and code will be released at https://github.com/JiaQiSJTU/EvolIF.
>
---
#### [replaced 075] AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于数学推理任务，旨在解决LLM训练数据质量低的问题。通过生成高质量的数学问答对，提升模型推理能力。**

- **链接: [https://arxiv.org/pdf/2510.19361v3](https://arxiv.org/pdf/2510.19361v3)**

> **作者:** Xianyang Liu; Yilin Liu; Shuai Wang; Hao Cheng; Andrew Estornell; Yuzhi Zhao; Jun Shu; Jiaheng Wei
>
> **备注:** 8 pages
>
> **摘要:** The creation of high-quality datasets to improve Large Language Model (LLM) reasoning remains a significant challenge, as current methods often suffer from generating low-quality/incorrect answers and limited information richness from available data sources. To address this, we propose AgenticMath, a novel agentic method for generating high-quality mathematical question-answer pairs to enhance the supervised fine-tuning of LLMs. Our method operates through four stages: (1) Seed Question Filter that selects questions with high information richness, complexity, and clarity; (2) an Agentic Question Rephrase step that employs a multi-agent system to generate diverse, logically consistent paraphrases; (3) an Answer Augment step where rewrite answers using chain-of-thought reasoning to enhance numerical and logical correctness, without reliance on human-provided labels; and (4) a final Question and Answer Evaluation that retains only the most superior pairs. Extensive experiments demonstrate that, fine-tuning 3B-8B parameter LLMs on AgenticMath generated datasets (comprising only 30-60K math samples) achieves competitive or superior performance on diverse in domain and out-of-domain mathematical reasoning benchmarks compared to baselines trained on much more data (e.g., 400K or 2.3M samples). Our work demonstrates that targeted, high-quality data generation is a more efficient path to improving mathematical reasoning in LLMs than large-scale, low-quality alternatives.
>
---
#### [replaced 076] Non-Linear Scoring Model for Translation Quality Evaluation
- **分类: cs.CL**

- **简介: 该论文属于机器翻译质量评估任务，解决线性评分模型在不同样本长度下的偏差问题，提出非线性评分模型以更准确反映人类对翻译质量的感知。**

- **链接: [https://arxiv.org/pdf/2511.13467v3](https://arxiv.org/pdf/2511.13467v3)**

> **作者:** Serge Gladkoff; Lifeng Han; Katerina Gasova
>
> **备注:** ongoing work, 32 pages
>
> **摘要:** Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition. Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size. Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model E(x) = a * ln(1 + b * x), a, b > 0, anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added. The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.
>
---
