# 音频 cs.SD;  eess.AS

- **最新发布 64 篇**

- **更新 25 篇**

## 最新发布

#### [new 001] Lightweight Self-Supervised Detection of Fundamental Frequency and Accurate Probability of Voicing in Monophonic Music
- **分类: eess.AS; cs.AI; cs.LG; cs.SD; eess.SP**

- **简介: 该论文属于语音处理任务，解决单音音乐中基频和有声度估计问题。提出一种轻量级自监督框架，提升估计可靠性与泛化能力。**

- **链接: [https://arxiv.org/pdf/2601.11768v1](https://arxiv.org/pdf/2601.11768v1)**

> **作者:** Venkat Suprabath Bitra; Homayoon Beigi
>
> **备注:** 12 pages, 6 figures, 3 tables, and an appendix, Accepted for publication at ICPRAM 2026 in Marbella, Spain, on March 2, 2026
>
> **摘要:** Reliable fundamental frequency (F 0) and voicing estimation is essential for neural synthesis, yet many pitch extractors depend on large labeled corpora and degrade under realistic recording artifacts. We propose a lightweight, fully self-supervised framework for joint F 0 estimation and voicing inference, designed for rapid single-instrument training from limited audio. Using transposition-equivariant learning on CQT features, we introduce an EM-style iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as a reliability signal to suppress uninformative noisy/unvoiced frames. The resulting weights provide confidence scores that enable pseudo-labeling for a separate lightweight voicing classifier without manual annotations. Trained on MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates cross-instrument generalization.
>
---
#### [new 002] Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving
- **分类: eess.AS; cs.MM; cs.RO**

- **简介: 该论文属于自动驾驶任务，旨在解决单一视觉输入导致的决策延迟问题。通过融合音频指令与视觉信息，提出EchoVLA模型，提升驾驶行为的适应性与准确性。**

- **链接: [https://arxiv.org/pdf/2601.12142v1](https://arxiv.org/pdf/2601.12142v1)**

> **作者:** Ziang Guo; Feng Yang; Xuefeng Zhang; Jiaqi Guo; Kun Zhao; Peng Lu; Zufeng Zhang; Sifa Zheng
>
> **备注:** Accepted by IV
>
> **摘要:** Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\%$ and the collision rate by $74.4\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user's speech.
>
---
#### [new 003] ICASSP 2026 URGENT Speech Enhancement Challenge
- **分类: eess.AS; cs.SD**

- **简介: 本文属于语音增强任务，旨在解决多样化噪声环境下语音增强问题。论文介绍了ICASSP 2026 URGENT挑战，包括任务定义、数据集、基线系统及评估方法。**

- **链接: [https://arxiv.org/pdf/2601.13531v1](https://arxiv.org/pdf/2601.13531v1)**

> **作者:** Chenda Li; Wei Wang; Marvin Sach; Wangyou Zhang; Kohei Saijo; Samuele Cornell; Yihui Fu; Zhaoheng Ni; Tim Fingscheidt; Shinji Watanabe; Yanmin Qian
>
> **备注:** The overview paper of the ICASSP 2026 URGENT Speech Enhancement Challenge
>
> **摘要:** The ICASSP 2026 URGENT Challenge advances the series by focusing on universal speech enhancement (SE) systems that handle diverse distortions, domains, and input conditions. This overview paper details the challenge's motivation, task definitions, datasets, baseline systems, evaluation protocols, and results. The challenge is divided into two complementary tracks. Track 1 focuses on universal speech enhancement, while Track 2 introduces speech quality assessment for enhanced speech. The challenge attracted over 80 team registrations, with 29 submitting valid entries, demonstrating significant community interest in robust SE technologies.
>
---
#### [new 004] Song Aesthetics Evaluation with Multi-Stem Attention and Hierarchical Uncertainty Modeling
- **分类: cs.SD; cs.MM; eess.AS**

- **简介: 该论文属于歌曲美学评估任务，解决AI生成音乐质量评价问题。提出多茎注意力融合和分层不确定性建模方法，提升多维评估效果。**

- **链接: [https://arxiv.org/pdf/2601.12222v1](https://arxiv.org/pdf/2601.12222v1)**

> **作者:** Yishan Lv; Jing Luo; Boyuan Ju; Yang Zhang; Xinda Wu; Bo Yuan; Xinyu Yang
>
> **摘要:** Music generative artificial intelligence (AI) is rapidly expanding music content, necessitating automated song aesthetics evaluation. However, existing studies largely focus on speech, audio or singing quality, leaving song aesthetics underexplored. Moreover, conventional approaches often predict a precise Mean Opinion Score (MOS) value directly, which struggles to capture the nuances of human perception in song aesthetics evaluation. This paper proposes a song-oriented aesthetics evaluation framework, featuring two novel modules: 1) Multi-Stem Attention Fusion (MSAF) builds bidirectional cross-attention between mixture-vocal and mixture-accompaniment pairs, fusing them to capture complex musical features; 2) Hierarchical Granularity-Aware Interval Aggregation (HiGIA) learns multi-granularity score probability distributions, aggregates them into a score interval, and applies a regression within the interval to produce the final score. We evaluated on two datasets of full-length songs: SongEval dataset (AI-generated) and an internal aesthetics dataset (human-created), and compared with two state-of-the-art (SOTA) models. Results show that the proposed method achieves stronger performance for multi-dimensional song aesthetics evaluation.
>
---
#### [new 005] Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection
- **分类: cs.SD; cs.AI**

- **简介: 该论文属于AI生成音乐检测任务，旨在解决全音频检测难题。通过改进的Segment Transformer模型，融合内容与结构信息，提升检测效果。**

- **链接: [https://arxiv.org/pdf/2601.13647v1](https://arxiv.org/pdf/2601.13647v1)**

> **作者:** Yumin Kim; Seonghyeon Go
>
> **摘要:** With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.
>
---
#### [new 006] Event Classification by Physics-informed Inpainting for Distributed Multichannel Acoustic Sensor with Partially Degraded Channels
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于声事件分类任务，解决分布式多通道麦克风阵列在部分通道退化和布局不一致时性能下降的问题。提出基于物理的反向时间迁移修复方法，提升分类准确率。**

- **链接: [https://arxiv.org/pdf/2601.13513v1](https://arxiv.org/pdf/2601.13513v1)**

> **作者:** Noriyuki Tonami; Wataru Kohno; Yoshiyuki Yajima; Sakiko Mishima; Yumi Arai; Reishi Kondo; Tomoyuki Hino
>
> **备注:** Accepted to ICASSP 2026
>
> **摘要:** Distributed multichannel acoustic sensing (DMAS) enables large-scale sound event classification (SEC), but performance drops when many channels are degraded and when sensor layouts at test time differ from training layouts. We propose a learning-free, physics-informed inpainting frontend based on reverse time migration (RTM). In this approach, observed multichannel spectrograms are first back-propagated on a 3D grid using an analytic Green's function to form a scene-consistent image, and then forward-projected to reconstruct inpainted signals before log-mel feature extraction and Transformer-based classification. We evaluate the method on ESC-50 with 50 sensors and three layouts (circular, linear, right-angle), where per-channel SNRs are sampled from -30 to 0 dB. Compared with an AST baseline, scaling-sparsemax channel selection, and channel-swap augmentation, the proposed RTM frontend achieves the best or competitive accuracy across all layouts, improving accuracy by 13.1 points on the right-angle layout (from 9.7% to 22.8%). Correlation analyses show that spatial weights align more strongly with SNR than with channel--source distance, and that higher SNR--weight correlation corresponds to higher SEC accuracy. These results demonstrate that a reconstruct-then-project, physics-based preprocessing effectively complements learning-only methods for DMAS under layout-open configurations and severe channel degradation.
>
---
#### [new 007] A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于文本到语音生成任务，旨在解决现有模型无法灵活控制语音属性的问题。提出SpeechEdit模型，实现对语音特征的局部可控编辑。**

- **链接: [https://arxiv.org/pdf/2601.12480v1](https://arxiv.org/pdf/2601.12480v1)**

> **作者:** Hanchen Pei; Shujie Liu; Yanqing Liu; Jianwei Yu; Yuanhang Qian; Gongping Huang; Sheng Zhao; Yan Lu
>
> **摘要:** Neural codec language models achieve impressive zero-shot Text-to-Speech (TTS) by fully imitating the acoustic characteristics of a short speech prompt, including timbre, prosody, and paralinguistic information. However, such holistic imitation limits their ability to isolate and control individual attributes. In this paper, we present a unified codec language model SpeechEdit that extends zero-shot TTS with a selective control mechanism. By default, SpeechEdit reproduces the complete acoustic profile inferred from the speech prompt, but it selectively overrides only the attributes specified by explicit control instructions. To enable controllable modeling, SpeechEdit is trained on our newly constructed LibriEdit dataset, which provides delta (difference-aware) training pairs derived from LibriHeavy. Experimental results show that our approach maintains naturalness and robustness while offering flexible and localized control over desired attributes. Audio samples are available at https://speech-editing.github.io/speech-editing/.
>
---
#### [new 008] A Survey on 30+ Years of Automatic Singing Assessment and Singing Information Processing
- **分类: eess.AS; cs.SD**

- **简介: 本文综述了30多年来自动歌唱评估与歌唱信息处理的发展，旨在解决歌唱性能客观评价与艺术表达捕捉的问题，分析了技术进展与现存挑战。**

- **链接: [https://arxiv.org/pdf/2601.12153v1](https://arxiv.org/pdf/2601.12153v1)**

> **作者:** Arthur N. dos Santos; Bruno S. Masiero
>
> **摘要:** Automatic Singing Assessment and Singing Information Processing have evolved over the past three decades to support singing pedagogy, performance analysis, and vocal training. While the first approach objectively evaluates a singer's performance through computational metrics ranging from real-time visual feedback and acoustical biofeedback to sophisticated pitch tracking and spectral analysis, the latter method compares a predictor vocal signal with a target reference to capture nuanced data embedded in the singing voice. Notable advancements include the development of interactive systems that have significantly improved real-time visual feedback, and the integration of machine learning and deep neural network architectures that enhance the precision of vocal signal processing. This survey critically examines the literature to map the historical evolution of these technologies, while identifying and discussing key gaps. The analysis reveals persistent challenges, such as the lack of standardized evaluation frameworks, difficulties in reliably separating vocal signals from various noise sources, and the underutilization of advanced digital signal processing and artificial intelligence methodologies for capturing artistic expressivity. By detailing these limitations and the corresponding technological advances, this review demonstrates how addressing these issues can bridge the gap between objective computational assessments and subjective human-like evaluations of singing performance, ultimately enhancing both the technical accuracy and pedagogical relevance of automated singing evaluation systems.
>
---
#### [new 009] Content Leakage in LibriSpeech and Its Impact on the Privacy Evaluation of Speaker Anonymization
- **分类: eess.AS; cs.SD**

- **简介: 该论文属于语音隐私保护任务，研究LibriSpeech数据集中因词汇差异导致的说话人身份泄露问题，指出现有匿名化技术无法解决此问题，并推荐更安全的EdAcc数据集。**

- **链接: [https://arxiv.org/pdf/2601.13107v1](https://arxiv.org/pdf/2601.13107v1)**

> **作者:** Carlos Franzreb; Arnab Das; Tim Polzehl; Sebastian Möller
>
> **备注:** Accepted to ICASSP 2026
>
> **摘要:** Speaker anonymization aims to conceal a speaker's identity, without considering the linguistic content. In this study, we reveal a weakness of Librispeech, the dataset that is commonly used to evaluate anonymizers: the books read by the Librispeech speakers are so distinct, that speakers can be identified by their vocabularies. Even perfect anonymizers cannot prevent this identity leakage. The EdAcc dataset is better in this regard: only a few speakers can be identified through their vocabularies, encouraging the attacker to look elsewhere for the identities of the anonymized speakers. EdAcc also comprises spontaneous speech and more diverse speakers, complementing Librispeech and giving more insights into how anonymizers work.
>
---
#### [new 010] Improving Audio Question Answering with Variational Inference
- **分类: eess.AS; cs.SD**

- **简介: 该论文属于音频问答任务，旨在提升模型预测的准确性和可靠性。通过引入变分推断优化器IVON，增强模型对权重不确定性的建模，改善预测校准度。**

- **链接: [https://arxiv.org/pdf/2601.12700v1](https://arxiv.org/pdf/2601.12700v1)**

> **作者:** Haolin Chen
>
> **备注:** ICASSP 2026
>
> **摘要:** Variational inference (VI) provides a principled framework for estimating posterior distributions over model parameters, enabling explicit modeling of weight uncertainty during optimization. By capturing this uncertainty, VI improves the reliability of predictions, yielding better calibrated outputs. In this work, we investigate the benefits of VI for challenging multimodal understanding and reasoning by applying the Improved Variational Online Newton (IVON), a recent VI optimizer, to fine-tuning a multimodal large language model on audio question answering tasks. Our results show that VI not only enhances predictive accuracy but also significantly improves calibration, reducing the model's overconfidence. These advances further support risk-sensitive applications such as selective prediction, where reliable confidence estimates are crucial.
>
---
#### [new 011] WenetSpeech-Wu: Datasets, Benchmarks, and Models for a Unified Chinese Wu Dialect Speech Processing Ecosystem
- **分类: cs.SD; eess.AS**

- **简介: 该论文针对吴语语音处理问题，提出首个大规模开源数据集WenetSpeech-Wu及基准测试平台，涵盖ASR、翻译等多任务模型，推动方言语音技术发展。**

- **链接: [https://arxiv.org/pdf/2601.11027v1](https://arxiv.org/pdf/2601.11027v1)**

> **作者:** Chengyou Wang; Mingchen Shao; Jingbin Hu; Zeyu Zhu; Hongfei Xue; Bingshen Mu; Xin Xu; Xingyi Duan; Binbin Zhang; Pengcheng Zhu; Chuang Ding; Xiaojun Zhang; Hui Bu; Lei Xie
>
> **摘要:** Speech processing for low-resource dialects remains a fundamental challenge in developing inclusive and robust speech technologies. Despite its linguistic significance and large speaker population, the Wu dialect of Chinese has long been hindered by the lack of large-scale speech data, standardized evaluation benchmarks, and publicly available models. In this work, we present WenetSpeech-Wu, the first large-scale, multi-dimensionally annotated open-source speech corpus for the Wu dialect, comprising approximately 8,000 hours of diverse speech data. Building upon this dataset, we introduce WenetSpeech-Wu-Bench, the first standardized and publicly accessible benchmark for systematic evaluation of Wu dialect speech processing, covering automatic speech recognition (ASR), Wu-to-Mandarin translation, speaker attribute prediction, speech emotion recognition, text-to-speech (TTS) synthesis, and instruction-following TTS (instruct TTS). Furthermore, we release a suite of strong open-source models trained on WenetSpeech-Wu, establishing competitive performance across multiple tasks and empirically validating the effectiveness of the proposed dataset. Together, these contributions lay the foundation for a comprehensive Wu dialect speech processing ecosystem, and we open-source proposed datasets, benchmarks, and models to support future research on dialectal speech intelligence.
>
---
#### [new 012] UNMIXX: Untangling Highly Correlated Singing Voices Mixtures
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于语音分离任务，旨在解决歌唱声源混合中数据稀缺和高度相关性问题。提出UNMIXX框架，通过音乐混音策略、交叉源注意力和幅度惩罚损失提升分离效果。**

- **链接: [https://arxiv.org/pdf/2601.12802v1](https://arxiv.org/pdf/2601.12802v1)**

> **作者:** Jihoo Jung; Ji-Hoon Kim; Doyeop Kwak; Junwon Lee; Juhan Nam; Joon Son Chung
>
> **备注:** Accepted by ICASSP 2026
>
> **摘要:** We introduce UNMIXX, a novel framework for multiple singing voices separation (MSVS). While related to speech separation, MSVS faces unique challenges: data scarcity and the highly correlated nature of singing voices mixture. To address these issues, we propose UNMIXX with three key components: (1) musically informed mixing strategy to construct highly correlated, music-like mixtures, (2) cross-source attention that drives representations of two singers apart via reverse attention, and (3) magnitude penalty loss penalizing erroneously assigned interfering energy. UNMIXX not only addresses data scarcity by simulating realistic training data, but also excels at separating highly correlated mixtures through cross-source interactions at both the architectural and loss levels. Our extensive experiments demonstrate that UNMIXX greatly enhances performance, with SDRi gains exceeding 2.2 dB over prior work.
>
---
#### [new 013] Toward Faithful Explanations in Acoustic Anomaly Detection
- **分类: cs.SD; cs.LG; eess.AS**

- **简介: 该论文属于音频异常检测任务，旨在提升模型的可解释性。通过比较标准自编码器与掩码自编码器，研究其解释能力，并提出一种基于扰动的忠实度评估方法。**

- **链接: [https://arxiv.org/pdf/2601.12660v1](https://arxiv.org/pdf/2601.12660v1)**

> **作者:** Maab Elrashid; Anthony Deschênes; Cem Subakan; Mirco Ravanelli; Rémi Georges; Michael Morin
>
> **备注:** Accepted at the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026. Code: https://github.com/Maab-Nimir/Faithful-Explanations-in-Acoustic-Anomaly-Detection
>
> **摘要:** Interpretability is essential for user trust in real-world anomaly detection applications. However, deep learning models, despite their strong performance, often lack transparency. In this work, we study the interpretability of autoencoder-based models for audio anomaly detection, by comparing a standard autoencoder (AE) with a mask autoencoder (MAE) in terms of detection performance and interpretability. We applied several attribution methods, including error maps, saliency maps, SmoothGrad, Integrated Gradients, GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it consistently provides more faithful and temporally precise explanations, suggesting a better alignment with true anomalies. To assess the relevance of the regions highlighted by the explanation method, we propose a perturbation-based faithfulness metric that replaces them with their reconstructions to simulate normal input. Our findings, based on experiments in a real industrial scenario, highlight the importance of incorporating interpretability into anomaly detection pipelines and show that masked training improves explanation quality without compromising performance.
>
---
#### [new 014] Performance and Complexity Trade-off Optimization of Speech Models During Training
- **分类: cs.SD; cs.AI; cs.LG; eess.AS**

- **简介: 该论文属于语音机器学习任务，旨在优化模型性能与计算复杂度的平衡。通过引入特征噪声重参数化方法，在训练中联合优化两者，无需后期剪枝。**

- **链接: [https://arxiv.org/pdf/2601.13704v1](https://arxiv.org/pdf/2601.13704v1)**

> **作者:** Esteban Gómez; Tom Bäckström
>
> **摘要:** In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research.
>
---
#### [new 015] ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models
- **分类: cs.SD; cs.AI; cs.LG**

- **简介: 该论文提出ConceptCaps数据集，解决音乐模型可解释性问题。通过分离语义建模与文本生成，提升音频-文本对齐效果，支持概念探测分析。**

- **链接: [https://arxiv.org/pdf/2601.14157v1](https://arxiv.org/pdf/2601.14157v1)**

> **作者:** Bruno Sienkiewicz; Łukasz Neumann; Mateusz Modrzejewski
>
> **摘要:** Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.
>
---
#### [new 016] SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training
- **分类: eess.AS; cs.AI; cs.SD**

- **简介: 该论文提出SLAP模型，解决CLAP在数据规模、音频时长限制及细粒度特征学习上的不足，通过多目标训练提升音频-文本预训练效果。**

- **链接: [https://arxiv.org/pdf/2601.12594v1](https://arxiv.org/pdf/2601.12594v1)**

> **作者:** Xinhao Mei; Gael Le Lan; Haohe Liu; Zhaoheng Ni; Varun Nagaraja; Yang Liu; Yangyang Shi; Vikas Chandra
>
> **备注:** Accepted to ICASSP 2026
>
> **摘要:** Contrastive language-audio pretraining (CLAP) has achieved notable success in learning semantically rich audio representations and is widely adopted for various audio-related tasks. However, current CLAP models face several key limitations. First, they are typically trained on relatively small datasets, often comprising a few million audio samples. Second, existing CLAP models are restricted to short and fixed duration, which constrains their usage in real-world scenarios with variable-duration audio. Third, the standard contrastive training objective operates on global representations, which may hinder the learning of dense, fine-grained audio features. To address these challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which scales language-audio pretraining to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives. SLAP unifies contrastive loss with additional self-supervised and captioning losses in a single-stage training, facilitating the learning of richer dense audio representations. The proposed SLAP model achieves new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks, demonstrating its effectiveness across diverse benchmarks.
>
---
#### [new 017] DistilMOS: Layer-Wise Self-Distillation For Self-Supervised Learning Model-Based MOS Prediction
- **分类: cs.SD**

- **简介: 该论文属于MOS预测任务，解决SSL模型在微调中的遗忘和过拟合问题。提出DistilMOS，通过层间自蒸馏提升预测精度与泛化能力。**

- **链接: [https://arxiv.org/pdf/2601.13700v1](https://arxiv.org/pdf/2601.13700v1)**

> **作者:** Jianing Yang; Wataru Nakata; Yuki Saito; Hiroshi Saruwatari
>
> **备注:** Accepted to ICASSP 2026
>
> **摘要:** With the advancement of self-supervised learning (SSL), fine-tuning pretrained SSL models for mean opinion score (MOS) prediction has achieved state-of-the-art performance. However, during fine-tuning, these SSL-based MOS prediction models often suffer from catastrophic forgetting of the pretrained knowledge and tend to overfit the training set, resulting in poor generalization performance. In this study, we propose DistilMOS, a novel method that learns to predict not only MOS but also token IDs obtained by clustering the hidden representations of each layer in the pretrained SSL model. These layer-wise token targets serve as self-distillation signals that enables the MOS prediction model to extract rich internal knowledge from SSL models, enhancing both prediction accuracy and generalization capability. Experimental evaluations demonstrate that our method significantly outperforms standard SSL-based MOS prediction models on both in-domain and out-of-domain evaluations, verifying the effectiveness and practicality of the proposed method.
>
---
#### [new 018] The Achilles' Heel of Angular Margins: A Chebyshev Polynomial Fix for Speaker Verification
- **分类: cs.SD**

- **简介: 该论文属于语音识别任务，针对角边距损失训练不稳定的问题，提出用切比雪夫多项式近似替代arccos操作，提升优化效果与模型性能。**

- **链接: [https://arxiv.org/pdf/2601.13198v1](https://arxiv.org/pdf/2601.13198v1)**

> **作者:** Yang Wang; Yiqi Liu; Chenghao Xiao; Chenghua Lin
>
> **备注:** Accepted for presentation at ICASSP 2026
>
> **摘要:** Angular margin losses, such as AAM-Softmax, have become the de facto in speaker and face verification. Their success hinges on directly manipulating the angle between features and class prototypes. However, this manipulation relies on the arccos function to recover the angle, introducing a significant yet overlooked source of training instability. The derivative of arccos explodes at its boundaries, causing gradient peaks during optimisation. Furthermore, the formulation fails to generate a sufficiently sharp gradient for hard-to-classify examples. We address these issues by proposing ChebyAAM, a loss that replaces the arccos operation with its Chebyshev polynomial approximation. This substitution eliminates gradient explosion and applies a stronger corrective signal to hard examples, leading to more effective optimisation. Experiments on three benchmarks (VoxCeleb, SITW, and CN-Celeb) demonstrate that our method resolves the instability and consistently improves performance. Our work suggests that approximating angular operations, rather than calculating them explicitly, offers a more robust path for designing future metric learning losses. Code is available at https://github.com/ExtraOrdinaryLab/vibe.
>
---
#### [new 019] Supervised Learning for Game Music Segmentation
- **分类: cs.SD**

- **简介: 该论文属于游戏音乐结构分割任务，旨在解决神经网络难以理解音乐结构的问题。通过结合卷积和循环神经网络，使用监督学习方法实现有效分割。**

- **链接: [https://arxiv.org/pdf/2601.12961v1](https://arxiv.org/pdf/2601.12961v1)**

> **作者:** Shangxuan Luo; Joshua Reiss
>
> **摘要:** At present, neural network-based models, including transformers, struggle to generate memorable and readily comprehensible music from unified and repetitive musical material due to a lack of understanding of musical structure. Consequently, these models are rarely employed by the games industry. It is hypothesised by many scholars that the modelling of musical structure may inform models at a higher level, thereby enhancing the quality of music generation. The aim of this study is to explore the performance of supervised learning methods in the task of structural segmentation, which is the initial step in music structure modelling. An audio game music dataset with 309 structural annotations was created to train the proposed method, which combines convolutional neural networks and recurrent neural networks, achieving performance comparable to the state-of-the-art unsupervised learning methods with fewer training resources.
>
---
#### [new 020] S$^2$Voice: Style-Aware Autoregressive Modeling with Enhanced Conditioning for Singing Style Conversion
- **分类: eess.AS; eess.SP**

- **简介: 该论文属于歌唱风格转换任务，旨在提升风格控制与音色相似性。通过引入风格嵌入、全局说话人嵌入及多阶段训练，增强模型的风格建模与泛化能力。**

- **链接: [https://arxiv.org/pdf/2601.13629v1](https://arxiv.org/pdf/2601.13629v1)**

> **作者:** Ziqian Wang; Xianjun Xia; Chuanzeng Huang; Lei Xie
>
> **备注:** accepted to ICASSP 2026
>
> **摘要:** We present S$^2$Voice, the winning system of the Singing Voice Conversion Challenge (SVCC) 2025 for both the in-domain and zero-shot singing style conversion tracks. Built on the strong two-stage Vevo baseline, S$^2$Voice advances style control and robustness through several contributions. First, we integrate style embeddings into the autoregressive large language model (AR LLM) via a FiLM-style layer-norm conditioning and a style-aware cross-attention for enhanced fine-grained style modeling. Second, we introduce a global speaker embedding into the flow-matching transformer to improve timbre similarity. Third, we curate a large, high-quality singing corpus via an automated pipeline for web harvesting, vocal separation, and transcript refinement. Finally, we employ a multi-stage training strategy combining supervised fine-tuning (SFT) and direct preference optimization (DPO). Subjective listening tests confirm our system's superior performance: leading in style similarity and singer similarity for Task 1, and across naturalness, style similarity, and singer similarity for Task 2. Ablation studies demonstrate the effectiveness of our contributions in enhancing style fidelity, timbre preservation, and generalization. Audio samples are available~\footnote{https://honee-w.github.io/SVC-Challenge-Demo/}.
>
---
#### [new 021] ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech
- **分类: cs.SD; cs.LG; eess.AS**

- **简介: 该论文提出ParaMETA，用于学习分离的语音风格表示，解决多任务语音风格识别与生成问题。通过子空间投影实现风格解耦，提升模型性能与控制能力。**

- **链接: [https://arxiv.org/pdf/2601.12289v1](https://arxiv.org/pdf/2601.12289v1)**

> **作者:** Haowei Lou; Hye-young Paik; Wen Hu; Lina Yao
>
> **备注:** 9 pages, 7 figures, Accepted to AAAI-26 (Main Technical Track)
>
> **摘要:** Learning representative embeddings for different types of speaking styles, such as emotion, age, and gender, is critical for both recognition tasks (e.g., cognitive computing and human-computer interaction) and generative tasks (e.g., style-controllable speech generation). In this work, we introduce ParaMETA, a unified and flexible framework for learning and controlling speaking styles directly from speech. Unlike existing methods that rely on single-task models or cross-modal alignment, ParaMETA learns disentangled, task-specific embeddings by projecting speech into dedicated subspaces for each type of style. This design reduces inter-task interference, mitigates negative transfer, and allows a single model to handle multiple paralinguistic tasks such as emotion, gender, age, and language classification. Beyond recognition, ParaMETA enables fine-grained style control in Text-To-Speech (TTS) generative models. It supports both speech- and text-based prompting and allows users to modify one speaking styles while preserving others. Extensive experiments demonstrate that ParaMETA outperforms strong baselines in classification accuracy and generates more natural and expressive speech, while maintaining a lightweight and efficient model suitable for real-world applications.
>
---
#### [new 022] SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and Neural Synthesis with Interactive 3D Visualization
- **分类: cs.SD; cs.LG**

- **简介: 该论文提出SoundPlot，一个用于鸟类声音分析与合成的开源框架，解决音频特征提取与可视化问题，实现3D交互展示和高保真音频重建。**

- **链接: [https://arxiv.org/pdf/2601.12752v1](https://arxiv.org/pdf/2601.12752v1)**

> **作者:** Naqcho Ali Mehdi; Mohammad Adeel; Aizaz Ali Larik
>
> **摘要:** We present SoundPlot, an open-source framework for analyzing avian vocalizations through acoustic feature extraction, dimensionality reduction, and neural audio synthesis. The system transforms audio signals into a multi-dimensional acoustic feature space, enabling real-time visualization of temporal dynamics in 3D using web-based interactive graphics. Our framework implements a complete analysis-synthesis pipeline that extracts spectral features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN (pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a unified timbre space for visualization. Audio reconstruction employs the Griffin-Lim phase estimation algorithm applied to mel spectrograms. The accompanying Three.js-based interface provides dual-viewport visualization comparing original and synthesized audio trajectories with independent playback controls. We demonstrate the framework's capabilities through comprehensive waveform analysis, spectrogram comparisons, and feature space evaluation using Principal Component Analysis (PCA). Quantitative evaluation shows mel spectrogram correlation scores exceeding 0.92, indicating high-fidelity preservation of perceptual acoustic structure. SoundPlot is released under the MIT License to facilitate research in bioacoustics, audio signal processing, and computational ethology.
>
---
#### [new 023] Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models
- **分类: eess.AS; cs.AI**

- **简介: 该论文属于语音匿名化任务，旨在保护在线语音应用中的说话人身份。针对现有方法隐私不足的问题，提出Stream-Voice-Anon，结合神经音频编解码器与语言模型，提升语音可懂度和情感保留，同时保障隐私。**

- **链接: [https://arxiv.org/pdf/2601.13948v1](https://arxiv.org/pdf/2601.13948v1)**

> **作者:** Nikita Kuzmin; Songting Liu; Kong Aik Lee; Eng Siong Chng
>
> **备注:** Accepted by ICASSP2026
>
> **摘要:** Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.
>
---
#### [new 024] ImmersiveFlow: Stereo-to-7.1.4 spatial audio generation with flow matching
- **分类: eess.AS**

- **简介: 该论文属于音频生成任务，解决从立体声到7.1.4空间音频的转换问题。通过Flow Matching技术实现端到端生成，提升音场沉浸感。**

- **链接: [https://arxiv.org/pdf/2601.12950v1](https://arxiv.org/pdf/2601.12950v1)**

> **作者:** Zining Liang; Runbang Wang; Xuzhou Ye; Qiuqiang Kong
>
> **备注:** 5 pages, 3 figures, 2 tables
>
> **摘要:** Immersive spatial audio has become increasingly critical for applications ranging from AR/VR to home entertainment and automotive sound systems. However, existing generative methods remain constrained to low-dimensional formats such as binaural audio and First-Order Ambisonics (FOA). Binaural rendering is inherently limited to headphone playback, while FOA suffers from spatial aliasing and insufficient resolution for high-frequency. To overcome these limitations, we introduce ImmersiveFlow, the first end-to-end generative framework that directly synthesizes discrete 7.1.4 format spatial audio from stereo input. ImmersiveFlow leverages Flow Matching to learn trajectories from stereo inputs to multichannel spatial features within a pretrained VAE latent space. At inference, the Flow Matching model predicted latent features are decoded by the VAE and converted into the final 7.1.4 waveform. Comprehensive objective and subjective evaluations demonstrate that our method produces perceptually rich sound fields and enhanced externalization, significantly outperforming traditional upmixing techniques. Code implementations and audio samples are provided at: https://github.com/violet-audio/ImmersiveFlow.
>
---
#### [new 025] RLBR: Reinforcement Learning with Biasing Rewards for Contextual Speech Large Language Models
- **分类: eess.AS**

- **简介: 该论文属于语音大语言模型的优化任务，旨在提升罕见词和专业术语的识别准确率。通过引入RLBR方法，利用偏置奖励和参考感知机制，显著改善了模型性能。**

- **链接: [https://arxiv.org/pdf/2601.13409v1](https://arxiv.org/pdf/2601.13409v1)**

> **作者:** Bo Ren; Ruchao Fan; Yelong Shen; Weizhu Chen; Jinyu Li
>
> **备注:** Accepted to the 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)
>
> **摘要:** Speech large language models (LLMs) have driven significant progress in end-to-end speech understanding and recognition, yet they continue to struggle with accurately recognizing rare words and domain-specific terminology. This paper presents a novel fine-tuning method, Reinforcement Learning with Biasing Rewards (RLBR), which employs a specialized biasing words preferred reward to explicitly emphasize biasing words in the reward calculation. In addition, we introduce reference-aware mechanisms that extend the reinforcement learning algorithm with reference transcription to strengthen the potential trajectory exploration space. Experiments on the LibriSpeech corpus across various biasing list sizes demonstrate that RLBR delivers substantial performance improvements over a strong supervised fine-tuning (SFT) baseline and consistently outperforms several recently published methods. The proposed approach achieves excellent performance on the LibriSpeech test-clean and test-other sets, reaching Biasing Word Error Rates (BWERs) of 0.59% / 2.11%, 1.09% / 3.24%, and 1.36% / 4.04% for biasing list sizes of 100, 500, and 1000, respectively, without compromising the overall WERs.
>
---
#### [new 026] LongSpeech: A Scalable Benchmark for Transcription, Translation and Understanding in Long Speech
- **分类: cs.SD**

- **简介: 该论文提出LongSpeech，一个用于长时语音任务的基准，解决语音识别、翻译和理解等多任务挑战。工作包括构建大规模数据集并评估模型性能。**

- **链接: [https://arxiv.org/pdf/2601.13539v1](https://arxiv.org/pdf/2601.13539v1)**

> **作者:** Fei Yang; Xuanfan Ni; Renyi Yang; Jiahui Geng; Qing Li; Chenyang Lyu; Yichao Du; Longyue Wang; Weihua Luo; Kaifu Zhang
>
> **备注:** ICASSP 2026
>
> **摘要:** Recent advances in audio-language models have demonstrated remarkable success on short, segment-level speech tasks. However, real-world applications such as meeting transcription, spoken document understanding, and conversational analysis require robust models capable of processing and reasoning over long-form audio. In this work, we present LongSpeech, a large-scale and scalable benchmark specifically designed to evaluate and advance the capabilities of speech models on long-duration audio. LongSpeech comprises over 100,000 speech segments, each approximately 10 minutes long, with rich annotations for ASR, speech translation, summarization, language detection, speaker counting, content separation, and question answering. We introduce a reproducible pipeline for constructing long-form speech benchmarks from diverse sources, enabling future extensions. Our initial experiments with state-of-the-art models reveal significant performance gaps, with models often specializing in one task at the expense of others and struggling with higher-level reasoning. These findings underscore the challenging nature of our benchmark. Our benchmark will be made publicly available to the research community.
>
---
#### [new 027] CodeSep: Low-Bitrate Codec-Driven Speech Separation with Base-Token Disentanglement and Auxiliary-Token Serial Prediction
- **分类: eess.AS**

- **简介: 该论文提出CodeSep，解决语音分离与压缩结合的任务，通过分离说话人并生成离散表示实现低比特率传输。**

- **链接: [https://arxiv.org/pdf/2601.12757v1](https://arxiv.org/pdf/2601.12757v1)**

> **作者:** Hui-Peng Du; Yang Ai; Xiao-Hang Jiang; Rui-Chen Zheng; Zhen-Hua Ling
>
> **备注:** Accepted by ICASSP 2026
>
> **摘要:** This paper targets a new scenario that integrates speech separation with speech compression, aiming to disentangle multiple speakers while producing discrete representations for efficient transmission or storage, with applications in online meetings and dialogue archiving. To address this scenario, we propose CodeSep, a codec-driven model that jointly performs speech separation and low-bitrate compression. CodeSep comprises a residual vector quantizer (RVQ)-based plain neural speech codec, a base-token disentanglement (BTD) module, and parallel auxiliary-token serial prediction (ATSP) modules. The BTD module disentangles mixed-speech mel-spectrograms into base tokens for each speaker, which are then refined by ATSP modules to serially predict auxiliary tokens, and finally, all tokens are decoded to reconstruct separated waveforms through the codec decoder. During training, the codec's RVQ provides supervision with permutation-invariant and teacher-forcing-based cross-entropy losses. As only base tokens are transmitted or stored, CodeSep achieves low-bitrate compression. Experimental results show that CodeSep attains satisfactory separation performance at only 1 kbps compared with baseline methods.
>
---
#### [new 028] SmoothCLAP: Soft-Target Enhanced Contrastive Language\--Audio Pretraining for Affective Computing
- **分类: cs.SD; eess.AS**

- **简介: 论文提出SmoothCLAP，用于情感计算任务，解决情绪识别中情感边界模糊的问题。通过引入软目标增强对比学习，提升模型对情绪关系的感知能力。**

- **链接: [https://arxiv.org/pdf/2601.12591v1](https://arxiv.org/pdf/2601.12591v1)**

> **作者:** Xin Jing; Jiadong Wang; Andreas Triantafyllopoulos; Maurice Gerczuk; Shahin Amiriparian; Jun Luo; Björn Schuller
>
> **备注:** 5 pages, accepted by ICASSP 2026
>
> **摘要:** The ambiguity of human emotions poses several challenges for machine learning models, as they often overlap and lack clear delineating boundaries. Contrastive language-audio pretraining (CLAP) has emerged as a key technique for generalisable emotion recognition. However, as conventional CLAP enforces a strict one-to-one alignment between paired audio-text samples, it overlooks intra-modal similarity and treats all non-matching pairs as equally negative. This conflicts with the fuzzy boundaries between different emotions. To address this limitation, we propose SmoothCLAP, which introduces softened targets derived from intra-modal similarity and paralinguistic features. By combining these softened targets with conventional contrastive supervision, SmoothCLAP learns embeddings that respect graded emotional relationships, while retaining the same inference pipeline as CLAP. Experiments on eight affective computing tasks across English and German demonstrate that SmoothCLAP is consistently achieving superior performance. Our results highlight that leveraging soft supervision is a promising strategy for building emotion-aware audio-text models.
>
---
#### [new 029] Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings
- **分类: cs.SD; cs.CL**

- **简介: 该论文属于语音合成任务，旨在解决如何为任意说话人生成可控风格的Lombard语音问题。通过分析风格嵌入与Lombard特征的关系，实现对语音风格的精确控制。**

- **链接: [https://arxiv.org/pdf/2601.12966v1](https://arxiv.org/pdf/2601.12966v1)**

> **作者:** Seymanur Akti; Alexander Waibel
>
> **摘要:** The Lombard effect plays a key role in natural communication, particularly in noisy environments or when addressing hearing-impaired listeners. We present a controllable text-to-speech (TTS) system capable of synthesizing Lombard speech for any speaker without requiring explicit Lombard data during training. Our approach leverages style embeddings learned from a large, prosodically diverse dataset and analyzes their correlation with Lombard attributes using principal component analysis (PCA). By shifting the relevant PCA components, we manipulate the style embeddings and incorporate them into our TTS model to generate speech at desired Lombard levels. Evaluations demonstrate that our method preserves naturalness and speaker identity, enhances intelligibility under noise, and provides fine-grained control over prosody, offering a robust solution for controllable Lombard TTS for any speaker.
>
---
#### [new 030] Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection
- **分类: cs.SD**

- **简介: 该论文属于音频深度伪造检测任务，旨在解决声学与情感特征不一致问题。提出EAI-ADD方法，通过分析跨层级不一致性提升检测效果。**

- **链接: [https://arxiv.org/pdf/2601.13847v1](https://arxiv.org/pdf/2601.13847v1)**

> **作者:** Jinhua Zhang; Zhenqi Jia; Rui Liu
>
> **备注:** Accepted by ICASSP 2026
>
> **摘要:** Audio Deepfake Detection (ADD) aims to detect spoof speech from bonafide speech. Most prior studies assume that stronger correlations within or across acoustic and emotional features imply authenticity, and thus focus on enhancing or measuring such correlations. However, existing methods often treat acoustic and emotional features in isolation or rely on correlation metrics, which overlook subtle desynchronization between them and smooth out abrupt discontinuities. To address these issues, we propose EAI-ADD, which treats cross level emotion acoustic inconsistency as the primary detection signal. We first project emotional and acoustic representations into a comparable space. Then we progressively integrate frame level and utterance level emotion features with acoustic features to capture cross level emotion acoustic inconsistencies across different temporal granularities. Experimental results on the ASVspoof 2019LA and 2021LA datasets demonstrate that the proposed EAI-ADD outperforms baselines, providing a more effective solution for audio anti spoofing detection.
>
---
#### [new 031] Confidence-based Filtering for Speech Dataset Curation with Generative Speech Enhancement Using Discrete Tokens
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于语音数据集清理任务，解决GSE模型产生的幻觉错误问题。通过利用生成token的置信度分数进行非侵入式过滤，有效提升TTS数据质量。**

- **链接: [https://arxiv.org/pdf/2601.12254v1](https://arxiv.org/pdf/2601.12254v1)**

> **作者:** Kazuki Yamauchi; Masato Murata; Shogo Seki
>
> **备注:** Accepted for ICASSP 2026
>
> **摘要:** Generative speech enhancement (GSE) models show great promise in producing high-quality clean speech from noisy inputs, enabling applications such as curating noisy text-to-speech (TTS) datasets into high-quality ones. However, GSE models are prone to hallucination errors, such as phoneme omissions and speaker inconsistency, which conventional error filtering based on non-intrusive speech quality metrics often fails to detect. To address this issue, we propose a non-intrusive method for filtering hallucination errors from discrete token-based GSE models. Our method leverages the log-probabilities of generated tokens as confidence scores to detect potential errors. Experimental results show that the confidence scores strongly correlate with a suite of intrusive SE metrics, and that our method effectively identifies hallucination errors missed by conventional filtering methods. Furthermore, we demonstrate the practical utility of our method: curating an in-the-wild TTS dataset with our confidence-based filtering improves the performance of subsequently trained TTS models.
>
---
#### [new 032] Adaptive Rotary Steering with Joint Autoregression for Robust Extraction of Closely Moving Speakers in Dynamic Scenarios
- **分类: eess.AS; cs.LG; cs.SD**

- **简介: 该论文属于语音增强任务，解决动态场景中紧密移动说话人的跟踪与分离问题。通过引入联合自回归框架，提升复杂场景下的跟踪与增强效果。**

- **链接: [https://arxiv.org/pdf/2601.12345v1](https://arxiv.org/pdf/2601.12345v1)**

> **作者:** Jakob Kienegger; Timo Gerkmann
>
> **备注:** Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026
>
> **摘要:** Latest advances in deep spatial filtering for Ambisonics demonstrate strong performance in stationary multi-speaker scenarios by rotating the sound field toward a target speaker prior to multi-channel enhancement. For applicability in dynamic acoustic conditions with moving speakers, we propose to automate this rotary steering using an interleaved tracking algorithm conditioned on the target's initial direction. However, for nearby or crossing speakers, robust tracking becomes difficult and spatial cues less effective for enhancement. By incorporating the processed recording as additional guide into both algorithms, our novel joint autoregressive framework leverages temporal-spectral correlations of speech to resolve spatially challenging speaker constellations. Consequently, our proposed method significantly improves tracking and enhancement of closely spaced speakers, consistently outperforming comparable non-autoregressive methods on a synthetic dataset. Real-world recordings complement these findings in complex scenarios with multiple speaker crossings and varying speaker-to-array distances.
>
---
#### [new 033] Towards Effective Negation Modeling in Joint Audio-Text Models for Music
- **分类: cs.SD; cs.IR; cs.LG**

- **简介: 该论文属于音乐检索任务，旨在解决联合音频-文本模型中否定表达建模不足的问题。通过文本增强和对比损失提升模型对否定语义的区分能力。**

- **链接: [https://arxiv.org/pdf/2601.13931v1](https://arxiv.org/pdf/2601.13931v1)**

> **作者:** Yannis Vasilakis; Rachel Bittner; Johan Pauwels
>
> **备注:** Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026
>
> **摘要:** Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., "with vocals" vs. "without vocals"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.
>
---
#### [new 034] SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition
- **分类: cs.SD; cs.CL; cs.LG; eess.AS**

- **简介: 该论文属于语音识别任务，解决参数高效微调中的效率与可扩展性问题。提出SSVD-O方法，通过结构化SVD实现更优的参数分配，提升模型适应新领域的能力。**

- **链接: [https://arxiv.org/pdf/2601.12600v1](https://arxiv.org/pdf/2601.12600v1)**

> **作者:** Pu Wang; Shinji Watanabe; Hugo Van hamme
>
> **备注:** Accepted by IEEE ICASSP 2026
>
> **摘要:** Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting.
>
---
#### [new 035] Purification Before Fusion: Toward Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition
- **分类: eess.AS; cs.AI; cs.LG; cs.MM; cs.SD**

- **简介: 该论文属于音频-视觉语音识别任务，旨在解决高噪声环境下语音增强问题。提出一种无需噪声掩码的端到端框架，通过融合模块提升识别鲁棒性。**

- **链接: [https://arxiv.org/pdf/2601.12436v1](https://arxiv.org/pdf/2601.12436v1)**

> **作者:** Linzhi Wu; Xingyu Zhang; Hao Yuan; Yakun Zhang; Changyan Zheng; Liang Xie; Tiejun Liu; Erwei Yin
>
> **备注:** Accepted by ICASSP2026
>
> **摘要:** Audio-visual speech recognition (AVSR) typically improves recognition accuracy in noisy environments by integrating noise-immune visual cues with audio signals. Nevertheless, high-noise audio inputs are prone to introducing adverse interference into the feature fusion process. To mitigate this, recent AVSR methods often adopt mask-based strategies to filter audio noise during feature interaction and fusion, yet such methods risk discarding semantically relevant information alongside noise. In this work, we propose an end-to-end noise-robust AVSR framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. This framework leverages a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance. By reducing modality redundancy and enhancing inter-modal interactions, our method preserves speech semantic integrity to achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that our method outperforms prior advanced mask-based baselines under noisy conditions.
>
---
#### [new 036] Robust Online Overdetermined Independent Vector Analysis Based on Bilinear Decomposition
- **分类: eess.AS; cs.SD**

- **简介: 该论文属于语音分离任务，解决大麦克风阵列下参数过多导致的在线估计精度下降问题。通过双线性分解减少参数数量，并设计迭代算法提升性能与鲁棒性。**

- **链接: [https://arxiv.org/pdf/2601.12485v1](https://arxiv.org/pdf/2601.12485v1)**

> **作者:** Kang Chen; Xianrui Wang; Yichen Yang; Andreas Brendel; Gongping Huang; Zbyněk Koldovský; Jingdong Chen; Jacob Benesty; Shoji Makino
>
> **摘要:** Online blind source separation is essential for both speech communication and human-machine interaction. Among existing approaches, overdetermined independent vector analysis (OverIVA) delivers strong performance by exploiting the statistical independence of source signals and the orthogonality between source and noise subspaces. However, when applied to large microphone arrays, the number of parameters grows rapidly, which can degrade online estimation accuracy. To overcome this challenge, we propose decomposing each long separation filter into a bilinear form of two shorter filters, thereby reducing the number of parameters. Because the two filters are closely coupled, we design an alternating iterative projection algorithm to update them in turn. Simulation results show that, with far fewer parameters, the proposed method achieves improved performance and robustness.
>
---
#### [new 037] Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis
- **分类: cs.SD**

- **简介: 该论文属于呼吸音分析任务，旨在提升哮喘等肺部疾病诊断的准确性。通过引入Transformer模型和多模态架构，提高诊断效果并整合临床信息。**

- **链接: [https://arxiv.org/pdf/2601.14227v1](https://arxiv.org/pdf/2601.14227v1)**

> **作者:** Theodore Aptekarev; Vladimir Sokolovsky; Gregory Furman
>
> **备注:** 7 pages, 4 figures
>
> **摘要:** Respiratory sound analysis is a crucial tool for screening asthma and other pulmonary pathologies, yet traditional auscultation remains subjective and experience-dependent. Our prior research established a CNN baseline using DenseNet201, which demonstrated high sensitivity in classifying respiratory sounds. In this work, we (i) adapt the Audio Spectrogram Transformer (AST) for respiratory sound analysis and (ii) evaluate a multimodal Vision-Language Model (VLM) that integrates spectrograms with structured patient metadata. AST is initialized from publicly available weights and fine-tuned on a medical dataset containing hundreds of recordings per diagnosis. The VLM experiment uses a compact Moondream-type model that processes spectrogram images alongside a structured text prompt (sex, age, recording site) to output a JSON-formatted diagnosis. Results indicate that AST achieves approximately 97% accuracy with an F1-score around 97% and ROC AUC of 0.98 for asthma detection, significantly outperforming both the internal CNN baseline and typical external benchmarks. The VLM reaches 86-87% accuracy, performing comparably to the CNN baseline while demonstrating the capability to integrate clinical context into the inference process. These results confirm the effectiveness of self-attention for acoustic screening and highlight the potential of multimodal architectures for holistic diagnostic tools.
>
---
#### [new 038] AMDM-SE: Attention-based Multichannel Diffusion Model for Speech Enhancement
- **分类: eess.AS**

- **简介: 该论文属于语音增强任务，旨在提升噪声环境下的语音质量。针对多麦克风输入，提出AMDM-SE模型，通过注意力机制增强空间信息，有效降低噪声。**

- **链接: [https://arxiv.org/pdf/2601.13140v1](https://arxiv.org/pdf/2601.13140v1)**

> **作者:** Renana Opochinsky; Sharon Gannot
>
> **摘要:** Diffusion models have recently achieved impressive results in reconstructing images from noisy inputs, and similar ideas have been applied to speech enhancement by treating time-frequency representations as images. With the ubiquity of multi-microphone devices, we extend state-of-the-art diffusion-based methods to exploit multichannel inputs for improved performance. Multichannel diffusion-based enhancement remains in its infancy, with prior work making limited use of advanced mechanisms such as attention for spatial modeling - a gap addressed in this paper. We propose AMDM-SE, an Attention-based Multichannel Diffusion Model for Speech Enhancement, designed specifically for noise reduction. AMDM-SE leverages spatial inter-channel information through a novel cross-channel time-frequency attention block, enabling faithful reconstruction of fine-grained signal details within a generative diffusion framework. On the CHiME-3 benchmark, AMDM-SE outperforms both a single-channel diffusion baseline and a multichannel model without attention, as well as a strong DNN-based predictive method. Simulated-data experiments further underscore the importance of the proposed multichannel attention mechanism. Overall, our results show that incorporating targeted multichannel attention into diffusion models substantially improves noise reduction. While multichannel diffusion-based speech enhancement is still an emerging field, our work contributes a new and complementary approach to the growing body of research in this direction.
>
---
#### [new 039] Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文研究神经音频编解码器的泛化能力，解决其在未见语言和非语音任务中的表现问题。通过控制实验，验证了预训练数据对性能的影响。**

- **链接: [https://arxiv.org/pdf/2601.12205v1](https://arxiv.org/pdf/2601.12205v1)**

> **作者:** Shih-Heng Wang; Jiatong Shi; Jinchuan Tian; Haibin Wu; Shinji Watanabe
>
> **摘要:** This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.
>
---
#### [new 040] VoCodec: An Efficient Lightweight Low-Bitrate Speech Codec
- **分类: eess.AS**

- **简介: 该论文提出VoCodec，一种高效低复杂度的语音编解码模型，解决低比特率语音压缩与实时通信问题。通过轻量级网络提升语音增强能力，实验表明性能优异。**

- **链接: [https://arxiv.org/pdf/2601.13055v1](https://arxiv.org/pdf/2601.13055v1)**

> **作者:** Leyan Yang; Ronghui Hu; Yang Xu; Jing Lu
>
> **摘要:** Recent advancements in end-to-end neural speech codecs enable compressing audio at extremely low bitrates while maintaining high-fidelity reconstruction. Meanwhile, low computational complexity and low latency are crucial for real-time communication. In this paper, we propose VoCodec, a speech codec model featuring a computational complexity of only 349.29M multiply-accumulate operations per second (MACs/s) and a latency of 30 ms. With the competitive vocoder Vocos as its backbone, the proposed model ranked fourth on Track 1 in the 2025 LRAC Challenge and achieved the highest subjective evaluation score (MUSHRA) on the clean speech test set. Additionally, we cascade a lightweight neural network at the front end to extend its capability of speech enhancement. Experimental results demonstrate that the two systems achieve competitive performance across multiple evaluation metrics. Speech samples can be found at https://acceleration123.github.io/.
>
---
#### [new 041] MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting
- **分类: eess.AS; cs.AI**

- **简介: 该论文提出MATE，用于开放词汇关键词检测任务，解决固定短语限制问题。通过多粒度嵌入框架，提升检测效果且无推理开销。**

- **链接: [https://arxiv.org/pdf/2601.14012v1](https://arxiv.org/pdf/2601.14012v1)**

> **作者:** Youngmoon Jung; Myunghun Jung; Joon-Young Yang; Yong-Hyeok Lee; Jaeyoung Roh; Hoon-Young Cho
>
> **备注:** 5 pages, 1 figure, Accepted at ICASSP 2026
>
> **摘要:** Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings ("prefixes"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.
>
---
#### [new 042] Synthetic Singers: A Review of Deep-Learning-based Singing Voice Synthesis Approaches
- **分类: eess.AS**

- **简介: 该论文属于语音合成任务，旨在系统梳理深度学习驱动的歌唱语音合成方法，分析其技术架构与核心问题，提供全面的综述与参考。**

- **链接: [https://arxiv.org/pdf/2601.13910v1](https://arxiv.org/pdf/2601.13910v1)**

> **作者:** Changhao Pan; Dongyu Yao; Yu Zhang; Wenxiang Guo; Jingyu Lu; Zhiyuan Zhu; Zhou Zhao
>
> **备注:** Accepetd by IJCNLP-AACL 2025(Oral)
>
> **摘要:** Recent advances in singing voice synthesis (SVS) have attracted substantial attention from both academia and industry. With the advent of large language models and novel generative paradigms, producing controllable, high-fidelity singing voices has become an attainable goal. Yet the field still lacks a comprehensive survey that systematically analyzes deep-learning-based singing voice synthesis systems and their enabling technologies. To address the aforementioned issue, this survey first categorizes existing systems by task type and then organizes current architectures into two major paradigms: cascaded and end-to-end approaches. Moreover, we provide an in-depth analysis of core technologies, covering singing modeling and control techniques. Finally, we review relevant datasets, annotation tools, and evaluation benchmarks that support training and assessment. In appendix, we introduce training strategies and further discussion of SVS. This survey provides an up-to-date review of the literature on SVS models, which would be a useful reference for both researchers and engineers. Related materials are available at https://github.com/David-Pigeon/SyntheticSingers.
>
---
#### [new 043] Harmonizing the Arabic Audio Space with Data Scheduling
- **分类: cs.SD; cs.AI; cs.CL; eess.AS**

- **简介: 该论文属于语音语言模型任务，旨在解决阿拉伯语语音理解与生成中的多任务适应问题。通过构建数据集并提出训练策略，提升模型在复杂环境下的性能。**

- **链接: [https://arxiv.org/pdf/2601.12494v1](https://arxiv.org/pdf/2601.12494v1)**

> **作者:** Hunzalah Hassan Bhatti; Firoj Alam; Shammur Absar Chowdhury
>
> **备注:** Foundation Models, Large Language Models, Native, Speech Models, Arabic
>
> **摘要:** Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.
>
---
#### [new 044] Ultra-Lightweight Network for Ship-Radiated Sound Classification on Embedded Deployment
- **分类: cs.SD**

- **简介: 该论文提出ShuffleFAC，用于船舶辐射声音分类的轻量级模型，解决嵌入式系统资源受限问题。通过高效卷积结构实现低计算成本与高精度。**

- **链接: [https://arxiv.org/pdf/2601.13679v1](https://arxiv.org/pdf/2601.13679v1)**

> **作者:** Sangwon Park; Dongjun Kim; Sung-Hoon Byun; Sangwook Park
>
> **备注:** This manuscript is under review at IEEE Geoscience and Remote Sensing Letters
>
> **摘要:** This letter presents ShuffleFAC, a lightweight acoustic model for ship-radiated sound classification in resource-constrained maritime monitoring systems. ShuffleFAC integrates Frequency-Aware convolution into an efficiency-oriented backbone using separable convolution, point-wise group convolution, and channel shuffle, enabling frequency-sensitive feature extraction with low computational cost. Experiments on the DeepShip dataset show that ShuffleFAC achieves competitive performance with substantially reduced complexity. In particular, ShuffleFAC ($γ=16$) attains a macro F1-score of 71.45 $\pm$ 1.18% using 39K parameters and 3.06M MACs, and achieves an inference latency of 6.05 $\pm$ 0.95ms on a Raspberry Pi. Compared with MicroNet0, it improves macro F1-score by 1.82 % while reducing model size by 9.7x and latency by 2.5x. These results indicate that ShuffleFAC is suitable for real-time embedded UATR.
>
---
#### [new 045] Embryonic Exposure to VPA Influences Chick Vocalisations: A Computational Study
- **分类: cs.SD**

- **简介: 该论文属于动物行为分析任务，旨在解决传统 vocalisation 分析方法的局限性。研究提出计算框架，自动分析鸡鸣叫声，发现 VPA 暴露对叫声特征的影响。**

- **链接: [https://arxiv.org/pdf/2601.12203v1](https://arxiv.org/pdf/2601.12203v1)**

> **作者:** Antonella M. C. Torrisi; Inês Nolasco; Paola Sgadò; Elisabetta Versace; Emmanouil Benetos
>
> **备注:** Main text (approx. 23 pages including references) with extensive Supplementary Material ( 20 pages) and multiple figures
>
> **摘要:** In young animals like poultry chicks (Gallus gallus), vocalisations convey information about affective and behavioural states. Traditional approaches to vocalisation analysis, relying on manual annotation and predefined categories, introduce biases, limit scalability, and fail to capture the full complexity of vocal repertoires. We introduce a computational framework for the automated detection, acoustic feature extraction, and unsupervised learning of chick vocalisations. Applying this framework to a dataset of newly hatched chicks, we identified two primary vocal clusters. We then tested our computational framework on an independent dataset of chicks exposed during embryonic development to vehicle or Valproic Acid (VPA), a compound that disrupts neural development and is linked to autistic-like symptoms. Clustering analysis on the experimental dataset confirmed two primary vocal clusters and revealed systematic differences between groups. VPA-exposed chicks showed an altered repertoire, with a relative increase in softer calls. VPA differentially affected call clusters, modulating temporal, frequency, and energy domain features. Overall, VPA-exposed chicks produced vocalisations with shorter duration, reduced pitch variability, and modified energy profiles, with the strongest alterations observed in louder calls. This study provides a computational framework for analysing animal vocalisations, advancing knowledge of early-life communication in typical and atypical vocal development.
>
---
#### [new 046] GOMPSNR: Reflourish the Signal-to-Noise Ratio Metric for Audio Generation Tasks
- **分类: cs.SD**

- **简介: 该论文属于音频生成任务，旨在解决SNR度量与人类感知不一致的问题。通过引入相位距离改进SNR，提出GOMPSNR及新型损失函数，提升音频质量评估与模型性能。**

- **链接: [https://arxiv.org/pdf/2601.13758v1](https://arxiv.org/pdf/2601.13758v1)**

> **作者:** Lingling Dai; Andong Li; Cheng Chi; Yifan Liang; Xiaodong Li; Chengshi Zheng
>
> **备注:** Accepted by AAAI 2026
>
> **摘要:** In the field of audio generation, signal-to-noise ratio (SNR) has long served as an objective metric for evaluating audio quality. Nevertheless, recent studies have shown that SNR and its variants are not always highly correlated with human perception, prompting us to raise the questions: Why does SNR fail in measuring audio quality? And how to improve its reliability as an objective metric? In this paper, we identify the inadequate measurement of phase distance as a pivotal factor and propose to reformulate SNR with specially designed phase-distance terms, yielding an improved metric named GOMPSNR. We further extend the newly proposed formulation to derive two novel categories of loss function, corresponding to magnitude-guided phase refinement and joint magnitude-phase optimization, respectively. Besides, extensive experiments are conducted for an optimal combination of different loss functions. Experimental results on advanced neural vocoders demonstrate that our proposed GOMPSNR exhibits more reliable error measurement than SNR. Meanwhile, our proposed loss functions yield substantial improvements in model performance, and our wellchosen combination of different loss functions further optimizes the overall model capability.
>
---
#### [new 047] Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control
- **分类: eess.AS; cs.LG; eess.SP**

- **简介: 该论文属于主动降噪（ANC）任务，解决环境变化下初始性能差的问题。通过元学习联合初始化控制滤波器和次路径模型，提升早期效果与适应速度。**

- **链接: [https://arxiv.org/pdf/2601.13849v1](https://arxiv.org/pdf/2601.13849v1)**

> **作者:** Ziyi Yang; Li Rao; Zhengding Luo; Dongyuan Shi; Qirui Huang; Woon-Seng Gan
>
> **摘要:** Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.
>
---
#### [new 048] Bone-conduction Guided Multimodal Speech Enhancement with Conditional Diffusion Models
- **分类: eess.AS; cs.LG; cs.SD**

- **简介: 该论文属于语音增强任务，旨在解决极端噪声环境下语音质量下降的问题。通过融合骨传导与空气传导信号，使用条件扩散模型提升语音增强效果。**

- **链接: [https://arxiv.org/pdf/2601.12354v1](https://arxiv.org/pdf/2601.12354v1)**

> **作者:** Sina Khanagha; Bunlong Lay; Timo Gerkmann
>
> **备注:** Accepted to IEEE ICASSP 2026
>
> **摘要:** Single-channel speech enhancement models face significant performance degradation in extremely noisy environments. While prior work has shown that complementary bone-conducted speech can guide enhancement, effective integration of this noise-immune modality remains a challenge. This paper introduces a novel multimodal speech enhancement framework that integrates bone-conduction sensors with air-conducted microphones using a conditional diffusion model. Our proposed model significantly outperforms previously established multimodal techniques and a powerful diffusion-based single-modal baseline across a wide range of acoustic conditions.
>
---
#### [new 049] DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification
- **分类: eess.AS; cs.AI**

- **简介: 该论文属于说话人验证任务，解决短语音验证困难的问题。提出DAME框架，通过多尺度嵌入捕捉不同长度语音中的说话人特征，提升短语音性能且无额外开销。**

- **链接: [https://arxiv.org/pdf/2601.13999v1](https://arxiv.org/pdf/2601.13999v1)**

> **作者:** Youngmoon Jung; Joon-Young Yang; Ju-ho Kim; Jaeyoung Roh; Chang Woo Han; Hoon-Young Cho
>
> **备注:** 5 pages, 2 figures, Accepted at ICASSP 2026
>
> **摘要:** Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.
>
---
#### [new 050] AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering
- **分类: eess.AS; cs.AI; cs.CL; cs.LG; cs.SD**

- **简介: 该论文提出AQUA-Bench，用于评估音频问答中的不可回答性问题。任务为音频问答，解决现有基准忽略不可回答问题的缺陷，通过三个场景测试模型可靠性。**

- **链接: [https://arxiv.org/pdf/2601.12248v1](https://arxiv.org/pdf/2601.12248v1)**

> **作者:** Chun-Yi Kuan; Hung-yi Lee
>
> **备注:** Accepted to ICASSP 2026. Project Website: https://kuan2jiu99.github.io/AQUA-Bench-demo/
>
> **摘要:** Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. Such cases are common in real-world settings, where questions may be misleading, ill-posed, or incompatible with the information. To address this gap, we present AQUA-Bench, a benchmark for Audio Question Unanswerability Assessment. It systematically evaluates three scenarios: Absent Answer Detection (the correct option is missing), Incompatible Answer Set Detection (choices are categorically mismatched with the question), and Incompatible Audio Question Detection (the question is irrelevant or lacks sufficient grounding in the audio). By assessing these cases, AQUA-Bench offers a rigorous measure of model reliability and promotes the development of audio-language systems that are more robust and trustworthy. Our experiments suggest that while models excel on standard answerable tasks, they often face notable challenges with unanswerable ones, pointing to a blind spot in current audio-language understanding.
>
---
#### [new 051] Lightweight Prompt Biasing for Contextualized End-to-End ASR Systems
- **分类: eess.AS; cs.CL**

- **简介: 该论文属于语音识别任务，旨在提升端到端ASR系统对罕见实体的识别准确率。通过引入轻量级提示偏置技术，结合多任务学习框架，有效提高实体识别效果。**

- **链接: [https://arxiv.org/pdf/2506.06252v2](https://arxiv.org/pdf/2506.06252v2)**

> **作者:** Bo Ren; Yu Shi; Jinyu Li
>
> **摘要:** End-to-End Automatic Speech Recognition (ASR) has advanced significantly yet still struggles with rare and domain-specific entities. This paper introduces a simple yet efficient prompt-based biasing technique for contextualized ASR, enhancing recognition accuracy by leverage a unified multitask learning framework. The approach comprises two key components: a prompt biasing model which is trained to determine when to focus on entities in prompt, and a entity filtering mechanism which efficiently filters out irrelevant entities. Our method significantly enhances ASR accuracy on entities, achieving a relative 30.7% and 18.0% reduction in Entity Word Error Rate compared to the baseline model with shallow fusion on in-house domain dataset with small and large entity lists, respectively. The primary advantage of this method lies in its efficiency and simplicity without any structure change, making it lightweight and highly efficient.
>
---
#### [new 052] A Similarity Network for Correlating Musical Structure to Military Strategy
- **分类: cs.SD**

- **简介: 该论文属于跨学科研究任务，旨在通过音乐结构与军事战略的相似性分析，探索音乐感知与美学教育的新视角。**

- **链接: [https://arxiv.org/pdf/2601.12314v1](https://arxiv.org/pdf/2601.12314v1)**

> **作者:** Yiwen Zhang; Hui Zhang; Fanqin Meng
>
> **备注:** This paper was completed in 2024
>
> **摘要:** Music perception, a multi-sensory process based on the synesthesia effect, is an essential component of music aesthetic education. Understanding music structure helps both perception and aesthetic education. Music structure incorporates a range of information, the coordination of which forms the melody, just as different military actions cooperate to produce a military strategy. However, there are a few ways for assessing music perception from the perspectives of system operation and information management. In this paper, we explore the similarities between music structure and military strategy while creating the Music Clips Correlation Network (MCCN) based on Mel-frequency Cepstral Coefficients (MFCCs). The inspiration comes from the comparison between a concert conductor's musical score and a military war commander's sand table exercise. Specifically, we create MCCNs for various kinds of war movie soundtracks, then relate military tactics (Sun Tzu's Art of War, etc.) and political institutions to military operations networks. Our primary findings suggest a few similarities, implying that music perception and aesthetic education can be approached from a military strategy and management perspective through this interdisciplinary research. Similarly, we can discover similarities between the art of military scheming and the art of musical structure based on network analysis in order to facilitate the understanding of the relationship between technology and art.
>
---
#### [new 053] Adaptive Speaker Embedding Self-Augmentation for Personal Voice Activity Detection with Short Enrollment Speech
- **分类: eess.AS**

- **简介: 该论文属于语音活动检测任务，解决短时注册语音导致的说话人嵌入质量低的问题。通过自增强策略和长期适应方法提升检测性能。**

- **链接: [https://arxiv.org/pdf/2601.12769v1](https://arxiv.org/pdf/2601.12769v1)**

> **作者:** Fuyuan Feng; Wenbin Zhang; Yu Gao; Longting Xu; Xiaofeng Mou; Yi Xu
>
> **备注:** Accepted by ICASSP 2026
>
> **摘要:** Personal Voice Activity Detection (PVAD) is crucial for identifying target speaker segments in the mixture, yet its performance heavily depends on the quality of speaker embeddings. A key practical limitation is the short enrollment speech--such as a wake-up word--which provides limited cues. This paper proposes a novel adaptive speaker embedding self-augmentation strategy that enhances PVAD performance by augmenting the original enrollment embeddings through additive fusion of keyframe embeddings extracted from mixed speech. Furthermore, we introduce a long-term adaptation strategy to iteratively refine embeddings during detection, mitigating speaker temporal variability. Experiments show significant gains in recall, precision, and F1-score under short enrollment conditions, matching full-length enrollment performance after five iterative updates. The source code is available at https://anonymous.4open.science/r/ASE-PVAD-E5D6 .
>
---
#### [new 054] MuseAgent-1: Interactive Grounded Multimodal Understanding of Music Scores and Performance Audio
- **分类: cs.MM; cs.SD; eess.AS**

- **简介: 该论文属于音乐理解任务，旨在解决现有模型对音乐符号和音频理解不足的问题。通过构建MuseAgent，结合乐谱与音频的结构化表示，提升音乐内容的多模态交互能力。**

- **链接: [https://arxiv.org/pdf/2601.11968v1](https://arxiv.org/pdf/2601.11968v1)**

> **作者:** Qihao Zhao; Yunqi Cao; Yangyu Huang; Hui Yi Leong; Fan Zhang; Kim-Hui Yap; Wei Hu
>
> **备注:** Tech Report
>
> **摘要:** Despite recent advances in multimodal large language models (MLLMs), their ability to understand and interact with music remains limited. Music understanding requires grounded reasoning over symbolic scores and expressive performance audio, which general-purpose MLLMs often fail to handle due to insufficient perceptual grounding. We introduce MuseAgent, a music-centric multimodal agent that augments language models with structured symbolic representations derived from sheet music images and performance audio. By integrating optical music recognition and automatic music transcription modules, MuseAgent enables multi-step reasoning and interaction over fine-grained musical content. To systematically evaluate music understanding capabilities, we further propose MuseBench, a benchmark covering music theory reasoning, score interpretation, and performance-level analysis across text, image, and audio modalities. Experiments show that existing MLLMs perform poorly on these tasks, while MuseAgent achieves substantial improvements, highlighting the importance of structured multimodal grounding for interactive music understanding.
>
---
#### [new 055] Context and Transcripts Improve Detection of Deepfake Audios of Public Figures
- **分类: cs.AI; cs.SD**

- **简介: 该论文属于音频深度伪造检测任务，旨在提升检测效果。通过引入上下文和字幕信息，提出CADD模型，有效提高检测准确性和鲁棒性。**

- **链接: [https://arxiv.org/pdf/2601.13464v1](https://arxiv.org/pdf/2601.13464v1)**

> **作者:** Chongyang Gao; Marco Postiglione; Julian Baldwin; Natalia Denisenko; Isabel Gortner; Luke Fosdick; Chiara Pulice; Sarit Kraus; V. S. Subrahmanian
>
> **摘要:** Humans use context to assess the veracity of information. However, current audio deepfake detectors only analyze the audio file without considering either context or transcripts. We create and analyze a Journalist-provided Deepfake Dataset (JDD) of 255 public deepfakes which were primarily contributed by over 70 journalists since early 2024. We also generate a synthetic audio dataset (SYN) of dead public figures and propose a novel Context-based Audio Deepfake Detector (CADD) architecture. In addition, we evaluate performance on two large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or the transcript can significantly improve the efficacy of audio deepfake detectors. Performance (measured via F1 score, AUC, and EER) of multiple baseline audio deepfake detectors and traditional classifiers can be improved by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We additionally show that CADD, via its use of context and/or transcripts, is more robust to 5 adversarial evasion strategies, limiting performance degradation to an average of just -0.71% across all experiments. Code, models, and datasets are available at our project page: https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection (access restricted during review).
>
---
#### [new 056] Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis
- **分类: cs.CL; cs.SD; eess.AS**

- **简介: 该论文属于多方言阿拉伯语语音合成任务，旨在解决统一建模和数据不足的问题。通过构建Habibi模型，利用开源数据提升生成质量并建立基准。**

- **链接: [https://arxiv.org/pdf/2601.13802v1](https://arxiv.org/pdf/2601.13802v1)**

> **作者:** Yushen Chen; Junzhe Liu; Yujie Tu; Zhikang Niu; Yuzhe Liang; Kai Yu; Chunyu Qiang; Chen Zhang; Xie Chen
>
> **摘要:** A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .
>
---
#### [new 057] VidTune: Creating Video Soundtracks with Generative Music and Contextual Thumbnails
- **分类: cs.HC; cs.MM; cs.SD; eess.AS**

- **简介: 论文提出VidTune系统，解决视频配乐匹配难度大的问题。通过生成音乐和上下文缩略图，帮助创作者高效选择合适配乐。**

- **链接: [https://arxiv.org/pdf/2601.12180v1](https://arxiv.org/pdf/2601.12180v1)**

> **作者:** Mina Huh; Ailie C. Fraser; Dingzeyu Li; Mira Dontcheva; Bryan Wang
>
> **备注:** Accepted to CHI 2026
>
> **摘要:** Music shapes the tone of videos, yet creators often struggle to find soundtracks that match their video's mood and narrative. Recent text-to-music models let creators generate music from text prompts, but our formative study (N=8) shows creators struggle to construct diverse prompts, quickly review and compare tracks, and understand their impact on the video. We present VidTune, a system that supports soundtrack creation by generating diverse music options from a creator's prompt and producing contextual thumbnails for rapid review. VidTune extracts representative video subjects to ground thumbnails in context, maps each track's valence and energy onto visual cues like color and brightness, and depicts prominent genres and instruments. Creators can refine tracks through natural language edits, which VidTune expands into new generations. In a controlled user study (N=12) and an exploratory case study (N=6), participants found VidTune helpful for efficiently reviewing and comparing music options and described the process as playful and enriching.
>
---
#### [new 058] On the Relation of State Space Models and Hidden Markov Models
- **分类: cs.LG; cs.CL; eess.AS; eess.SY**

- **简介: 该论文属于模型比较任务，旨在厘清状态空间模型与隐马尔可夫模型的关系，分析其结构与学习方法的异同。**

- **链接: [https://arxiv.org/pdf/2601.13357v1](https://arxiv.org/pdf/2601.13357v1)**

> **作者:** Aydin Ghojogh; M. Hadi Sepanj; Benyamin Ghojogh
>
> **摘要:** State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational frameworks for modeling sequential data with latent variables and are widely used in signal processing, control theory, and machine learning. Despite their shared temporal structure, they differ fundamentally in the nature of their latent states, probabilistic assumptions, inference procedures, and training paradigms. Recently, deterministic state space models have re-emerged in natural language processing through architectures such as S4 and Mamba, raising new questions about the relationship between classical probabilistic SSMs, HMMs, and modern neural sequence models. In this paper, we present a unified and systematic comparison of HMMs, linear Gaussian state space models, Kalman filtering, and contemporary NLP state space models. We analyze their formulations through the lens of probabilistic graphical models, examine their inference algorithms -- including forward-backward inference and Kalman filtering -- and contrast their learning procedures via Expectation-Maximization and gradient-based optimization. By highlighting both structural similarities and semantic differences, we clarify when these models are equivalent, when they fundamentally diverge, and how modern NLP SSMs relate to classical probabilistic models. Our analysis bridges perspectives from control theory, probabilistic modeling, and modern deep learning.
>
---
#### [new 059] CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration
- **分类: cs.LG; cs.AI; cs.CL; cs.SD; eess.AS**

- **简介: 该论文提出CSyMR-Bench，一个用于评估符号音乐推理的基准数据集，解决传统基准缺乏综合推理的问题。通过整合音乐分析工具，提升模型在复杂音乐问题上的表现。**

- **链接: [https://arxiv.org/pdf/2601.11556v1](https://arxiv.org/pdf/2601.11556v1)**

> **作者:** Boyang Wang; Yash Vishe; Xin Xu; Zachary Novack; Julian McAuley; Junda Wu
>
> **摘要:** Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.
>
---
#### [new 060] Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification
- **分类: cs.AI; cs.SD**

- **简介: 该论文属于情感驱动的内容生成任务，旨在实时生成安全、适合年龄的响应内容。通过多智能体系统实现情绪识别、策略决策、参数生成与安全验证，解决情感到响应的转化问题。**

- **链接: [https://arxiv.org/pdf/2601.13589v1](https://arxiv.org/pdf/2601.13589v1)**

> **作者:** HyeYoung Lee
>
> **摘要:** This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.
>
---
#### [new 061] The Third VoicePrivacy Challenge: Preserving Emotional Expressiveness and Linguistic Content in Voice Anonymization
- **分类: cs.CL; cs.SD; eess.AS**

- **简介: 该论文属于语音匿名化任务，旨在保护说话人身份的同时保留语言内容和情感表达。工作包括分析挑战框架、评估系统性能并总结创新方法。**

- **链接: [https://arxiv.org/pdf/2601.11846v1](https://arxiv.org/pdf/2601.11846v1)**

> **作者:** Natalia Tomashenko; Xiaoxiao Miao; Pierre Champion; Sarina Meyer; Michele Panariello; Xin Wang; Nicholas Evans; Emmanuel Vincent; Junichi Yamagishi; Massimiliano Todisco
>
> **备注:** under review
>
> **摘要:** We present results and analyses from the third VoicePrivacy Challenge held in 2024, which focuses on advancing voice anonymization technologies. The task was to develop a voice anonymization system for speech data that conceals a speaker's voice identity while preserving linguistic content and emotional state. We provide a systematic overview of the challenge framework, including detailed descriptions of the anonymization task and datasets used for both system development and evaluation. We outline the attack model and objective evaluation metrics for assessing privacy protection (concealing speaker voice identity) and utility (content and emotional state preservation). We describe six baseline anonymization systems and summarize the innovative approaches developed by challenge participants. Finally, we provide key insights and observations to guide the design of future VoicePrivacy challenges and identify promising directions for voice anonymization research.
>
---
#### [new 062] Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs
- **分类: cs.MM; cs.AI; cs.IR; cs.LG; cs.SD**

- **简介: 该论文属于跨模态嵌入学习任务，旨在解决音频-视觉信号对齐中的误判问题。通过引入软标签和潜在交互图，提升嵌入的鲁棒性和语义一致性。**

- **链接: [https://arxiv.org/pdf/2601.11995v1](https://arxiv.org/pdf/2601.11995v1)**

> **作者:** Donghuo Zeng; Hao Niu; Yanan Wang; Masato Taya
>
> **备注:** 16 pages, 5 figures, 2 tables
>
> **摘要:** Learning robust audio-visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences - background noise, unrelated elements, or unannotated events. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. For example, a video labeled "train" might also contain motorcycle audio and visual, because "motorcycle" is not the chosen annotation; standard methods treat these co-occurrences as negatives to true motorcycle anchors elsewhere, creating false negatives and missing true cross-modal dependencies. We propose a framework that leverages soft-label predictions and inferred latent interactions to address these issues: (1) Audio-Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to produce aligned soft-label distributions across modalities, assigning nonzero probability to co-occurring but unannotated events and enriching the supervision signal. (2) Inferred Latent Interaction Graph (ILI) applies the GRaSP algorithm to teacher soft labels to infer a sparse, directed dependency graph among classes. This graph highlights directional dependencies (e.g., "Train (visual)" -> "Motorcycle (audio)") that expose likely semantic or conditional relationships between classes; these are interpreted as estimated dependency patterns. (3) Latent Interaction Regularizer (LIR): A student network is trained with both metric loss and a regularizer guided by the ILI graph, pulling together embeddings of dependency-linked but unlabeled pairs in proportion to their soft-label probabilities. Experiments on AVE and VEGAS benchmarks show consistent improvements in mean average precision (mAP), demonstrating that integrating inferred latent interactions into embedding learning enhances robustness and semantic coherence.
>
---
#### [new 063] Sound2Hap: Learning Audio-to-Vibrotactile Haptic Generation from Human Ratings
- **分类: cs.HC; cs.SD; eess.AS**

- **简介: 该论文属于音频到触觉生成任务，旨在解决现有方法无法有效泛化于多样化环境声音的问题。通过用户研究和深度学习模型Sound2Hap，提升音频驱动触觉的感知一致性与体验质量。**

- **链接: [https://arxiv.org/pdf/2601.12245v1](https://arxiv.org/pdf/2601.12245v1)**

> **作者:** Yinan Li; Hasti Seifi
>
> **摘要:** Environmental sounds like footsteps, keyboard typing, or dog barking carry rich information and emotional context, making them valuable for designing haptics in user applications. Existing audio-to-vibration methods, however, rely on signal-processing rules tuned for music or games and often fail to generalize across diverse sounds. To address this, we first investigated user perception of four existing audio-to-haptic algorithms, then created a data-driven model for environmental sounds. In Study 1, 34 participants rated vibrations generated by the four algorithms for 1,000 sounds, revealing no consistent algorithm preferences. Using this dataset, we trained Sound2Hap, a CNN-based autoencoder, to generate perceptually meaningful vibrations from diverse sounds with low latency. In Study 2, 15 participants rated its output higher than signal-processing baselines on both audio-vibration match and Haptic Experience Index (HXI), finding it more harmonious with diverse sounds. This work demonstrates a perceptually validated approach to audio-haptic translation, broadening the reach of sound-driven haptics.
>
---
#### [new 064] PRiSM: Benchmarking Phone Realization in Speech Models
- **分类: cs.CL; cs.SD**

- **简介: 该论文属于语音模型中的发音识别任务，旨在解决现有评估仅关注表面准确率的问题。通过PRiSM基准测试，分析模型在不同语言环境下的表现及下游应用效果。**

- **链接: [https://arxiv.org/pdf/2601.14046v1](https://arxiv.org/pdf/2601.14046v1)**

> **作者:** Shikhar Bharadwaj; Chin-Jou Li; Yoonjae Kim; Kwanghee Choi; Eunjung Yeo; Ryan Soh-Eun Shim; Hanyu Zhou; Brendon Boldt; Karen Rosero Jacome; Kalvin Chang; Darsh Agrawal; Keer Xu; Chao-Han Huck Yang; Jian Zhu; Shinji Watanabe; David R. Mortensen
>
> **摘要:** Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.
>
---
## 更新

#### [replaced 001] Event2Audio: Event-Based Optical Vibration Sensing
- **分类: eess.IV; cs.CV; eess.AS**

- **简介: 该论文属于音频恢复任务，解决从视频中提取声音的问题。通过事件相机提升振动感知，实现快速、高质量的音频重建。**

- **链接: [https://arxiv.org/pdf/2507.03273v2](https://arxiv.org/pdf/2507.03273v2)**

> **作者:** Mingxuan Cai; Dekel Galor; Amit Pal Singh Kohli; Jacob L. Yates; Laura Waller
>
> **备注:** 14 pages, 13 figures
>
> **摘要:** Small vibrations observed in video can unveil information beyond what is visual, such as sound and material properties. It is possible to passively record these vibrations when they are visually perceptible, or actively amplify their visual contribution with a laser beam when they are not perceptible. In this paper, we improve upon the active sensing approach by leveraging event-based cameras, which are designed to efficiently capture fast motion. We demonstrate our method experimentally by recovering audio from vibrations, even for multiple simultaneous sources, and in the presence of environmental distortions. Our approach matches the state-of-the-art reconstruction quality at much faster speeds, approaching real-time processing.
>
---
#### [replaced 002] QASTAnet: A DNN-based Quality Metric for Spatial Audio
- **分类: eess.AS; cs.LG**

- **简介: 论文提出QASTAnet，一种基于深度神经网络的立体声音频质量评估模型，解决现有方法在真实信号上泛化能力差的问题。通过结合听觉系统建模与神经网络，提升质量预测准确性。**

- **链接: [https://arxiv.org/pdf/2509.16715v2](https://arxiv.org/pdf/2509.16715v2)**

> **作者:** Adrien Llave; Emma Granier; Grégory Pallone
>
> **备注:** Minor typo corrections, figure updates for clarity, add two references, no modifications of the results and their interpretation
>
> **摘要:** In the development of spatial audio technologies, reliable and shared methods for evaluating audio quality are essential. Listening tests are currently the standard but remain costly in terms of time and resources. Several models predicting subjective scores have been proposed, but they do not generalize well to real-world signals. In this paper, we propose QASTAnet (Quality Assessment for SpaTial Audio network), a new metric based on a deep neural network, specialized on spatial audio (ambisonics and binaural). As training data is scarce, we aim for the model to be trainable with a small amount of data. To do so, we propose to rely on expert modeling of the low-level auditory system and use a neurnal network to model the high-level cognitive function of the quality judgement. We compare its performance to two reference metrics on a wide range of content types (speech, music, ambiance, anechoic, reverberated) and focusing on codec artifacts. Results demonstrate that QASTAnet overcomes the aforementioned limitations of the existing methods. The strong correlation between the proposed metric prediction and subjective scores makes it a good candidate for comparing codecs in their development.
>
---
#### [replaced 003] K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function
- **分类: cs.CL; cs.AI; cs.SD; eess.AS**

- **简介: 该论文提出K-Function，用于评估儿童语言能力。解决自动语音识别中儿童语音识别困难的问题，结合子词转录与LLM评分，提升评估准确性。**

- **链接: [https://arxiv.org/pdf/2507.03043v2](https://arxiv.org/pdf/2507.03043v2)**

> **作者:** Shuhe Li; Chenxu Guo; Jiachen Lian; Cheol Jun Cho; Wenshuo Zhao; Xiner Xu; Ruiyu Jin; Xiaoyu Shi; Xuanru Zhou; Dingkun Zhou; Sam Wang; Grace Wang; Jingze Yang; Jingyi Xu; Ruohan Bao; Xingrui Chen; Elise Brenner; Brandon In; Francesca Pei; Maria Luisa Gorno-Tempini; Gopala Anumanchipalli
>
> **摘要:** Evaluating young children's language is challenging for automatic speech recognizers due to high-pitched voices, prolonged sounds, and limited data. We introduce K-Function, a framework that combines accurate sub-word transcription with objective, Large Language Model (LLM)-driven scoring. Its core, Kids-Weighted Finite State Transducer (K-WFST), merges an acoustic phoneme encoder with a phoneme-similarity model to capture child-specific speech errors while remaining fully interpretable. K-WFST achieves a 1.39 % phoneme error rate on MyST and 8.61 % on Multitudes-an absolute improvement of 10.47 % and 7.06 % over a greedy-search decoder. These high-quality transcripts are used by an LLM to grade verbal skills, developmental milestones, reading, and comprehension, with results that align closely with human evaluators. Our findings show that precise phoneme recognition is essential for creating an effective assessment framework, enabling scalable language screening for children.
>
---
#### [replaced 004] Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens
- **分类: eess.AS; cs.LG; cs.SD**

- **简介: 该论文属于语音合成评估任务，旨在解决现有指标与人类感知关联弱的问题。提出TTScore框架，通过条件预测离散标记，分别评估可懂性和韵律性。**

- **链接: [https://arxiv.org/pdf/2509.20485v2](https://arxiv.org/pdf/2509.20485v2)**

> **作者:** Ismail Rasim Ulgen; Zongyang Du; Junchen Lu; Philipp Koehn; Berrak Sisman
>
> **备注:** Accepted in IEEE Open Journal of Signal Processing, 2026
>
> **摘要:** Objective evaluation of synthesized speech is critical for advancing speech generation systems, yet existing metrics for intelligibility and prosody remain limited in scope and weakly correlated with human perception. Word Error Rate (WER) provides only a coarse text-based measure of intelligibility, while F0-RMSE and related pitch-based metrics offer a narrow, reference-dependent view of prosody. To address these limitations, we propose TTScore, a targeted and reference-free evaluation framework based on conditional prediction of discrete speech tokens. TTScore employs two sequence-to-sequence predictors conditioned on input text: TTScore-int, which measures intelligibility through content tokens, and TTScore-pro, which evaluates prosody through prosody tokens. For each synthesized utterance, the predictors compute the likelihood of the corresponding token sequences, yielding interpretable scores that capture alignment with intended linguistic content and prosodic structure. Experiments on the SOMOS, VoiceMOS, and TTSArena benchmarks demonstrate that TTScore-int and TTScore-pro provide reliable, aspect-specific evaluation and achieve stronger correlations with human judgments of overall quality than existing intelligibility and prosody-focused metrics.
>
---
#### [replaced 005] Fun-Audio-Chat Technical Report
- **分类: cs.CL; cs.AI; cs.SD; eess.AS**

- **简介: 该论文提出Fun-Audio-Chat，解决语音与文本模型间的时间分辨率不匹配及知识遗忘问题，通过双分辨率语音表示和核心鸡尾酒训练提升音频理解与生成能力。**

- **链接: [https://arxiv.org/pdf/2512.20156v4](https://arxiv.org/pdf/2512.20156v4)**

> **作者:** Tongyi Fun Team; Qian Chen; Luyao Cheng; Chong Deng; Xiangang Li; Jiaqing Liu; Chao-Hong Tan; Wen Wang; Junhao Xu; Jieping Ye; Qinglin Zhang; Qiquan Zhang; Jingren Zhou
>
> **备注:** Authors are listed in alphabetical order, 21 pages, open-source at https://github.com/FunAudioLLM/Fun-Audio-Chat
>
> **摘要:** Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo, at https://github.com/FunAudioLLM/Fun-Audio-Chat .
>
---
#### [replaced 006] Emotional Dimension Control in Language Model-Based Text-to-Speech: Spanning a Broad Spectrum of Human Emotions
- **分类: eess.AS; cs.CL; cs.SD**

- **简介: 该论文属于情感文本转语音任务，旨在解决情感表达范围有限的问题。通过引入PAD维度控制，实现更丰富的情感合成，提升语音自然度与多样性。**

- **链接: [https://arxiv.org/pdf/2409.16681v3](https://arxiv.org/pdf/2409.16681v3)**

> **作者:** Kun Zhou; You Zhang; Dianwen Ng; Shengkui Zhao; Hao Wang; Bin Ma
>
> **备注:** ICASSP 2026
>
> **摘要:** Emotional text-to-speech (TTS) systems sturggle to capture the full spectrum of human emotions due to the inherent complexity of emotional expressions and the limited coverage of existing emotion labels. To address this, we propose a language model-based TTS framework that synthesizes speech across a broad range of emotional styles. Our approach enables flexible user control along three continuous dimensions - pleasure, arousal, and dominance (PAD). To enable this, we train an emotional dimension predictor that maps categorical emotion labels in speech datasets into the PAD space, grounded in established psychological research. Importantly, while the emotional dimension predictor leverages categorical labels, the TTS framework itself does not require explict emotion labels during training. Objective and subjective evaluations demonstrate that our framework effectively generates more expressive emotional styles and enhances both naturalness and diversity compared to baselines.
>
---
#### [replaced 007] ESDD2: Environment-Aware Speech and Sound Deepfake Detection Challenge Evaluation Plan
- **分类: cs.SD**

- **简介: 该论文属于语音与声音深度伪造检测任务，旨在解决真实环境中组件级伪造检测难题。通过构建数据集和联合学习框架，提出ESDD2挑战赛以提升检测能力。**

- **链接: [https://arxiv.org/pdf/2601.07303v3](https://arxiv.org/pdf/2601.07303v3)**

> **作者:** Xueping Zhang; Han Yin; Yang Xiao; Lin Zhang; Ting Dang; Rohan Kumar Das; Ming Li
>
> **摘要:** Audio recorded in real-world environments often contains a mixture of foreground speech and background environmental sounds. With rapid advances in text-to-speech, voice conversion, and other generation models, either component can now be modified independently. Such component-level manipulations are harder to detect, as the remaining unaltered component can mislead the systems designed for whole deepfake audio, and they often sound more natural to human listeners. To address this gap, we have proposed CompSpoofV2 dataset and a separation-enhanced joint learning framework. CompSpoofV2 is a large-scale curated dataset designed for component-level audio anti-spoofing, which contains over 250k audio samples, with a total duration of approximately 283 hours. Based on the CompSpoofV2 and the separation-enhanced joint learning framework, we launch the Environment-Aware Speech and Sound Deepfake Detection Challenge (ESDD2), focusing on component-level spoofing, where both speech and environmental sounds may be manipulated or synthesized, creating a more challenging and realistic detection scenario. The challenge will be held in conjunction with the IEEE International Conference on Multimedia and Expo 2026 (ICME 2026).
>
---
#### [replaced 008] Super Monotonic Alignment Search
- **分类: eess.AS; cs.AI**

- **简介: 该论文属于文本到语音任务，解决MAS算法效率低的问题，通过GPU加速实现更高效的对齐搜索。**

- **链接: [https://arxiv.org/pdf/2409.07704v2](https://arxiv.org/pdf/2409.07704v2)**

> **作者:** Junhyeok Lee; Hyeongju Kim
>
> **备注:** Accepted to ICASSP 2026
>
> **摘要:** Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most popular algorithm in text-to-speech to estimate unknown alignments between text and speech. Since this algorithm needs to search for the most probable alignment with dynamic programming by caching all possible paths, the time complexity of the algorithm is $O(T \times S)$, where $T$ is the length of text and $S$ is the length of speech representation. The authors of Glow-TTS run this algorithm on CPU, and while they mentioned it is difficult to parallelize, we found that MAS can be parallelized in text length dimension and CPU execution consumes an inordinate amount of time for inter-device copy. Therefore, we implemented a Triton kernel and PyTorch JIT script to accelerate MAS on GPU without inter-device copy. As a result, Super-MAS Triton kernel is up to 72 times faster in the extreme-length case. The code is available at https://github.com/supertone-inc/super-monotonic-align.
>
---
#### [replaced 009] XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark
- **分类: cs.SD; cs.AI; cs.CL; cs.LG; eess.AS**

- **简介: 该论文属于音频深度伪造检测任务，旨在解决现有检测方法在跨领域场景下性能下降的问题。作者构建了XMAD-Bench基准，用于评估模型在不同语言、说话人和生成方法下的泛化能力。**

- **链接: [https://arxiv.org/pdf/2506.00462v2](https://arxiv.org/pdf/2506.00462v2)**

> **作者:** Ioan-Paul Ciobanu; Andrei-Iulian Hiji; Nicolae-Catalin Ristea; Paul Irofti; Cristian Rusu; Radu Tudor Ionescu
>
> **备注:** Accepted at EACL 2026
>
> **摘要:** Recent advances in audio generation led to an increasing number of deepfakes, making the general public more vulnerable to financial scams, identity theft, and misinformation. Audio deepfake detectors promise to alleviate this issue, with many recent studies reporting accuracy rates close to 99%. However, these methods are typically tested in an in-domain setup, where the deepfake samples from the training and test sets are produced by the same generative models. To this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In our novel dataset, the speakers, the generative methods, and the real audio sources are distinct across training and test splits. This leads to a challenging cross-domain evaluation setup, where audio deepfake detectors can be tested "in the wild". Our in-domain and cross-domain experiments indicate a clear disparity between the in-domain performance of deepfake detectors, which is usually as high as 100%, and the cross-domain performance of the same models, which is sometimes similar to random chance. Our benchmark highlights the need for the development of robust audio deepfake detectors, which maintain their generalization capacity across different languages, speakers, generative methods, and data sources. Our benchmark is publicly released at https://github.com/ristea/xmad-bench/.
>
---
#### [replaced 010] VoiceSculptor: Your Voice, Designed By You
- **分类: eess.AS**

- **简介: 该论文提出VoiceSculptor，解决TTS中缺乏细粒度语音控制的问题。整合指令设计与高保真克隆，实现语音属性的精准控制与迭代优化。**

- **链接: [https://arxiv.org/pdf/2601.10629v2](https://arxiv.org/pdf/2601.10629v2)**

> **作者:** Jingbin Hu; Huakang Chen; Linhan Ma; Dake Guo; Qirui Zhan; Wenhao Li; Haoyu Zhang; Kangxiang Xia; Ziyu Zhang; Wenjie Tian; Chengyou Wang; Jinrui Liang; Shuhan Guo; Zihang Yang; Bengu Wu; Binbin Zhang; Pengcheng Zhu; Pengyuan Xie; Chuan Xie; Qiang Zhang; Jie Liu; Lei Xie
>
> **备注:** 13 pages, 4 figures
>
> **摘要:** Despite rapid progress in text-to-speech (TTS), open-source systems still lack truly instruction-following, fine-grained control over core speech attributes (e.g., pitch, speaking rate, age, emotion, and style). We present VoiceSculptor, an open-source unified system that bridges this gap by integrating instruction-based voice design and high-fidelity voice cloning in a single framework. It generates controllable speaker timbre directly from natural-language descriptions, supports iterative refinement via Retrieval-Augmented Generation (RAG), and provides attribute-level edits across multiple dimensions. The designed voice is then rendered into a prompt waveform and fed into a cloning model to enable high-fidelity timbre transfer for downstream speech synthesis. VoiceSculptor achieves open-source state-of-the-art (SOTA) on InstructTTSEval-Zh, and is fully open-sourced, including code and pretrained models, to advance reproducible instruction-controlled TTS research.
>
---
#### [replaced 011] DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis
- **分类: eess.AS; cs.SD**

- **简介: 该论文属于文本到语音合成任务，旨在实现环境感知的个性化语音生成。通过分离语音与环境信息，并进行音频补全，提升语音自然度与环境匹配度。**

- **链接: [https://arxiv.org/pdf/2509.14684v2](https://arxiv.org/pdf/2509.14684v2)**

> **作者:** Ye-Xin Lu; Yu Gu; Kun Wei; Hui-Peng Du; Yang Ai; Zhen-Hua Ling
>
> **备注:** Accepted by ICASSP 2026
>
> **摘要:** This paper presents DAIEN-TTS, a zero-shot text-to-speech (TTS) framework that enables ENvironment-aware synthesis through Disentangled Audio Infilling. By leveraging separate speaker and environment prompts, DAIEN-TTS allows independent control over the timbre and the background environment of the synthesized speech. Built upon F5-TTS, the proposed DAIEN-TTS first incorporates a pretrained speech-environment separation (SES) module to disentangle the environmental speech into mel-spectrograms of clean speech and environment audio. Two random span masks of varying lengths are then applied to both mel-spectrograms, which, together with the text embedding, serve as conditions for infilling the masked environmental mel-spectrogram, enabling the simultaneous continuation of personalized speech and time-varying environmental audio. To further enhance controllability during inference, we adopt dual classifier-free guidance (DCFG) for the speech and environment components and introduce a signal-to-noise ratio (SNR) adaptation strategy to align the synthesized speech with the environment prompt. Experimental results demonstrate that DAIEN-TTS generates environmental personalized speech with high naturalness, strong speaker similarity, and high environmental fidelity.
>
---
#### [replaced 012] AnyRIR: Robust Non-intrusive Room Impulse Response Estimation in the Wild
- **分类: eess.AS**

- **简介: 该论文属于声学环境中的房间脉冲响应（RIR）估计任务，旨在解决噪声和非平稳干扰下的RIR估计问题。工作包括提出AnyRIR方法，利用音乐信号和L1范数回归提升鲁棒性。**

- **链接: [https://arxiv.org/pdf/2510.17788v2](https://arxiv.org/pdf/2510.17788v2)**

> **作者:** Kyung Yun Lee; Nils Meyer-Kahlen; Karolina Prawda; Vesa Välimäki; Sebastian J. Schlecht
>
> **备注:** ICASSP 2026
>
> **摘要:** We address the problem of estimating room impulse responses (RIRs) in noisy, uncontrolled environments where non-stationary sounds such as speech or footsteps corrupt conventional deconvolution. We propose AnyRIR, a non-intrusive method that uses music as the excitation signal instead of a dedicated test signal, and formulate RIR estimation as an L1-norm regression in the time-frequency domain. Solved efficiently with Iterative Reweighted Least Squares (IRLS) and Least-Squares Minimal Residual (LSMR) methods, this approach exploits the sparsity of non-stationary noise to suppress its influence. Experiments on simulated and measured data show that AnyRIR outperforms L2-based and frequency-domain deconvolution, under in-the-wild noisy scenarios and codec mismatch, enabling robust RIR estimation for AR/VR and related applications.
>
---
#### [replaced 013] SoundCompass: Navigating Target Sound Extraction With Effective Directional Clue Integration In Complex Acoustic Scenes
- **分类: eess.AS; cs.AI; cs.SD**

- **简介: 该论文属于目标声音提取任务，解决复杂声场中方向信息利用不足的问题。提出SoundCompass框架，融合空间相关性和方向信息，提升目标声音提取效果。**

- **链接: [https://arxiv.org/pdf/2509.18561v2](https://arxiv.org/pdf/2509.18561v2)**

> **作者:** Dayun Choi; Jung-Woo Choi
>
> **备注:** 5 pages, 4 figures, accepted to ICASSP 2026
>
> **摘要:** Recent advances in target sound extraction (TSE) utilize directional clues derived from direction of arrival (DoA), which represent an inherent spatial property of sound available in any acoustic scene. However, previous DoA-based methods rely on hand-crafted features or discrete encodings, which lose fine-grained spatial information and limit adaptability. We propose SoundCompass, an effective directional clue integration framework centered on a Spectral Pairwise INteraction (SPIN) module that captures cross-channel spatial correlations in the complex spectrogram domain to preserve full spatial information in multichannel signals. The input feature expressed in terms of spatial correlations is fused with a DoA clue represented as spherical harmonics (SH) encoding. The fusion is carried out across overlapping frequency subbands, inheriting the benefits reported in the previous band-split architectures. We also incorporate the iterative refinement strategy, chain-of-inference (CoI), in the TSE framework, which recursively fuses DoA with sound event activation estimated from the previous inference stage. Experiments demonstrate that SoundCompass, combining SPIN, SH embedding, and CoI, robustly extracts target sources across diverse signal classes and spatial configurations.
>
---
#### [replaced 014] Direction-of-Arrival and Noise Covariance Matrix joint estimation for beamforming
- **分类: eess.AS; math.OC**

- **简介: 该论文属于波束成形任务，旨在联合估计到达方向（DoA）和噪声协方差矩阵（NCM）。通过提出一种简化方法，提升估计精度与鲁棒性。**

- **链接: [https://arxiv.org/pdf/2511.10639v4](https://arxiv.org/pdf/2511.10639v4)**

> **作者:** Vitor Gelsleichter Probst Curtarelli; Stephan Paul; Anderson Wedderhoff Spengler
>
> **摘要:** We propose a joint estimation method for the Direction-of-Arrival (DoA) and the Noise Covariance Matrix (NCM) tailored for beamforming applications. Building upon an existing NCM framework, our approach simplifies the estimation procedure by deriving an quasi-linear solution, instead of the traditional exhaustive search. Additionally, we introduce a novel DoA estimation technique that operates across all frequency bins, improving robustness in reverberant environments. Simulation results demonstrate that our method outperforms classical techniques, such as MUSIC, in mid- to high-angle scenarios, achieving lower angular errors and superior signal enhancement through beamforming. The proposed framework was also fared against other techniques for signal enhancement, having better noise rejection and interference canceling capabilities. These improvements are validated using both theoretical and empirical performance metrics.
>
---
#### [replaced 015] MOSS Transcribe Diarize Technical Report
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文属于语音处理任务，旨在解决会议转录中说话人识别与时间戳定位的问题。提出MOSS Transcribe Diarize模型，实现端到端的说话人带时标的转录。**

- **链接: [https://arxiv.org/pdf/2601.01554v5](https://arxiv.org/pdf/2601.01554v5)**

> **作者:** MOSI. AI; :; Donghua Yu; Zhengyuan Lin; Chen Yang; Yiyang Zhang; Hanfu Chen; Jingqi Chen; Ke Chen; Liwei Fan; Yi Jiang; Jie Zhu; Muchen Li; Wenxuan Wang; Yang Wang; Zhe Xu; Yitian Gong; Yuqian Zhang; Wenbo Zhang; Songlin Wang; Zhiyu Wu; Zhaoye Fei; Qinyuan Cheng; Shimin Li; Xipeng Qiu
>
> **摘要:** Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.
>
---
#### [replaced 016] Aligning Generative Speech Enhancement with Perceptual Feedback
- **分类: eess.AS; cs.AI; cs.LG**

- **简介: 该论文属于语音增强任务，旨在解决传统方法与人类感知不匹配的问题。通过引入DPO和UTMOS，直接优化模型以提升语音自然度和听感。**

- **链接: [https://arxiv.org/pdf/2507.09929v2](https://arxiv.org/pdf/2507.09929v2)**

> **作者:** Haoyang Li; Nana Hou; Yuchen Hu; Jixun Yao; Sabato Marco Siniscalchi; Xuyi Zhuang; Deheng Ye; Wei Yang; Eng Siong Chng
>
> **备注:** Accepted to ICASSP 2026
>
> **摘要:** Language Model (LM)-based speech enhancement (SE) has recently emerged as a promising direction, but existing approaches predominantly rely on token-level likelihood objectives that weakly reflect human perception. This mismatch limits progress, as optimizing signal accuracy does not always improve naturalness or listening comfort. We address this gap by introducing a perceptually aligned LM-based SE approach. Our method applies Direct Preference Optimization (DPO) with UTMOS, a neural MOS predictor, as a proxy for human ratings, directly steering models toward perceptually preferred outputs. This design directly connects model training to perceptual quality and is broadly applicable within LM-based SE frameworks. On the Deep Noise Suppression Challenge 2020 test sets, our approach consistently improves speech quality metrics, achieving relative gains of up to 56%. To our knowledge, this is the first integration of perceptual feedback into LM-based SE and the first application of DPO in the SE domain, establishing a new paradigm for perceptually aligned enhancement with SE.
>
---
#### [replaced 017] A Stage-Wise Learning Strategy with Fixed Anchors for Robust Speaker Verification
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于语音识别中的说话人验证任务，旨在提升噪声环境下说话人表示的鲁棒性。通过分阶段学习和固定锚点策略，增强区分性和抗噪能力。**

- **链接: [https://arxiv.org/pdf/2510.18530v2](https://arxiv.org/pdf/2510.18530v2)**

> **作者:** Bin Gu; Lipeng Dai; Huipeng Du; Haitao Zhao; Jibo Wei
>
> **备注:** submitted to ICASSP 2026
>
> **摘要:** Learning robust speaker representations under noisy conditions presents significant challenges, which requires careful handling of both discriminative and noise-invariant properties. In this work, we proposed an anchor-based stage-wise learning strategy for robust speaker representation learning. Specifically, our approach begins by training a base model to establish discriminative speaker boundaries, and then extract anchor embeddings from this model as stable references. Finally, a copy of the base model is fine-tuned on noisy inputs, regularized by enforcing proximity to their corresponding fixed anchor embeddings to preserve speaker identity under distortion. Experimental results suggest that this strategy offers advantages over conventional joint optimization, particularly in maintaining discrimination while improving noise robustness. The proposed method demonstrates consistent improvements across various noise conditions, potentially due to its ability to handle boundary stabilization and variation suppression separately.
>
---
#### [replaced 018] GLAP: General contrastive audio-text pretraining across domains and languages
- **分类: cs.SD; cs.CL; eess.AS**

- **简介: 该论文提出GLAP，解决音频与文本跨语言、跨领域预训练问题。通过扩展CLAP，提升多语言语音和音乐理解能力，在多个基准上取得优异效果。**

- **链接: [https://arxiv.org/pdf/2506.11350v2](https://arxiv.org/pdf/2506.11350v2)**

> **作者:** Heinrich Dinkel; Zhiyong Yan; Tianzi Wang; Yongqing Wang; Xingwei Sun; Yadong Niu; Jizhong Liu; Gang Li; Junbo Zhang; Jian Luan
>
> **备注:** ICASSP 2026
>
> **摘要:** Contrastive Language Audio Pretraining (CLAP) is a widely-used method to bridge the gap between audio and text domains. Current CLAP methods enable sound and music retrieval in English, ignoring multilingual spoken content. To address this, we introduce general language audio pretraining (GLAP), which expands CLAP with multilingual and multi-domain abilities. GLAP demonstrates its versatility by achieving competitive performance on standard audio-text retrieval benchmarks like Clotho and AudioCaps, while significantly surpassing existing methods in speech retrieval and classification tasks. Additionally, GLAP achieves strong results on widely used sound-event zero-shot benchmarks, while simultaneously outperforming previous methods on speech content benchmarks. Further keyword spotting evaluations across 50 languages emphasize GLAP's advanced multilingual capabilities. Finally, multilingual sound and music understanding is evaluated across four languages. Checkpoints and Source: https://github.com/xiaomi-research/dasheng-glap.
>
---
#### [replaced 019] Multimodal Emotion Recognition using Audio-Video Transformer Fusion with Cross Attention
- **分类: cs.MM; cs.CL; cs.CV; cs.LG; cs.SD; eess.AS**

- **简介: 该论文属于多模态情感识别任务，旨在解决时序错位、特征表达弱和模态融合不佳的问题。提出AVT-CA模型，通过跨注意力机制融合音频与视频特征，提升情感识别效果。**

- **链接: [https://arxiv.org/pdf/2407.18552v4](https://arxiv.org/pdf/2407.18552v4)**

> **作者:** Joe Dhanith P R; Shravan Venkatraman; Vigya Sharma; Santhosh Malarvannan
>
> **摘要:** Multimodal emotion recognition (MER) aims to infer human affect by jointly modeling audio and visual cues; however, existing approaches often struggle with temporal misalignment, weakly discriminative feature representations, and suboptimal fusion of heterogeneous modalities. To address these challenges, we propose AVT-CA, an Audio-Video Transformer architecture with cross attention for robust emotion recognition. The proposed model introduces a hierarchical video feature representation that combines channel attention, spatial attention, and local feature extraction to emphasize emotionally salient regions while suppressing irrelevant information. These refined visual features are integrated with audio representations through an intermediate transformer-based fusion mechanism that captures interlinked temporal dependencies across modalities. Furthermore, a cross-attention module selectively reinforces mutually consistent audio-visual cues, enabling effective feature selection and noise-aware fusion. Extensive experiments on three benchmark datasets, CMU-MOSEI, RAVDESS, and CREMA-D, demonstrate that AVT-CA consistently outperforms state-of-the-art baselines, achieving significant improvements in both accuracy and F1-score. Our source code is publicly available at https://github.com/shravan-18/AVTCA.
>
---
#### [replaced 020] Improving the Speaker Anonymization Evaluation's Robustness to Target Speakers with Adversarial Learning
- **分类: eess.AS; cs.LG**

- **简介: 该论文属于语音隐私保护任务，解决现有评估方法在同性别目标选择时高估隐私的问题。通过引入目标分类器和对抗学习，提升评估的鲁棒性。**

- **链接: [https://arxiv.org/pdf/2508.09803v2](https://arxiv.org/pdf/2508.09803v2)**

> **作者:** Carlos Franzreb; Arnab Das; Tim Polzehl; Sebastian Möller
>
> **备注:** Accepted to ICASSP 2026
>
> **摘要:** The current privacy evaluation for speaker anonymization often overestimates privacy when a same-gender target selection algorithm (TSA) is used, although this TSA leaks the speaker's gender and should hence be more vulnerable. We hypothesize that this occurs because the evaluation does not account for the fact that anonymized speech contains information from both the source and target speakers. To address this, we propose to add a target classifier that measures the influence of target speaker information in the evaluation, which can also be removed with adversarial learning. Experiments demonstrate that this approach is effective for multiple anonymizers, particularly when using a same-gender TSA, leading to a more reliable assessment.
>
---
#### [replaced 021] MMEDIT: A Unified Framework for Multi-Type Audio Editing via Audio Language Model
- **分类: cs.SD**

- **简介: 该论文提出MMEdit，解决多类型音频编辑任务中的内容保留与精准控制问题。通过扩展任务定义和构建数据集，结合跨模态模型实现高效音频编辑。**

- **链接: [https://arxiv.org/pdf/2512.20339v3](https://arxiv.org/pdf/2512.20339v3)**

> **作者:** Ye Tao; Wen Wu; Chao Zhang; Mengyue Wu; Shuai Wang; Xuenan Xu
>
> **备注:** Under review
>
> **摘要:** Text-guided audio editing aims to modify specific acoustic events while strictly preserving non-target content. Despite recent progress, existing approaches remain fundamentally limited. Training-free methods often suffer from signal degradation caused by diffusion inversion, while training-based methods, although achieving higher generation quality, are severely constrained by the scarcity of high-quality paired data and task formulations that cover only a narrow subset of editing operations. In addition, standard architectures typically decouple text and audio processing, limiting the ability to align instructions with specific acoustic contexts. To address these challenges, we propose MMEdit, an audio-language-model-driven framework for unified audio editing. We systematically extend task definitions to cover a comprehensive range of editing operations, including addition, replacement, removal, reordering, and attribute modification. Furthermore, we design a scalable data synthesis pipeline to construct large-scale paired datasets with fine-grained event-level annotations. To capture complex editing semantics, we integrate a Qwen2-Audio encoder with an MMDiT-based generator, enabling precise cross-modal alignment and localized editing. Experimental results demonstrate that our method achieves superior editing localization accuracy, robust instruction following, and high fidelity in non-edited regions.
>
---
#### [replaced 022] How Does Instrumental Music Help SingFake Detection?
- **分类: cs.SD; eess.AS; eess.SP**

- **简介: 该论文属于SingFake检测任务，探讨 instrumental music 对检测的影响。研究发现，伴奏主要作为数据增强，而非提供内在线索，且微调会改变模型对语音特征的依赖。**

- **链接: [https://arxiv.org/pdf/2509.14675v2](https://arxiv.org/pdf/2509.14675v2)**

> **作者:** Xuanjun Chen; Chia-Yu Hu; I-Ming Lin; Yi-Cheng Lin; I-Hsiang Chiu; You Zhang; Sung-Feng Huang; Yi-Hsuan Yang; Haibin Wu; Hung-yi Lee; Jyh-Shing Roger Jang
>
> **备注:** Accepted by ICASSP 2026
>
> **摘要:** Although many models exist to detect singing voice deepfakes (SingFake), how these models operate, particularly with instrumental accompaniment, is unclear. We investigate how instrumental music affects SingFake detection from two perspectives. To investigate the behavioral effect, we test different backbones, unpaired instrumental tracks, and frequency subbands. To analyze the representational effect, we probe how fine-tuning alters encoders' speech and music capabilities. Our results show that instrumental accompaniment acts mainly as data augmentation rather than providing intrinsic cues (e.g., rhythm or harmony). Furthermore, fine-tuning increases reliance on shallow speaker features while reducing sensitivity to content, paralinguistic, and semantic information. These insights clarify how models exploit vocal versus instrumental cues and can inform the design of more interpretable and robust SingFake detection systems.
>
---
#### [replaced 023] TurnGuide: Enhancing Meaningful Full Duplex Spoken Interactions via Dynamic Turn-Level Text-Speech Interleaving
- **分类: cs.CL; cs.SD; eess.AS**

- **简介: 该论文属于语音对话系统任务，旨在提升全双工语音语言模型的对话质量。针对语音序列过长和数据不足导致的对话能力下降问题，提出TurnGuide方法，通过动态文本与语音交织生成实现更自然的对话交互。**

- **链接: [https://arxiv.org/pdf/2508.07375v2](https://arxiv.org/pdf/2508.07375v2)**

> **作者:** Wenqian Cui; Lei Zhu; Xiaohui Li; Zhihan Guo; Haoli Bai; Lu Hou; Irwin King
>
> **备注:** Work in progress
>
> **摘要:** Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation models designed to enable natural, real-time spoken interactions by modeling complex conversational turn-taking such as interruptions, backchannels, and overlapping speech. End-to-end (e2e) FD-SLMs leverage real-world double-channel conversational data to capture nuanced two-speaker dialogue patterns for human-like interactions, but their conversational abilities often degrade compared to pure-text conversation due to prolonged speech sequences and limited high-quality spoken dialogue data. Although interleaved text-speech generation could mitigate this degradation, integrating discrete text tokens into continuous double-channel audio streams could disrupt the precise time alignment required for fluid interaction. To address this, we propose TurnGuide, a novel text-speech interleaved generation approach for e2e FD-SLMs that dynamically segments assistant speech into dialogue turns and interleaves turn-level text and speech generation. This approach allows FD-SLMs to integrate the semantic intelligence of LLMs without compromising the natural acoustic flow. Extensive experiments show that TurnGuide not only significantly improves e2e FD-SLMs to produce semantically meaningful, coherent speech but also achieves state-of-the-art performance on various turn-taking events. Demos are available at https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at https://github.com/dreamtheater123/TurnGuide.
>
---
#### [replaced 024] SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition
- **分类: cs.SD; cs.LG**

- **简介: 该论文属于语音命令识别任务，旨在解决SNN在时间依赖和上下文建模上的不足。提出SpikCommander架构，结合多视角学习与时空注意力机制，提升识别效果与效率。**

- **链接: [https://arxiv.org/pdf/2511.07883v3](https://arxiv.org/pdf/2511.07883v3)**

> **作者:** Jiaqi Wang; Liutao Yu; Xiongri Shen; Sihang Guo; Chenlin Zhou; Leilei Zhao; Yi Zhong; Zhiguo Zhang; Zhengyu Ma
>
> **备注:** Accepted by The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)
>
> **摘要:** Spiking neural networks (SNNs) offer a promising path toward energy-efficient speech command recognition (SCR) by leveraging their event-driven processing paradigm. However, existing SNN-based SCR methods often struggle to capture rich temporal dependencies and contextual information from speech due to limited temporal modeling and binary spike-based representations. To address these challenges, we first introduce the multi-view spiking temporal-aware self-attention (MSTASA) module, which combines effective spiking temporal-aware attention with a multi-view learning framework to model complementary temporal dependencies in speech commands. Building on MSTASA, we further propose SpikCommander, a fully spike-driven transformer architecture that integrates MSTASA with a spiking contextual refinement channel MLP (SCR-MLP) to jointly enhance temporal context modeling and channel-wise feature integration. We evaluate our method on three benchmark datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC), and the Google Speech Commands V2 (GSC). Extensive experiments demonstrate that SpikCommander consistently outperforms state-of-the-art (SOTA) SNN approaches with fewer parameters under comparable time steps, highlighting its effectiveness and efficiency for robust speech command recognition.
>
---
#### [replaced 025] From Hype to Insight: Rethinking Large Language Model Integration in Visual Speech Recognition
- **分类: cs.SD**

- **简介: 该论文属于视觉语音识别任务，旨在解决大语言模型集成效果的疑问。通过系统评估不同配置，发现模型提升主要来自语言建模而非视觉理解，强调更强视觉编码器的重要性。**

- **链接: [https://arxiv.org/pdf/2509.14880v3](https://arxiv.org/pdf/2509.14880v3)**

> **作者:** Rishabh Jain; Naomi Harte
>
> **备注:** Accepted for publication in ICASSP 2026
>
> **摘要:** Advances in self-supervised encoders have improved Visual Speech Recognition (VSR). Recent approaches integrating these encoders with LLM decoders improves transcription accuracy; however, it remains unclear whether these gains stem from visual understanding or stronger language modeling. In this work, we systematically evaluate LLM decoders by freezing or selectively updating the visual encoder, scaling decoder size, comparing adaptation strategies and architectures, and varying training data across LRS2, LRS3, and their combination. Evaluation on LRS2, LRS3, and WildVSR shows that scaling and adaptation yield limited improvements, while combining datasets enhances generalization. Semantic analysis reveals that gains arise primarily from lexical rather than semantic processing. Our Llama-2-13B model trained on the combined set achieves 24.7% WER on LRS3 and 47.0% on WildVSR, establishing SOTA among models trained without additional supervision. Our findings indicate LLM decoders refine contextual reasoning rather than visual features, emphasizing the need for stronger visual encoders to drive meaningful progress.
>
---
