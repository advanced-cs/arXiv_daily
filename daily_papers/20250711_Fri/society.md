# 计算机与社会 cs.CY

- **最新发布 14 篇**

- **更新 9 篇**

## 最新发布

#### [new 001] Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance
- **分类: cs.CY; cs.HC**

- **简介: 该论文属于AI行为研究任务，探讨教育水平如何影响用户对生成式AI的回避行为，通过实验分析发现低教育群体更倾向于避免使用CAI。**

- **链接: [http://arxiv.org/pdf/2507.07881v1](http://arxiv.org/pdf/2507.07881v1)**

> **作者:** Roberto Ulloa; Juhi Kulshrestha; Celina Kacperski
>
> **摘要:** The rise of conversational AI (CAI), powered by large language models, is transforming how individuals access and interact with digital information. However, these tools may inadvertently amplify existing digital inequalities. This study investigates whether differences in formal education are associated with CAI avoidance, leveraging behavioral data from an online experiment (N = 1,636). Participants were randomly assigned to a control or an information-seeking task, either a traditional online search or a CAI (Perplexity AI). Task avoidance (operationalized as survey abandonment or providing unrelated responses during task assignment) was significantly higher in the CAI group (51%) compared to the search (30.9%) and control (16.8%) groups, with the highest CAI avoidance among participants with lower education levels (~74.4%). Structural equation modeling based on the theoretical framework UTAUT2 and LASSO regressions reveal that education is strongly associated with CAI avoidance, even after accounting for various cognitive and affective predictors of technology adoption. These findings underscore education's central role in shaping AI adoption and the role of self-selection biases in AI-related research, stressing the need for inclusive design to ensure equitable access to emerging technologies.
>
---
#### [new 002] The Evolution of Scientific Credit: When Authorship Norms Impede Collaboration
- **分类: cs.CY**

- **简介: 该论文属于社会科学领域，研究科学作者权规范对合作的影响。通过博弈论模型分析不同规范如何形成及影响合作意愿，揭示非贡献敏感规范可能阻碍科学协作。**

- **链接: [http://arxiv.org/pdf/2507.07364v1](http://arxiv.org/pdf/2507.07364v1)**

> **作者:** Toby Handfield; Kevin Zollman
>
> **备注:** 45 pages, 18 figures. Code: https://github.com/ghostleopold/author_order
>
> **摘要:** Scientific authorship norms vary dramatically across disciplines, from contribution-sensitive systems where first author is the greatest contributor and subsequent author order reflects relative input, to contribution-insensitive conventions like alphabetical ordering or senior-author-last. We develop evolutionary game-theoretic models to examine both how these divergent norms emerge and their subsequent effects on collaborative behavior. Our first model reveals that contribution-insensitive norms evolve when researchers who sacrifice positional advantage face the strongest adaptive pressure -- for example senior authors managing larger collaboration portfolios or bearing heavier reputational stakes. This "Red King" dynamic potentially explains why fields in which senior researchers command large labs, major grants, and extensive collaboration portfolios may paradoxically evolve conventions that favour junior-author positioning. Our second model demonstrates that established norms influence researchers' willingness to collaborate, with contribution-sensitive norms consistently outperforming insensitive alternatives in fostering successful partnerships. Contribution-insensitive norms create systematic coordination failures through two mechanisms: "main contributor resentment" when exceptional work goes unrecognized, and "second contributor resentment" when comparable efforts receive unequal credit. These findings suggest that widely adopted practices like senior-last positioning and alphabetical ordering may function as institutional frictions that impede valuable scientific collaborations rather than neutral organizational conventions, potentially reducing overall scientific productivity across affected disciplines.
>
---
#### [new 003] Short-Term Gains, Long-Term Gaps: The Impact of GenAI and Search Technologies on Retention
- **分类: cs.CY**

- **简介: 该论文属于教育技术研究，探讨GenAI和搜索工具对学生学习成果与记忆的影响。通过实验分析不同工具在不同认知任务中的效果，旨在解决技术辅助学习的长期效果问题。**

- **链接: [http://arxiv.org/pdf/2507.07357v1](http://arxiv.org/pdf/2507.07357v1)**

> **作者:** Mahir Akgun; Sacip Toker
>
> **备注:** To appear in the proceedings of the 26th International Conference on Artificial Intelligence in Education (AIED 2025)
>
> **摘要:** The rise of Generative AI (GenAI) tools, such as ChatGPT, has transformed how students access and engage with information, raising questions about their impact on learning outcomes and retention. This study investigates how GenAI (ChatGPT), search engines (Google), and e-textbooks influence student performance across tasks of varying cognitive complexity, based on Bloom's Taxonomy. Using a sample of 123 students, we examined performance in three tasks: [1] knowing and understanding, [2] applying, and [3] synthesizing, evaluating, and creating. Results indicate that ChatGPT and Google groups outperformed the control group in immediate assessments for lower-order cognitive tasks, benefiting from quick access to structured information. However, their advantage diminished over time, with retention test scores aligning with those of the e-textbook group. For higher-order cognitive tasks, no significant differences were observed among groups, with the control group demonstrating the highest retention. These findings suggest that while AI-driven tools facilitate immediate performance, they do not inherently reinforce long-term retention unless supported by structured learning strategies. The study highlights the need for balanced technology integration in education, ensuring that AI tools are paired with pedagogical approaches that promote deep cognitive engagement and knowledge retention.
>
---
#### [new 004] Vaccine Hesitancy on YouTube: a Competition between Health and Politics
- **分类: cs.CY**

- **简介: 该论文属于内容分析任务，旨在研究YouTube上疫苗犹豫现象。通过分析3个月内的视频，揭示健康信息与政治评论之间的竞争，以及平台对犹豫内容的监管不足。**

- **链接: [http://arxiv.org/pdf/2507.07517v1](http://arxiv.org/pdf/2507.07517v1)**

> **作者:** Yelena Mejova; Michele Tizzani
>
> **备注:** Digital Public Health Conference 2025
>
> **摘要:** YouTube has rapidly emerged as a predominant platform for content consumption, effectively displacing conventional media such as television and news outlets. A part of the enormous video stream uploaded to this platform includes health-related content, both from official public health organizations, and from any individual or group that can make an account. The quality of information available on YouTube is a critical point of public health safety, especially when concerning major interventions, such as vaccination. This study differentiates itself from previous efforts of auditing YouTube videos on this topic by conducting a systematic daily collection of posted videos mentioning vaccination for the duration of 3 months. We show that the competition for the public's attention is between public health messaging by institutions and individual educators on one side, and commentators on society and politics on the other, the latest contributing the most to the videos expressing stances against vaccination. Videos opposing vaccination are more likely to mention politicians and publication media such as podcasts, reports, and news analysis, on the other hand, videos in favor are more likely to mention specific diseases or health-related topics. Finally, we find that, at the time of analysis, only 2.7% of the videos have been taken down (by the platform or the channel), despite 20.8% of the collected videos having a vaccination hesitant stance, pointing to a lack of moderation activity for hesitant content. The availability of high-quality information is essential to improve awareness and compliance with public health interventions. Our findings help characterize the public discourse around vaccination on one of the largest media platforms, disentangling the role of the different creators and their stances, and as such, they provide important insights for public health communication policy.
>
---
#### [new 005] AI Human Impact: Toward a Model for Ethical Investing in AI-Intensive Companies
- **分类: cs.CY**

- **简介: 该论文属于伦理投资任务，旨在解决AI公司评估不足的问题。提出九项指标构建伦理评估模型，以指导符合价值观的投资决策。**

- **链接: [http://arxiv.org/pdf/2507.07703v1](http://arxiv.org/pdf/2507.07703v1)**

> **作者:** James Brusseau
>
> **摘要:** Does AI conform to humans, or will we conform to AI? An ethical evaluation of AI-intensive companies will allow investors to knowledgeably participate in the decision. The evaluation is built from nine performance indicators that can be analyzed and scored to reflect a technology's human-centering. The result is objective investment guidance, as well as investors empowered to act in accordance with their own values. Incorporating ethics into financial decisions is a strategy that will be recognized by participants in environmental, social, and governance investing, however, this paper argues that conventional ESG frameworks are inadequate to companies that function with AI at their core. Fully accounting for contemporary big data, predictive analytics, and machine learning requires specialized metrics customized from established AI ethics principles. With these metrics established, the larger goal is a model for humanist investing in AI-intensive companies that is intellectually robust, manageable for analysts, useful for portfolio managers, and credible for investors.
>
---
#### [new 006] Distributed and Decentralised Training: Technical Governance Challenges in a Shifting AI Landscape
- **分类: cs.CY; cs.LG**

- **简介: 该论文属于AI治理研究，探讨分布式与去中心化训练带来的技术挑战，分析其对计算结构、能力扩散及可检测性的影响，旨在支持更精准的政策制定。**

- **链接: [http://arxiv.org/pdf/2507.07765v1](http://arxiv.org/pdf/2507.07765v1)**

> **作者:** Jakub Kryś; Yashvardhan Sharma; Janet Egan
>
> **备注:** Accepted as an oral presentation at the Technical AI Governance Workshop (ICML 2025)
>
> **摘要:** Advances in low-communication training algorithms are enabling a shift from centralised model training to compute setups that are either distributed across multiple clusters or decentralised via community-driven contributions. This paper distinguishes these two scenarios - distributed and decentralised training - which are little understood and often conflated in policy discourse. We discuss how they could impact technical AI governance through an increased risk of compute structuring, capability proliferation, and the erosion of detectability and shutdownability. While these trends foreshadow a possible new paradigm that could challenge key assumptions of compute governance, we emphasise that certain policy levers, like export controls, remain relevant. We also acknowledge potential benefits of decentralised AI, including privacy-preserving training runs that could unlock access to more data, and mitigating harmful power concentration. Our goal is to support more precise policymaking around compute, capability proliferation, and decentralised AI development.
>
---
#### [new 007] Structured Prompts, Better Outcomes? Exploring the Effects of a Structured Interface with ChatGPT in a Graduate Robotics Course
- **分类: cs.CY**

- **简介: 该论文属于教育技术任务，探讨结构化界面如何影响学生使用ChatGPT学习机器人学的效果。研究通过实验比较干预组与对照组，分析 prompting 行为、学习成效及学生反馈，旨在优化LLM在教学中的应用策略。**

- **链接: [http://arxiv.org/pdf/2507.07767v1](http://arxiv.org/pdf/2507.07767v1)**

> **作者:** Jerome Brender; Laila El-Hamamsy; Kim Uittenhove; Francesco Mondada; Engin Bumbacher
>
> **备注:** Accepted, to appear in the proceedings of the EC-TEL 2025 conference
>
> **摘要:** Prior research shows that how students engage with Large Language Models (LLMs) influences their problem-solving and understanding, reinforcing the need to support productive LLM-uses that promote learning. This study evaluates the impact of a structured GPT platform designed to promote 'good' prompting behavior with data from 58 students in a graduate-level robotics course. The students were assigned to either an intervention group using the structured platform or a control group using ChatGPT freely for two practice lab sessions, before a third session where all students could freely use ChatGPT. We analyzed student perception (pre-post surveys), prompting behavior (logs), performance (task scores), and learning (pre-post tests). Although we found no differences in performance or learning between groups, we identified prompting behaviors - such as having clear prompts focused on understanding code - that were linked with higher learning gains and were more prominent when students used the structured platform. However, such behaviors did not transfer once students were no longer constrained to use the structured platform. Qualitative survey data showed mixed perceptions: some students perceived the value of the structured platform, but most did not perceive its relevance and resisted changing their habits. These findings contribute to ongoing efforts to identify effective strategies for integrating LLMs into learning and question the effectiveness of bottom-up approaches that temporarily alter user interfaces to influence students' interaction. Future research could instead explore top-down strategies that address students' motivations and explicitly demonstrate how certain interaction patterns support learning.
>
---
#### [new 008] Working with AI: Measuring the Occupational Implications of Generative AI
- **分类: cs.AI; cs.CY; econ.GN; q-fin.EC**

- **简介: 该论文研究AI对职业的影响，分析用户与AI互动的活动，计算各职业的AI适用性得分，旨在理解AI对经济的影响。**

- **链接: [http://arxiv.org/pdf/2507.07935v1](http://arxiv.org/pdf/2507.07935v1)**

> **作者:** Kiran Tomlinson; Sonia Jaffe; Will Wang; Scott Counts; Siddharth Suri
>
> **备注:** 40 pages
>
> **摘要:** Given the rapid adoption of generative AI and its potential to impact a wide range of tasks, understanding the effects of AI on the economy is one of society's most important questions. In this work, we take a step toward that goal by analyzing the work activities people do with AI, how successfully and broadly those activities are done, and combine that with data on what occupations do those activities. We analyze a dataset of 200k anonymized and privacy-scrubbed conversations between users and Microsoft Bing Copilot, a publicly available generative AI system. We find the most common work activities people seek AI assistance for involve gathering information and writing, while the most common activities that AI itself is performing are providing information and assistance, writing, teaching, and advising. Combining these activity classifications with measurements of task success and scope of impact, we compute an AI applicability score for each occupation. We find the highest AI applicability scores for knowledge work occupation groups such as computer and mathematical, and office and administrative support, as well as occupations such as sales whose work activities involve providing and communicating information. Additionally, we characterize the types of work activities performed most successfully, how wage and education correlate with AI applicability, and how real-world usage compares to predictions of occupational AI impact.
>
---
#### [new 009] HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN
- **分类: cs.NI; cs.CY; cs.SY; eess.SY; 68M10, 68M12, 68W15; C.2.1; C.2.2; C.2.3; C.2.6; H.5.5; K.4.1**

- **简介: 该论文属于智能城市通信任务，旨在解决灾后通信问题。提出HaLert架构，结合Wi-Fi HaLow和SDN技术，实现稳定应急通信。**

- **链接: [http://arxiv.org/pdf/2507.07841v1](http://arxiv.org/pdf/2507.07841v1)**

> **作者:** Ana Rita Ortigoso; Gabriel Vieira; Daniel Fuentes; Luís Frazão; Nuno Costa; António Pereira
>
> **摘要:** Events such as catastrophes and disasters are, in most cases, unpredictable. Consequently, reusing existing infrastructures to develop alternative communication strategies after disasters is essential to minimise the impact of these events on the population's ability to communicate and promptly receive alerts from authorities. In this context, the emergence of smart cities, characterised by dense and geographically distributed IoT networks, presents significant potential for such reuse. This work proposes HaLert, a resilient architecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network, whose resources can be readily reallocated to support a emergency communication system to exchange messages (including text, location, image, audio, and video) between citizens, authorities, and between both parties. To facilitate remote monitoring and configuration of the network, the architecture incorporates the SDN (Software-Defined Networking) paradigm, supported by a LoRa controlled flooding mesh network. A prototype was developed based on this architecture and tested in a real urban scenario comprising both indoor and outdoor environments. The results demonstrated that, despite the significant impact of obstacles, lack of line-of-sight, and terrain slopes on the latency (average latency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and 726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow network, it remained stable and resilient, successfully providing all functionalities associated with the HaLert architecture. The tests conducted on the LoRa network revealed a high average message success rate of 94.96%.
>
---
#### [new 010] Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses
- **分类: cs.CL; cs.AI; cs.CY; J.4**

- **简介: 该论文属于自然语言处理任务，研究LLM在调查中的响应偏差问题。通过测试不同LLM对问卷的敏感性，发现其存在类似人类的近期偏差，并强调提示设计的重要性。**

- **链接: [http://arxiv.org/pdf/2507.07188v1](http://arxiv.org/pdf/2507.07188v1)**

> **作者:** Jens Rupprecht; Georg Ahnert; Markus Strohmaier
>
> **备注:** 18 pages, 17 figures
>
> **摘要:** Large Language Models (LLMs) are increasingly used as proxies for human subjects in social science surveys, but their reliability and susceptibility to known response biases are poorly understood. This paper investigates the response robustness of LLMs in normative survey contexts -- we test nine diverse LLMs on questions from the World Values Survey (WVS), applying a comprehensive set of 11 perturbations to both question phrasing and answer option structure, resulting in over 167,000 simulated interviews. In doing so, we not only reveal LLMs' vulnerabilities to perturbations but also reveal that all tested models exhibit a consistent \textit{recency bias} varying in intensity, disproportionately favoring the last-presented answer option. While larger models are generally more robust, all models remain sensitive to semantic variations like paraphrasing and to combined perturbations. By applying a set of perturbations, we reveal that LLMs partially align with survey response biases identified in humans. This underscores the critical importance of prompt design and robustness testing when using LLMs to generate synthetic survey data.
>
---
#### [new 011] Improving Clustering on Occupational Text Data through Dimensionality Reduction
- **分类: cs.LG; cs.CL; cs.CY**

- **简介: 该论文属于文本聚类任务，旨在解决不同机构间职业定义不一致的问题。通过BERT和降维技术提升聚类效果，实现职业自动映射。**

- **链接: [http://arxiv.org/pdf/2507.07582v1](http://arxiv.org/pdf/2507.07582v1)**

> **作者:** Iago Xabier Vázquez García; Damla Partanaz; Emrullah Fatih Yetkin
>
> **备注:** Preprint, 10 figures
>
> **摘要:** In this study, we focused on proposing an optimal clustering mechanism for the occupations defined in the well-known US-based occupational database, O*NET. Even though all occupations are defined according to well-conducted surveys in the US, their definitions can vary for different firms and countries. Hence, if one wants to expand the data that is already collected in O*NET for the occupations defined with different tasks, a map between the definitions will be a vital requirement. We proposed a pipeline using several BERT-based techniques with various clustering approaches to obtain such a map. We also examined the effect of dimensionality reduction approaches on several metrics used in measuring performance of clustering algorithms. Finally, we improved our results by using a specialized silhouette approach. This new clustering-based mapping approach with dimensionality reduction may help distinguish the occupations automatically, creating new paths for people wanting to change their careers.
>
---
#### [new 012] FLoRA: An Advanced AI-Powered Engine to Facilitate Hybrid Human-AI Regulated Learning
- **分类: cs.HC; cs.CY**

- **简介: 该论文属于教育技术领域，旨在解决传统AI工具在支持自主学习与人机协同中的不足。提出FLoRA引擎，融合生成式AI与学习分析，提供动态自适应学习支持。**

- **链接: [http://arxiv.org/pdf/2507.07362v1](http://arxiv.org/pdf/2507.07362v1)**

> **作者:** Xinyu Li; Tongguang Li; Lixiang Yan; Yuheng Li; Linxuan Zhao; Mladen Raković; Inge Molenaar; Dragan Gašević; Yizhou Fan
>
> **摘要:** SRL, defined as learners' ability to systematically plan, monitor, and regulate their learning activities, is crucial for sustained academic achievement and lifelong learning competencies. Emerging Artificial Intelligence (AI) developments profoundly influence SRL interactions by potentially either diminishing or strengthening learners' opportunities to exercise their own regulatory skills. Recent literature emphasizes a balanced approach termed Hybrid Human-AI Regulated Learning (HHAIRL), in which AI provides targeted, timely scaffolding while preserving the learners' role as active decision-makers and reflective monitors of their learning process. Nevertheless, existing digital tools frequently fall short, lacking adaptability, focusing narrowly on isolated SRL phases, and insufficiently support meaningful human-AI interactions. In response, this paper introduces the enhanced \flora Engine, which incorporates advanced Generative Artificial Intelligence (GenAI) features and state-of-the-art learning analytics, explicitly grounded in SRL and HHAIRL theories. The \flora Engine offers instrumentation tools such as collaborative writing, multi-agents chatbot, and detailed learning trace logging to support dynamic, adaptive scaffolding tailored to individual needs in real time. We further present a summary of several research studies that provide the validations for and illustrate how these instrumentation tools can be utilized in real-world educational and experimental contexts. These studies demonstrate the effectiveness of \flora Engine in fostering SRL and HHAIRL, providing both theoretical insights and practical solutions for the future of AI-enhanced learning context.
>
---
#### [new 013] Dirty Data in the Newsroom: Comparing Data Preparation in Journalism and Data Science
- **分类: cs.HC; cs.CY; A.0**

- **简介: 该论文属于数据新闻研究任务，旨在探讨数据准备过程中的问题。通过混合分析方法，识别了数据来源的四个挑战，并提出新的脏数据分类体系。**

- **链接: [http://arxiv.org/pdf/2507.07238v1](http://arxiv.org/pdf/2507.07238v1)**

> **作者:** Stephen Kasica; Charles Berret; Tamara Munzner
>
> **备注:** 18 pages, 3 figures, Published in proceedings of the 2023 CHI Conference on Human Factors in Computing Systems
>
> **摘要:** The work involved in gathering, wrangling, cleaning, and otherwise preparing data for analysis is often the most time consuming and tedious aspect of data work. Although many studies describe data preparation within the context of data science workflows, there has been little research on data preparation in data journalism. We address this gap with a hybrid form of thematic analysis that combines deductive codes derived from existing accounts of data science workflows and inductive codes arising from an interview study with 36 professional data journalists. We extend a previous model of data science work to incorporate detailed activities of data preparation. We synthesize 60 dirty data issues from 16 taxonomies on dirty data and our interview data, and we provide a novel taxonomy to characterize these dirty data issues as discrepancies between mental models. We also identify four challenges faced by journalists: diachronic, regional, fragmented, and disparate data sources.
>
---
#### [new 014] Meek Models Shall Inherit the Earth
- **分类: cs.AI; cs.CY; I.2.0; K.4.1**

- **简介: 该论文属于AI性能研究任务，探讨模型规模与能力的关系。论文指出计算扩展的边际收益下降，导致小模型也能接近顶级模型性能，需重新审视AI战略。**

- **链接: [http://arxiv.org/pdf/2507.07931v1](http://arxiv.org/pdf/2507.07931v1)**

> **作者:** Hans Gundlach; Jayson Lynch; Neil Thompson
>
> **备注:** 13 pages, 9 figures, longer version of the paper presented at TAIG ICML 2025
>
> **摘要:** The past decade has seen incredible scaling of AI systems by a few companies, leading to inequality in AI model performance. This paper argues that, contrary to prevailing intuition, the diminishing returns to compute scaling will lead to a convergence of AI model capabilities. In other words, meek models (those with limited computation budget) shall inherit the earth, approaching the performance level of the best models overall. We develop a model illustrating that under a fixed-distribution next-token objective, the marginal capability returns to raw compute shrink substantially. Given current scaling practices, we argue that these diminishing returns are strong enough that even companies that can scale their models exponentially faster than other organizations will eventually have little advantage in capabilities. As part of our argument, we give several reasons that proxies like training loss differences capture important capability measures using evidence from benchmark data and theoretical performance models. In addition, we analyze empirical data on the capability difference of AI models over time. Finally, in light of the increasing ability of meek models, we argue that AI strategy and policy require reexamination, and we outline the areas this shift will affect.
>
---
## 更新

#### [replaced 001] Ethical Concerns of Generative AI and Mitigation Strategies: A Systematic Mapping Study
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2502.00015v2](http://arxiv.org/pdf/2502.00015v2)**

> **作者:** Yutan Huang; Chetan Arora; Wen Cheng Houng; Tanjila Kanij; Anuradha Madulgalla; John Grundy
>
> **摘要:** [Context] Generative AI technologies, particularly Large Language Models (LLMs), have transformed numerous domains by enhancing convenience and efficiency in information retrieval, content generation, and decision-making processes. However, deploying LLMs also presents diverse ethical challenges, and their mitigation strategies remain complex and domain-dependent. [Objective] This paper aims to identify and categorize the key ethical concerns associated with using LLMs, examine existing mitigation strategies, and assess the outstanding challenges in implementing these strategies across various domains. [Method] We conducted a systematic mapping study, reviewing 39 studies that discuss ethical concerns and mitigation strategies related to LLMs. We analyzed these ethical concerns using five ethical dimensions that we extracted based on various existing guidelines, frameworks, and an analysis of the mitigation strategies and implementation challenges. [Results] Our findings reveal that ethical concerns in LLMs are multi-dimensional and context-dependent. While proposed mitigation strategies address some of these concerns, significant challenges still remain. [Conclusion] Our results highlight that ethical issues often hinder the practical implementation of the mitigation strategies, particularly in high-stake areas like healthcare and public governance; existing frameworks often lack adaptability, failing to accommodate evolving societal expectations and diverse contexts.
>
---
#### [replaced 002] Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components
- **分类: cs.LG; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2506.02357v2](http://arxiv.org/pdf/2506.02357v2)**

> **作者:** Ram Potham
>
> **备注:** Preprint. This work has been submitted to the Technical AI Governance Workshop at ICML 2025 for review
>
> **摘要:** Credible safety plans for advanced AI development require methods to verify agent behavior and detect potential control deficiencies early. A fundamental aspect is ensuring agents adhere to safety-critical principles, especially when these conflict with operational goals. This paper introduces a lightweight, interpretable benchmark to evaluate an LLM agent's ability to uphold a high-level safety principle when faced with conflicting task instructions. Our evaluation of six LLMs reveals two primary findings: (1) a quantifiable "cost of compliance" where safety constraints degrade task performance even when compliant solutions exist, and (2) an "illusion of compliance" where high adherence often masks task incompetence rather than principled choice. These findings provide initial evidence that while LLMs can be influenced by hierarchical directives, current approaches lack the consistency required for reliable safety governance.
>
---
#### [replaced 003] The Thin Line Between Comprehension and Persuasion in LLMs
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.01936v2](http://arxiv.org/pdf/2507.01936v2)**

> **作者:** Adrian de Wynter; Tangming Yuan
>
> **备注:** Preprint
>
> **摘要:** Large language models (LLMs) are excellent at maintaining high-level, convincing dialogues. They are being fast deployed as chatbots and evaluators in sensitive areas, such as peer review and mental health applications. This, along with the disparate accounts on their reasoning capabilities, calls for a closer examination of LLMs and their comprehension of dialogue. In this work we begin by evaluating LLMs' ability to maintain a debate--one of the purest yet most complex forms of human communication. Then we measure how this capability relates to their understanding of what is being talked about, namely, their comprehension of dialogical structures and the pragmatic context. We find that LLMs are capable of maintaining coherent, persuasive debates, often swaying the beliefs of participants and audiences alike. We also note that awareness or suspicion of AI involvement encourage people to be more critical of the arguments made. When polling LLMs on their comprehension of deeper structures of dialogue, however, they cannot demonstrate said understanding. Our findings tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand the context. More broadly, for the field of argumentation theory we posit that, if an agent can convincingly maintain a dialogue, it is not necessary for it to know what it is talking about. Hence, the modelling of pragmatic context and coherence are secondary to effectiveness.
>
---
#### [replaced 004] Beyond Overcorrection: Evaluating Diversity in T2I Models with DivBench
- **分类: cs.CL; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.03015v2](http://arxiv.org/pdf/2507.03015v2)**

> **作者:** Felix Friedrich; Thiemo Ganesha Welsch; Manuel Brack; Patrick Schramowski; Kristian Kersting
>
> **摘要:** Current diversification strategies for text-to-image (T2I) models often ignore contextual appropriateness, leading to over-diversification where demographic attributes are modified even when explicitly specified in prompts. This paper introduces DIVBENCH, a benchmark and evaluation framework for measuring both under- and over-diversification in T2I generation. Through systematic evaluation of state-of-the-art T2I models, we find that while most models exhibit limited diversity, many diversification approaches overcorrect by inappropriately altering contextually-specified attributes. We demonstrate that context-aware methods, particularly LLM-guided FairDiffusion and prompt rewriting, can already effectively address under-diversity while avoiding over-diversification, achieving a better balance between representation and semantic fidelity.
>
---
#### [replaced 005] Decoding AI Judgment: How LLMs Assess News Credibility and Bias
- **分类: cs.CL; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2502.04426v2](http://arxiv.org/pdf/2502.04426v2)**

> **作者:** Edoardo Loru; Jacopo Nudo; Niccolò Di Marco; Alessandro Santirocchi; Roberto Atzeni; Matteo Cinelli; Vincenzo Cestari; Clelia Rossi-Arnaud; Walter Quattrociocchi
>
> **摘要:** Large Language Models (LLMs) are increasingly embedded in workflows that involve evaluative processes. This raises the need to examine how such evaluations are built, what assumptions they rely on, and how their strategies diverge from those of humans. We benchmark six LLMs against expert ratings--NewsGuard and Media Bias/Fact Check (MBFC)--and against human judgments collected through a controlled experiment. To enable direct comparison, we implement a structured agentic framework in which both models and non-expert participants follow the same evaluation procedure: selecting criteria, retrieving content, and producing justifications. Despite output alignment, LLMs rely on different mechanisms: lexical associations and statistical priors replace contextual reasoning. This reliance produces systematic effects: political asymmetries, opaque justifications, and a tendency to confuse linguistic form with epistemic validity. Delegating judgment to such systems does not merely automate evaluation--it redefines it, shifting from normative reasoning to pattern-based approximation.
>
---
#### [replaced 006] Revisiting the Predictability of Performative, Social Events
- **分类: cs.CY; cs.LG; econ.TH; stat.ML**

- **链接: [http://arxiv.org/pdf/2503.11713v2](http://arxiv.org/pdf/2503.11713v2)**

> **作者:** Juan C. Perdomo
>
> **备注:** 21 pages, accepted to ICML 2025
>
> **摘要:** Social predictions do not passively describe the future; they actively shape it. They inform actions and change individual expectations in ways that influence the likelihood of the predicted outcome. Given these dynamics, to what extent can social events be predicted? This question was discussed throughout the 20th century by authors like Merton, Morgenstern, Simon, and others who considered it a central issue in social science methodology. In this work, we provide a modern answer to this old problem. Using recent ideas from performative prediction and outcome indistinguishability, we establish that one can always efficiently predict social events accurately, regardless of how predictions influence data. While achievable, we also show that these predictions are often undesirable, highlighting the limitations of previous desiderata. We end with a discussion of various avenues forward.
>
---
#### [replaced 007] MAEBE: Multi-Agent Emergent Behavior Framework
- **分类: cs.MA; cs.AI; cs.CL; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2506.03053v2](http://arxiv.org/pdf/2506.03053v2)**

> **作者:** Sinem Erisken; Timothy Gothard; Martin Leitgab; Ram Potham
>
> **备注:** Preprint. This work has been submitted to the Multi-Agent Systems Workshop at ICML 2025 for review
>
> **摘要:** Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts.
>
---
#### [replaced 008] A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search
- **分类: cs.LG; cs.AI; cs.CY; cs.PF**

- **链接: [http://arxiv.org/pdf/2507.00004v2](http://arxiv.org/pdf/2507.00004v2)**

> **作者:** Austin R. Ellis-Mohr; Anuj K. Nayak; Lav R. Varshney
>
> **摘要:** Large language models (LLMs) demand considerable computational, energy, and financial resources during both training and deployment. While scaling laws for training have guided much of the field's recent progress, inference costs now represent a significant and growing component of the overall resource burden, particularly for reasoning-focused models. Existing characterizations of compute-optimality that consider model size, dataset size, and inference tokens in isolation or in fixed combinations risk overlooking more efficient operating points. We introduce directed stochastic skill search (DS3), a general framework that represents inference as stochastic traversal over a learned skill graph. From a simplified yet expressive instantiation, we derive closed-form expressions for task success and compute cost across a wide range of inference strategies -- including chain-of-thought (CoT) and tree-of-thought (ToT) -- enabling comparative analysis as a function of task difficulty and model capability. To that end, we extend a prior first-principles tripartite graph framework of LLM training to incorporate inference, and separately bridge DS3 with empirical methods that characterize LLM scaling behavior. We theoretically recover empirically observed patterns, including: linear accuracy scaling with logarithmic compute; variation in preferred inference strategies as a function of task difficulty and model capability; emergent behavior elicited by reasoning even when performance plateaus under parameter scaling; and both best-of-N (BoN) and majority voting behavior captured within a unified analytical framework. By explicitly characterizing training-inference interdependencies, our framework deepens theoretical understanding and supports principled algorithmic design and resource allocation.
>
---
#### [replaced 009] Anchoring AI Capabilities in Market Valuations: The Capability Realization Rate Model and Valuation Misalignment Risk
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.10590v2](http://arxiv.org/pdf/2505.10590v2)**

> **作者:** Xinmin Fang; Lingfeng Tao; Zhengxiong Li
>
> **备注:** 11 pages, 3 figures, NeurIPS
>
> **摘要:** Recent breakthroughs in artificial intelligence (AI) have triggered surges in market valuations for AI-related companies, often outpacing the realization of underlying capabilities. We examine the anchoring effect of AI capabilities on equity valuations and propose a Capability Realization Rate (CRR) model to quantify the gap between AI potential and realized performance. Using data from the 2023--2025 generative AI boom, we analyze sector-level sensitivity and conduct case studies (OpenAI, Adobe, NVIDIA, Meta, Microsoft, Goldman Sachs) to illustrate patterns of valuation premium and misalignment. Our findings indicate that AI-native firms commanded outsized valuation premiums anchored to future potential, while traditional companies integrating AI experienced re-ratings subject to proof of tangible returns. We argue that CRR can help identify valuation misalignment risk-where market prices diverge from realized AI-driven value. We conclude with policy recommendations to improve transparency, mitigate speculative bubbles, and align AI innovation with sustainable market value.
>
---
