# 计算机视觉 cs.CV

- **最新发布 179 篇**

- **更新 119 篇**

## 最新发布

#### [new 001] One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory
- **分类: cs.CV; cs.AI; cs.GR; cs.LG**

- **简介: 该论文属视频分析任务，针对现有时空分块方法计算冗余、动态场景效果差的问题，提出TrajViT方法。通过基于全景子物体轨迹生成语义标记，减少冗余并保持时序一致性，提升视频理解、检索及QA性能，同时加速训练与推理，实现高效视频编码。**

- **链接: [http://arxiv.org/pdf/2505.23617v1](http://arxiv.org/pdf/2505.23617v1)**

> **作者:** Chenhao Zheng; Jieyu Zhang; Mohammadreza Salehi; Ziqi Gao; Vishnu Iyengar; Norimasa Kobori; Quan Kong; Ranjay Krishna
>
> **摘要:** Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 18x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution.
>
---
#### [new 002] Beam-Guided Knowledge Replay for Knowledge-Rich Image Captioning using Vision-Language Model
- **分类: cs.CV**

- **简介: 该论文属于图像描述生成任务，旨在解决现有模型生成的描述缺乏具体性和知识深度问题。提出KRCapVLM框架，结合知识重播、beam搜索优化、注意力增强图像编码器及训练调度器，提升生成质量与知识识别能力，增强模型对新知识的泛化。**

- **链接: [http://arxiv.org/pdf/2505.23358v1](http://arxiv.org/pdf/2505.23358v1)**

> **作者:** Reem AlJunaid; Muzammil Behzad
>
> **摘要:** Generating informative and knowledge-rich image captions remains a challenge for many existing captioning models, which often produce generic descriptions that lack specificity and contextual depth. To address this limitation, we propose KRCapVLM, a knowledge replay-based novel image captioning framework using vision-language model. We incorporate beam search decoding to generate more diverse and coherent captions. We also integrate attention-based modules into the image encoder to enhance feature representation. Finally, we employ training schedulers to improve stability and ensure smoother convergence during training. These proposals accelerate substantial gains in both caption quality and knowledge recognition. Our proposed model demonstrates clear improvements in both the accuracy of knowledge recognition and the overall quality of generated captions. It shows a stronger ability to generalize to previously unseen knowledge concepts, producing more informative and contextually relevant descriptions. These results indicate the effectiveness of our approach in enhancing the model's capacity to generate meaningful, knowledge-grounded captions across a range of scenarios.
>
---
#### [new 003] RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated Remote Sensing Forgeries
- **分类: cs.CV**

- **简介: 该论文提出RSFAKE-1M数据集，用于检测扩散模型生成的遥感图像伪造。针对现有伪造检测方法难以应对扩散模型生成的遥感图像问题，构建含50万真实与伪造图像的数据集（基于10种扩散模型生成），并实验表明现有方法效果有限，而基于该数据集训练的模型检测性能显著提升，为后续研究提供基础。**

- **链接: [http://arxiv.org/pdf/2505.23283v1](http://arxiv.org/pdf/2505.23283v1)**

> **作者:** Zhihong Tan; Jiayi Wang; Huiying Shi; Binyuan Huang; Hongchen Wei; Zhenzhong Chen
>
> **摘要:** Detecting forged remote sensing images is becoming increasingly critical, as such imagery plays a vital role in environmental monitoring, urban planning, and national security. While diffusion models have emerged as the dominant paradigm for image generation, their impact on remote sensing forgery detection remains underexplored. Existing benchmarks primarily target GAN-based forgeries or focus on natural images, limiting progress in this critical domain. To address this gap, we introduce RSFAKE-1M, a large-scale dataset of 500K forged and 500K real remote sensing images. The fake images are generated by ten diffusion models fine-tuned on remote sensing data, covering six generation conditions such as text prompts, structural guidance, and inpainting. This paper presents the construction of RSFAKE-1M along with a comprehensive experimental evaluation using both existing detectors and unified baselines. The results reveal that diffusion-based remote sensing forgeries remain challenging for current methods, and that models trained on RSFAKE-1M exhibit notably improved generalization and robustness. Our findings underscore the importance of RSFAKE-1M as a foundation for developing and evaluating next-generation forgery detection approaches in the remote sensing domain. The dataset and other supplementary materials are available at https://huggingface.co/datasets/TZHSW/RSFAKE/.
>
---
#### [new 004] MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification
- **分类: cs.CV**

- **简介: 该论文属于细粒度图像分类任务，旨在解决多模态间隐式依赖导致的语义交互捕捉困难问题。提出MCFNet，通过正则化融合模块提升单模态特征并利用混合注意力对齐语义；设计多模态决策模块整合跨模态相关性与单模态特征，采用多损失加权投票，实验验证其有效性。**

- **链接: [http://arxiv.org/pdf/2505.23365v1](http://arxiv.org/pdf/2505.23365v1)**

> **作者:** Yang Qiao; Xiaoyu Zhong; Xiaofeng Gu; Zhiguo Yu
>
> **摘要:** Multimodal information processing has become increasingly important for enhancing image classification performance. However, the intricate and implicit dependencies across different modalities often hinder conventional methods from effectively capturing fine-grained semantic interactions, thereby limiting their applicability in high-precision classification tasks. To address this issue, we propose a novel Multimodal Collaborative Fusion Network (MCFNet) designed for fine-grained classification. The proposed MCFNet architecture incorporates a regularized integrated fusion module that improves intra-modal feature representation through modality-specific regularization strategies, while facilitating precise semantic alignment via a hybrid attention mechanism. Additionally, we introduce a multimodal decision classification module, which jointly exploits inter-modal correlations and unimodal discriminative features by integrating multiple loss functions within a weighted voting paradigm. Extensive experiments and ablation studies on benchmark datasets demonstrate that the proposed MCFNet framework achieves consistent improvements in classification accuracy, confirming its effectiveness in modeling subtle cross-modal semantics.
>
---
#### [new 005] TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance
- **分类: cs.CV**

- **简介: 该论文提出TextSR模型，针对多语言场景文本图像超分辨率任务。解决现有扩散模型在文本区域定位不准、字符形状建模不足导致生成失真问题。通过结合OCR提取文本、UTF-8编码器转换字符形状，并设计鲁棒性方法，提升文本细节与可读性，创STISR新基准。**

- **链接: [http://arxiv.org/pdf/2505.23119v1](http://arxiv.org/pdf/2505.23119v1)**

> **作者:** Keren Ye; Ignacio Garcia Dorado; Michalis Raptis; Mauricio Delbracio; Irene Zhu; Peyman Milanfar; Hossein Talebi
>
> **摘要:** While recent advancements in Image Super-Resolution (SR) using diffusion models have shown promise in improving overall image quality, their application to scene text images has revealed limitations. These models often struggle with accurate text region localization and fail to effectively model image and multilingual character-to-shape priors. This leads to inconsistencies, the generation of hallucinated textures, and a decrease in the perceived quality of the super-resolved text. To address these issues, we introduce TextSR, a multimodal diffusion model specifically designed for Multilingual Scene Text Image Super-Resolution. TextSR leverages a text detector to pinpoint text regions within an image and then employs Optical Character Recognition (OCR) to extract multilingual text from these areas. The extracted text characters are then transformed into visual shapes using a UTF-8 based text encoder and cross-attention. Recognizing that OCR may sometimes produce inaccurate results in real-world scenarios, we have developed two innovative methods to enhance the robustness of our model. By integrating text character priors with the low-resolution text images, our model effectively guides the super-resolution process, enhancing fine details within the text and improving overall legibility. The superior performance of our model on both the TextZoom and TextVQA datasets sets a new benchmark for STISR, underscoring the efficacy of our approach.
>
---
#### [new 006] Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting
- **分类: cs.CV**

- **简介: 该论文属于大规模场景重建任务，旨在解决现有分治方法导致的全局信息丢失与参数调优复杂问题。提出MixGS框架，通过整合相机姿态与高斯参数的全局表示，并采用混合操作平衡全局一致性和局部细节，实现高效高质量重建，支持单GPU训练。**

- **链接: [http://arxiv.org/pdf/2505.23280v1](http://arxiv.org/pdf/2505.23280v1)**

> **作者:** Chuandong Liu; Huijiao Wang; Lei Yu; Gui-Song Xia
>
> **摘要:** Recent advances in 3D Gaussian Splatting have shown remarkable potential for novel view synthesis. However, most existing large-scale scene reconstruction methods rely on the divide-and-conquer paradigm, which often leads to the loss of global scene information and requires complex parameter tuning due to scene partitioning and local optimization. To address these limitations, we propose MixGS, a novel holistic optimization framework for large-scale 3D scene reconstruction. MixGS models the entire scene holistically by integrating camera pose and Gaussian attributes into a view-aware representation, which is decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation combines decoded and original Gaussians to jointly preserve global coherence and local fidelity. Extensive experiments on large-scale scenes demonstrate that MixGS achieves state-of-the-art rendering quality and competitive speed, while significantly reducing computational requirements, enabling large-scale scene reconstruction training on a single 24GB VRAM GPU. The code will be released at https://github.com/azhuantou/MixGS.
>
---
#### [new 007] Multi-Sourced Compositional Generalization in Visual Question Answering
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视觉问题回答（VQA）任务，旨在解决多源组合泛化（MSCG）问题，即模型需泛化跨模态新组合（如文本与图像元素组合）。提出检索增强训练框架，通过聚合跨模态语义等价原始特征提升统一表征能力，并构建GQA-MSCG数据集进行评估，实验验证了方法有效性。**

- **链接: [http://arxiv.org/pdf/2505.23045v1](http://arxiv.org/pdf/2505.23045v1)**

> **作者:** Chuanhao Li; Wenbo Ye; Zhen Li; Yuwei Wu; Yunde Jia
>
> **备注:** Accepted by IJCAI 2025
>
> **摘要:** Compositional generalization is the ability of generalizing novel compositions from seen primitives, and has received much attention in vision-and-language (V\&L) recently. Due to the multi-modal nature of V\&L tasks, the primitives composing compositions source from different modalities, resulting in multi-sourced novel compositions. However, the generalization ability over multi-sourced novel compositions, \textit{i.e.}, multi-sourced compositional generalization (MSCG) remains unexplored. In this paper, we explore MSCG in the context of visual question answering (VQA), and propose a retrieval-augmented training framework to enhance the MSCG ability of VQA models by learning unified representations for primitives from different modalities. Specifically, semantically equivalent primitives are retrieved for each primitive in the training samples, and the retrieved features are aggregated with the original primitive to refine the model. This process helps the model learn consistent representations for the same semantic primitives across different modalities. To evaluate the MSCG ability of VQA models, we construct a new GQA-MSCG dataset based on the GQA dataset, in which samples include three types of novel compositions composed of primitives from different modalities. Experimental results demonstrate the effectiveness of the proposed framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.
>
---
#### [new 008] DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于多任务学习在胸部X光分类的任务，旨在解决任务间贡献平衡困难及传统梯度方法效率低的问题。提出DeepChest框架，通过动态无梯度权重机制自适应调整任务重要性，提升训练速度3倍，整体准确率超现有一7%，减少负迁移，促进医疗应用。**

- **链接: [http://arxiv.org/pdf/2505.23595v1](http://arxiv.org/pdf/2505.23595v1)**

> **作者:** Youssef Mohamed; Noran Mohamed; Khaled Abouhashad; Feilong Tang; Sara Atito; Shoaib Jameel; Imran Razzak; Ahmed B. Zaky
>
> **摘要:** While Multi-Task Learning (MTL) offers inherent advantages in complex domains such as medical imaging by enabling shared representation learning, effectively balancing task contributions remains a significant challenge. This paper addresses this critical issue by introducing DeepChest, a novel, computationally efficient and effective dynamic task-weighting framework specifically designed for multi-label chest X-ray (CXR) classification. Unlike existing heuristic or gradient-based methods that often incur substantial overhead, DeepChest leverages a performance-driven weighting mechanism based on effective analysis of task-specific loss trends. Given a network architecture (e.g., ResNet18), our model-agnostic approach adaptively adjusts task importance without requiring gradient access, thereby significantly reducing memory usage and achieving a threefold increase in training speed. It can be easily applied to improve various state-of-the-art methods. Extensive experiments on a large-scale CXR dataset demonstrate that DeepChest not only outperforms state-of-the-art MTL methods by 7% in overall accuracy but also yields substantial reductions in individual task losses, indicating improved generalization and effective mitigation of negative transfer. The efficiency and performance gains of DeepChest pave the way for more practical and robust deployment of deep learning in critical medical diagnostic applications. The code is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL
>
---
#### [new 009] HiGarment: Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image
- **分类: cs.CV**

- **简介: 论文提出FS2RG任务，通过融合平面草图与文本生成逼真服装图像，解决文本指导视觉细节不足及模态冲突问题。提出HiGarment框架，包含多模态语义增强和和谐注意力机制，平衡输入并支持可控生成，同时构建新数据集验证有效性。**

- **链接: [http://arxiv.org/pdf/2505.23186v1](http://arxiv.org/pdf/2505.23186v1)**

> **作者:** Junyi Guo; Jingxuan Zhang; Fangyu Wu; Huanda Lu; Qiufeng Wang; Wenmian Yang; Eng Gee Lim; Dongming Lu
>
> **摘要:** Diffusion-based garment synthesis tasks primarily focus on the design phase in the fashion domain, while the garment production process remains largely underexplored. To bridge this gap, we introduce a new task: Flat Sketch to Realistic Garment Image (FS2RG), which generates realistic garment images by integrating flat sketches and textual guidance. FS2RG presents two key challenges: 1) fabric characteristics are solely guided by textual prompts, providing insufficient visual supervision for diffusion-based models, which limits their ability to capture fine-grained fabric details; 2) flat sketches and textual guidance may provide conflicting information, requiring the model to selectively preserve or modify garment attributes while maintaining structural coherence. To tackle this task, we propose HiGarment, a novel framework that comprises two core components: i) a multi-modal semantic enhancement mechanism that enhances fabric representation across textual and visual modalities, and ii) a harmonized cross-attention mechanism that dynamically balances information from flat sketches and text prompts, allowing controllable synthesis by generating either sketch-aligned (image-biased) or text-guided (text-biased) outputs. Furthermore, we collect Multi-modal Detailed Garment, the largest open-source dataset for garment generation. Experimental results and user studies demonstrate the effectiveness of HiGarment in garment synthesis. The code and dataset will be released.
>
---
#### [new 010] SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images
- **分类: cs.CV**

- **简介: 该论文属于语义3D重建任务，针对现有方法在稀疏图像中语义表达不足和冗余问题，提出SpatialSplat。通过分解语义为粗细特征场（粗略编码全局语义，精细捕捉局部关系），并采用选择性高斯机制去除冗余，实现参数减少60%且性能更优。**

- **链接: [http://arxiv.org/pdf/2505.23044v1](http://arxiv.org/pdf/2505.23044v1)**

> **作者:** Yu Sheng; Jiajun Deng; Xinran Zhang; Yu Zhang; Bei Hua; Yanyong Zhang; Jianmin Ji
>
> **摘要:** A major breakthrough in 3D reconstruction is the feedforward paradigm to generate pixel-wise 3D points or Gaussian primitives from sparse, unposed images. To further incorporate semantics while avoiding the significant memory and storage costs of high-dimensional semantic features, existing methods extend this paradigm by associating each primitive with a compressed semantic feature vector. However, these methods have two major limitations: (a) the naively compressed feature compromises expressiveness, affecting the model's ability to capture fine-grained semantics, and (b) the pixel-wise primitive prediction introduces redundancy in overlapping areas, causing unnecessary memory overhead. To this end, we introduce \textbf{SpatialSplat}, a feedforward framework that produces redundancy-aware Gaussians and capitalizes on a dual-field semantic representation. Particularly, with the insight that primitives within the same instance exhibit high semantic consistency, we decompose the semantic representation into a coarse feature field that encodes uncompressed semantics with minimal primitives, and a fine-grained yet low-dimensional feature field that captures detailed inter-instance relationships. Moreover, we propose a selective Gaussian mechanism, which retains only essential Gaussians in the scene, effectively eliminating redundant primitives. Our proposed Spatialsplat learns accurate semantic information and detailed instances prior with more compact 3D Gaussians, making semantic 3D reconstruction more applicable. We conduct extensive experiments to evaluate our method, demonstrating a remarkable 60\% reduction in scene representation parameters while achieving superior performance over state-of-the-art methods. The code will be made available for future investigation.
>
---
#### [new 011] 4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians
- **分类: cs.CV**

- **简介: 该论文提出4D-TAM方法，属于4D SLAM任务，解决动态非刚性场景的相机定位与表面重建问题。通过动态表面高斯、MLP变形场及可微分渲染优化，提升深度信号利用与时空重建。同时创建新合成数据集，弥补评估协议不足。**

- **链接: [http://arxiv.org/pdf/2505.22859v1](http://arxiv.org/pdf/2505.22859v1)**

> **作者:** Hidenobu Matsuki; Gwangbin Bae; Andrew J. Davison
>
> **备注:** CVPR 2025. Project Page: https://muskie82.github.io/4dtam/
>
> **摘要:** We propose the first 4D tracking and mapping method that jointly performs camera localization and non-rigid surface reconstruction via differentiable rendering. Our approach captures 4D scenes from an online stream of color images with depth measurements or predictions by jointly optimizing scene geometry, appearance, dynamics, and camera ego-motion. Although natural environments exhibit complex non-rigid motions, 4D-SLAM remains relatively underexplored due to its inherent challenges; even with 2.5D signals, the problem is ill-posed because of the high dimensionality of the optimization space. To overcome these challenges, we first introduce a SLAM method based on Gaussian surface primitives that leverages depth signals more effectively than 3D Gaussians, thereby achieving accurate surface reconstruction. To further model non-rigid deformations, we employ a warp-field represented by a multi-layer perceptron (MLP) and introduce a novel camera pose estimation technique along with surface regularization terms that facilitate spatio-temporal reconstruction. In addition to these algorithmic challenges, a significant hurdle in 4D SLAM research is the lack of reliable ground truth and evaluation protocols, primarily due to the difficulty of 4D capture using commodity sensors. To address this, we present a novel open synthetic dataset of everyday objects with diverse motions, leveraging large-scale object models and animation modeling. In summary, we open up the modern 4D-SLAM research by introducing a novel method and evaluation protocols grounded in modern vision and rendering techniques.
>
---
#### [new 012] Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object
- **分类: cs.CV**

- **简介: 该论文属于部分视角图像到3D重建任务，解决有限视角范围下传统插值失效及生成不一致问题。提出无训练方法Zero-P-to-3，融合局部观测与多源先验，通过DDIM采样对齐策略生成多视角一致图像，并利用几何结构迭代优化，提升不可见区域重建质量，超越现有方法。**

- **链接: [http://arxiv.org/pdf/2505.23054v1](http://arxiv.org/pdf/2505.23054v1)**

> **作者:** Yuxuan Lin; Ruihang Chu; Zhenyu Chen; Xiao Tang; Lei Ke; Haoling Li; Yingji Zhong; Zhihao Li; Shiyong Liu; Xiaofei Wu; Jianzhuang Liu; Yujiu Yang
>
> **摘要:** Generative 3D reconstruction shows strong potential in incomplete observations. While sparse-view and single-image reconstruction are well-researched, partial observation remains underexplored. In this context, dense views are accessible only from a specific angular range, with other perspectives remaining inaccessible. This task presents two main challenges: (i) limited View Range: observations confined to a narrow angular scope prevent effective traditional interpolation techniques that require evenly distributed perspectives. (ii) inconsistent Generation: views created for invisible regions often lack coherence with both visible regions and each other, compromising reconstruction consistency. To address these challenges, we propose \method, a novel training-free approach that integrates the local dense observations and multi-source priors for reconstruction. Our method introduces a fusion-based strategy to effectively align these priors in DDIM sampling, thereby generating multi-view consistent images to supervise invisible views. We further design an iterative refinement strategy, which uses the geometric structures of the object to enhance reconstruction quality. Extensive experiments on multiple datasets show the superiority of our method over SOTAs, especially in invisible regions.
>
---
#### [new 013] To Trust Or Not To Trust Your Vision-Language Model's Prediction
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 论文属于模型可信度评估任务，解决视觉语言模型（VLM）高自信误分类风险。提出训练-free框架TrustVLM，通过分析图像嵌入空间中概念区分度设计评分函数，提升误判检测性能，在17个数据集上实现SOTA，助力安全部署。**

- **链接: [http://arxiv.org/pdf/2505.23745v1](http://arxiv.org/pdf/2505.23745v1)**

> **作者:** Hao Dong; Moru Liu; Jian Liang; Eleni Chatzi; Olga Fink
>
> **摘要:** Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.
>
---
#### [new 014] Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks
- **分类: cs.CV; cs.AI; cs.CR**

- **简介: 该论文属于AIGC检测对抗任务，旨在通过语义提示攻击绕过检测器。针对现有检测器对文本到图像模型生成内容的识别漏洞，提出结合语法树与蒙特卡洛树搜索的自动化对抗提示生成框架，生成可规避多类检测器的可控提示，并构建对抗数据集提升检测系统。**

- **链接: [http://arxiv.org/pdf/2505.23192v1](http://arxiv.org/pdf/2505.23192v1)**

> **作者:** Run Hao; Peng Ying
>
> **备注:** 9 pages
>
> **摘要:** The rise of text-to-image (T2I) models has enabled the synthesis of photorealistic human portraits, raising serious concerns about identity misuse and the robustness of AIGC detectors. In this work, we propose an automated adversarial prompt generation framework that leverages a grammar tree structure and a variant of the Monte Carlo tree search algorithm to systematically explore the semantic prompt space. Our method generates diverse, controllable prompts that consistently evade both open-source and commercial AIGC detectors. Extensive experiments across multiple T2I models validate its effectiveness, and the approach ranked first in a real-world adversarial AIGC detection competition. Beyond attack scenarios, our method can also be used to construct high-quality adversarial datasets, providing valuable resources for training and evaluating more robust AIGC detection and defense systems.
>
---
#### [new 015] Leveraging Diffusion Models for Synthetic Data Augmentation in Protein Subcellular Localization Classification
- **分类: cs.CV**

- **简介: 该论文研究利用扩散模型生成合成数据提升蛋白质亚细胞定位多标签分类的任务。通过构建条件扩散模型生成标签一致的合成图像，并提出Mix Loss和Mix Representation两种混合训练策略。实验显示验证集性能提升，但测试集泛化效果差，表明合成数据需更真实且需强监督，传统方法更稳定，强调了数据质量和监督在生物医学图像分类中的重要性。**

- **链接: [http://arxiv.org/pdf/2505.22926v1](http://arxiv.org/pdf/2505.22926v1)**

> **作者:** Sylvey Lin; Zhi-Yi Cao
>
> **摘要:** We investigate whether synthetic images generated by diffusion models can enhance multi-label classification of protein subcellular localization. Specifically, we implement a simplified class-conditional denoising diffusion probabilistic model (DDPM) to produce label-consistent samples and explore their integration with real data via two hybrid training strategies: Mix Loss and Mix Representation. While these approaches yield promising validation performance, our proposed MixModel exhibits poor generalization to unseen test data, underscoring the challenges of leveraging synthetic data effectively. In contrast, baseline classifiers built on ResNet backbones with conventional loss functions demonstrate greater stability and test-time performance. Our findings highlight the importance of realistic data generation and robust supervision when incorporating generative augmentation into biomedical image classification.
>
---
#### [new 016] MIAS-SAM: Medical Image Anomaly Segmentation without thresholding
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文提出MIAS-SAM方法，用于医疗图像异常区域分割任务。针对传统方法依赖阈值分割的问题，其通过SAM编码器提取正常图像特征构建记忆库，推理时对比异常特征生成异常图，并利用质心定位引导解码器直接分割，无需阈值设定。实验在脑MRI、肝CT和视网膜OCT数据集上验证了高Dice分数的分割效果。**

- **链接: [http://arxiv.org/pdf/2505.22762v1](http://arxiv.org/pdf/2505.22762v1)**

> **作者:** Marco Colussi; Dragan Ahmetovic; Sergio Mascetti
>
> **摘要:** This paper presents MIAS-SAM, a novel approach for the segmentation of anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to store relevant image features, which are extracted from normal data using the SAM encoder. At inference time, the embedding patches extracted from the SAM encoder are compared with those in the memory bank to obtain the anomaly map. Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt the SAM decoder, obtaining an accurate segmentation from the previously extracted features. Differently from prior works, MIAS-SAM does not require to define a threshold value to obtain the segmentation from the anomaly map. Experimental results conducted on three publicly available datasets, each with a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show accurate anomaly segmentation capabilities measured using DICE score. The code is available at: https://github.com/warpcut/MIAS-SAM
>
---
#### [new 017] HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring
- **分类: cs.CV**

- **简介: 该论文属于端到端自动驾驶任务，旨在解决生成多样合规轨迹及选择最优路径的挑战。提出HMAD框架，结合BEV轨迹提案机制与多准则评分模块：通过锚定偏移解码生成稳定轨迹，利用模拟监督评分评估安全性、合规性等指标，实现44.5%的测试得分，强调生成与评分解耦的优势。**

- **链接: [http://arxiv.org/pdf/2505.23129v1](http://arxiv.org/pdf/2505.23129v1)**

> **作者:** Bin Wang; Pingjun Li; Jinkun Liu; Jun Cheng; Hailong Lei; Yinze Rong; Huan-ang Gao; Kangliang Chen; Xing Pan; Weihao Gu
>
> **摘要:** End-to-end autonomous driving faces persistent challenges in both generating diverse, rule-compliant trajectories and robustly selecting the optimal path from these options via learned, multi-faceted evaluation. To address these challenges, we introduce HMAD, a framework integrating a distinctive Bird's-Eye-View (BEV) based trajectory proposal mechanism with learned multi-criteria scoring. HMAD leverages BEVFormer and employs learnable anchored queries, initialized from a trajectory dictionary and refined via iterative offset decoding (inspired by DiffusionDrive), to produce numerous diverse and stable candidate trajectories. A key innovation, our simulation-supervised scorer module, then evaluates these proposals against critical metrics including no at-fault collisions, drivable area compliance, comfortableness, and overall driving quality (i.e., extended PDM score). Demonstrating its efficacy, HMAD achieves a 44.5% driving score on the CVPR 2025 private test set. This work highlights the benefits of effectively decoupling robust trajectory generation from comprehensive, safety-aware learned scoring for advanced autonomous driving.
>
---
#### [new 018] A Reverse Causal Framework to Mitigate Spurious Correlations for Debiasing Scene Graph Generation
- **分类: cs.CV; I.2.10; I.4.8**

- **简介: 该论文针对场景图生成任务，提出Reverse Causal Framework（RcSGG）解决现有两阶段框架因因果链结构导致的虚假相关问题，如尾部关系误判为头部关系、前景关系误为背景。通过反转因果结构，采用Active Reverse Estimation（ARE）和Maximum Information Sampling（MIS）干预混淆变量，消除预测偏差以提升生成精度。**

- **链接: [http://arxiv.org/pdf/2505.23451v1](http://arxiv.org/pdf/2505.23451v1)**

> **作者:** Shuzhou Sun; Li Liu; Tianpeng Liu; Shuaifeng Zhi; Ming-Ming Cheng; Janne Heikkilä; Yongxiang Liu
>
> **备注:** Accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 21 pages, 11 figures, 12 tables
>
> **摘要:** Existing two-stage Scene Graph Generation (SGG) frameworks typically incorporate a detector to extract relationship features and a classifier to categorize these relationships; therefore, the training paradigm follows a causal chain structure, where the detector's inputs determine the classifier's inputs, which in turn influence the final predictions. However, such a causal chain structure can yield spurious correlations between the detector's inputs and the final predictions, i.e., the prediction of a certain relationship may be influenced by other relationships. This influence can induce at least two observable biases: tail relationships are predicted as head ones, and foreground relationships are predicted as background ones; notably, the latter bias is seldom discussed in the literature. To address this issue, we propose reconstructing the causal chain structure into a reverse causal structure, wherein the classifier's inputs are treated as the confounder, and both the detector's inputs and the final predictions are viewed as causal variables. Specifically, we term the reconstructed causal paradigm as the Reverse causal Framework for SGG (RcSGG). RcSGG initially employs the proposed Active Reverse Estimation (ARE) to intervene on the confounder to estimate the reverse causality, \ie the causality from final predictions to the classifier's inputs. Then, the Maximum Information Sampling (MIS) is suggested to enhance the reverse causality estimation further by considering the relationship information. Theoretically, RcSGG can mitigate the spurious correlations inherent in the SGG framework, subsequently eliminating the induced biases. Comprehensive experiments on popular benchmarks and diverse SGG frameworks show the state-of-the-art mean recall rate.
>
---
#### [new 019] Improving Contrastive Learning for Referring Expression Counting
- **分类: cs.CV**

- **简介: 该论文针对Referring Expression Counting（REC）任务，解决视觉相似但属性不同的物体计数问题。提出C-REX框架，通过纯图像空间的对比学习增强判别性特征，利用大量负样本提升鲁棒性，并采用物体中心点检测基线。实验显示其在REC和类别无关计数中达SOTA，MAE/RMSE提升超22%/10%。**

- **链接: [http://arxiv.org/pdf/2505.22850v1](http://arxiv.org/pdf/2505.22850v1)**

> **作者:** Kostas Triaridis; Panagiotis Kaliosis; E-Ro Nguyen; Jingyi Xu; Hieu Le; Dimitris Samaras
>
> **备注:** 9 pages, 4 figures
>
> **摘要:** Object counting has progressed from class-specific models, which count only known categories, to class-agnostic models that generalize to unseen categories. The next challenge is Referring Expression Counting (REC), where the goal is to count objects based on fine-grained attributes and contextual differences. Existing methods struggle with distinguishing visually similar objects that belong to the same category but correspond to different referring expressions. To address this, we propose C-REX, a novel contrastive learning framework, based on supervised contrastive learning, designed to enhance discriminative representation learning. Unlike prior works, C-REX operates entirely within the image space, avoiding the misalignment issues of image-text contrastive learning, thus providing a more stable contrastive signal. It also guarantees a significantly larger pool of negative samples, leading to improved robustness in the learned representations. Moreover, we showcase that our framework is versatile and generic enough to be applied to other similar tasks like class-agnostic counting. To support our approach, we analyze the key components of sota detection-based models and identify that detecting object centroids instead of bounding boxes is the key common factor behind their success in counting tasks. We use this insight to design a simple yet effective detection-based baseline to build upon. Our experiments show that C-REX achieves state-of-the-art results in REC, outperforming previous methods by more than 22\% in MAE and more than 10\% in RMSE, while also demonstrating strong performance in class-agnostic counting. Code is available at https://github.com/cvlab-stonybrook/c-rex.
>
---
#### [new 020] Federated Unsupervised Semantic Segmentation
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于联邦无监督语义分割任务，旨在解决分布式异构数据下特征与聚类中心对齐难题。提出FUSS框架，通过新型联邦策略优化全局特征与原型一致性，实现去中心化无监督分割训练，实验显示优于传统方法。**

- **链接: [http://arxiv.org/pdf/2505.23292v1](http://arxiv.org/pdf/2505.23292v1)**

> **作者:** Evangelos Charalampakis; Vasileios Mygdalis; Ioannis Pitas
>
> **摘要:** This work explores the application of Federated Learning (FL) in Unsupervised Semantic image Segmentation (USS). Recent USS methods extract pixel-level features using frozen visual foundation models and refine them through self-supervised objectives that encourage semantic grouping. These features are then grouped to semantic clusters to produce segmentation masks. Extending these ideas to federated settings requires feature representation and cluster centroid alignment across distributed clients -- an inherently difficult task under heterogeneous data distributions in the absence of supervision. To address this, we propose FUSS Federated Unsupervised image Semantic Segmentation) which is, to our knowledge, the first framework to enable fully decentralized, label-free semantic segmentation training. FUSS introduces novel federation strategies that promote global consistency in feature and prototype space, jointly optimizing local segmentation heads and shared semantic centroids. Experiments on both benchmark and real-world datasets, including binary and multi-class segmentation tasks, show that FUSS consistently outperforms local-only client trainings as well as extensions of classical FL algorithms under varying client data distributions. To support reproducibility, full code will be released upon manuscript acceptance.
>
---
#### [new 021] Video Editing for Audio-Visual Dubbing
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文提出EdiDub框架，解决现有视觉配音方法在保留原始场景信息（如遮挡、光照）和同步精度上的不足。通过内容感知编辑任务，利用专用条件方案精准修改原视频而非生成/修复，提升身份保持与同步性能，在多个数据集上表现更优。**

- **链接: [http://arxiv.org/pdf/2505.23406v1](http://arxiv.org/pdf/2505.23406v1)**

> **作者:** Binyamin Manela; Sharon Gannot; Ethan Fetyaya
>
> **摘要:** Visual dubbing, the synchronization of facial movements with new speech, is crucial for making content accessible across different languages, enabling broader global reach. However, current methods face significant limitations. Existing approaches often generate talking faces, hindering seamless integration into original scenes, or employ inpainting techniques that discard vital visual information like partial occlusions and lighting variations. This work introduces EdiDub, a novel framework that reformulates visual dubbing as a content-aware editing task. EdiDub preserves the original video context by utilizing a specialized conditioning scheme to ensure faithful and accurate modifications rather than mere copying. On multiple benchmarks, including a challenging occluded-lip dataset, EdiDub significantly improves identity preservation and synchronization. Human evaluations further confirm its superiority, achieving higher synchronization and visual naturalness scores compared to the leading methods. These results demonstrate that our content-aware editing approach outperforms traditional generation or inpainting, particularly in maintaining complex visual elements while ensuring accurate lip synchronization.
>
---
#### [new 022] MAGREF: Masked Guidance for Any-Reference Video Generation
- **分类: cs.CV; cs.AI**

- **简介: 该论文提出MAGREF框架，针对多参考视频生成任务，解决多主体一致性与高质量生成问题。通过区域感知动态掩码机制灵活处理各类主体，及像素级通道拼接保留外观特征，实现高保真、可控的多主体视频合成，并建立多主体评估基准。**

- **链接: [http://arxiv.org/pdf/2505.23742v1](http://arxiv.org/pdf/2505.23742v1)**

> **作者:** Yufan Deng; Xun Guo; Yuanyang Yin; Jacob Zhiyuan Fang; Yiding Yang; Yizhi Wang; Shenghai Yuan; Angtian Wang; Bo Liu; Haibin Huang; Chongyang Ma
>
> **备注:** Project website: https://magref-video.github.io/magref.github.io/
>
> **摘要:** Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF
>
---
#### [new 023] GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion
- **分类: cs.CV; cs.AI; eess.IV**

- **简介: GeoMan提出基于图像到视频扩散模型的时序一致人体3D几何估计方法，解决现有方法在视频中存在时间不一致及动态细节丢失的问题。通过首帧图像模型初始化深度/法线，结合视频扩散模型生成连续几何，并采用相对根节点深度表示提升人体尺寸精度，在少4D数据下实现视频人体几何的高质量估计。**

- **链接: [http://arxiv.org/pdf/2505.23085v1](http://arxiv.org/pdf/2505.23085v1)**

> **作者:** Gwanghyun Kim; Xueting Li; Ye Yuan; Koki Nagano; Tianye Li; Jan Kautz; Se Young Chun; Umar Iqbal
>
> **备注:** Project page: https://research.nvidia.com/labs/dair/geoman
>
> **摘要:** Estimating accurate and temporally consistent 3D human geometry from videos is a challenging problem in computer vision. Existing methods, primarily optimized for single images, often suffer from temporal inconsistencies and fail to capture fine-grained dynamic details. To address these limitations, we present GeoMan, a novel architecture designed to produce accurate and temporally consistent depth and normal estimations from monocular human videos. GeoMan addresses two key challenges: the scarcity of high-quality 4D training data and the need for metric depth estimation to accurately model human size. To overcome the first challenge, GeoMan employs an image-based model to estimate depth and normals for the first frame of a video, which then conditions a video diffusion model, reframing video geometry estimation task as an image-to-video generation problem. This design offloads the heavy lifting of geometric estimation to the image model and simplifies the video model's role to focus on intricate details while using priors learned from large-scale video datasets. Consequently, GeoMan improves temporal consistency and generalizability while requiring minimal 4D training data. To address the challenge of accurate human size estimation, we introduce a root-relative depth representation that retains critical human-scale details and is easier to be estimated from monocular inputs, overcoming the limitations of traditional affine-invariant and metric depth representations. GeoMan achieves state-of-the-art performance in both qualitative and quantitative evaluations, demonstrating its effectiveness in overcoming longstanding challenges in 3D human geometry estimation from videos.
>
---
#### [new 024] URWKV: Unified RWKV Model with Multi-state Perspective for Low-light Image Restoration
- **分类: cs.CV**

- **简介: 该论文属于低光图像修复任务，针对现有模型难以处理动态耦合退化的问题，提出URWKV模型。通过多状态机制（Luminance-adaptive Normalization、多阶段状态聚合、State-aware Selective Fusion模块），实现自适应亮度调节、特征动态融合，提升复杂退化场景的修复效果，参数与计算成本更低。**

- **链接: [http://arxiv.org/pdf/2505.23068v1](http://arxiv.org/pdf/2505.23068v1)**

> **作者:** Rui Xu; Yuzhen Niu; Yuezhou Li; Huangbiao Xu; Wenxi Liu; Yuzhong Chen
>
> **备注:** This paper has been accepted to CVPR 2025
>
> **摘要:** Existing low-light image enhancement (LLIE) and joint LLIE and deblurring (LLIE-deblur) models have made strides in addressing predefined degradations, yet they are often constrained by dynamically coupled degradations. To address these challenges, we introduce a Unified Receptance Weighted Key Value (URWKV) model with multi-state perspective, enabling flexible and effective degradation restoration for low-light images. Specifically, we customize the core URWKV block to perceive and analyze complex degradations by leveraging multiple intra- and inter-stage states. First, inspired by the pupil mechanism in the human visual system, we propose Luminance-adaptive Normalization (LAN) that adjusts normalization parameters based on rich inter-stage states, allowing for adaptive, scene-aware luminance modulation. Second, we aggregate multiple intra-stage states through exponential moving average approach, effectively capturing subtle variations while mitigating information loss inherent in the single-state mechanism. To reduce the degradation effects commonly associated with conventional skip connections, we propose the State-aware Selective Fusion (SSF) module, which dynamically aligns and integrates multi-state features across encoder stages, selectively fusing contextual information. In comparison to state-of-the-art models, our URWKV model achieves superior performance on various benchmarks, while requiring significantly fewer parameters and computational resources.
>
---
#### [new 025] UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors
- **分类: cs.CV**

- **简介: 该论文属于城市场景外推视图合成（EVS）任务。解决现有方法在训练外视角（如侧视、下视）性能差及图像扩散控制粗糙的问题。提出UrbanCraft，通过分层语义-几何先验（占用网格建场景级基底、3D框增细节）及HSG-VSD模型，融合先验约束提升未见视角重建精度。**

- **链接: [http://arxiv.org/pdf/2505.23434v1](http://arxiv.org/pdf/2505.23434v1)**

> **作者:** Tianhang Wang; Fan Lu; Sanqing Qu; Guo Yu; Shihang Du; Ya Wu; Yuan Huang; Guang Chen
>
> **摘要:** Existing neural rendering-based urban scene reconstruction methods mainly focus on the Interpolated View Synthesis (IVS) setting that synthesizes from views close to training camera trajectory. However, IVS can not guarantee the on-par performance of the novel view outside the training camera distribution (\textit{e.g.}, looking left, right, or downwards), which limits the generalizability of the urban reconstruction application. Previous methods have optimized it via image diffusion, but they fail to handle text-ambiguous or large unseen view angles due to coarse-grained control of text-only diffusion. In this paper, we design UrbanCraft, which surmounts the Extrapolated View Synthesis (EVS) problem using hierarchical sem-geometric representations serving as additional priors. Specifically, we leverage the partially observable scene to reconstruct coarse semantic and geometric primitives, establishing a coarse scene-level prior through an occupancy grid as the base representation. Additionally, we incorporate fine instance-level priors from 3D bounding boxes to enhance object-level details and spatial relationships. Building on this, we propose the \textbf{H}ierarchical \textbf{S}emantic-Geometric-\textbf{G}uided Variational Score Distillation (HSG-VSD), which integrates semantic and geometric constraints from pretrained UrbanCraft2D into the score distillation sampling process, forcing the distribution to be consistent with the observable scene. Qualitative and quantitative comparisons demonstrate the effectiveness of our methods on EVS problem.
>
---
#### [new 026] D-AR: Diffusion via Autoregressive Models
- **分类: cs.CV**

- **简介: 该论文提出D-AR方法，将图像扩散过程转化为标准自回归序列生成任务。针对传统扩散模型步骤繁琐的问题，通过设计图像分词器将图像编码为离散token序列，利用自回归模型预测token顺序，实现逐步去噪。支持流式生成中间结果及零样本布局控制，在ImageNet上达2.09 FID，推动视觉合成与大模型融合。**

- **链接: [http://arxiv.org/pdf/2505.23660v1](http://arxiv.org/pdf/2505.23660v1)**

> **作者:** Ziteng Gao; Mike Zheng Shou
>
> **备注:** Technical report
>
> **摘要:** This paper presents Diffusion via Autoregressive models (D-AR), a new paradigm recasting the image diffusion process as a vanilla autoregressive procedure in the standard next-token-prediction fashion. We start by designing the tokenizer that converts images into sequences of discrete tokens, where tokens in different positions can be decoded into different diffusion denoising steps in the pixel space. Thanks to the diffusion properties, these tokens naturally follow a coarse-to-fine order, which directly lends itself to autoregressive modeling. Therefore, we apply standard next-token prediction on these tokens, without modifying any underlying designs (either causal masks or training/inference strategies), and such sequential autoregressive token generation directly mirrors the diffusion procedure in image space. That is, once the autoregressive model generates an increment of tokens, we can directly decode these tokens into the corresponding diffusion denoising step in the streaming manner. Our pipeline naturally reveals several intriguing properties, for example, it supports consistent previews when generating only a subset of tokens and enables zero-shot layout-controlled synthesis. On the standard ImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone with 256 discrete tokens. We hope our work can inspire future research on unified autoregressive architectures of visual synthesis, especially with large language models. Code and models will be available at https://github.com/showlab/D-AR
>
---
#### [new 027] MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence
- **分类: cs.CV; cs.CL**

- **简介: 该论文提出MMSI-Bench，一个多图像空间智能基准，解决现有评估仅测试单图像关系的问题。通过构建1000道多选题（含干扰项与推理步骤），评估34个模型，揭示其与人类（97%）的显著差距，并分析四大错误模式，推动多图像空间推理研究。**

- **链接: [http://arxiv.org/pdf/2505.23764v1](http://arxiv.org/pdf/2505.23764v1)**

> **作者:** Sihan Yang; Runsen Xu; Yiman Xie; Sizhe Yang; Mo Li; Jingli Lin; Chenming Zhu; Xiaochen Chen; Haodong Duan; Xiangyu Yue; Dahua Lin; Tai Wang; Jiangmiao Pang
>
> **备注:** 34 pages. A comprehensive, fully human-curated, multi-image-based spatial intelligence benchmark with reasoning annotation for MLLMs. Project page: https://runsenxu.com/projects/MMSI_Bench
>
> **摘要:** Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench .
>
---
#### [new 028] Using Cross-Domain Detection Loss to Infer Multi-Scale Information for Improved Tiny Head Tracking
- **分类: cs.CV**

- **简介: 该论文属于目标检测与追踪任务，旨在解决现有方法在小目标（如微小头部）检测中计算资源消耗大、实时性差的问题。提出跨域检测损失、多尺度模块及小感受野检测机制，优化性能与效率平衡，提升拥挤场景下的检测精度与追踪稳定性，实验显示MOTA和mAP指标提升。**

- **链接: [http://arxiv.org/pdf/2505.22677v1](http://arxiv.org/pdf/2505.22677v1)**

> **作者:** Jisu Kim; Alex Mattingly; Eung-Joo Lee; Benjamin S. Riggan
>
> **备注:** To appear at IEEE International Conference on Automatic Face and Gesture 2025 (FG2025)
>
> **摘要:** Head detection and tracking are essential for downstream tasks, but current methods often require large computational budgets, which increase latencies and ties up resources (e.g., processors, memory, and bandwidth). To address this, we propose a framework to enhance tiny head detection and tracking by optimizing the balance between performance and efficiency. Our framework integrates (1) a cross-domain detection loss, (2) a multi-scale module, and (3) a small receptive field detection mechanism. These innovations enhance detection by bridging the gap between large and small detectors, capturing high-frequency details at multiple scales during training, and using filters with small receptive fields to detect tiny heads. Evaluations on the CroHD and CrowdHuman datasets show improved Multiple Object Tracking Accuracy (MOTA) and mean Average Precision (mAP), demonstrating the effectiveness of our approach in crowded scenes.
>
---
#### [new 029] PreFM: Online Audio-Visual Event Parsing via Predictive Future Modeling
- **分类: cs.CV**

- **简介: 该论文提出在线视听事件解析任务（On-AVEP），解决传统方法依赖离线处理、实时性差的问题。通过PreFM框架，利用预测未来多模态信息与模态无关表征，实现高效精准的视频流实时解析，在参数更少的情况下超越现有方法。**

- **链接: [http://arxiv.org/pdf/2505.23155v1](http://arxiv.org/pdf/2505.23155v1)**

> **作者:** Xiao Yu; Yan Fang; Xiaojie Jin; Yao Zhao; Yunchao Wei
>
> **备注:** 20 pages, 8 figures
>
> **摘要:** Audio-visual event parsing plays a crucial role in understanding multimodal video content, but existing methods typically rely on offline processing of entire videos with huge model sizes, limiting their real-time applicability. We introduce Online Audio-Visual Event Parsing (On-AVEP), a novel paradigm for parsing audio, visual, and audio-visual events by sequentially analyzing incoming video streams. The On-AVEP task necessitates models with two key capabilities: (1) Accurate online inference, to effectively distinguish events with unclear and limited context in online settings, and (2) Real-time efficiency, to balance high performance with computational constraints. To cultivate these, we propose the Predictive Future Modeling (PreFM) framework featured by (a) predictive multimodal future modeling to infer and integrate beneficial future audio-visual cues, thereby enhancing contextual understanding and (b) modality-agnostic robust representation along with focal temporal prioritization to improve precision and generalization. Extensive experiments on the UnAV-100 and LLP datasets show PreFM significantly outperforms state-of-the-art methods by a large margin with significantly fewer parameters, offering an insightful approach for real-time multimodal video understanding. Code is available at https://github.com/XiaoYu-1123/PreFM.
>
---
#### [new 030] Advancing Image Super-resolution Techniques in Remote Sensing: A Comprehensive Survey
- **分类: cs.CV**

- **简介: 该论文综述遥感图像超分辨率（RSISR）技术，旨在系统分析现有方法以解决高分辨率图像重建中的细节与结构保留不足问题。工作包括分类方法（监督/无监督/质量评估）、评估其优劣、指出大尺度降级下的局限性，并提出需开发领域专用架构及 robust 评估协议的未来方向。**

- **链接: [http://arxiv.org/pdf/2505.23248v1](http://arxiv.org/pdf/2505.23248v1)**

> **作者:** Yunliang Qi; Meng Lou; Yimin Liu; Lu Li; Zhen Yang; Wen Nie
>
> **备注:** 31 pages,7 figures, an survey
>
> **摘要:** Remote sensing image super-resolution (RSISR) is a crucial task in remote sensing image processing, aiming to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts. Despite the growing number of RSISR methods proposed in recent years, a systematic and comprehensive review of these methods is still lacking. This paper presents a thorough review of RSISR algorithms, covering methodologies, datasets, and evaluation metrics. We provide an in-depth analysis of RSISR methods, categorizing them into supervised, unsupervised, and quality evaluation approaches, to help researchers understand current trends and challenges. Our review also discusses the strengths, limitations, and inherent challenges of these techniques. Notably, our analysis reveals significant limitations in existing methods, particularly in preserving fine-grained textures and geometric structures under large-scale degradation. Based on these findings, we outline future research directions, highlighting the need for domain-specific architectures and robust evaluation protocols to bridge the gap between synthetic and real-world RSISR scenarios.
>
---
#### [new 031] CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting
- **分类: cs.CV**

- **简介: 论文提出CLIPGaussian，首个基于高斯散射的多模态风格迁移框架。解决现有方法在3D/4D场景中难以实现复杂风格迁移的挑战，支持文本/图像引导的2D、视频、3D对象及4D场景的样式转换。方法通过直接优化高斯原语，集成现有渲染流程为插件，无需大模型或重训，实现颜色几何联合优化与视频时序连贯性。**

- **链接: [http://arxiv.org/pdf/2505.22854v1](http://arxiv.org/pdf/2505.22854v1)**

> **作者:** Kornel Howil; Joanna Waczyńska; Piotr Borycki; Tadeusz Dziarmaga; Marcin Mazur; Przemysław Spurek
>
> **摘要:** Gaussian Splatting (GS) has recently emerged as an efficient representation for rendering 3D scenes from 2D images and has been extended to images, videos, and dynamic 4D content. However, applying style transfer to GS-based representations, especially beyond simple color changes, remains challenging. In this work, we introduce CLIPGaussians, the first unified style transfer framework that supports text- and image-guided stylization across multiple modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates directly on Gaussian primitives and integrates into existing GS pipelines as a plug-in module, without requiring large generative models or retraining from scratch. CLIPGaussians approach enables joint optimization of color and geometry in 3D and 4D settings, and achieves temporal coherence in videos, while preserving a model size. We demonstrate superior style fidelity and consistency across all tasks, validating CLIPGaussians as a universal and efficient solution for multimodal style transfer.
>
---
#### [new 032] Position Paper: Metadata Enrichment Model: Integrating Neural Networks and Semantic Knowledge Graphs for Cultural Heritage Applications
- **分类: cs.CV**

- **简介: 该论文提出元数据增强模型（MEM），结合神经网络与语义知识图谱，解决文化遗产数字化中元数据不足导致的可访问性与协作问题。通过多层视觉机制（MVM）动态提取嵌套视觉特征，应用于古籍数据集，并探讨实际应用挑战，如领域微调与计算成本。**

- **链接: [http://arxiv.org/pdf/2505.23543v1](http://arxiv.org/pdf/2505.23543v1)**

> **作者:** Jan Ignatowicz; Krzysztof Kutt; Grzegorz J. Nalepa
>
> **摘要:** The digitization of cultural heritage collections has opened new directions for research, yet the lack of enriched metadata poses a substantial challenge to accessibility, interoperability, and cross-institutional collaboration. In several past years neural networks models such as YOLOv11 and Detectron2 have revolutionized visual data analysis, but their application to domain-specific cultural artifacts - such as manuscripts and incunabula - remains limited by the absence of methodologies that address structural feature extraction and semantic interoperability. In this position paper, we argue, that the integration of neural networks with semantic technologies represents a paradigm shift in cultural heritage digitization processes. We present the Metadata Enrichment Model (MEM), a conceptual framework designed to enrich metadata for digitized collections by combining fine-tuned computer vision models, large language models (LLMs) and structured knowledge graphs. The Multilayer Vision Mechanism (MVM) appears as the key innovation of MEM. This iterative process improves visual analysis by dynamically detecting nested features, such as text within seals or images within stamps. To expose MEM's potential, we apply it to a dataset of digitized incunabula from the Jagiellonian Digital Library and release a manually annotated dataset of 105 manuscript pages. We examine the practical challenges of MEM's usage in real-world GLAM institutions, including the need for domain-specific fine-tuning, the adjustment of enriched metadata with Linked Data standards and computational costs. We present MEM as a flexible and extensible methodology. This paper contributes to the discussion on how artificial intelligence and semantic web technologies can advance cultural heritage research, and also use these technologies in practice.
>
---
#### [new 033] Fast Isotropic Median Filtering
- **分类: cs.CV; cs.DS**

- **简介: 该论文提出一种快速各向同性中值滤波方法，解决传统算法受图像位深、滤波核尺寸及形状（如方形导致条纹artifact）限制的问题，实现对任意位深、核大小及凸形状（含圆形）的高效处理，消除条纹并提升抗噪能力。**

- **链接: [http://arxiv.org/pdf/2505.22938v1](http://arxiv.org/pdf/2505.22938v1)**

> **作者:** Ben Weiss
>
> **备注:** Supplemental material: https://github.com/google/fast-isotropic-median-filter
>
> **摘要:** Median filtering is a cornerstone of computational image processing. It provides an effective means of image smoothing, with minimal blurring or softening of edges, invariance to monotonic transformations such as gamma adjustment, and robustness to noise and outliers. However, known algorithms have all suffered from practical limitations: the bit depth of the image data, the size of the filter kernel, or the kernel shape itself. Square-kernel implementations tend to produce streaky cross-hatching artifacts, and nearly all known efficient algorithms are in practice limited to square kernels. We present for the first time a method that overcomes all of these limitations. Our method operates efficiently on arbitrary bit-depth data, arbitrary kernel sizes, and arbitrary convex kernel shapes, including circular shapes.
>
---
#### [new 034] PixelThink: Towards Efficient Chain-of-Pixel Reasoning
- **分类: cs.CV; cs.MM**

- **简介: 该论文属于多模态分割任务，旨在解决现有方法在分布外场景泛化能力不足及推理冗余导致效率低下的问题。提出PixelThink，通过整合任务难度与模型不确定性，在强化学习框架中动态调节推理链长度，提升推理效率与分割性能，并构建ReasonSeg-Diff基准进行综合评估。**

- **链接: [http://arxiv.org/pdf/2505.23727v1](http://arxiv.org/pdf/2505.23727v1)**

> **作者:** Song Wang; Gongfan Fang; Lingdong Kong; Xiangtai Li; Jianyun Xu; Sheng Yang; Qiang Li; Jianke Zhu; Xinchao Wang
>
> **备注:** Project Page: https://PixelThink.github.io
>
> **摘要:** Existing reasoning segmentation approaches typically fine-tune multimodal large language models (MLLMs) using image-text pairs and corresponding mask labels. However, they exhibit limited generalization to out-of-distribution scenarios without an explicit reasoning process. Although recent efforts leverage reinforcement learning through group-relative policy optimization (GRPO) to enhance reasoning ability, they often suffer from overthinking - producing uniformly verbose reasoning chains irrespective of task complexity. This results in elevated computational costs and limited control over reasoning quality. To address this problem, we propose PixelThink, a simple yet effective scheme that integrates externally estimated task difficulty and internally measured model uncertainty to regulate reasoning generation within a reinforcement learning paradigm. The model learns to compress reasoning length in accordance with scene complexity and predictive confidence. To support comprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark with annotated reasoning references and difficulty scores, along with a suite of metrics designed to assess segmentation accuracy, reasoning quality, and efficiency jointly. Experimental results demonstrate that the proposed approach improves both reasoning efficiency and overall segmentation performance. Our work contributes novel perspectives towards efficient and interpretable multimodal understanding. The code and model will be publicly available.
>
---
#### [new 035] SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于红外小目标检测（ISTD）任务，旨在解决小目标占比极低（<0.15%）、背景复杂导致的区分困难，以及现有方法信息丢失和全局建模效率低的问题。提出SAMamba框架，通过FS-Adapter实现跨领域自适应，CSI模块高效建模全局上下文，DPCF模块融合多尺度特征，显著提升检测性能。**

- **链接: [http://arxiv.org/pdf/2505.23214v1](http://arxiv.org/pdf/2505.23214v1)**

> **作者:** Wenhao Xu; Shuchen Zheng; Changwei Wang; Zherui Zhang; Chuan Ren; Rongtao Xu; Shibiao Xu
>
> **备注:** Information Fusion 2025
>
> **摘要:** Infrared small target detection (ISTD) is vital for long-range surveillance in military, maritime, and early warning applications. ISTD is challenged by targets occupying less than 0.15% of the image and low distinguishability from complex backgrounds. Existing deep learning methods often suffer from information loss during downsampling and inefficient global context modeling. This paper presents SAMamba, a novel framework integrating SAM2's hierarchical feature learning with Mamba's selective sequence modeling. Key innovations include: (1) A Feature Selection Adapter (FS-Adapter) for efficient natural-to-infrared domain adaptation via dual-stage selection (token-level with a learnable task embedding and channel-wise adaptive transformations); (2) A Cross-Channel State-Space Interaction (CSI) module for efficient global context modeling with linear complexity using selective state space modeling; and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively combines multi-scale features with a gating mechanism to balance high-resolution and low-resolution feature contributions. SAMamba addresses core ISTD challenges by bridging the domain gap, maintaining fine-grained details, and efficiently modeling long-range dependencies. Experiments on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly outperforms state-of-the-art methods, especially in challenging scenarios with heterogeneous backgrounds and varying target scales. Code: https://github.com/zhengshuchen/SAMamba.
>
---
#### [new 036] Point or Line? Using Line-based Representation for Panoptic Symbol Spotting in CAD Drawings
- **分类: cs.CV**

- **简介: 该论文针对CAD图纸中的全景符号检测任务，解决现有方法计算成本高、结构信息丢失问题，提出VecFormer，采用线基表示保留几何连续性，并设计分支融合模块整合预测，实现91.1 PQ的SOTA性能，显著提升语义区域检测效果。**

- **链接: [http://arxiv.org/pdf/2505.23395v1](http://arxiv.org/pdf/2505.23395v1)**

> **作者:** Xingguang Wei; Haomin Wang; Shenglong Ye; Ruifeng Luo; Yanting Zhang; Lixin Gu; Jifeng Dai; Yu Qiao; Wenhai Wang; Hongjie Zhang
>
> **摘要:** We study the task of panoptic symbol spotting, which involves identifying both individual instances of countable things and the semantic regions of uncountable stuff in computer-aided design (CAD) drawings composed of vector graphical primitives. Existing methods typically rely on image rasterization, graph construction, or point-based representation, but these approaches often suffer from high computational costs, limited generality, and loss of geometric structural information. In this paper, we propose VecFormer, a novel method that addresses these challenges through line-based representation of primitives. This design preserves the geometric continuity of the original primitive, enabling more accurate shape representation while maintaining a computation-friendly structure, making it well-suited for vector graphic understanding tasks. To further enhance prediction reliability, we introduce a Branch Fusion Refinement module that effectively integrates instance and semantic predictions, resolving their inconsistencies for more coherent panoptic outputs. Extensive experiments demonstrate that our method establishes a new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and 21.2 points over the second-best results under settings with and without prior information, respectively, highlighting the strong potential of line-based representation as a foundation for vector graphic understanding.
>
---
#### [new 037] HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions
- **分类: cs.CV**

- **简介: 该论文属于姿势引导的人体图像动画生成任务，针对复杂动态动作（如非标准高动态动作）中结构不稳定和外观不一致的问题，提出Open-HyperMotionX数据集与评估基准，并设计DiT基线模型及空间低频增强RoPE模块，提升复杂动作动画的生成质量。**

- **链接: [http://arxiv.org/pdf/2505.22977v1](http://arxiv.org/pdf/2505.22977v1)**

> **作者:** Shuolin Xu; Siming Zheng; Ziyi Wang; HC Yu; Jinwei Chen; Huaqi Zhang; Bo Li; Peng-Tao Jiang
>
> **备注:** 17 pages, 7 figures
>
> **摘要:** Recent advances in diffusion models have significantly improved conditional video generation, particularly in the pose-guided human image animation task. Although existing methods are capable of generating high-fidelity and time-consistent animation sequences in regular motions and static scenes, there are still obvious limitations when facing complex human body motions (Hypermotion) that contain highly dynamic, non-standard motions, and the lack of a high-quality benchmark for evaluation of complex human motion animations. To address this challenge, we introduce the \textbf{Open-HyperMotionX Dataset} and \textbf{HyperMotionX Bench}, which provide high-quality human pose annotations and curated video clips for evaluating and improving pose-guided human image animation models under complex human motion conditions. Furthermore, we propose a simple yet powerful DiT-based video generation baseline and design spatial low-frequency enhanced RoPE, a novel module that selectively enhances low-frequency spatial feature modeling by introducing learnable frequency scaling. Our method significantly improves structural stability and appearance consistency in highly dynamic human motion sequences. Extensive experiments demonstrate the effectiveness of our dataset and proposed approach in advancing the generation quality of complex human motion image animations. Code and dataset will be made publicly available.
>
---
#### [new 038] Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving
- **分类: cs.CV**

- **简介: 该论文属于自动驾驶中的3D占用预测任务，旨在解决现有判别方法在噪声、不完整观测及复杂3D结构下的不足。提出基于扩散模型的生成方法，通过学习数据分布和3D场景先验，提升预测一致性与抗噪性，实验表明其在遮挡/低能见区域的预测更准确，且能优化路径规划等下游任务。**

- **链接: [http://arxiv.org/pdf/2505.23115v1](http://arxiv.org/pdf/2505.23115v1)**

> **作者:** Yunshen Wang; Yicheng Liu; Tianyuan Yuan; Yucheng Mao; Yingshi Liang; Xiuyu Yang; Honggang Zhang; Hang Zhao
>
> **备注:** ICRA 2025
>
> **摘要:** Accurately predicting 3D occupancy grids from visual inputs is critical for autonomous driving, but current discriminative methods struggle with noisy data, incomplete observations, and the complex structures inherent in 3D scenes. In this work, we reframe 3D occupancy prediction as a generative modeling task using diffusion models, which learn the underlying data distribution and incorporate 3D scene priors. This approach enhances prediction consistency, noise robustness, and better handles the intricacies of 3D spatial structures. Our extensive experiments show that diffusion-based generative models outperform state-of-the-art discriminative approaches, delivering more realistic and accurate occupancy predictions, especially in occluded or low-visibility regions. Moreover, the improved predictions significantly benefit downstream planning tasks, highlighting the practical advantages of our method for real-world autonomous driving applications.
>
---
#### [new 039] LeMoRe: Learn More Details for Lightweight Semantic Segmentation
- **分类: cs.CV**

- **简介: 该论文属于轻量级语义分割任务，旨在解决现有方法在效率与性能间失衡的问题。通过融合显式建模（笛卡尔方向、多视角）与隐式表征学习，并引入嵌套注意力机制捕获全局依赖，提出LeMoRe框架，在保持高效计算的同时提升表征能力。实验显示其在多个数据集上实现性能与效率的平衡。**

- **链接: [http://arxiv.org/pdf/2505.23093v1](http://arxiv.org/pdf/2505.23093v1)**

> **作者:** Mian Muhammad Naeem Abid; Nancy Mehta; Zongwei Wu; Radu Timofte
>
> **备注:** Accepted at IEEE ICIP 2025
>
> **摘要:** Lightweight semantic segmentation is essential for many downstream vision tasks. Unfortunately, existing methods often struggle to balance efficiency and performance due to the complexity of feature modeling. Many of these existing approaches are constrained by rigid architectures and implicit representation learning, often characterized by parameter-heavy designs and a reliance on computationally intensive Vision Transformer-based frameworks. In this work, we introduce an efficient paradigm by synergizing explicit and implicit modeling to balance computational efficiency with representational fidelity. Our method combines well-defined Cartesian directions with explicitly modeled views and implicitly inferred intermediate representations, efficiently capturing global dependencies through a nested attention mechanism. Extensive experiments on challenging datasets, including ADE20K, CityScapes, Pascal Context, and COCO-Stuff, demonstrate that LeMoRe strikes an effective balance between performance and efficiency.
>
---
#### [new 040] VModA: An Effective Framework for Adaptive NSFW Image Moderation
- **分类: cs.CV; cs.AI**

- **简介: 该论文提出VModA框架，解决复杂NSFW图像逃避检测及跨平台法规差异导致的偏差问题。通过自适应多场景、模型与语义分析，提升检测精度（最高54.3%），修正公共数据集标注并验证实际效果。**

- **链接: [http://arxiv.org/pdf/2505.23386v1](http://arxiv.org/pdf/2505.23386v1)**

> **作者:** Han Bao; Qinying Wang; Zhi Chen; Qingming Li; Xuhong Zhang; Changjiang Li; Zonghui Wang; Shouling Ji; Wenzhi Chen
>
> **摘要:** Not Safe/Suitable for Work (NSFW) content is rampant on social networks and poses serious harm to citizens, especially minors. Current detection methods mainly rely on deep learning-based image recognition and classification. However, NSFW images are now presented in increasingly sophisticated ways, often using image details and complex semantics to obscure their true nature or attract more views. Although still understandable to humans, these images often evade existing detection methods, posing a significant threat. Further complicating the issue, varying regulations across platforms and regions create additional challenges for effective moderation, leading to detection bias and reduced accuracy. To address this, we propose VModA, a general and effective framework that adapts to diverse moderation rules and handles complex, semantically rich NSFW content across categories. Experimental results show that VModA significantly outperforms existing methods, achieving up to a 54.3% accuracy improvement across NSFW types, including those with complex semantics. Further experiments demonstrate that our method exhibits strong adaptability across categories, scenarios, and base VLMs. We also identified inconsistent and controversial label samples in public NSFW benchmark datasets, re-annotated them, and submitted corrections to the original maintainers. Two datasets have confirmed the updates so far. Additionally, we evaluate VModA in real-world scenarios to demonstrate its practical effectiveness.
>
---
#### [new 041] A Probabilistic Jump-Diffusion Framework for Open-World Egocentric Activity Recognition
- **分类: cs.CV**

- **简介: 该论文属于开放世界自顶向下活动识别任务，旨在解决模型在未见活动和复杂搜索空间中的推理难题。提出ProbRes框架，融合结构化常识先验与视觉语言模型，通过概率跳-扩散机制平衡探索与利用，高效定位高可能性活动标签，在多开放层级数据集上达最优性能。**

- **链接: [http://arxiv.org/pdf/2505.22858v1](http://arxiv.org/pdf/2505.22858v1)**

> **作者:** Sanjoy Kundu; Shanmukha Vellamcheti; Sathyanarayanan N. Aakur
>
> **备注:** Extended abstract of arXiv:2504.03948 for CVPR 2025 EgoVis Workshop
>
> **摘要:** Open-world egocentric activity recognition poses a fundamental challenge due to its unconstrained nature, requiring models to infer unseen activities from an expansive, partially observed search space. We introduce ProbRes, a Probabilistic Residual search framework based on jump-diffusion that efficiently navigates this space by balancing prior-guided exploration with likelihood-driven exploitation. Our approach integrates structured commonsense priors to construct a semantically coherent search space, adaptively refines predictions using Vision-Language Models (VLMs) and employs a stochastic search mechanism to locate high-likelihood activity labels while minimizing exhaustive enumeration efficiently. We systematically evaluate ProbRes across multiple openness levels (L0--L3), demonstrating its adaptability to increasing search space complexity. In addition to achieving state-of-the-art performance on benchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we establish a clear taxonomy for open-world recognition, delineating the challenges and methodological advancements necessary for egocentric activity understanding.
>
---
#### [new 042] Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于行人属性识别（PAR）任务，旨在探索其对抗脆弱性并提出攻击与防御方法。针对PAR抗干扰能力不足的问题，提出基于CLIP的对抗语义及标签扰动攻击（ASL-PAR）框架，通过全局/局部图像块扰动和语义偏移防御策略，验证了方法在多个数据集的有效性。**

- **链接: [http://arxiv.org/pdf/2505.23313v1](http://arxiv.org/pdf/2505.23313v1)**

> **作者:** Weizhe Kong; Xiao Wang; Ruichong Gao; Chenglong Li; Yu Zhang; Xing Yang; Yaowei Wang; Jin Tang
>
> **摘要:** Pedestrian Attribute Recognition (PAR) is an indispensable task in human-centered research and has made great progress in recent years with the development of deep neural networks. However, the potential vulnerability and anti-interference ability have still not been fully explored. To bridge this gap, this paper proposes the first adversarial attack and defense framework for pedestrian attribute recognition. Specifically, we exploit both global- and patch-level attacks on the pedestrian images, based on the pre-trained CLIP-based PAR framework. It first divides the input pedestrian image into non-overlapping patches and embeds them into feature embeddings using a projection layer. Meanwhile, the attribute set is expanded into sentences using prompts and embedded into attribute features using a pre-trained CLIP text encoder. A multi-modal Transformer is adopted to fuse the obtained vision and text tokens, and a feed-forward network is utilized for attribute recognition. Based on the aforementioned PAR framework, we adopt the adversarial semantic and label-perturbation to generate the adversarial noise, termed ASL-PAR. We also design a semantic offset defense strategy to suppress the influence of adversarial attacks. Extensive experiments conducted on both digital domains (i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the effectiveness of our proposed adversarial attack and defense strategies for the pedestrian attribute recognition. The source code of this paper will be released on https://github.com/Event-AHU/OpenPAR.
>
---
#### [new 043] Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视觉语言模型（VLM）研究，旨在验证统一模型在视觉理解与生成任务间的必要性及泛化能力。针对"混合训练能否促进任务互惠"的问题，设计贴近现实的数据集，实验多模型验证发现：混合数据训练提升跨任务性能且随数据量增长，模态对齐与生成知识向理解任务的迁移（通过语言模型而非适配器）是关键。结论支持统一VLM的设计需求。**

- **链接: [http://arxiv.org/pdf/2505.23043v1](http://arxiv.org/pdf/2505.23043v1)**

> **作者:** Jihai Zhang; Tianle Li; Linjie Li; Zhengyuan Yang; Yu Cheng
>
> **摘要:** Recent advancements in unified vision-language models (VLMs), which integrate both visual understanding and generation capabilities, have attracted significant attention. The underlying hypothesis is that a unified architecture with mixed training on both understanding and generation tasks can enable mutual enhancement between understanding and generation. However, this hypothesis remains underexplored in prior works on unified VLMs. To address this gap, this paper systematically investigates the generalization across understanding and generation tasks in unified VLMs. Specifically, we design a dataset closely aligned with real-world scenarios to facilitate extensive experiments and quantitative evaluations. We evaluate multiple unified VLM architectures to validate our findings. Our key findings are as follows. First, unified VLMs trained with mixed data exhibit mutual benefits in understanding and generation tasks across various architectures, and this mutual benefits can scale up with increased data. Second, better alignment between multimodal input and output spaces will lead to better generalization. Third, the knowledge acquired during generation tasks can transfer to understanding tasks, and this cross-task generalization occurs within the base language model, beyond modality adapters. Our findings underscore the critical necessity of unifying understanding and generation in VLMs, offering valuable insights for the design and optimization of unified VLMs.
>
---
#### [new 044] Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence
- **分类: cs.CV; cs.AI; cs.LG; I.2.6; I.2**

- **简介: 该论文提出Spatial-MLLM框架，旨在仅通过2D数据提升MLLM的空间智能。针对现有3D MLLM依赖额外数据的问题，其采用双编码器融合2D语义与3D几何特征，并设计空间感知采样策略，结合新数据集训练，实现视觉空间推理的SOTA性能。**

- **链接: [http://arxiv.org/pdf/2505.23747v1](http://arxiv.org/pdf/2505.23747v1)**

> **作者:** Diankun Wu; Fangfu Liu; Yi-Hsin Hung; Yueqi Duan
>
> **备注:** 21 pages
>
> **摘要:** Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.
>
---
#### [new 045] A Divide-and-Conquer Approach for Global Orientation of Non-Watertight Scene-Level Point Clouds Using 0-1 Integer Optimization
- **分类: cs.CV**

- **简介: 该论文提出DACPO框架，解决大规模非水密场景点云定向问题。通过分治策略将场景划分为小块，分别估计法线方向，再构建图模型并用0-1整数优化整合块间一致性，实现高效全局定向。**

- **链接: [http://arxiv.org/pdf/2505.23469v1](http://arxiv.org/pdf/2505.23469v1)**

> **作者:** Zhuodong Li; Fei Hou; Wencheng Wang; Xuequan Lu; Ying He
>
> **备注:** accepted to SIGGRAPH 2025
>
> **摘要:** Orienting point clouds is a fundamental problem in computer graphics and 3D vision, with applications in reconstruction, segmentation, and analysis. While significant progress has been made, existing approaches mainly focus on watertight, object-level 3D models. The orientation of large-scale, non-watertight 3D scenes remains an underexplored challenge. To address this gap, we propose DACPO (Divide-And-Conquer Point Orientation), a novel framework that leverages a divide-and-conquer strategy for scalable and robust point cloud orientation. Rather than attempting to orient an unbounded scene at once, DACPO segments the input point cloud into smaller, manageable blocks, processes each block independently, and integrates the results through a global optimization stage. For each block, we introduce a two-step process: estimating initial normal orientations by a randomized greedy method and refining them by an adapted iterative Poisson surface reconstruction. To achieve consistency across blocks, we model inter-block relationships using an an undirected graph, where nodes represent blocks and edges connect spatially adjacent blocks. To reliably evaluate orientation consistency between adjacent blocks, we introduce the concept of the visible connected region, which defines the region over which visibility-based assessments are performed. The global integration is then formulated as a 0-1 integer-constrained optimization problem, with block flip states as binary variables. Despite the combinatorial nature of the problem, DACPO remains scalable by limiting the number of blocks (typically a few hundred for 3D scenes) involved in the optimization. Experiments on benchmark datasets demonstrate DACPO's strong performance, particularly in challenging large-scale, non-watertight scenarios where existing methods often fail. The source code is available at https://github.com/zd-lee/DACPO.
>
---
#### [new 046] iHDR: Iterative HDR Imaging with Arbitrary Number of Exposures
- **分类: cs.CV**

- **简介: 该论文属于高动态范围（HDR）成像任务，解决现有方法仅适配固定输入数量的问题。提出iHDR框架，通过迭代融合任意数量曝光图像：利用DiHDR网络融合两输入生成中间HDR图像，ToneNet将其映射回非线性域作为下次融合的参考，循环直至所有帧用完，实现灵活输入的高质量HDR合成。**

- **链接: [http://arxiv.org/pdf/2505.22971v1](http://arxiv.org/pdf/2505.22971v1)**

> **作者:** Yu Yuan; Yiheng Chi; Xingguang Zhang; Stanley Chan
>
> **备注:** To be appear in IEEE ICIP 2025
>
> **摘要:** High dynamic range (HDR) imaging aims to obtain a high-quality HDR image by fusing information from multiple low dynamic range (LDR) images. Numerous learning-based HDR imaging methods have been proposed to achieve this for static and dynamic scenes. However, their architectures are mostly tailored for a fixed number (e.g., three) of inputs and, therefore, cannot apply directly to situations beyond the pre-defined limited scope. To address this issue, we propose a novel framework, iHDR, for iterative fusion, which comprises a ghost-free Dual-input HDR fusion network (DiHDR) and a physics-based domain mapping network (ToneNet). DiHDR leverages a pair of inputs to estimate an intermediate HDR image, while ToneNet maps it back to the nonlinear domain and serves as the reference input for the next pairwise fusion. This process is iteratively executed until all input frames are utilized. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed method as compared to existing state-of-the-art HDR deghosting approaches given flexible numbers of input frames.
>
---
#### [new 047] VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?
- **分类: cs.CV**

- **简介: 该论文提出VideoReasonBench基准，填补评估多模态大模型在视觉主导视频复杂推理任务的空白。针对现有视频理解任务缺乏深度推理和视觉依赖问题，设计需逐步推理潜在状态的视频任务，测试18个模型发现多数表现差，但Gemini-2.5-Pro表现优异，并揭示延长推理时间对新任务关键。**

- **链接: [http://arxiv.org/pdf/2505.23359v1](http://arxiv.org/pdf/2505.23359v1)**

> **作者:** Yuanxin Liu; Kun Ouyang; Haoning Wu; Yi Liu; Lin Sui; Xinhao Li; Yan Zhong; Y. Charles; Xinyu Zhou; Xu Sun
>
> **备注:** Project Page: https://llyx97.github.io/video_reason_bench/
>
> **摘要:** Recent studies have shown that long chain-of-thought (CoT) reasoning can significantly enhance the performance of large language models (LLMs) on complex tasks. However, this benefit is yet to be demonstrated in the domain of video understanding, since most existing benchmarks lack the reasoning depth required to demonstrate the advantages of extended CoT chains. While recent efforts have proposed benchmarks aimed at video reasoning, the tasks are often knowledge-driven and do not rely heavily on visual content. To bridge this gap, we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric, complex video reasoning. To ensure visual richness and high reasoning complexity, each video in VideoReasonBench depicts a sequence of fine-grained operations on a latent state that is only visible in part of the video. The questions evaluate three escalating levels of video reasoning skills: recalling observed visual information, inferring the content of latent states, and predicting information beyond the video. Under such task setting, models have to precisely recall multiple operations in the video, and perform step-by-step reasoning to get correct final answers for these questions. Using VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal LLMs (MLLMs), finding that most perform poorly on complex video reasoning, e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our investigations on "test-time scaling" further reveal that extended thinking budget, while offering none or minimal benefits on existing video benchmarks, is essential for improving the performance on VideoReasonBench.
>
---
#### [new 048] ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS
- **分类: cs.CV**

- **简介: 该论文属于新颖视角合成任务，解决3DGS模型因编码器容量限制导致的扩展性差和内存消耗大的问题。提出ZPressor模块，通过将多视角图像划分为锚点与支持集，利用交叉注意力压缩冗余信息至紧凑潜态Z，使模型可处理超百视图输入，在大规模数据集上提升性能与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2505.23734v1](http://arxiv.org/pdf/2505.23734v1)**

> **作者:** Weijie Wang; Donny Y. Chen; Zeyu Zhang; Duochao Shi; Akide Liu; Bohan Zhuang
>
> **备注:** Project Page: https://lhmd.top/zpressor, Code: https://github.com/ziplab/ZPressor
>
> **摘要:** Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their encoders, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor.
>
---
#### [new 049] EAD: An EEG Adapter for Automated Classification
- **分类: cs.CV; cs.AI**

- **简介: 该论文提出EEG Adapter（EAD）框架，解决EEG信号分类中因设备或通道数差异导致的模型通用性问题。通过适配EEG基础模型学习鲁棒表示，实现跨设备/通道数的分类，在两个数据集达最优准确率，并验证零样本泛化能力。**

- **链接: [http://arxiv.org/pdf/2505.23107v1](http://arxiv.org/pdf/2505.23107v1)**

> **作者:** Pushapdeep Singh; Jyoti Nigam; Medicherla Vamsi Krishna; Arnav Bhavsar; Aditya Nigam
>
> **摘要:** While electroencephalography (EEG) has been a popular modality for neural decoding, it often involves task specific acquisition of the EEG data. This poses challenges for the development of a unified pipeline to learn embeddings for various EEG signal classification, which is often involved in various decoding tasks. Traditionally, EEG classification involves the step of signal preprocessing and the use of deep learning techniques, which are highly dependent on the number of EEG channels in each sample. However, the same pipeline cannot be applied even if the EEG data is collected for the same experiment but with different acquisition devices. This necessitates the development of a framework for learning EEG embeddings, which could be highly beneficial for tasks involving multiple EEG samples for the same task but with varying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a flexible framework compatible with any signal acquisition device. More specifically, we leverage a recent EEG foundational model with significant adaptations to learn robust representations from the EEG data for the classification task. We evaluate EAD on two publicly available datasets achieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and BrainLat respectively. This illustrates the effectiveness of the proposed framework across diverse EEG datasets containing two different perception tasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG classification on EEG-ImageNet task to demonstrate the generalization capability of the proposed approach.
>
---
#### [new 050] Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape
- **分类: cs.CV**

- **简介: 该论文属于视觉生成任务，针对扩散Transformer（DiT）中注意力机制计算复杂度随分辨率二次增长的问题，提出Re-ttention方法。通过统计重塑注意力分数分布，利用扩散模型的时间冗余性，在仅保留3.1% token时仍保持生成质量，实现超稀疏计算并降低92%自注意力延迟。**

- **链接: [http://arxiv.org/pdf/2505.22918v1](http://arxiv.org/pdf/2505.22918v1)**

> **作者:** Ruichen Chen; Keith G. Mills; Liyao Jiang; Chao Gao; Di Niu
>
> **摘要:** Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. % To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. % Experimental results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1\% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference. Further, we measure latency to show that our method can attain over 45\% end-to-end % and over 92\% self-attention latency reduction on an H100 GPU at negligible overhead cost. Code available online here: \href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}
>
---
#### [new 051] Spatio-Temporal Joint Density Driven Learning for Skeleton-Based Action Recognition
- **分类: cs.CV**

- **简介: 该论文针对骨骼序列动作识别，提出空间-时间关节密度(STJD)量化动静骨骼交互，解决传统方法忽略其区分潜力的问题。通过识别关键"主关节"，开发STJD-CL对比学习与STJD-MP重建方法，实验显示较现有方法提升3.5-3.6%（NTU 120数据集）。**

- **链接: [http://arxiv.org/pdf/2505.23012v1](http://arxiv.org/pdf/2505.23012v1)**

> **作者:** Shanaka Ramesh Gunasekara; Wanqing Li; Philip Ogunbona; Jack Yang
>
> **摘要:** Traditional approaches in unsupervised or self supervised learning for skeleton-based action classification have concentrated predominantly on the dynamic aspects of skeletal sequences. Yet, the intricate interaction between the moving and static elements of the skeleton presents a rarely tapped discriminative potential for action classification. This paper introduces a novel measurement, referred to as spatial-temporal joint density (STJD), to quantify such interaction. Tracking the evolution of this density throughout an action can effectively identify a subset of discriminative moving and/or static joints termed "prime joints" to steer self-supervised learning. A new contrastive learning strategy named STJD-CL is proposed to align the representation of a skeleton sequence with that of its prime joints while simultaneously contrasting the representations of prime and nonprime joints. In addition, a method called STJD-MP is developed by integrating it with a reconstruction-based framework for more effective learning. Experimental evaluations on the NTU RGB+D 60, NTU RGB+D 120, and PKUMMD datasets in various downstream tasks demonstrate that the proposed STJD-CL and STJD-MP improved performance, particularly by 3.5 and 3.6 percentage points over the state-of-the-art contrastive methods on the NTU RGB+D 120 dataset using X-sub and X-set evaluations, respectively.
>
---
#### [new 052] Hierarchical Material Recognition from Local Appearance
- **分类: cs.CV**

- **简介: 该论文提出基于局部外观的层次化材料识别方法，任务为通过材料物理特性分类。构建含图像和深度图的真实场景数据集，采用图注意力网络建模类别层级关系，实现SOTA性能，验证恶劣条件泛化及少样本学习能力。**

- **链接: [http://arxiv.org/pdf/2505.22911v1](http://arxiv.org/pdf/2505.22911v1)**

> **作者:** Matthew Beveridge; Shree K. Nayar
>
> **摘要:** We introduce a taxonomy of materials for hierarchical recognition from local appearance. Our taxonomy is motivated by vision applications and is arranged according to the physical traits of materials. We contribute a diverse, in-the-wild dataset with images and depth maps of the taxonomy classes. Utilizing the taxonomy and dataset, we present a method for hierarchical material recognition based on graph attention networks. Our model leverages the taxonomic proximity between classes and achieves state-of-the-art performance. We demonstrate the model's potential to generalize to adverse, real-world imaging conditions, and that novel views rendered using the depth maps can enhance this capability. Finally, we show the model's capacity to rapidly learn new materials in a few-shot learning setting.
>
---
#### [new 053] Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于皮肤病变分类任务，旨在解决仅依赖图像进行黑色素瘤检测时因成像条件差异和缺乏临床上下文导致的性能不足问题。提出SLIMP方法，通过嵌套多模态对比学习，融合个体病灶图像、病灶级元数据及患者级临床信息，提升下游任务表现。**

- **链接: [http://arxiv.org/pdf/2505.23709v1](http://arxiv.org/pdf/2505.23709v1)**

> **作者:** Dionysis Christopoulos; Sotiris Spanos; Eirini Baltzi; Valsamis Ntouskos; Konstantinos Karantzalos
>
> **摘要:** We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning rich representations of skin lesions through a novel nested contrastive learning approach that captures complex relationships between images and metadata. Melanoma detection and skin lesion classification based solely on images, pose significant challenges due to large variations in imaging conditions (lighting, color, resolution, distance, etc.) and lack of clinical and phenotypical context. Clinicians typically follow a holistic approach for assessing the risk level of the patient and for deciding which lesions may be malignant and need to be excised, by considering the patient's medical history as well as the appearance of other lesions of the patient. Inspired by this, SLIMP combines the appearance and the metadata of individual skin lesions with patient-level metadata relating to their medical record and other clinically relevant information. By fully exploiting all available data modalities throughout the learning process, the proposed pre-training strategy improves performance compared to other pre-training strategies on downstream skin lesions classification tasks highlighting the learned representations quality.
>
---
#### [new 054] HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer
- **分类: cs.CV; cs.MM**

- **简介: 该论文提出HiDream-I1，一种高效图像生成模型，解决高质量生成与计算效率的矛盾。通过稀疏扩散Transformer和动态MoE架构，实现快速生成；提供三种变体，并扩展为指令编辑模型HiDream-E1和交互式图像代理HiDream-A1，开源代码及模型。**

- **链接: [http://arxiv.org/pdf/2505.22705v1](http://arxiv.org/pdf/2505.22705v1)**

> **作者:** Qi Cai; Jingwen Chen; Yang Chen; Yehao Li; Fuchen Long; Yingwei Pan; Zhaofan Qiu; Yiheng Zhang; Fengbin Gao; Peihan Xu; Yimeng Wang; Kai Yu; Wenxuan Chen; Ziwei Feng; Zijian Gong; Jianzhuang Pan; Yi Peng; Rui Tian; Siyu Wang; Bo Zhao; Ting Yao; Tao Mei
>
> **备注:** Source codes and models are available at https://github.com/HiDream-ai/HiDream-I1 and https://github.com/HiDream-ai/HiDream-E1
>
> **摘要:** Recent advancements in image generative foundation models have prioritized quality improvements but often at the cost of increased computational complexity and inference latency. To address this critical trade-off, we introduce HiDream-I1, a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer (DiT) structure. Specifically, it starts with a dual-stream decoupled design of sparse DiT with dynamic Mixture-of-Experts (MoE) architecture, in which two separate encoders are first involved to independently process image and text tokens. Then, a single-stream sparse DiT structure with dynamic MoE architecture is adopted to trigger multi-model interaction for image generation in a cost-efficient manner. To support flexiable accessibility with varied model capabilities, we provide HiDream-I1 in three variants: HiDream-I1-Full, HiDream-I1-Dev, and HiDream-I1-Fast. Furthermore, we go beyond the typical text-to-image generation and remould HiDream-I1 with additional image conditions to perform precise, instruction-based editing on given images, yielding a new instruction-based image editing model namely HiDream-E1. Ultimately, by integrating text-to-image generation and instruction-based image editing, HiDream-I1 evolves to form a comprehensive image agent (HiDream-A1) capable of fully interactive image creation and refinement. To accelerate multi-modal AIGC research, we have open-sourced all the codes and model weights of HiDream-I1-Full, HiDream-I1-Dev, HiDream-I1-Fast, HiDream-E1 through our project websites: https://github.com/HiDream-ai/HiDream-I1 and https://github.com/HiDream-ai/HiDream-E1. All features can be directly experienced via https://vivago.ai/studio.
>
---
#### [new 055] Synthetic Document Question Answering in Hungarian
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文聚焦匈牙利语文档视觉问答任务，针对其数据稀缺问题，构建了合成数据集HuDocVQA、人工精筛数据集HuDocVQA-manual及OCR训练数据集HuCCPDF，通过质量过滤提升数据质量，实验表明微调后模型性能显著提升，推动多语言文档问答研究。**

- **链接: [http://arxiv.org/pdf/2505.23008v1](http://arxiv.org/pdf/2505.23008v1)**

> **作者:** Jonathan Li; Zoltan Csaki; Nidhi Hiremath; Etash Guha; Fenglu Hong; Edward Ma; Urmish Thakker
>
> **摘要:** Modern VLMs have achieved near-saturation accuracy in English document visual question-answering (VQA). However, this task remains challenging in lower resource languages due to a dearth of suitable training and evaluation data. In this paper we present scalable methods for curating such datasets by focusing on Hungarian, approximately the 17th highest resource language on the internet. Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets that modern VLMs significantly underperform on compared to English DocVQA. HuDocVQA-manual is a small manually curated dataset based on Hungarian documents from Common Crawl, while HuDocVQA is a larger synthetically generated VQA data set from the same source. We apply multiple rounds of quality filtering and deduplication to HuDocVQA in order to match human-level quality in this dataset. We also present HuCCPDF, a dataset of 117k pages from Hungarian Common Crawl PDFs along with their transcriptions, which can be used for training a model for Hungarian OCR. To validate the quality of our datasets, we show how finetuning on a mixture of these datasets can improve accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code will be released to the public to foster further research in multilingual DocVQA.
>
---
#### [new 056] 3DGS Compression with Sparsity-guided Hierarchical Transform Coding
- **分类: cs.CV**

- **简介: 该论文属于3DGS压缩任务，针对其内存占用大导致传输存储开销高的问题，提出首个端到端变换编码框架SHTC。通过基层KLT去相关与稀疏增强层压缩残差，联合优化变换与轻量模型，提升率失真性能，参数少且计算高效。**

- **链接: [http://arxiv.org/pdf/2505.22908v1](http://arxiv.org/pdf/2505.22908v1)**

> **作者:** Hao Xu; Xiaolin Wu; Xi Zhang
>
> **摘要:** 3D Gaussian Splatting (3DGS) has gained popularity for its fast and high-quality rendering, but it has a very large memory footprint incurring high transmission and storage overhead. Recently, some neural compression methods, such as Scaffold-GS, were proposed for 3DGS but they did not adopt the approach of end-to-end optimized analysis-synthesis transforms which has been proven highly effective in neural signal compression. Without an appropriate analysis transform, signal correlations cannot be removed by sparse representation. Without such transforms the only way to remove signal redundancies is through entropy coding driven by a complex and expensive context modeling, which results in slower speed and suboptimal rate-distortion (R-D) performance. To overcome this weakness, we propose Sparsity-guided Hierarchical Transform Coding (SHTC), the first end-to-end optimized transform coding framework for 3DGS compression. SHTC jointly optimizes the 3DGS, transforms and a lightweight context model. This joint optimization enables the transform to produce representations that approach the best R-D performance possible. The SHTC framework consists of a base layer using KLT for data decorrelation, and a sparsity-coded enhancement layer that compresses the KLT residuals to refine the representation. The enhancement encoder learns a linear transform to project high-dimensional inputs into a low-dimensional space, while the decoder unfolds the Iterative Shrinkage-Thresholding Algorithm (ISTA) to reconstruct the residuals. All components are designed to be interpretable, allowing the incorporation of signal priors and fewer parameters than black-box transforms. This novel design significantly improves R-D performance with minimal additional parameters and computational overhead.
>
---
#### [new 057] TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models
- **分类: cs.CV**

- **简介: 该论文属于概念擦除任务，解决扩散模型生成不良内容（如色情、敏感信息）的问题。提出TRACE方法，通过修改注意力层隐藏表征并约束后期去噪轨迹，抑制目标概念同时保持生成质量，超越现有方法。**

- **链接: [http://arxiv.org/pdf/2505.23312v1](http://arxiv.org/pdf/2505.23312v1)**

> **作者:** Finn Carter
>
> **备注:** In peer review
>
> **摘要:** Text-to-image diffusion models have shown unprecedented generative capability, but their ability to produce undesirable concepts (e.g.~pornographic content, sensitive identities, copyrighted styles) poses serious concerns for privacy, fairness, and safety. {Concept erasure} aims to remove or suppress specific concept information in a generative model. In this paper, we introduce \textbf{TRACE (Trajectory-Constrained Attentional Concept Erasure)}, a novel method to erase targeted concepts from diffusion models while preserving overall generative quality. Our approach combines a rigorous theoretical framework, establishing formal conditions under which a concept can be provably suppressed in the diffusion process, with an effective fine-tuning procedure compatible with both conventional latent diffusion (Stable Diffusion) and emerging rectified flow models (e.g.~FLUX). We first derive a closed-form update to the model's cross-attention layers that removes hidden representations of the target concept. We then introduce a trajectory-aware finetuning objective that steers the denoising process away from the concept only in the late sampling stages, thus maintaining the model's fidelity on unrelated content. Empirically, we evaluate TRACE on multiple benchmarks used in prior concept erasure studies (object classes, celebrity faces, artistic styles, and explicit content from the I2P dataset). TRACE achieves state-of-the-art performance, outperforming recent methods such as ANT, EraseAnything, and MACE in terms of removal efficacy and output quality.
>
---
#### [new 058] CLDTracker: A Comprehensive Language Description for Visual Tracking
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视觉目标跟踪（VOT）任务，旨在解决动态外观变化、遮挡和背景干扰下的跟踪问题。针对视觉语言模型（VLM）在VOT中存在文本表征不足、特征融合低效及缺乏时序建模的缺陷，提出CLDTracker框架：通过双分支架构融合VLM生成的丰富语义描述与视觉特征，并建模目标时序变化，实现SOTA跟踪性能。**

- **链接: [http://arxiv.org/pdf/2505.23704v1](http://arxiv.org/pdf/2505.23704v1)**

> **作者:** Mohamad Alansari; Sajid Javed; Iyyakutti Iyappan Ganapathi; Sara Alansari; Muzammal Naseer
>
> **备注:** 47 pages, 9 figures, Information Fusion Journal
>
> **摘要:** VOT remains a fundamental yet challenging task in computer vision due to dynamic appearance changes, occlusions, and background clutter. Traditional trackers, relying primarily on visual cues, often struggle in such complex scenarios. Recent advancements in VLMs have shown promise in semantic understanding for tasks like open-vocabulary detection and image captioning, suggesting their potential for VOT. However, the direct application of VLMs to VOT is hindered by critical limitations: the absence of a rich and comprehensive textual representation that semantically captures the target object's nuances, limiting the effective use of language information; inefficient fusion mechanisms that fail to optimally integrate visual and textual features, preventing a holistic understanding of the target; and a lack of temporal modeling of the target's evolving appearance in the language domain, leading to a disconnect between the initial description and the object's subsequent visual changes. To bridge these gaps and unlock the full potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive Language Description framework for robust visual Tracking. Our tracker introduces a dual-branch architecture consisting of a textual and a visual branch. In the textual branch, we construct a rich bag of textual descriptions derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with semantic and contextual cues to address the lack of rich textual representation. Experiments on six standard VOT benchmarks demonstrate that CLDTracker achieves SOTA performance, validating the effectiveness of leveraging robust and temporally-adaptive vision-language representations for tracking. Code and models are publicly available at: https://github.com/HamadYA/CLDTracker
>
---
#### [new 059] VidText: Towards Comprehensive Evaluation for Video Text Understanding
- **分类: cs.CV**

- **简介: 该论文提出VidText基准，填补视频文本理解评估的空白，解决现有方法忽视动态视觉与文本交互的问题。通过多场景覆盖、分层任务框架及跨模态推理任务，全面评测模型性能，实验显示当前模型表现欠佳，分析指出改进方向。**

- **链接: [http://arxiv.org/pdf/2505.22810v1](http://arxiv.org/pdf/2505.22810v1)**

> **作者:** Zhoufaran Yang; Yan Shu; Zhifei Yang; Yan Zhang; Yu Li; Keyang Lu; Gangyan Zeng; Shaohui Liu; Yu Zhou; Nicu Sebe
>
> **摘要:** Visual texts embedded in videos carry rich semantic information, which is crucial for both holistic video understanding and fine-grained reasoning about local human actions. However, existing video understanding benchmarks largely overlook textual information, while OCR-specific benchmarks are constrained to static images, limiting their ability to capture the interaction between text and dynamic visual contexts. To address this gap, we propose VidText, a new benchmark designed for comprehensive and in-depth evaluation of video text understanding. VidText offers the following key features: 1) It covers a wide range of real-world scenarios and supports multilingual content, encompassing diverse settings where video text naturally appears. 2) It introduces a hierarchical evaluation framework with video-level, clip-level, and instance-level tasks, enabling assessment of both global summarization and local retrieval capabilities. 3) The benchmark also introduces a set of paired perception reasoning tasks, ranging from visual text perception to cross-modal reasoning between textual and visual information. Extensive experiments on 18 state-of-the-art Large Multimodal Models (LMMs) reveal that current models struggle across most tasks, with significant room for improvement. Further analysis highlights the impact of both model-intrinsic factors, such as input resolution and OCR capability, and external factors, including the use of auxiliary information and Chain-of-Thought reasoning strategies. We hope VidText will fill the current gap in video understanding benchmarks and serve as a foundation for future research on multimodal reasoning with video text in dynamic environments.
>
---
#### [new 060] Grounded Reinforcement Learning for Visual Reasoning
- **分类: cs.CV**

- **简介: 该论文属于视觉推理任务，旨在解决模型在视觉任务中有效结合空间注意力与抽象推理的难题。提出ViGoRL模型，通过强化学习使每步推理锚定特定视觉坐标，并采用多回合动态缩放机制，提升对小元素定位和视觉搜索性能，在多个基准测试中表现优异。**

- **链接: [http://arxiv.org/pdf/2505.23678v1](http://arxiv.org/pdf/2505.23678v1)**

> **作者:** Gabriel Sarch; Snigdha Saha; Naitik Khandelwal; Ayush Jain; Michael J. Tarr; Aviral Kumar; Katerina Fragkiadaki
>
> **备注:** Project website: https://visually-grounded-rl.github.io/
>
> **摘要:** While reinforcement learning (RL) over chains of thought has significantly advanced language models in tasks such as mathematics and coding, visual reasoning introduces added complexity by requiring models to direct visual attention, interpret perceptual inputs, and ground abstract reasoning in spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement Learning), a vision-language model trained with RL to explicitly anchor each reasoning step to specific visual coordinates. Inspired by human visual decision-making, ViGoRL learns to produce spatially grounded reasoning traces, guiding visual attention to task-relevant regions at each step. When fine-grained exploration is required, our novel multi-turn RL framework enables the model to dynamically zoom into predicted coordinates as reasoning unfolds. Across a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK for spatial reasoning, V*bench for visual search, and ScreenSpot and VisualWebArena for web-based grounding--ViGoRL consistently outperforms both supervised fine-tuning and conventional RL baselines that lack explicit grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual feedback significantly improves ViGoRL's performance on localizing small GUI elements and visual search, achieving 86.4% on V*Bench. Additionally, we find that grounding amplifies other visual behaviors such as region exploration, grounded subgoal setting, and visual verification. Finally, human evaluations show that the model's visual references are not only spatially accurate but also helpful for understanding model reasoning steps. Our results show that visually grounded RL is a strong paradigm for imbuing models with general-purpose visual reasoning.
>
---
#### [new 061] Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis
- **分类: cs.CV**

- **简介: 该论文属于3D重建与新视角合成任务，针对Gaussian splats在颜色插值和表面连续性上的局限，提出基于三角形Soup的优化框架，通过软连接力约束促进三角面片间的表面连续性，实现更优的几何与视觉效果。**

- **链接: [http://arxiv.org/pdf/2505.23642v1](http://arxiv.org/pdf/2505.23642v1)**

> **作者:** Nathaniel Burgdorfer; Philippos Mordohai
>
> **摘要:** In this work, we introduce an inference-time optimization framework utilizing triangles to represent the geometry and appearance of the scene. More specifically, we develop a scene optimization algorithm for triangle soup, a collection of disconnected semi-transparent triangle primitives. Compared to the current most-widely used primitives for 3D scene representation, namely Gaussian splats, triangles allow for more expressive color interpolation, and benefit from a large algorithmic infrastructure for downstream tasks. Triangles, unlike full-rank Gaussian kernels, naturally combine to form surfaces. We formulate connectivity forces between triangles during optimization, encouraging explicit, but soft, surface continuity in 3D. We perform experiments on a representative 3D reconstruction dataset and show competitive photometric and geometric results.
>
---
#### [new 062] Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging
- **分类: cs.CV**

- **简介: 该论文针对单像素成像重建任务，提出结合PnP与unrolling方法优势的proximal算法展开网络。通过设计高效深度图像修复器及PT损失函数，使模型在单模型适配不同压缩比的同时，实现更高精度与速度，解决传统方法灵活性与性能的 trade-off。**

- **链接: [http://arxiv.org/pdf/2505.23180v1](http://arxiv.org/pdf/2505.23180v1)**

> **作者:** Ping Wang; Lishun Wang; Gang Qu; Xiaodong Wang; Yulun Zhang; Xin Yuan
>
> **备注:** Accepted by CVPR 2025
>
> **摘要:** Deep-unrolling and plug-and-play (PnP) approaches have become the de-facto standard solvers for single-pixel imaging (SPI) inverse problem. PnP approaches, a class of iterative algorithms where regularization is implicitly performed by an off-the-shelf deep denoiser, are flexible for varying compression ratios (CRs) but are limited in reconstruction accuracy and speed. Conversely, unrolling approaches, a class of multi-stage neural networks where a truncated iterative optimization process is transformed into an end-to-end trainable network, typically achieve better accuracy with faster inference but require fine-tuning or even retraining when CR changes. In this paper, we address the challenge of integrating the strengths of both classes of solvers. To this end, we design an efficient deep image restorer (DIR) for the unrolling of HQS (half quadratic splitting) and ADMM (alternating direction method of multipliers). More importantly, a general proximal trajectory (PT) loss function is proposed to train HQS/ADMM-unrolling networks such that learned DIR approximates the proximal operator of an ideal explicit restoration regularizer. Extensive experiments demonstrate that, the resulting proximal unrolling networks can not only flexibly handle varying CRs with a single model like PnP algorithms, but also outperform previous CR-specific unrolling networks in both reconstruction accuracy and speed. Source codes and models are available at https://github.com/pwangcs/ProxUnroll.
>
---
#### [new 063] Pose-free 3D Gaussian splatting via shape-ray estimation
- **分类: cs.CV**

- **简介: 该论文属于通用3D场景重建与渲染任务，旨在解决现有Gaussian splatting方法依赖精确相机位姿导致几何错位的问题。提出SHARE框架，通过联合估计形状和相机光线，构建姿态感知的规范体素表示，并采用锚点对齐的高斯预测优化局部几何，实现无需位姿输入的鲁棒场景重建。**

- **链接: [http://arxiv.org/pdf/2505.22978v1](http://arxiv.org/pdf/2505.22978v1)**

> **作者:** Youngju Na; Taeyeon Kim; Jumin Lee; Kyu Beom Han; Woo Jae Kim; Sung-eui Yoon
>
> **备注:** ICIP 2025
>
> **摘要:** While generalizable 3D Gaussian splatting enables efficient, high-quality rendering of unseen scenes, it heavily depends on precise camera poses for accurate geometry. In real-world scenarios, obtaining accurate poses is challenging, leading to noisy pose estimates and geometric misalignments. To address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting framework that overcomes these ambiguities by joint shape and camera rays estimation. Instead of relying on explicit 3D transformations, SHARE builds a pose-aware canonical volume representation that seamlessly integrates multi-view information, reducing misalignment caused by inaccurate pose estimates. Additionally, anchor-aligned Gaussian prediction enhances scene reconstruction by refining local geometry around coarse anchors, allowing for more precise Gaussian placement. Extensive experiments on diverse real-world datasets show that our method achieves robust performance in pose-free generalizable Gaussian splatting.
>
---
#### [new 064] VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation
- **分类: cs.CV**

- **简介: 该论文提出VCapsBench，首个大规模细粒度视频字幕评估基准，含5677个视频和109796个QA对，覆盖21个关键维度（如相机运动）。针对现有评估无法捕捉时空细节的问题，设计准确率、不一致率、覆盖率三指标及LLM驱动的自动化评估pipeline，助力优化字幕质量与视频生成模型。**

- **链接: [http://arxiv.org/pdf/2505.23484v1](http://arxiv.org/pdf/2505.23484v1)**

> **作者:** Shi-Xue Zhang; Hongfa Wang; Duojun Huang; Xin Li; Xiaobin Zhu; Xu-Cheng Yin
>
> **备注:** submitting
>
> **摘要:** Video captions play a crucial role in text-to-video generation tasks, as their quality directly influences the semantic coherence and visual fidelity of the generated videos. Although large vision-language models (VLMs) have demonstrated significant potential in caption generation, existing benchmarks inadequately address fine-grained evaluation, particularly in capturing spatial-temporal details critical for video generation. To address this gap, we introduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the first large-scale fine-grained benchmark comprising 5,677 (5K+) videos and 109,796 (100K+) question-answer pairs. These QA-pairs are systematically annotated across 21 fine-grained dimensions (e.g., camera movement, and shot type) that are empirically proven critical for text-to-video generation. We further introduce three metrics (Accuracy (AR), Inconsistency Rate (IR), Coverage Rate (CR)), and an automated evaluation pipeline leveraging large language model (LLM) to verify caption quality via contrastive QA-pairs analysis. By providing actionable insights for caption optimization, our benchmark can advance the development of robust text-to-video models. The dataset and codes are available at website: https://github.com/GXYM/VCapsBench.
>
---
#### [new 065] CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization
- **分类: cs.CV**

- **简介: 该论文属于无监督时序动作定位（UTAL）任务，旨在解决现有方法过度关注高区分区域及视觉单模态难以判定边界的问题。提出CLIP-AE方法，结合视觉语言预训练与分类预训练协同增强，并引入音频信息与自监督跨视角学习，提升多模态上下文边界检测性能。**

- **链接: [http://arxiv.org/pdf/2505.23524v1](http://arxiv.org/pdf/2505.23524v1)**

> **作者:** Rui Xia; Dan Jiang; Quan Zhang; Ke Zhang; Chun Yuan
>
> **摘要:** Temporal Action Localization (TAL) has garnered significant attention in information retrieval. Existing supervised or weakly supervised methods heavily rely on labeled temporal boundaries and action categories, which are labor-intensive and time-consuming. Consequently, unsupervised temporal action localization (UTAL) has gained popularity. However, current methods face two main challenges: 1) Classification pre-trained features overly focus on highly discriminative regions; 2) Solely relying on visual modality information makes it difficult to determine contextual boundaries. To address these issues, we propose a CLIP-assisted cross-view audiovisual enhanced UTAL method. Specifically, we introduce visual language pre-training (VLP) and classification pre-training-based collaborative enhancement to avoid excessive focus on highly discriminative regions; we also incorporate audio perception to provide richer contextual boundary information. Finally, we introduce a self-supervised cross-view learning paradigm to achieve multi-view perceptual enhancement without additional annotations. Extensive experiments on two public datasets demonstrate our model's superiority over several state-of-the-art competitors.
>
---
#### [new 066] FMG-Det: Foundation Model Guided Robust Object Detection
- **分类: cs.CV**

- **简介: 该论文属于目标检测任务，旨在解决标注噪声（如边界标注主观、不完整）导致检测性能下降的问题，尤其在小样本场景。提出FMG-Det方法，结合基础模型的预处理管道修正标注，并采用多实例学习框架与检测头优化，在多数据集上实现高效且 state-of-the-art 性能。**

- **链接: [http://arxiv.org/pdf/2505.23726v1](http://arxiv.org/pdf/2505.23726v1)**

> **作者:** Darryl Hannan; Timothy Doster; Henry Kvinge; Adam Attarian; Yijing Watkins
>
> **备注:** 10 pages, ICIP 2025
>
> **摘要:** Collecting high quality data for object detection tasks is challenging due to the inherent subjectivity in labeling the boundaries of an object. This makes it difficult to not only collect consistent annotations across a dataset but also to validate them, as no two annotators are likely to label the same object using the exact same coordinates. These challenges are further compounded when object boundaries are partially visible or blurred, which can be the case in many domains. Training on noisy annotations significantly degrades detector performance, rendering them unusable, particularly in few-shot settings, where just a few corrupted annotations can impact model performance. In this work, we propose FMG-Det, a simple, efficient methodology for training models with noisy annotations. More specifically, we propose combining a multiple instance learning (MIL) framework with a pre-processing pipeline that leverages powerful foundation models to correct labels prior to training. This pre-processing pipeline, along with slight modifications to the detector head, results in state-of-the-art performance across a number of datasets, for both standard and few-shot scenarios, while being much simpler and more efficient than other approaches.
>
---
#### [new 067] Frequency-Adaptive Discrete Cosine-ViT-ResNet Architecture for Sparse-Data Vision
- **分类: cs.CV**

- **简介: 该论文针对稀有动物图像分类中数据稀缺问题，提出结合自适应DCT频率选择、ViT-B16与ResNet50的混合网络。通过自适应划分频率域特征，融合全局上下文（ViT）与局部空间（ResNet）信息，并采用贝叶斯分类器，在仅50类小样本数据集上达最优分类精度。**

- **链接: [http://arxiv.org/pdf/2505.22701v1](http://arxiv.org/pdf/2505.22701v1)**

> **作者:** Ziyue Kang; Weichuan Zhang
>
> **摘要:** A major challenge in rare animal image classification is the scarcity of data, as many species usually have only a small number of labeled samples. To address this challenge, we designed a hybrid deep-learning framework comprising a novel adaptive DCT preprocessing module, ViT-B16 and ResNet50 backbones, and a Bayesian linear classification head. To our knowledge, we are the first to introduce an adaptive frequency-domain selection mechanism that learns optimal low-, mid-, and high-frequency boundaries suited to the subsequent backbones. Our network first captures image frequency-domain cues via this adaptive DCT partitioning. The adaptively filtered frequency features are then fed into ViT-B16 to model global contextual relationships, while ResNet50 concurrently extracts local, multi-scale spatial representations from the original image. A cross-level fusion strategy seamlessly integrates these frequency- and spatial-domain embeddings, and the fused features are passed through a Bayesian linear classifier to output the final category predictions. On our self-built 50-class wildlife dataset, this approach outperforms conventional CNN and fixed-band DCT pipelines, achieving state-of-the-art accuracy under extreme sample scarcity.
>
---
#### [new 068] VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning
- **分类: cs.CV**

- **简介: 论文属于视频异常理解（VAU）任务，解决现有方法缺乏可解释性、因果推理不足及无推理评估基准的问题。提出数据高效的VAU-R1框架（基于多模态大模型与强化微调）和首个Chain-of-Thought基准VAU-Bench，提升问答、时间定位及推理连贯性。**

- **链接: [http://arxiv.org/pdf/2505.23504v1](http://arxiv.org/pdf/2505.23504v1)**

> **作者:** Liyun Zhu; Qixiang Chen; Xi Shen; Xiaodong Cun
>
> **摘要:** Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.
>
---
#### [new 069] Fast Trajectory-Independent Model-Based Reconstruction Algorithm for Multi-Dimensional Magnetic Particle Imaging
- **分类: cs.CV; cs.NA; math.NA; physics.med-ph**

- **简介: 该论文提出一种轨迹无关的快速模型基磁粒子成像（MPI）重建算法，解决传统方法依赖特定扫描轨迹或耗时校准的问题。通过改进零次射击PnP算法（自动噪声估计）并引入自然图像训练的去噪器，在真实2D MPI数据及自定义场景验证了其通用性与鲁棒性，实现多维度灵活重建。**

- **链接: [http://arxiv.org/pdf/2505.22797v1](http://arxiv.org/pdf/2505.22797v1)**

> **作者:** Vladyslav Gapyak; Thomas März; Andreas Weinmann
>
> **备注:** 10 pages, 5 figures. This work has been submitted to the IEEE for possible publication
>
> **摘要:** Magnetic Particle Imaging (MPI) is a promising tomographic technique for visualizing the spatio-temporal distribution of superparamagnetic nanoparticles, with applications ranging from cancer detection to real-time cardiovascular monitoring. Traditional MPI reconstruction relies on either time-consuming calibration (measured system matrix) or model-based simulation of the forward operator. Recent developments have shown the applicability of Chebyshev polynomials to multi-dimensional Lissajous Field-Free Point (FFP) scans. This method is bound to the particular choice of sinusoidal scanning trajectories. In this paper, we present the first reconstruction on real 2D MPI data with a trajectory-independent model-based MPI reconstruction algorithm. We further develop the zero-shot Plug-and-Play (PnP) algorithm of the authors -- with automatic noise level estimation -- to address the present deconvolution problem, leveraging a state-of-the-art denoiser trained on natural images without retraining on MPI-specific data. We evaluate our method on the publicly available 2D FFP MPI dataset ``MPIdata: Equilibrium Model with Anisotropy", featuring scans of six phantoms acquired using a Bruker preclinical scanner. Moreover, we show reconstruction performed on custom data on a 2D scanner with additional high-frequency excitation field and partial data. Our results demonstrate strong reconstruction capabilities across different scanning scenarios -- setting a precedent for general-purpose, flexible model-based MPI reconstruction.
>
---
#### [new 070] Rhetorical Text-to-Image Generation via Two-layer Diffusion Policy Optimization
- **分类: cs.CV**

- **简介: 该论文提出Rhet2Pix框架，解决修辞文本生成图像时隐含语义理解不足的问题。现有模型因侧重字面词嵌对齐，导致比喻表达生成偏离意图。方法通过两层扩散策略优化：外层分解提示为子句逐步生成，内层优化扩散过程中的动作对奖励，提升修辞图像生成效果，超越现有模型。**

- **链接: [http://arxiv.org/pdf/2505.22792v1](http://arxiv.org/pdf/2505.22792v1)**

> **作者:** Yuxi Zhang; Yueting Li; Xinyu Du; Sibo Wang
>
> **摘要:** Generating images from rhetorical languages remains a critical challenge for text-to-image models. Even state-of-the-art (SOTA) multimodal large language models (MLLM) fail to generate images based on the hidden meaning inherent in rhetorical language--despite such content being readily mappable to visual representations by humans. A key limitation is that current models emphasize object-level word embedding alignment, causing metaphorical expressions to steer image generation towards their literal visuals and overlook the intended semantic meaning. To address this, we propose Rhet2Pix, a framework that formulates rhetorical text-to-image generation as a multi-step policy optimization problem, incorporating a two-layer MDP diffusion module. In the outer layer, Rhet2Pix converts the input prompt into incrementally elaborated sub-sentences and executes corresponding image-generation actions, constructing semantically richer visuals. In the inner layer, Rhet2Pix mitigates reward sparsity during image generation by discounting the final reward and optimizing every adjacent action pair along the diffusion denoising trajectory. Extensive experiments demonstrate the effectiveness of Rhet2Pix in rhetorical text-to-image generation. Our model outperforms SOTA MLLMs such as GPT-4o, Grok-3 and leading academic baselines across both qualitative and quantitative evaluations. The code and dataset used in this work are publicly available.
>
---
#### [new 071] Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition
- **分类: cs.CV**

- **简介: 该论文针对手写数学表达式识别（HMER）任务，解决符号布局自由和书写风格多变导致的识别难题。基于预训练视觉语言模型（VLM），提出Uni-MuMER，通过多任务微调整合结构化空间推理（Tree-CoT）、错误驱动学习（EDL）和符号计数（SC），在CROHME和HME100K数据集上实现新SOTA，零样本设置下超现有模型16.31%-24.42%。**

- **链接: [http://arxiv.org/pdf/2505.23566v1](http://arxiv.org/pdf/2505.23566v1)**

> **作者:** Yu Li; Jin Jiang; Jianhua Zhu; Shuai Peng; Baole Wei; Yuxuan Zhou; Liangcai Gao
>
> **摘要:** Handwritten Mathematical Expression Recognition (HMER) remains a persistent challenge in Optical Character Recognition (OCR) due to the inherent freedom of symbol layout and variability in handwriting styles. Prior methods have faced performance bottlenecks, proposing isolated architectural modifications that are difficult to integrate coherently into a unified framework. Meanwhile, recent advances in pretrained vision-language models (VLMs) have demonstrated strong cross-task generalization, offering a promising foundation for developing unified solutions. In this paper, we introduce Uni-MuMER, which fully fine-tunes a VLM for the HMER task without modifying its architecture, effectively injecting domain-specific knowledge into a generalist framework. Our method integrates three data-driven tasks: Tree-Aware Chain-of-Thought (Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for reducing confusion among visually similar characters, and Symbol Counting (SC) for improving recognition consistency in long expressions. Experiments on the CROHME and HME100K datasets show that Uni-MuMER achieves new state-of-the-art performance, surpassing the best lightweight specialized model SSAN by 16.31% and the top-performing VLM Gemini2.5-flash by 24.42% in the zero-shot setting. Our datasets, models, and code are open-sourced at: https://github.com/BFlameSwift/Uni-MuMER
>
---
#### [new 072] Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于视觉语言模型（VLM）文化分析任务，针对其在文化评估中表现不足的问题，提出基于文化理论的五个框架，系统识别图像中的文化维度，以完善VLM的文化能力评估。**

- **链接: [http://arxiv.org/pdf/2505.22793v1](http://arxiv.org/pdf/2505.22793v1)**

> **作者:** Srishti Yadav; Lauren Tilton; Maria Antoniak; Taylor Arnold; Jiaang Li; Siddhesh Milind Pawar; Antonia Karamolegkou; Stella Frank; Zhaochong An; Negar Rostamzadeh; Daniel Hershcovich; Serge Belongie; Ekaterina Shutova
>
> **摘要:** Modern vision-language models (VLMs) often fail at cultural competency evaluations and benchmarks. Given the diversity of applications built upon VLMs, there is renewed interest in understanding how they encode cultural nuances. While individual aspects of this problem have been studied, we still lack a comprehensive framework for systematically identifying and annotating the nuanced cultural dimensions present in images for VLMs. This position paper argues that foundational methodologies from visual culture studies (cultural studies, semiotics, and visual studies) are necessary for cultural analysis of images. Building upon this review, we propose a set of five frameworks, corresponding to cultural dimensions, that must be considered for a more complete analysis of the cultural competencies of VLMs.
>
---
#### [new 073] LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering
- **分类: cs.CV**

- **简介: 该论文提出LODGE方法，属于大规模3D场景实时渲染任务。针对内存受限设备下高效渲染难题，提出分层细节层次（LOD）表示，通过相机距离动态选择最优高斯集合，并结合空间分块加载与边缘混合技术，减少内存占用与渲染延迟，同时保持视觉保真度，实现高效高质量渲染。**

- **链接: [http://arxiv.org/pdf/2505.23158v1](http://arxiv.org/pdf/2505.23158v1)**

> **作者:** Jonas Kulhanek; Marie-Julie Rakotosaona; Fabian Manhardt; Christina Tsalicoglou; Michael Niemeyer; Torsten Sattler; Songyou Peng; Federico Tombari
>
> **备注:** Web: https://lodge-gs.github.io/
>
> **摘要:** In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements.
>
---
#### [new 074] OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation
- **分类: cs.CV**

- **简介: 论文提出OpenUni，一种轻量级开源基线模型，旨在统一多模态理解和生成任务。针对多模态任务训练复杂度高问题，通过连接现成的多模态LLM和扩散模型，采用可学习查询与轻量Transformer架构，实现高质量图像生成及在GenEval等基准测试中的优异表现（1.1B/3.1B参数），并开源模型与数据集支持研究。**

- **链接: [http://arxiv.org/pdf/2505.23661v1](http://arxiv.org/pdf/2505.23661v1)**

> **作者:** Size Wu; Zhonghua Wu; Zerui Gong; Qingyi Tao; Sheng Jin; Qinyue Li; Wei Li; Chen Change Loy
>
> **摘要:** In this report, we present OpenUni, a simple, lightweight, and fully open-source baseline for unifying multimodal understanding and generation. Inspired by prevailing practices in unified model learning, we adopt an efficient training strategy that minimizes the training complexity and overhead by bridging the off-the-shelf multimodal large language models (LLMs) and diffusion models through a set of learnable queries and a light-weight transformer-based connector. With a minimalist choice of architecture, we demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned images, and 2) achieve exceptional performance on standard benchmarks such as GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To support open research and community advancement, we release all model weights, training code, and our curated training datasets (including 23M image-text pairs) at https://github.com/wusize/OpenUni.
>
---
#### [new 075] A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis
- **分类: cs.CV**

- **简介: 该论文属于多模态大语言模型（MLLMs）在内窥镜分析的评估任务。针对现有基准覆盖场景和临床任务不足的问题，提出EndoBench基准，包含4种内窥镜场景、12项临床任务及5级视觉提示，构建6832个VQA对，评估23种模型，发现专有模型表现最优但逊于人类专家，强调医疗微调和任务复杂度对性能影响，建立新评估标准。**

- **链接: [http://arxiv.org/pdf/2505.23601v1](http://arxiv.org/pdf/2505.23601v1)**

> **作者:** Shengyuan Liu; Boyun Zheng; Wenting Chen; Zhihao Peng; Zhenfei Yin; Jing Shao; Jiancong Hu; Yixuan Yuan
>
> **备注:** 36 pages, 18 figures
>
> **摘要:** Endoscopic procedures are essential for diagnosing and treating internal diseases, and multi-modal large language models (MLLMs) are increasingly applied to assist in endoscopy analysis. However, current benchmarks are limited, as they typically cover specific endoscopic scenarios and a small set of clinical tasks, failing to capture the real-world diversity of endoscopic scenarios and the full range of skills needed in clinical workflows. To address these issues, we introduce EndoBench, the first comprehensive benchmark specifically designed to assess MLLMs across the full spectrum of endoscopic practice with multi-dimensional capacities. EndoBench encompasses 4 distinct endoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks, and 5 levels of visual prompting granularities, resulting in 6,832 rigorously validated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation framework mirrors the clinical workflow--spanning anatomical recognition, lesion analysis, spatial localization, and surgical operations--to holistically gauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios. We benchmark 23 state-of-the-art models, including general-purpose, medical-specialized, and proprietary MLLMs, and establish human clinician performance as a reference standard. Our extensive experiments reveal: (1) proprietary MLLMs outperform open-source and medical-specialized models overall, but still trail human experts; (2) medical-domain supervised fine-tuning substantially boosts task-specific accuracy; and (3) model performance remains sensitive to prompt format and clinical task complexity. EndoBench establishes a new standard for evaluating and advancing MLLMs in endoscopy, highlighting both progress and persistent gaps between current models and expert clinical reasoning. We publicly release our benchmark and code.
>
---
#### [new 076] IRS: Incremental Relationship-guided Segmentation for Digital Pathology
- **分类: cs.CV**

- **简介: 该论文提出IRS方法，针对数字病理中持续学习的分割任务，解决时间上部分标注数据及处理新类别（如未知病灶）的挑战。通过增量关系矩阵建模类间解剖关系，实现时空OOD持续学习，提升多尺度病理结构分割与领域泛化能力。**

- **链接: [http://arxiv.org/pdf/2505.22855v1](http://arxiv.org/pdf/2505.22855v1)**

> **作者:** Ruining Deng; Junchao Zhu; Juming Xiong; Can Cui; Tianyuan Yao; Junlin Guo; Siqi Lu; Marilyn Lionts; Mengmeng Yin; Yu Wang; Shilin Zhao; Yucheng Tang; Yihe Yang; Paul Dennis Simonson; Mert R. Sabuncu; Haichun Yang; Yuankai Huo
>
> **摘要:** Continual learning is rapidly emerging as a key focus in computer vision, aiming to develop AI systems capable of continuous improvement, thereby enhancing their value and practicality in diverse real-world applications. In healthcare, continual learning holds great promise for continuously acquired digital pathology data, which is collected in hospitals on a daily basis. However, panoramic segmentation on digital whole slide images (WSIs) presents significant challenges, as it is often infeasible to obtain comprehensive annotations for all potential objects, spanning from coarse structures (e.g., regions and unit objects) to fine structures (e.g., cells). This results in temporally and partially annotated data, posing a major challenge in developing a holistic segmentation framework. Moreover, an ideal segmentation model should incorporate new phenotypes, unseen diseases, and diverse populations, making this task even more complex. In this paper, we introduce a novel and unified Incremental Relationship-guided Segmentation (IRS) learning scheme to address temporally acquired, partially annotated data while maintaining out-of-distribution (OOD) continual learning capacity in digital pathology. The key innovation of IRS lies in its ability to realize a new spatial-temporal OOD continual learning paradigm by mathematically modeling anatomical relationships between existing and newly introduced classes through a simple incremental universal proposition matrix. Experimental results demonstrate that the IRS method effectively handles the multi-scale nature of pathological segmentation, enabling precise kidney segmentation across various structures (regions, units, and cells) as well as OOD disease lesions at multiple magnifications. This capability significantly enhances domain generalization, making IRS a robust approach for real-world digital pathology applications.
>
---
#### [new 077] VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于视觉语言模型（LVLMs）效率优化任务，旨在解决细粒度视觉编码导致计算成本过高问题。提出VScan框架，在视觉编码阶段通过全局/局部扫描合并冗余token，并在语言模型中间层进行剪枝，实现加速推理（如LLaVA模型预填速提升2.91倍，计算量降90%）同时保持性能。**

- **链接: [http://arxiv.org/pdf/2505.22654v1](http://arxiv.org/pdf/2505.22654v1)**

> **作者:** Ce Zhang; Kaixin Ma; Tianqing Fang; Wenhao Yu; Hongming Zhang; Zhisong Zhang; Yaqi Xie; Katia Sycara; Haitao Mi; Dong Yu
>
> **摘要:** Recent Large Vision-Language Models (LVLMs) have advanced multi-modal understanding by incorporating finer-grained visual perception and encoding. However, such methods incur significant computational costs due to longer visual token sequences, posing challenges for real-time deployment. To mitigate this, prior studies have explored pruning unimportant visual tokens either at the output layer of the visual encoder or at the early layers of the language model. In this work, we revisit these design choices and reassess their effectiveness through comprehensive empirical studies of how visual tokens are processed throughout the visual encoding and language decoding stages. Guided by these insights, we propose VScan, a two-stage visual token reduction framework that addresses token redundancy by: (1) integrating complementary global and local scans with token merging during visual encoding, and (2) introducing pruning at intermediate layers of the language model. Extensive experimental results across four LVLMs validate the effectiveness of VScan in accelerating inference and demonstrate its superior performance over current state-of-the-arts on sixteen benchmarks. Notably, when applied to LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a 10$\times$ reduction in FLOPs, while retaining 95.4% of the original performance.
>
---
#### [new 078] GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation
- **分类: cs.CV**

- **简介: 该论文属于3D CAD生成任务，旨在解决GenCAD生成的B-reps约10%不可行问题。提出GenCAD-Self-Repairing框架，通过扩散引导去噪与回归修正机制，将不可行设计修复率达66%，提升可行性同时保持几何精度，增强AI驱动CAD在制造等领域的应用潜力。**

- **链接: [http://arxiv.org/pdf/2505.23287v1](http://arxiv.org/pdf/2505.23287v1)**

> **作者:** Chikaha Tsuji; Enrique Flores Medina; Harshit Gupta; Md Ferdous Alam
>
> **摘要:** With the advancement of generative AI, research on its application to 3D model generation has gained traction, particularly in automating the creation of Computer-Aided Design (CAD) files from images. GenCAD is a notable model in this domain, leveraging an autoregressive transformer-based architecture with a contrastive learning framework to generate CAD programs. However, a major limitation of GenCAD is its inability to consistently produce feasible boundary representations (B-reps), with approximately 10% of generated designs being infeasible. To address this, we propose GenCAD-Self-Repairing, a framework that enhances the feasibility of generative CAD models through diffusion guidance and a self-repairing pipeline. This framework integrates a guided diffusion denoising process in the latent space and a regression-based correction mechanism to refine infeasible CAD command sequences while preserving geometric accuracy. Our approach successfully converted two-thirds of infeasible designs in the baseline method into feasible ones, significantly improving the feasibility rate while simultaneously maintaining a reasonable level of geometric accuracy between the point clouds of ground truth models and generated models. By significantly improving the feasibility rate of generating CAD models, our approach helps expand the availability of high-quality training data and enhances the applicability of AI-driven CAD generation in manufacturing, architecture, and product design.
>
---
#### [new 079] FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于无逆向流式图像编辑任务，旨在解决现有方法编辑轨迹不稳定与源图像一致性差的问题。提出FlowAlign框架，引入流匹配损失正则化优化轨迹平滑性，平衡编辑语义与结构保真，并支持可逆编辑。**

- **链接: [http://arxiv.org/pdf/2505.23145v1](http://arxiv.org/pdf/2505.23145v1)**

> **作者:** Jeongsol Kim; Yeobin Hong; Jong Chul Ye
>
> **摘要:** Recent inversion-free, flow-based image editing methods such as FlowEdit leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3, enabling text-driven manipulation by solving an ordinary differential equation (ODE). While the lack of exact latent inversion is a core advantage of these methods, it often results in unstable editing trajectories and poor source consistency. To address this limitation, we propose FlowAlign, a novel inversion-free flow-based framework for consistent image editing with principled trajectory control. FlowAlign introduces a flow-matching loss as a regularization mechanism to promote smoother and more stable trajectories during the editing process. Notably, the flow-matching loss is shown to explicitly balance semantic alignment with the edit prompt and structural consistency with the source image along the trajectory. Furthermore, FlowAlign naturally supports reverse editing by simply reversing the ODE trajectory, highlighting the reversible and consistent nature of the transformation. Extensive experiments demonstrate that FlowAlign outperforms existing methods in both source preservation and editing controllability.
>
---
#### [new 080] Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch
- **分类: cs.CV**

- **简介: 该论文针对细粒度草图到图像检索（FG-SBIR）任务，解决草图数据高效网络缺失的问题。提出跨模态知识蒸馏适配现有轻量模型，并设计RL画布选择器动态调整计算，使FLOPs减少99.37%（40.18G→0.254G），参数减少84.89%，同时保持准确率（33.03%→32.77%）。**

- **链接: [http://arxiv.org/pdf/2505.23763v1](http://arxiv.org/pdf/2505.23763v1)**

> **作者:** Aneeshan Sain; Subhajit Maity; Pinaki Nath Chowdhury; Subhadeep Koley; Ayan Kumar Bhunia; Yi-Zhe Song
>
> **备注:** Accepted at CVPR 2025, Project Page: https://subhajitmaity.me/SketchDownTheFLOPs
>
> **摘要:** As sketch research has collectively matured over time, its adaptation for at-mass commercialisation emerges on the immediate horizon. Despite an already mature research endeavour for photos, there is no research on the efficient inference specifically designed for sketch data. In this paper, we first demonstrate existing state-of-the-art efficient light-weight models designed for photos do not work on sketches. We then propose two sketch-specific components which work in a plug-n-play manner on any photo efficient network to adapt them to work on sketch data. We specifically chose fine-grained sketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised sketch problem with immediate commercial value. Technically speaking, we first propose a cross-modal knowledge distillation network to transfer existing photo efficient networks to be compatible with sketch, which brings down number of FLOPs and model parameters by 97.96% percent and 84.89% respectively. We then exploit the abstract trait of sketch to introduce a RL-based canvas selector that dynamically adjusts to the abstraction level which further cuts down number of FLOPs by two thirds. The end result is an overall reduction of 99.37% of FLOPs (from 40.18G to 0.254G) when compared with a full network, while retaining the accuracy (33.03% vs 32.77%) -- finally making an efficient network for the sparse sketch data that exhibit even fewer FLOPs than the best photo counterpart.
>
---
#### [new 081] Identification of Patterns of Cognitive Impairment for Early Detection of Dementia
- **分类: cs.CV**

- **简介: 该论文提出通过识别个体认知衰退模式实现早期痴呆检测。针对传统测试耗时且无法大规模应用及不同人群模式差异问题，采用特征选择与聚类分析方法，从24000人数据中提取认知模式，预测个体发展路径，支持个性化随访。**

- **链接: [http://arxiv.org/pdf/2505.23109v1](http://arxiv.org/pdf/2505.23109v1)**

> **作者:** Anusha A. S.; Uma Ranjan; Medha Sharma; Siddharth Dutt
>
> **备注:** 4 pages, 2 figures, to be published in IEEE EMBC 2020
>
> **摘要:** Early detection of dementia is crucial to devise effective interventions. Comprehensive cognitive tests, while being the most accurate means of diagnosis, are long and tedious, thus limiting their applicability to a large population, especially when periodic assessments are needed. The problem is compounded by the fact that people have differing patterns of cognitive impairment as they progress to different forms of dementia. This paper presents a novel scheme by which individual-specific patterns of impairment can be identified and used to devise personalized tests for periodic follow-up. Patterns of cognitive impairment are initially learned from a population cluster of combined normals and MCIs, using a set of standardized cognitive tests. Impairment patterns in the population are identified using a 2-step procedure involving an ensemble wrapper feature selection followed by cluster identification and analysis. These patterns have been shown to correspond to clinically accepted variants of MCI, a prodrome of dementia. The learned clusters of patterns can subsequently be used to identify the most likely route of cognitive impairment, even for pre-symptomatic and apparently normal people. Baseline data of 24,000 subjects from the NACC database was used for the study.
>
---
#### [new 082] UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes
- **分类: cs.CV**

- **简介: 该论文提出UniTEX框架，解决3D纹理生成中UV映射导致的拓扑模糊问题。任务为高保真3D纹理生成，通过两阶段方法：首阶段用LoRA适配扩散模型合成多视角纹理，次阶段以Transformer直接预测3D纹理函数，绕过UV映射，在统一3D空间生成高质量一致纹理。**

- **链接: [http://arxiv.org/pdf/2505.23253v1](http://arxiv.org/pdf/2505.23253v1)**

> **作者:** Yixun Liang; Kunming Luo; Xiao Chen; Rui Chen; Hongyu Yan; Weiyu Li; Jiarui Liu; Ping Tan
>
> **备注:** 10 pages, 9 figures
>
> **摘要:** We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX.
>
---
#### [new 083] UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning
- **分类: cs.CV**

- **简介: 该论文属于统一多模态模型优化任务，解决现有模型依赖大规模预训练数据及外部资源的问题。提出UniRL方法，通过自生成图像与文本交互训练，结合监督微调和强化学习，减少任务间性能差距并降低计算需求，实现模型自我迭代提升。**

- **链接: [http://arxiv.org/pdf/2505.23380v1](http://arxiv.org/pdf/2505.23380v1)**

> **作者:** Weijia Mao; Zhenheng Yang; Mike Zheng Shou
>
> **摘要:** Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL.
>
---
#### [new 084] WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver-Assistance Systems
- **分类: cs.CV**

- **简介: 该论文提出WTEFNet框架，解决ADAS中低光环境下的实时物体检测问题。针对RGB摄像头在弱光下性能下降，设计低光增强、小波特征提取与自适应融合检测模块，并构建GSN数据集。实验显示其在低光场景达态性能且支持嵌入式实时部署。**

- **链接: [http://arxiv.org/pdf/2505.23201v1](http://arxiv.org/pdf/2505.23201v1)**

> **作者:** Hao Wu; Junzhou Chen; Ronghui Zhang; Nengchao Lyu; Hongyu Hu; Yanyong Guo; Tony Z. Qiu
>
> **备注:** This paper is expected to be submitted to IEEE Transactions on Instrumentation and Measurement
>
> **摘要:** Object detection is a cornerstone of environmental perception in advanced driver assistance systems(ADAS). However, most existing methods rely on RGB cameras, which suffer from significant performance degradation under low-light conditions due to poor image quality. To address this challenge, we proposes WTEFNet, a real-time object detection framework specifically designed for low-light scenarios, with strong adaptability to mainstream detectors. WTEFNet comprises three core modules: a Low-Light Enhancement (LLE) module, a Wavelet-based Feature Extraction (WFE) module, and an Adaptive Fusion Detection (AFFD) module. The LLE enhances dark regions while suppressing overexposed areas; the WFE applies multi-level discrete wavelet transforms to isolate high- and low-frequency components, enabling effective denoising and structural feature retention; the AFFD fuses semantic and illumination features for robust detection. To support training and evaluation, we introduce GSN, a manually annotated dataset covering both clear and rainy night-time scenes. Extensive experiments on BDD100K, SHIFT, nuScenes, and GSN demonstrate that WTEFNet achieves state-of-the-art accuracy under low-light conditions. Furthermore, deployment on a embedded platform (NVIDIA Jetson AGX Orin) confirms the framework's suitability for real-time ADAS applications.
>
---
#### [new 085] Can Large Language Models Challenge CNNS in Medical Image Analysis?
- **分类: cs.CV; cs.AI; cs.ET**

- **简介: 该论文属医学影像分类任务，探讨大型语言模型（LLMs）能否挑战CNN。通过构建多模态框架，对比两者在准确率、F1值、能耗等指标，发现CNN更优但LLMs经后处理可显著提升性能，旨在优化医疗诊断的可靠性与可持续性。**

- **链接: [http://arxiv.org/pdf/2505.23503v1](http://arxiv.org/pdf/2505.23503v1)**

> **作者:** Shibbir Ahmed; Shahnewaz Karim Sakib; Anindya Bijoy Das
>
> **摘要:** This study presents a multimodal AI framework designed for precisely classifying medical diagnostic images. Utilizing publicly available datasets, the proposed system compares the strengths of convolutional neural networks (CNNs) and different large language models (LLMs). This in-depth comparative analysis highlights key differences in diagnostic performance, execution efficiency, and environmental impacts. Model evaluation was based on accuracy, F1-score, average execution time, average energy consumption, and estimated $CO_2$ emission. The findings indicate that although CNN-based models can outperform various multimodal techniques that incorporate both images and contextual information, applying additional filtering on top of LLMs can lead to substantial performance gains. These findings highlight the transformative potential of multimodal AI systems to enhance the reliability, efficiency, and scalability of medical diagnostics in clinical settings.
>
---
#### [new 086] Implicit Inversion turns CLIP into a Decoder
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于文本到图像生成任务。旨在仅用CLIP模型实现图像合成，无需解码器或微调。通过隐式神经表示、对抗初始化、正交投影及混合损失稳定逆映射，实现文本生成图像、风格迁移与重建，证明判别模型潜在生成能力。**

- **链接: [http://arxiv.org/pdf/2505.23161v1](http://arxiv.org/pdf/2505.23161v1)**

> **作者:** Antonio D'Orazio; Maria Rosaria Briglia; Donato Crisostomi; Dario Loi; Emanuele Rodolà; Iacopo Masi
>
> **摘要:** CLIP is a discriminative model trained to align images and text in a shared embedding space. Due to its multimodal structure, it serves as the backbone of many generative pipelines, where a decoder is trained to map from the shared space back to images. In this work, we show that image synthesis is nevertheless possible using CLIP alone -- without any decoder, training, or fine-tuning. Our approach optimizes a frequency-aware implicit neural representation that encourages coarse-to-fine generation by stratifying frequencies across network layers. To stabilize this inverse mapping, we introduce adversarially robust initialization, a lightweight Orthogonal Procrustes projection to align local text and image embeddings, and a blending loss that anchors outputs to natural image statistics. Without altering CLIP's weights, this framework unlocks capabilities such as text-to-image generation, style transfer, and image reconstruction. These findings suggest that discriminative models may hold untapped generative potential, hidden in plain sight.
>
---
#### [new 087] How Animals Dance (When You're Not Looking)
- **分类: cs.CV; cs.GR**

- **简介: 该论文提出基于关键帧的动物舞蹈视频生成框架，通过图优化技术从少量输入关键帧（如6个）构建符合音乐节奏的编舞结构，并引入镜像姿势生成方法，利用视频扩散模型合成流畅中间帧，实现音乐同步、编舞自然的动物舞蹈视频生成，适用于多种动物与音乐类型。**

- **链接: [http://arxiv.org/pdf/2505.23738v1](http://arxiv.org/pdf/2505.23738v1)**

> **作者:** Xiaojuan Wang; Aleksander Holynski; Brian Curless; Ira Kemelmacher; Steve Seitz
>
> **备注:** Project page: https://how-animals-dance.github.io/
>
> **摘要:** We present a keyframe-based framework for generating music-synchronized, choreography aware animal dance videos. Starting from a few keyframes representing distinct animal poses -- generated via text-to-image prompting or GPT-4o -- we formulate dance synthesis as a graph optimization problem: find the optimal keyframe structure that satisfies a specified choreography pattern of beats, which can be automatically estimated from a reference dance video. We also introduce an approach for mirrored pose image generation, essential for capturing symmetry in dance. In-between frames are synthesized using an video diffusion model. With as few as six input keyframes, our method can produce up to 30 second dance videos across a wide range of animals and music tracks.
>
---
#### [new 088] Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于视觉语言推理任务，旨在解决长推理导致视觉信息注意力衰减及幻觉问题。提出Qwen-LA模型，通过视觉-文本反思机制（含BRPO优化）及视觉标记复制/路由技术，强制模型重新关注视觉信息，提升性能并减少幻觉。**

- **链接: [http://arxiv.org/pdf/2505.23558v1](http://arxiv.org/pdf/2505.23558v1)**

> **作者:** Xu Chu; Xinrong Chen; Guanyu Wang; Zhijie Tan; Kui Huang; Wenyu Lv; Tong Mo; Weiping Li
>
> **摘要:** Inference time scaling drives extended reasoning to enhance the performance of Vision-Language Models (VLMs), thus forming powerful Vision-Language Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens, causing visual information to receive less attention and may trigger hallucinations. Although introducing text-only reflection processes shows promise in language models, we demonstrate that it is insufficient to suppress hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain (Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a vision-text reflection process that guides the model to re-attention visual information during reasoning. We first propose a reinforcement learning method Balanced Reflective Policy Optimization (BRPO), which guides the model to decide when to generate vision-text reflection on its own and balance the number and length of reflections. Then, we formally prove that VLRMs lose attention to visual tokens as reasoning progresses, and demonstrate that supplementing visual information during reflection enhances visual attention. Therefore, during training and inference, Visual Token COPY and Visual Token ROUTE are introduced to force the model to re-attention visual information at the visual level, addressing the limitations of text-only reflection. Experiments on multiple visual QA datasets and hallucination metrics indicate that Qwen-LA achieves leading accuracy performance while reducing hallucinations. Our code is available at: https://github.com/Liar406/Look_Again.
>
---
#### [new 089] PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based Artist Agents
- **分类: cs.CV**

- **简介: 该论文提出PhotoArtAgent系统，属于智能照片修图任务。旨在解决非专业用户使用自动化工具时缺乏艺术深度与交互透明性的问题。通过结合视觉语言模型与自然语言推理，模拟专业艺术家的创作流程，实现艺术分析、策略规划、参数生成及迭代优化，并提供透明解释，实验显示其效果超越现有工具并接近人类专业水平。**

- **链接: [http://arxiv.org/pdf/2505.23130v1](http://arxiv.org/pdf/2505.23130v1)**

> **作者:** Haoyu Chen; Keda Tao; Yizao Wang; Xinlei Wang; Lei Zhu; Jinjin Gu
>
> **摘要:** Photo retouching is integral to photographic art, extending far beyond simple technical fixes to heighten emotional expression and narrative depth. While artists leverage expertise to create unique visual effects through deliberate adjustments, non-professional users often rely on automated tools that produce visually pleasing results but lack interpretative depth and interactive transparency. In this paper, we introduce PhotoArtAgent, an intelligent system that combines Vision-Language Models (VLMs) with advanced natural language reasoning to emulate the creative process of a professional artist. The agent performs explicit artistic analysis, plans retouching strategies, and outputs precise parameters to Lightroom through an API. It then evaluates the resulting images and iteratively refines them until the desired artistic vision is achieved. Throughout this process, PhotoArtAgent provides transparent, text-based explanations of its creative rationale, fostering meaningful interaction and user control. Experimental results show that PhotoArtAgent not only surpasses existing automated tools in user studies but also achieves results comparable to those of professional human artists.
>
---
#### [new 090] Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\textsuperscript{\textregistered} Assessment
- **分类: cs.CV**

- **简介: 该论文提出结合YOLOv11n检测器与预训练FUSegNet模型的轻量级管道，仅用500标注边界框实现跨身体部位伤口分割，无需针对新部位微调。解决现有模型泛化性差及依赖像素级标注的问题，将DESIGN-R评分精度从71%提升至94%，减少临床标注需求。**

- **链接: [http://arxiv.org/pdf/2505.23392v1](http://arxiv.org/pdf/2505.23392v1)**

> **作者:** Yun-Cheng Tsai
>
> **摘要:** Purpose: Accurate wound segmentation is essential for automated DESIGN-R scoring. However, existing models such as FUSegNet, which are trained primarily on foot ulcer datasets, often fail to generalize to wounds on other body sites. Methods: We propose an annotation-efficient pipeline that combines a lightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation model. Instead of relying on pixel-level annotations or retraining for new anatomical regions, our method achieves robust performance using only 500 manually labeled bounding boxes. This zero fine-tuning approach effectively bridges the domain gap and enables direct deployment across diverse wound types. This is an advance not previously demonstrated in the wound segmentation literature. Results: Evaluated on three real-world test sets spanning foot, sacral, and trochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23 percentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size estimation accuracy from 71 percent to 94 percent (see Table 3 for details). Conclusion: Our pipeline generalizes effectively across body sites without task-specific fine-tuning, demonstrating that minimal supervision, with 500 annotated ROIs, is sufficient for scalable, annotation-light wound segmentation. This capability paves the way for real-world DESIGN-R automation, reducing reliance on pixel-wise labeling, streamlining documentation workflows, and supporting objective and consistent wound scoring in clinical practice. We will publicly release the trained detector weights and configuration to promote reproducibility and facilitate downstream deployment.
>
---
#### [new 091] ATI: Any Trajectory Instruction for Controllable Video Generation
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于可控视频生成任务，解决多类型运动控制分离且效果不佳的问题。提出ATI框架，通过轻量级运动注入器将用户定义的轨迹（关键点路径）投影到预训练模型的潜在空间，统一实现相机运动、物体移动及局部变形控制，提升运动连贯性与视觉质量。**

- **链接: [http://arxiv.org/pdf/2505.22944v1](http://arxiv.org/pdf/2505.22944v1)**

> **作者:** Angtian Wang; Haibin Huang; Jacob Zhiyuan Fang; Yiding Yang; Chongyang Ma
>
> **摘要:** We propose a unified framework for motion control in video generation that seamlessly integrates camera movement, object-level translation, and fine-grained local motion using trajectory-based inputs. In contrast to prior methods that address these motion types through separate modules or task-specific designs, our approach offers a cohesive solution by projecting user-defined trajectories into the latent space of pre-trained image-to-video generation models via a lightweight motion injector. Users can specify keypoints and their motion paths to control localized deformations, entire object motion, virtual camera dynamics, or combinations of these. The injected trajectory signals guide the generative process to produce temporally consistent and semantically aligned motion sequences. Our framework demonstrates superior performance across multiple video motion control tasks, including stylized motion effects (e.g., motion brushes), dynamic viewpoint changes, and precise local motion manipulation. Experiments show that our method provides significantly better controllability and visual quality compared to prior approaches and commercial solutions, while remaining broadly compatible with various state-of-the-art video generation backbones. Project page: https://anytraj.github.io/.
>
---
#### [new 092] Language-guided Learning for Object Detection Tackling Multiple Variations in Aerial Images
- **分类: cs.CV**

- **简介: 该论文属于航空图像目标检测任务，旨在解决光照、视角等变化导致的场景与实例级多样性对检测精度的影响。提出LANGO框架，通过视觉语义推理器解析场景条件（如天气），结合语言引导的关系学习损失建模类别间关系，提升复杂变化下的检测性能。**

- **链接: [http://arxiv.org/pdf/2505.23193v1](http://arxiv.org/pdf/2505.23193v1)**

> **作者:** Sungjune Park; Hyunjun Kim; Beomchan Park; Yong Man Ro
>
> **摘要:** Despite recent advancements in computer vision research, object detection in aerial images still suffers from several challenges. One primary challenge to be mitigated is the presence of multiple types of variation in aerial images, for example, illumination and viewpoint changes. These variations result in highly diverse image scenes and drastic alterations in object appearance, so that it becomes more complicated to localize objects from the whole image scene and recognize their categories. To address this problem, in this paper, we introduce a novel object detection framework in aerial images, named LANGuage-guided Object detection (LANGO). Upon the proposed language-guided learning, the proposed framework is designed to alleviate the impacts from both scene and instance-level variations. First, we are motivated by the way humans understand the semantics of scenes while perceiving environmental factors in the scenes (e.g., weather). Therefore, we design a visual semantic reasoner that comprehends visual semantics of image scenes by interpreting conditions where the given images were captured. Second, we devise a training objective, named relation learning loss, to deal with instance-level variations, such as viewpoint angle and scale changes. This training objective aims to learn relations in language representations of object categories, with the help of the robust characteristics against such variations. Through extensive experiments, we demonstrate the effectiveness of the proposed method, and our method obtains noticeable detection performance improvements.
>
---
#### [new 093] DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP
- **分类: cs.CV**

- **简介: 该论文提出DarkDiff框架，通过重新适配预训练扩散模型与相机ISP，解决低光raw图像增强中的细节过平滑与色彩失真问题，实验显示其性能优于现有方法。**

- **链接: [http://arxiv.org/pdf/2505.23743v1](http://arxiv.org/pdf/2505.23743v1)**

> **作者:** Amber Yijia Zheng; Yu Zhang; Jun Hu; Raymond A. Yeh; Chen Chen
>
> **摘要:** High-quality photography in extreme low-light conditions is challenging but impactful for digital cameras. With advanced computing hardware, traditional camera image signal processor (ISP) algorithms are gradually being replaced by efficient deep networks that enhance noisy raw images more intelligently. However, existing regression-based models often minimize pixel errors and result in oversmoothing of low-light photos or deep shadows. Recent work has attempted to address this limitation by training a diffusion model from scratch, yet those models still struggle to recover sharp image details and accurate colors. We introduce a novel framework to enhance low-light raw images by retasking pre-trained generative diffusion models with the camera ISP. Extensive experiments demonstrate that our method outperforms the state-of-the-art in perceptual quality across three challenging low-light raw image benchmarks.
>
---
#### [new 094] Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching
- **分类: cs.CV**

- **简介: 该论文属于Flow Matching（流匹配）任务，旨在解决现有方法因路径交叉或耦合方向不匹配导致生成效率低的问题。提出Model-Aligned Coupling（MAC），结合几何距离与模型预测误差选择最优耦合，通过训练误差最低的样本，提升生成质量和效率。**

- **链接: [http://arxiv.org/pdf/2505.23346v1](http://arxiv.org/pdf/2505.23346v1)**

> **作者:** Yexiong Lin; Yu Yao; Tongliang Liu
>
> **摘要:** Flow Matching (FM) is an effective framework for training a model to learn a vector field that transports samples from a source distribution to a target distribution. To train the model, early FM methods use random couplings, which often result in crossing paths and lead the model to learn non-straight trajectories that require many integration steps to generate high-quality samples. To address this, recent methods adopt Optimal Transport (OT) to construct couplings by minimizing geometric distances, which helps reduce path crossings. However, we observe that such geometry-based couplings do not necessarily align with the model's preferred trajectories, making it difficult to learn the vector field induced by these couplings, which prevents the model from learning straight trajectories. Motivated by this, we propose Model-Aligned Coupling (MAC), an effective method that matches training couplings based not only on geometric distance but also on alignment with the model's preferred transport directions based on its prediction error. To avoid the time-costly match process, MAC proposes to select the top-$k$ fraction of couplings with the lowest error for training. Extensive experiments show that MAC significantly improves generation quality and efficiency in few-step settings compared to existing methods. Project page: https://yexionglin.github.io/mac
>
---
#### [new 095] Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation
- **分类: cs.CV**

- **简介: 该论文属于半监督语义分割任务，针对现有方法忽视空间增强（如旋转/平移）的问题，提出自适应空间增强策略ASAug。通过基于熵的动态调整，缓解空间增强导致的掩膜不一致，提升模型性能，实现SOTA结果。**

- **链接: [http://arxiv.org/pdf/2505.23438v1](http://arxiv.org/pdf/2505.23438v1)**

> **作者:** Lingyan Ran; Yali Li; Tao Zhuo; Shizhou Zhang; Yanning Zhang
>
> **备注:** 10 pages, 8 figures
>
> **摘要:** In semi-supervised semantic segmentation (SSSS), data augmentation plays a crucial role in the weak-to-strong consistency regularization framework, as it enhances diversity and improves model generalization. Recent strong augmentation methods have primarily focused on intensity-based perturbations, which have minimal impact on the semantic masks. In contrast, spatial augmentations like translation and rotation have long been acknowledged for their effectiveness in supervised semantic segmentation tasks, but they are often ignored in SSSS. In this work, we demonstrate that spatial augmentation can also contribute to model training in SSSS, despite generating inconsistent masks between the weak and strong augmentations. Furthermore, recognizing the variability among images, we propose an adaptive augmentation strategy that dynamically adjusts the augmentation for each instance based on entropy. Extensive experiments show that our proposed Adaptive Spatial Augmentation (\textbf{ASAug}) can be integrated as a pluggable module, consistently improving the performance of existing methods and achieving state-of-the-art results on benchmark datasets such as PASCAL VOC 2012, Cityscapes, and COCO.
>
---
#### [new 096] CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models
- **分类: cs.CV; cs.LG; q-bio.BM**

- **简介: 该论文提出CFP-Gen，一种基于扩散模型的蛋白质生成方法，解决多模态约束下蛋白质设计难题。通过AGFM模块动态整合功能注释（如GO、EC号）及RCFE模块捕捉残基交互，并结合3D结构约束，实现高效生成多功能蛋白，性能接近天然蛋白。任务为蛋白质从头设计，目标是突破单一条件限制，提升多约束满足能力。**

- **链接: [http://arxiv.org/pdf/2505.22869v1](http://arxiv.org/pdf/2505.22869v1)**

> **作者:** Junbo Yin; Chao Zha; Wenjia He; Chencheng Xu; Xin Gao
>
> **备注:** Accepted at ICML 2025. Code is available at https://github.com/yinjunbo/cfpgen
>
> **摘要:** Existing PLMs generate protein sequences based on a single-condition constraint from a specific modality, struggling to simultaneously satisfy multiple constraints across different modalities. In this work, we introduce CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein GENeration. CFP-Gen facilitates the de novo protein design by integrating multimodal conditions with functional, sequence, and structural constraints. Specifically, an Annotation-Guided Feature Modulation (AGFM) module is introduced to dynamically adjust the protein feature distribution based on composable functional annotations, e.g., GO terms, IPR domains and EC numbers. Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures residue-wise interaction to ensure more precise control. Additionally, off-the-shelf 3D structure encoders can be seamlessly integrated to impose geometric constraints. We demonstrate that CFP-Gen enables high-throughput generation of novel proteins with functionality comparable to natural proteins, while achieving a high success rate in designing multifunctional proteins. Code and data available at https://github.com/yinjunbo/cfpgen.
>
---
#### [new 097] Color Image Set Recognition Based on Quaternionic Grassmannians
- **分类: cs.CV; math.AG**

- **简介: 该论文提出基于四元数草曼尼安的颜色图像集识别方法，解决传统方法难以有效融合颜色信息与集合结构的问题。通过四元数建模颜色特征，将图像集映射为流形上的点，推导流形空间的距离公式并构建分类框架，在ETH-80数据集验证有效性，指出稳定性局限并提出改进方向。**

- **链接: [http://arxiv.org/pdf/2505.23629v1](http://arxiv.org/pdf/2505.23629v1)**

> **作者:** Xiang Xiang Wang; Tin-Yau Tam
>
> **摘要:** We propose a new method for recognizing color image sets using quaternionic Grassmannians, which use the power of quaternions to capture color information and represent each color image set as a point on the quaternionic Grassmannian. We provide a direct formula to calculate the shortest distance between two points on the quaternionic Grassmannian, and use this distance to build a new classification framework. Experiments on the ETH-80 benchmark dataset show that our method achieves good recognition results. We also discuss some limitations in stability and suggest ways the method can be improved in the future.
>
---
#### [new 098] ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks
- **分类: cs.CV**

- **简介: 该论文提出ThinkGeo基准，评估LLM驱动代理在遥感任务中的工具使用与多步骤规划能力。针对现有评估缺乏领域特定遥感测试的问题，设计包含436个结构化任务的基准，覆盖城市规划、灾害评估等场景，要求代理结合卫星影像与工具集推理。通过ReAct框架测试多模型，分析工具准确性和规划差异，填补遥感领域工具增强代理评估的空白。**

- **链接: [http://arxiv.org/pdf/2505.23752v1](http://arxiv.org/pdf/2505.23752v1)**

> **作者:** Akashah Shabbir; Muhammad Akhtar Munir; Akshay Dudhane; Muhammad Umer Sheikh; Muhammad Haris Khan; Paolo Fraccaro; Juan Bernabe Moreno; Fahad Shahbaz Khan; Salman Khan
>
> **摘要:** Recent progress in large language models (LLMs) has enabled tool-augmented agents capable of solving complex real-world tasks through step-by-step reasoning. However, existing evaluations often focus on general-purpose or multimodal scenarios, leaving a gap in domain-specific benchmarks that assess tool-use capabilities in complex remote sensing use cases. We present ThinkGeo, an agentic benchmark designed to evaluate LLM-driven agents on remote sensing tasks via structured tool use and multi-step planning. Inspired by tool-interaction paradigms, ThinkGeo includes human-curated queries spanning a wide range of real-world applications such as urban planning, disaster assessment and change analysis, environmental monitoring, transportation analysis, aviation monitoring, recreational infrastructure, and industrial site analysis. Each query is grounded in satellite or aerial imagery and requires agents to reason through a diverse toolset. We implement a ReAct-style interaction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o, Qwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise execution metrics and final answer correctness. Our analysis reveals notable disparities in tool accuracy and planning consistency across models. ThinkGeo provides the first extensive testbed for evaluating how tool-enabled LLMs handle spatial reasoning in remote sensing. Our code and dataset are publicly available
>
---
#### [new 099] VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models
- **分类: cs.CV**

- **简介: 该论文属于文本到视频生成任务，旨在解决现有模型生成物理不真实内容的问题。提出VideoREPA框架，通过token级关系对齐将视频基础模型的物理理解能力蒸馏到T2V模型，引入TRD损失实现时空对齐微调，首次将REPA方法应用于T2V物理知识注入，显著提升生成视频的物理合理性。**

- **链接: [http://arxiv.org/pdf/2505.23656v1](http://arxiv.org/pdf/2505.23656v1)**

> **作者:** Xiangdong Zhang; Jiaqi Liao; Shaofeng Zhang; Fanqing Meng; Xiangpeng Wan; Junchi Yan; Yu Cheng
>
> **摘要:** Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.
>
---
#### [new 100] DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification
- **分类: cs.CV**

- **简介: 该论文提出DSAGL框架，针对弱监督全幻灯片图像分类任务，解决大规模WSI标注不足与实例歧义问题。通过双流注意力引导学习、伪标签生成及轻量级编码器，提升诊断区域关注与模型一致性，在多数据集上超越现有方法。**

- **链接: [http://arxiv.org/pdf/2505.23341v1](http://arxiv.org/pdf/2505.23341v1)**

> **作者:** Daoxi Cao; Hangbei Cheng; Yijin Li; Ruolin Zhou; Xinyi Li; Xuehan Zhang; Binwei Li; Xuancheng Gu; Xueyu Liu; Yongfei Wu
>
> **摘要:** Whole-slide images (WSIs) are critical for cancer diagnosis due to their ultra-high resolution and rich semantic content. However, their massive size and the limited availability of fine-grained annotations pose substantial challenges for conventional supervised learning. We propose DSAGL (Dual-Stream Attention-Guided Learning), a novel weakly supervised classification framework that combines a teacher-student architecture with a dual-stream design. DSAGL explicitly addresses instance-level ambiguity and bag-level semantic consistency by generating multi-scale attention-based pseudo labels and guiding instance-level learning. A shared lightweight encoder (VSSMamba) enables efficient long-range dependency modeling, while a fusion-attentive module (FASA) enhances focus on sparse but diagnostically relevant regions. We further introduce a hybrid loss to enforce mutual consistency between the two streams. Experiments on CIFAR-10, NCT-CRC, and TCGA-Lung datasets demonstrate that DSAGL consistently outperforms state-of-the-art MIL baselines, achieving superior discriminative performance and robustness under weak supervision.
>
---
#### [new 101] Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文提出使用组相对策略优化（GRPO）微调视觉自回归模型，解决对齐复杂奖励信号（如美学评分和CLIP嵌入）及扩展生成分布的问题。通过RL优化，提升图像质量、风格控制，并使模型生成预训练未见的风格图像，验证了RL微调在效率和效果上的优势。**

- **链接: [http://arxiv.org/pdf/2505.23331v1](http://arxiv.org/pdf/2505.23331v1)**

> **作者:** Matteo Gallici; Haitz Sáez de Ocáriz Borde
>
> **摘要:** Fine-tuning pre-trained generative models with Reinforcement Learning (RL) has emerged as an effective approach for aligning outputs more closely with nuanced human preferences. In this paper, we investigate the application of Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual autoregressive (VAR) models. Our empirical results demonstrate that this approach enables alignment to intricate reward signals derived from aesthetic predictors and CLIP embeddings, significantly enhancing image quality and enabling precise control over the generation style. Interestingly, by leveraging CLIP, our method can help VAR models generalize beyond their initial ImageNet distribution: through RL-driven exploration, these models can generate images aligned with prompts referencing image styles that were absent during pre-training. In summary, we show that RL-based fine-tuning is both efficient and effective for VAR models, benefiting particularly from their fast inference speeds, which are advantageous for online sampling, an aspect that poses significant challenges for diffusion-based alternatives.
>
---
#### [new 102] Deep Modeling and Optimization of Medical Image Classification
- **分类: cs.CV**

- **简介: 该论文针对医疗图像分类任务，解决数据隐私与模型性能问题。提出三种方法：1)设计CLIP变体结合CNN和ViT进行癌症分类；2)用联邦学习保护隐私；3)融合传统机器学习提升模型泛化。实验显示MaxViT和ConvNeXt在皮肤癌数据集表现最优，SVM辅助提升2%性能。**

- **链接: [http://arxiv.org/pdf/2505.23040v1](http://arxiv.org/pdf/2505.23040v1)**

> **作者:** Yihang Wu; Muhammad Owais; Reem Kateb; Ahmad Chaddad
>
> **备注:** Accepted in ISBI2025
>
> **摘要:** Deep models, such as convolutional neural networks (CNNs) and vision transformer (ViT), demonstrate remarkable performance in image classification. However, those deep models require large data to fine-tune, which is impractical in the medical domain due to the data privacy issue. Furthermore, despite the feasible performance of contrastive language image pre-training (CLIP) in the natural domain, the potential of CLIP has not been fully investigated in the medical field. To face these challenges, we considered three scenarios: 1) we introduce a novel CLIP variant using four CNNs and eight ViTs as image encoders for the classification of brain cancer and skin cancer, 2) we combine 12 deep models with two federated learning techniques to protect data privacy, and 3) we involve traditional machine learning (ML) methods to improve the generalization ability of those deep models in unseen domain data. The experimental results indicate that maxvit shows the highest averaged (AVG) test metrics (AVG = 87.03\%) in HAM10000 dataset with multimodal learning, while convnext\_l demonstrates remarkable test with an F1-score of 83.98\% compared to swin\_b with 81.33\% in FL model. Furthermore, the use of support vector machine (SVM) can improve the overall test metrics with AVG of $\sim 2\%$ for swin transformer series in ISIC2018. Our codes are available at https://github.com/AIPMLab/SkinCancerSimulation.
>
---
#### [new 103] Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs
- **分类: cs.CV**

- **简介: 该论文提出医学图像美学推理新基准，针对MLLMs在图像筛选中因数据不足和美学推理能力弱导致的性能问题，构建含1500+样本的医疗图像数据集（评估四维度）并采用长思维链与DPA-GRPO优化方法，提升小模型超越大模型的推理能力。**

- **链接: [http://arxiv.org/pdf/2505.23265v1](http://arxiv.org/pdf/2505.23265v1)**

> **作者:** Zheng Sun; Yi Wei; Long Yu
>
> **摘要:** Multimodal Large Language Models (MLLMs) are of great application across many domains, such as multimodal understanding and generation. With the development of diffusion models (DM) and unified MLLMs, the performance of image generation has been significantly improved, however, the study of image screening is rare and its performance with MLLMs is unsatisfactory due to the lack of data and the week image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive medical image screening dataset with 1500+ samples, each sample consists of a medical image, four generated images, and a multiple-choice answer. The dataset evaluates the aesthetic reasoning ability under four aspects: \textit{(1) Appearance Deformation, (2) Principles of Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}. For methodology, we utilize long chains of thought (CoT) and Group Relative Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO, to enhance the image aesthetic reasoning ability of MLLMs. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the reinforcement learning approach, we are able to surpass the score of both large-scale models and leading closed-source models using a much smaller model. We hope our attempt on medical image screening will serve as a regular configuration in image aesthetic reasoning in the future.
>
---
#### [new 104] DIP-R1: Deep Inspection and Perception with RL Looking Through and Understanding Complex Scenes
- **分类: cs.CV**

- **简介: 该论文提出DIP-R1框架，通过强化学习提升多模态大模型在复杂场景（如拥挤区域）的细粒度视觉感知能力。针对现有模型在密集环境中的感知局限，设计三类奖励机制（推理奖励、不确定性区域观察奖励、精准决策奖励），引导模型逐步分析场景并聚焦模糊区域，显著优于基线方法。**

- **链接: [http://arxiv.org/pdf/2505.23179v1](http://arxiv.org/pdf/2505.23179v1)**

> **作者:** Sungjune Park; Hyunjun Kim; Junho Kim; Seongho Kim; Yong Man Ro
>
> **摘要:** Multimodal Large Language Models (MLLMs) have demonstrated significant visual understanding capabilities, yet their fine-grained visual perception in complex real-world scenarios, such as densely crowded public areas, remains limited. Inspired by the recent success of reinforcement learning (RL) in both LLMs and MLLMs, in this paper, we explore how RL can enhance visual perception ability of MLLMs. Then we develop a novel RL-based framework, Deep Inspection and Perception with RL (DIP-R1) designed to enhance the visual perception capabilities of MLLMs, by comprehending complex scenes and looking through visual instances closely. DIP-R1 guides MLLMs through detailed inspection of visual scene via three simply designed rule-based reward modelings. First, we adopt a standard reasoning reward encouraging the model to include three step-by-step processes: 1) reasoning for understanding visual scenes, 2) observing for looking through interested but ambiguous regions, and 3) decision-making for predicting answer. Second, a variance-guided looking reward is designed to examine uncertain regions for the second observing process. It explicitly enables the model to inspect ambiguous areas, improving its ability to mitigate perceptual uncertainties. Third, we model a weighted precision-recall accuracy reward enhancing accurate decision-making. We explore its effectiveness across diverse fine-grained object detection data consisting of challenging real-world environments, such as densely crowded scenes. Built upon existing MLLMs, DIP-R1 achieves consistent and significant improvement across various in-domain and out-of-domain scenarios. It also outperforms various existing baseline models and supervised fine-tuning methods. Our findings highlight the substantial potential of integrating RL into MLLMs for enhancing capabilities in complex real-world perception tasks.
>
---
#### [new 105] Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文研究多模态大语言模型（MLLMs）的规则化视觉强化学习（RL），以拼图任务为实验框架，解决其在感知任务中的表现与泛化问题。通过分析模型在拼图任务中的微调、任务迁移、推理模式及训练策略，揭示RL优于监督微调（SFT）的泛化能力，并探讨模型推理机制与预存复杂模式的影响。**

- **链接: [http://arxiv.org/pdf/2505.23590v1](http://arxiv.org/pdf/2505.23590v1)**

> **作者:** Zifu Wang; Junyi Zhu; Bo Tang; Zhiyu Li; Feiyu Xiong; Jiaqian Yu; Matthew B. Blaschko
>
> **摘要:** The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL using jigsaw puzzles as a structured experimental framework, revealing several key findings. \textit{Firstly,} we find that MLLMs, initially performing near to random guessing on simple puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. \textit{Secondly,} training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. \textit{Fourthly,} we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. \textit{Finally,} our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: \href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.
>
---
#### [new 106] Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models
- **分类: cs.CV**

- **简介: 该论文属于自动驾驶视觉语言动作（VLA）模型优化任务。针对现有模型在非结构化场景表现差的问题，提出Impromptu VLA数据集，包含8万余精选视频片段，基于四类挑战性场景设计，提供规划导向问答与轨迹标注。实验显示其显著提升模型性能，开源数据与代码。**

- **链接: [http://arxiv.org/pdf/2505.23757v1](http://arxiv.org/pdf/2505.23757v1)**

> **作者:** Haohan Chi; Huan-ang Gao; Ziming Liu; Jianing Liu; Chenyu Liu; Jinwei Li; Kaisen Yang; Yangcheng Yu; Zeda Wang; Wenyi Li; Leichen Wang; Xingtao Hu; Hao Sun; Hang Zhao; Hao Zhao
>
> **备注:** Project page: https://github.com/ahydchh/Impromptu-VLA
>
> **摘要:** Vision-Language-Action (VLA) models for autonomous driving show promise but falter in unstructured corner case scenarios, largely due to a scarcity of targeted benchmarks. To address this, we introduce Impromptu VLA. Our core contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets. This dataset is built upon our novel taxonomy of four challenging unstructured categories and features rich, planning-oriented question-answering annotations and action trajectories. Crucially, experiments demonstrate that VLAs trained with our dataset achieve substantial performance gains on established benchmarks--improving closed-loop NeuroNCAP scores and collision rates, and reaching near state-of-the-art L2 accuracy in open-loop nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an effective diagnostic, revealing clear VLM improvements in perception, prediction, and planning. Our code, data and models are available at https://github.com/ahydchh/Impromptu-VLA.
>
---
#### [new 107] Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features
- **分类: cs.CV; cs.MM; eess.IV**

- **简介: 该论文属于图像操纵区域弱监督定位任务，旨在解决现有方法在可解释性和定位精度上的不足及缺乏像素级标注的问题。提出结合图像级操纵检测网络的激活图与预训练分割模型的分割图，通过多视角特征融合和贝叶斯推理细化定位，实现无需像素标签的操纵区域定位。**

- **链接: [http://arxiv.org/pdf/2505.23586v1](http://arxiv.org/pdf/2505.23586v1)**

> **作者:** Ziyong Wang; Charith Abhayaratne
>
> **备注:** This paper was presented at the British Machine Vision Conference 2024 workshop on Media authenticity in the age of artificial intelligence
>
> **摘要:** The explosive growth of digital images and the widespread availability of image editing tools have made image manipulation detection an increasingly critical challenge. Current deep learning-based manipulation detection methods excel in achieving high image-level classification accuracy, they often fall short in terms of interpretability and localization of manipulated regions. Additionally, the absence of pixel-wise annotations in real-world scenarios limits the existing fully-supervised manipulation localization techniques. To address these challenges, we propose a novel weakly-supervised approach that integrates activation maps generated by image-level manipulation detection networks with segmentation maps from pre-trained models. Specifically, we build on our previous image-level work named WCBnet to produce multi-view feature maps which are subsequently fused for coarse localization. These coarse maps are then refined using detailed segmented regional information provided by pre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet), with Bayesian inference employed to enhance the manipulation localization. Experimental results demonstrate the effectiveness of our approach, highlighting the feasibility to localize image manipulations without relying on pixel-level labels.
>
---
#### [new 108] ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer
- **分类: cs.CV**

- **简介: 该论文提出ImmunoDiff模型，用于肺癌免疫治疗反应预测。针对现有方法依赖治疗前影像、难以捕捉动态变化的问题，其通过生成治疗后CT并整合解剖结构及临床数据（如PD-L1表达），提升预测准确率（21.24%）和生存预测c-index（+0.03）。**

- **链接: [http://arxiv.org/pdf/2505.23675v1](http://arxiv.org/pdf/2505.23675v1)**

> **作者:** Moinak Bhattacharya; Judy Huang; Amna F. Sher; Gagandeep Singh; Chao Chen; Prateek Prasanna
>
> **摘要:** Accurately predicting immunotherapy response in Non-Small Cell Lung Cancer (NSCLC) remains a critical unmet need. Existing radiomics and deep learning-based predictive models rely primarily on pre-treatment imaging to predict categorical response outcomes, limiting their ability to capture the complex morphological and textural transformations induced by immunotherapy. This study introduces ImmunoDiff, an anatomy-aware diffusion model designed to synthesize post-treatment CT scans from baseline imaging while incorporating clinically relevant constraints. The proposed framework integrates anatomical priors, specifically lobar and vascular structures, to enhance fidelity in CT synthesis. Additionally, we introduce a novel cbi-Adapter, a conditioning module that ensures pairwise-consistent multimodal integration of imaging and clinical data embeddings, to refine the generative process. Additionally, a clinical variable conditioning mechanism is introduced, leveraging demographic data, blood-based biomarkers, and PD-L1 expression to refine the generative process. Evaluations on an in-house NSCLC cohort treated with immune checkpoint inhibitors demonstrate a 21.24% improvement in balanced accuracy for response prediction and a 0.03 increase in c-index for survival prediction. Code will be released soon.
>
---
#### [new 109] R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation
- **分类: cs.CV; cs.CL**

- **简介: 该论文提出R2I-Bench基准，评估文本到图像生成模型的推理能力。针对现有模型推理不足问题，构建覆盖常识、数学等推理类别的评测数据集，并设计R2IScore指标，通过问答形式评估文本-图像对齐、推理准确性和图像质量。实验显示当前模型推理性能有限，强调需开发更强大的推理感知架构。**

- **链接: [http://arxiv.org/pdf/2505.23493v1](http://arxiv.org/pdf/2505.23493v1)**

> **作者:** Kaijie Chen; Zihao Lin; Zhiyang Xu; Ying Shen; Yuguang Yao; Joy Rimchala; Jiaxin Zhang; Lifu Huang
>
> **备注:** Project Page: https://r2i-bench.github.io
>
> **摘要:** Reasoning is a fundamental capability often required in real-world text-to-image (T2I) generation, e.g., generating ``a bitten apple that has been left in the air for more than a week`` necessitates understanding temporal decay and commonsense concepts. While recent T2I models have made impressive progress in producing photorealistic images, their reasoning capability remains underdeveloped and insufficiently evaluated. To bridge this gap, we introduce R2I-Bench, a comprehensive benchmark specifically designed to rigorously assess reasoning-driven T2I generation. R2I-Bench comprises meticulously curated data instances, spanning core reasoning categories, including commonsense, mathematical, logical, compositional, numerical, causal, and concept mixing. To facilitate fine-grained evaluation, we design R2IScore, a QA-style metric based on instance-specific, reasoning-oriented evaluation questions that assess three critical dimensions: text-image alignment, reasoning accuracy, and image quality. Extensive experiments with 16 representative T2I models, including a strong pipeline-based framework that decouples reasoning and generation using the state-of-the-art language and image generation models, demonstrate consistently limited reasoning performance, highlighting the need for more robust, reasoning-aware architectures in the next generation of T2I systems. Project Page: https://r2i-bench.github.io
>
---
#### [new 110] Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging
- **分类: cs.CV; cs.AI**

- **简介: 该论文属医学影像分类任务，对比持久图聚合与特征拼接方法对模型性能的影响。针对如何有效整合多条形码拓扑特征的问题，通过多数据集实验发现特征拼接保留细节且提升分类效果，推荐该方法。**

- **链接: [http://arxiv.org/pdf/2505.23637v1](http://arxiv.org/pdf/2505.23637v1)**

> **作者:** Dashti A. Ali; Richard K. G. Do; William R. Jarnagin; Aras T. Asaad; Amber L. Simpson
>
> **备注:** 16 pages, 8 figures
>
> **摘要:** In medical image analysis, feature engineering plays an important role in the design and performance of machine learning models. Persistent homology (PH), from the field of topological data analysis (TDA), demonstrates robustness and stability to data perturbations and addresses the limitation from traditional feature extraction approaches where a small change in input results in a large change in feature representation. Using PH, we store persistent topological and geometrical features in the form of the persistence barcode whereby large bars represent global topological features and small bars encapsulate geometrical information of the data. When multiple barcodes are computed from 2D or 3D medical images, two approaches can be used to construct the final topological feature vector in each dimension: aggregating persistence barcodes followed by featurization or concatenating topological feature vectors derived from each barcode. In this study, we conduct a comprehensive analysis across diverse medical imaging datasets to compare the effects of the two aforementioned approaches on the performance of classification models. The results of this analysis indicate that feature concatenation preserves detailed topological information from individual barcodes, yields better classification performance and is therefore a preferred approach when conducting similar experiments.
>
---
#### [new 111] Towards Privacy-Preserving Fine-Grained Visual Classification via Hierarchical Learning from Label Proportions
- **分类: cs.CV**

- **简介: 该论文属于隐私保护的细粒度视觉分类任务，旨在解决现有方法依赖实例级标签导致隐私泄露的问题。提出LHFGLP框架，利用分层标签比例学习，通过分层稀疏字典学习和层级损失函数，在仅使用包级标签的情况下提升分类精度，实验显示优于现有方法。**

- **链接: [http://arxiv.org/pdf/2505.23031v1](http://arxiv.org/pdf/2505.23031v1)**

> **作者:** Jinyi Chang; Dongliang Chang; Lei Chen; Bingyao Yu; Zhanyu Ma
>
> **备注:** 10 pages, 5 figures, 5 tables
>
> **摘要:** In recent years, Fine-Grained Visual Classification (FGVC) has achieved impressive recognition accuracy, despite minimal inter-class variations. However, existing methods heavily rely on instance-level labels, making them impractical in privacy-sensitive scenarios such as medical image analysis. This paper aims to enable accurate fine-grained recognition without direct access to instance labels. To achieve this, we leverage the Learning from Label Proportions (LLP) paradigm, which requires only bag-level labels for efficient training. Unlike existing LLP-based methods, our framework explicitly exploits the hierarchical nature of fine-grained datasets, enabling progressive feature granularity refinement and improving classification accuracy. We propose Learning from Hierarchical Fine-Grained Label Proportions (LHFGLP), a framework that incorporates Unrolled Hierarchical Fine-Grained Sparse Dictionary Learning, transforming handcrafted iterative approximation into learnable network optimization. Additionally, our proposed Hierarchical Proportion Loss provides hierarchical supervision, further enhancing classification performance. Experiments on three widely-used fine-grained datasets, structured in a bag-based manner, demonstrate that our framework consistently outperforms existing LLP-based methods. We will release our code and datasets to foster further research in privacy-preserving fine-grained classification.
>
---
#### [new 112] Unsupervised Transcript-assisted Video Summarization and Highlight Detection
- **分类: cs.CV; cs.AI; cs.MM**

- **简介: 该论文属于视频摘要和亮点检测任务，旨在解决现有方法未融合多模态与强化学习（RL）及标注数据不足的问题。提出结合视频帧与文本的多模态RL框架，通过无监督训练优化摘要多样性和文本相关性，实验表明优于仅视觉方法。**

- **链接: [http://arxiv.org/pdf/2505.23268v1](http://arxiv.org/pdf/2505.23268v1)**

> **作者:** Spyros Barbakos; Charalampos Antoniadis; Gerasimos Potamianos; Gianluca Setti
>
> **摘要:** Video consumption is a key part of daily life, but watching entire videos can be tedious. To address this, researchers have explored video summarization and highlight detection to identify key video segments. While some works combine video frames and transcripts, and others tackle video summarization and highlight detection using Reinforcement Learning (RL), no existing work, to the best of our knowledge, integrates both modalities within an RL framework. In this paper, we propose a multimodal pipeline that leverages video frames and their corresponding transcripts to generate a more condensed version of the video and detect highlights using a modality fusion mechanism. The pipeline is trained within an RL framework, which rewards the model for generating diverse and representative summaries while ensuring the inclusion of video segments with meaningful transcript content. The unsupervised nature of the training allows for learning from large-scale unannotated datasets, overcoming the challenge posed by the limited size of existing annotated datasets. Our experiments show that using the transcript in video summarization and highlight detection achieves superior results compared to relying solely on the visual content of the video.
>
---
#### [new 113] VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration
- **分类: cs.CV**

- **简介: 该论文属于虚拟试衣任务，旨在解决现有方法因姿态差异、自遮挡导致衣物变形失真和细节丢失的问题。提出VITON-DRR方法，通过双金字塔特征提取器重建人体语义分割，设计变形模块提取衣物关键点并采用非刚性配准精准变形，结合图像合成模块生成细节保留的试穿结果。**

- **链接: [http://arxiv.org/pdf/2505.23439v1](http://arxiv.org/pdf/2505.23439v1)**

> **作者:** Ben Li; Minqi Li; Jie Ren; Kaibing Zhang
>
> **备注:** 31 pages, 12 figures, Accepted by Computers & Graphics
>
> **摘要:** Image-based virtual try-on aims to fit a target garment to a specific person image and has attracted extensive research attention because of its huge application potential in the e-commerce and fashion industries. To generate high-quality try-on results, accurately warping the clothing item to fit the human body plays a significant role, as slight misalignment may lead to unrealistic artifacts in the fitting image. Most existing methods warp the clothing by feature matching and thin-plate spline (TPS). However, it often fails to preserve clothing details due to self-occlusion, severe misalignment between poses, etc. To address these challenges, this paper proposes a detail retention virtual try-on method via accurate non-rigid registration (VITON-DRR) for diverse human poses. Specifically, we reconstruct a human semantic segmentation using a dual-pyramid-structured feature extractor. Then, a novel Deformation Module is designed for extracting the cloth key points and warping them through an accurate non-rigid registration algorithm. Finally, the Image Synthesis Module is designed to synthesize the deformed garment image and generate the human pose information adaptively. {Compared with} traditional methods, the proposed VITON-DRR can make the deformation of fitting images more accurate and retain more garment details. The experimental results demonstrate that the proposed method performs better than state-of-the-art methods.
>
---
#### [new 114] Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping
- **分类: cs.CV**

- **简介: 该论文属于室内3D物体检测与SLAM任务，解决未配准图像下的场景级3D检测与定位建图问题。提出Rooms from Motion（RfM）方法，通过基于3D框的对象匹配替代传统关键点匹配，联合估计相机位姿与物体轨迹，生成语义3D地图，在数据集上优于现有方法，实现高效稀疏定位与对象参数化建图。**

- **链接: [http://arxiv.org/pdf/2505.23756v1](http://arxiv.org/pdf/2505.23756v1)**

> **作者:** Justin Lazarow; Kai Kang; Afshin Dehghan
>
> **摘要:** We revisit scene-level 3D object detection as the output of an object-centric framework capable of both localization and mapping using 3D oriented boxes as the underlying geometric primitive. While existing 3D object detection approaches operate globally and implicitly rely on the a priori existence of metric camera poses, our method, Rooms from Motion (RfM) operates on a collection of un-posed images. By replacing the standard 2D keypoint-based matcher of structure-from-motion with an object-centric matcher based on image-derived 3D boxes, we estimate metric camera poses, object tracks, and finally produce a global, semantic 3D object map. When a priori pose is available, we can significantly improve map quality through optimization of global 3D boxes against individual observations. RfM shows strong localization performance and subsequently produces maps of higher quality than leading point-based and multi-view 3D object detection methods on CA-1M and ScanNet++, despite these global methods relying on overparameterization through point clouds or dense volumes. Rooms from Motion achieves a general, object-centric representation which not only extends the work of Cubify Anything to full scenes but also allows for inherently sparse localization and parametric mapping proportional to the number of objects in a scene.
>
---
#### [new 115] RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer
- **分类: cs.CV**

- **简介: 该论文属于机器人视觉策略迁移任务，旨在解决模拟到现实（sim-to-real）的性能差距问题。提出RoboTransfer框架，通过结合多视角几何约束与扩散模型生成几何一致的多视角视频，实现对场景元素（如背景、物体属性）的精细控制，提升合成数据的视觉保真度与跨视角几何一致性，从而显著提高机器人策略在目标域的迁移成功率。**

- **链接: [http://arxiv.org/pdf/2505.23171v1](http://arxiv.org/pdf/2505.23171v1)**

> **作者:** Liu Liu; Xiaofeng Wang; Guosheng Zhao; Keyu Li; Wenkang Qin; Jiaxiong Qiu; Zheng Zhu; Guan Huang; Zhizhong Su
>
> **备注:** 20 pages, 15 figures
>
> **摘要:** Imitation Learning has become a fundamental approach in robotic manipulation. However, collecting large-scale real-world robot demonstrations is prohibitively expensive. Simulators offer a cost-effective alternative, but the sim-to-real gap make it extremely challenging to scale. Therefore, we introduce RoboTransfer, a diffusion-based video generation framework for robotic data synthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry with explicit control over scene components, such as background and object attributes. By incorporating cross-view feature interactions and global depth/normal conditions, RoboTransfer ensures geometry consistency across views. This framework allows fine-grained control, including background edits and object swaps. Experiments demonstrate that RoboTransfer is capable of generating multi-view videos with enhanced geometric consistency and visual fidelity. In addition, policies trained on the data generated by RoboTransfer achieve a 33.3% relative improvement in the success rate in the DIFF-OBJ setting and a substantial 251% relative improvement in the more challenging DIFF-ALL scenario. Explore more demos on our project page: https://horizonrobotics.github.io/robot_lab/robotransfer
>
---
#### [new 116] VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文提出VF-Eval基准，评估多模态LLMs在AIGC视频反馈中的能力，填补现有评估忽视合成视频的空白。通过四个任务（连贯性验证、错误识别等）测试13个模型，发现GPT-4.1表现仍不均衡，证明基准挑战性，并通过RePrompt实验验证其改进视频生成的潜力。**

- **链接: [http://arxiv.org/pdf/2505.23693v1](http://arxiv.org/pdf/2505.23693v1)**

> **作者:** Tingyu Song; Tongyan Hu; Guo Gan; Yilun Zhao
>
> **备注:** ACL 2025 Main
>
> **摘要:** MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.
>
---
#### [new 117] Toward Memory-Aided World Models: Benchmarking via Spatial Consistency
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于世界模型研究任务，旨在解决现有基准忽视长期空间一致性的问题。通过构建基于Minecraft的导航视频数据集（2000万帧），设计渐进式序列长度基准，评估四种世界模型在空间一致性上的表现，推动记忆模块发展。**

- **链接: [http://arxiv.org/pdf/2505.22976v1](http://arxiv.org/pdf/2505.22976v1)**

> **作者:** Kewei Lian; Shaofei Cai; Yilun Du; Yitao Liang
>
> **摘要:** The ability to simulate the world in a spatially consistent manner is a crucial requirements for effective world models. Such a model enables high-quality visual generation, and also ensures the reliability of world models for downstream tasks such as simulation and planning. Designing a memory module is a crucial component for addressing spatial consistency: such a model must not only retain long-horizon observational information, but also enables the construction of explicit or implicit internal spatial representations. However, there are no dataset designed to promote the development of memory modules by explicitly enforcing spatial consistency constraints. Furthermore, most existing benchmarks primarily emphasize visual coherence or generation quality, neglecting the requirement of long-range spatial consistency. To bridge this gap, we construct a dataset and corresponding benchmark by sampling 150 distinct locations within the open-world environment of Minecraft, collecting about 250 hours (20 million frames) of loop-based navigation videos with actions. Our dataset follows a curriculum design of sequence lengths, allowing models to learn spatial consistency on increasingly complex navigation trajectories. Furthermore, our data collection pipeline is easily extensible to new Minecraft environments and modules. Four representative world model baselines are evaluated on our benchmark. Dataset, benchmark, and code are open-sourced to support future research.
>
---
#### [new 118] Diffusion Sampling Path Tells More: An Efficient Plug-and-Play Strategy for Sample Filtering
- **分类: cs.CV**

- **简介: 该论文提出CFG-Rejection方法，解决扩散模型采样质量不一致及计算成本高的问题。发现样本质量与去噪轨迹的累积得分差异(ASD)相关，通过早期筛选低质样本提升效率，无需额外训练或修改模型，兼容现有框架。**

- **链接: [http://arxiv.org/pdf/2505.23343v1](http://arxiv.org/pdf/2505.23343v1)**

> **作者:** Sixian Wang; Zhiwei Tang; Tsung-Hui Chang
>
> **摘要:** Diffusion models often exhibit inconsistent sample quality due to stochastic variations inherent in their sampling trajectories. Although training-based fine-tuning (e.g. DDPO [1]) and inference-time alignment techniques[2] aim to improve sample fidelity, they typically necessitate full denoising processes and external reward signals. This incurs substantial computational costs, hindering their broader applicability. In this work, we unveil an intriguing phenomenon: a previously unobserved yet exploitable link between sample quality and characteristics of the denoising trajectory during classifier-free guidance (CFG). Specifically, we identify a strong correlation between high-density regions of the sample distribution and the Accumulated Score Differences (ASD)--the cumulative divergence between conditional and unconditional scores. Leveraging this insight, we introduce CFG-Rejection, an efficient, plug-and-play strategy that filters low-quality samples at an early stage of the denoising process, crucially without requiring external reward signals or model retraining. Importantly, our approach necessitates no modifications to model architectures or sampling schedules and maintains full compatibility with existing diffusion frameworks. We validate the effectiveness of CFG-Rejection in image generation through extensive experiments, demonstrating marked improvements on human preference scores (HPSv2, PickScore) and challenging benchmarks (GenEval, DPG-Bench). We anticipate that CFG-Rejection will offer significant advantages for diverse generative modalities beyond images, paving the way for more efficient and reliable high-quality sample generation.
>
---
#### [new 119] CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis
- **分类: cs.CV; cs.AI**

- **简介: 该论文提出CryoCCD框架，解决冷冻电镜数据稀缺及噪声建模不足问题。通过整合生物物理模型与条件扩散模型，结合循环一致性约束和空间自适应噪声学习，生成结构多样、噪声真实的合成微图，提升颗粒检测和三维重构等任务性能。**

- **链接: [http://arxiv.org/pdf/2505.23444v1](http://arxiv.org/pdf/2505.23444v1)**

> **作者:** Runmin Jiang; Genpei Zhang; Yuntian Yang; Siqi Wu; Yuheng Zhang; Wanyue Feng; Yizhou Zhao; Xi Xiao; Xiao Wang; Tianyang Wang; Xingjian Li; Min Xu
>
> **摘要:** Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of macromolecules, but developing robust models for downstream analysis is hindered by the scarcity of high-quality annotated data. While synthetic data generation has emerged as a potential solution, existing methods often fail to capture both the structural diversity of biological specimens and the complex, spatially varying noise inherent in cryo-EM imaging. To overcome these limitations, we propose CryoCCD, a synthesis framework that integrates biophysical modeling with generative techniques. Specifically, CryoCCD produces multi-scale cryo-EM micrographs that reflect realistic biophysical variability through compositional heterogeneity, cellular context, and physics-informed imaging. To generate realistic noise, we employ a conditional diffusion model, enhanced by cycle consistency to preserve structural fidelity and mask-aware contrastive learning to capture spatially adaptive noise patterns. Extensive experiments show that CryoCCD generates structurally accurate micrographs and enhances performance in downstream tasks, outperforming state-of-the-art baselines in both particle picking and reconstruction.
>
---
#### [new 120] TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning
- **分类: cs.CV; cs.LG**

- **简介: 该论文提出TimePoint方法，解决时间序列对齐中DTW速度慢、抗噪差的问题。通过自监督学习从合成数据中提取关键点和描述符，结合1D微分同胚生成数据及卷积架构，实现稀疏表示下的快速高精度对齐，泛化至真实数据表现优异。**

- **链接: [http://arxiv.org/pdf/2505.23475v1](http://arxiv.org/pdf/2505.23475v1)**

> **作者:** Ron Shapira Weber; Shahar Ben Ishay; Andrey Lavrinenko; Shahaf E. Finder; Oren Freifeld
>
> **备注:** ICML 2025
>
> **摘要:** Fast and scalable alignment of time series is a fundamental challenge in many domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor scalability and sensitivity to noise. We introduce TimePoint, a self-supervised method that dramatically accelerates DTW-based alignment while typically improving alignment accuracy by learning keypoints and descriptors from synthetic data. Inspired by 2D keypoint detection but carefully adapted to the unique challenges of 1D signals, TimePoint leverages efficient 1D diffeomorphisms, which effectively model nonlinear time warping, to generate realistic training data. This approach, along with fully convolutional and wavelet convolutional architectures, enables the extraction of informative keypoints and descriptors. Applying DTW to these sparse representations yield major speedups and typically higher alignment accuracy than standard DTW applied to the full signals. TimePoint demonstrates strong generalization to real-world time series when trained solely on synthetic data, and further improves with fine-tuning on real data. Extensive experiments demonstrate that TimePoint consistently achieves faster and more accurate alignments than standard DTW, making it a scalable solution for time-series analysis. Our code is available at https://github.com/BGU-CS-VIL/TimePoint
>
---
#### [new 121] HyperPointFormer: Multimodal Fusion in 3D Space with Dual-Branch Cross-Attention Transformers
- **分类: cs.CV**

- **简介: 该论文提出HyperPointFormer方法，针对城市场景土地利用/覆盖分类任务，解决传统2D处理无法充分利用3D点云特征的问题。通过双分支Transformer融合几何与光谱信息，采用3D跨注意力机制多尺度整合多模态特征，实验证明其3D预测效果优于2D方法且更具灵活性。**

- **链接: [http://arxiv.org/pdf/2505.23206v1](http://arxiv.org/pdf/2505.23206v1)**

> **作者:** Aldino Rizaldy; Richard Gloaguen; Fabian Ewald Fassnacht; Pedram Ghamisi
>
> **摘要:** Multimodal remote sensing data, including spectral and lidar or photogrammetry, is crucial for achieving satisfactory land-use / land-cover classification results in urban scenes. So far, most studies have been conducted in a 2D context. When 3D information is available in the dataset, it is typically integrated with the 2D data by rasterizing the 3D data into 2D formats. Although this method yields satisfactory classification results, it falls short in fully exploiting the potential of 3D data by restricting the model's ability to learn 3D spatial features directly from raw point clouds. Additionally, it limits the generation of 3D predictions, as the dimensionality of the input data has been reduced. In this study, we propose a fully 3D-based method that fuses all modalities within the 3D point cloud and employs a dedicated dual-branch Transformer model to simultaneously learn geometric and spectral features. To enhance the fusion process, we introduce a cross-attention-based mechanism that fully operates on 3D points, effectively integrating features from various modalities across multiple scales. The purpose of cross-attention is to allow one modality to assess the importance of another by weighing the relevant features. We evaluated our method by comparing it against both 3D and 2D methods using the 2018 IEEE GRSS Data Fusion Contest (DFC2018) dataset. Our findings indicate that 3D fusion delivers competitive results compared to 2D methods and offers more flexibility by providing 3D predictions. These predictions can be projected onto 2D maps, a capability that is not feasible in reverse. Additionally, we evaluated our method on different datasets, specifically the ISPRS Vaihingen 3D and the IEEE 2019 Data Fusion Contest. Our code will be published here: https://github.com/aldinorizaldy/hyperpointformer.
>
---
#### [new 122] SeG-SR: Integrating Semantic Knowledge into Remote Sensing Image Super-Resolution via Vision-Language Model
- **分类: cs.CV**

- **简介: 该论文属于遥感图像超分辨率任务，旨在解决现有方法忽视语义信息导致重建伪影的问题。提出SeG-SR框架，利用视觉语言模型提取语义知识，通过语义特征提取、定位及可学习调制模块，将高层场景理解融入超分过程，提升重建质量。**

- **链接: [http://arxiv.org/pdf/2505.23010v1](http://arxiv.org/pdf/2505.23010v1)**

> **作者:** Bowen Chen; Keyan Chen; Mohan Yang; Zhengxia Zou; Zhenwei Shi
>
> **摘要:** High-resolution (HR) remote sensing imagery plays a vital role in a wide range of applications, including urban planning and environmental monitoring. However, due to limitations in sensors and data transmission links, the images acquired in practice often suffer from resolution degradation. Remote Sensing Image Super-Resolution (RSISR) aims to reconstruct HR images from low-resolution (LR) inputs, providing a cost-effective and efficient alternative to direct HR image acquisition. Existing RSISR methods primarily focus on low-level characteristics in pixel space, while neglecting the high-level understanding of remote sensing scenes. This may lead to semantically inconsistent artifacts in the reconstructed results. Motivated by this observation, our work aims to explore the role of high-level semantic knowledge in improving RSISR performance. We propose a Semantic-Guided Super-Resolution framework, SeG-SR, which leverages Vision-Language Models (VLMs) to extract semantic knowledge from input images and uses it to guide the super resolution (SR) process. Specifically, we first design a Semantic Feature Extraction Module (SFEM) that utilizes a pretrained VLM to extract semantic knowledge from remote sensing images. Next, we propose a Semantic Localization Module (SLM), which derives a series of semantic guidance from the extracted semantic knowledge. Finally, we develop a Learnable Modulation Module (LMM) that uses semantic guidance to modulate the features extracted by the SR network, effectively incorporating high-level scene understanding into the SR pipeline. We validate the effectiveness and generalizability of SeG-SR through extensive experiments: SeG-SR achieves state-of-the-art performance on two datasets and consistently delivers performance improvements across various SR architectures. Codes can be found at https://github.com/Mr-Bamboo/SeG-SR.
>
---
#### [new 123] Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis
- **分类: cs.CV**

- **简介: 该论文提出"维度缩减攻击(DRA-Ctrl)"方法，探索将视频生成模型应用于可控图像合成任务。针对视频模型与静态图像生成的适配难题，通过混合过渡策略和注意力机制优化，实现视频模型在图像生成中的高效复用。实验表明复用视频模型优于专用图像模型，揭示视频生成器在跨模态任务中的潜力。**

- **链接: [http://arxiv.org/pdf/2505.23325v1](http://arxiv.org/pdf/2505.23325v1)**

> **作者:** Hengyuan Cao; Yutong Feng; Biao Gong; Yijing Tian; Yunhong Lu; Chuang Liu; Bin Wang
>
> **摘要:** Video generative models can be regarded as world simulators due to their ability to capture dynamic, continuous changes inherent in real-world environments. These models integrate high-dimensional information across visual, temporal, spatial, and causal dimensions, enabling predictions of subjects in various status. A natural and valuable research direction is to explore whether a fully trained video generative model in high-dimensional space can effectively support lower-dimensional tasks such as controllable image generation. In this work, we propose a paradigm for video-to-image knowledge compression and task adaptation, termed \textit{Dimension-Reduction Attack} (\texttt{DRA-Ctrl}), which utilizes the strengths of video models, including long-range context modeling and flatten full-attention, to perform various generation tasks. Specially, to address the challenging gap between continuous video frames and discrete image generation, we introduce a mixup-based transition strategy that ensures smooth adaptation. Moreover, we redesign the attention structure with a tailored masking mechanism to better align text prompts with image-level control. Experiments across diverse image generation tasks, such as subject-driven and spatially conditioned generation, show that repurposed video models outperform those trained directly on images. These results highlight the untapped potential of large-scale video generators for broader visual applications. \texttt{DRA-Ctrl} provides new insights into reusing resource-intensive video models and lays foundation for future unified generative models across visual modalities. The project page is https://dra-ctrl-2025.github.io/DRA-Ctrl/.
>
---
#### [new 124] LAFR: Efficient Diffusion-based Blind Face Restoration via Latent Codebook Alignment Adapter
- **分类: cs.CV**

- **简介: 该论文提出LAFR模型，针对盲面部修复任务中扩散模型VAE模块对低质量输入的语义不匹配问题，设计潜代码本对齐适配器优化潜空间分布，结合多级恢复损失增强身份与结构保真度，并通过仅微调0.9%FFHQ数据实现高效训练（节省70%时间），实现高质量、身份保留的修复效果。**

- **链接: [http://arxiv.org/pdf/2505.23462v1](http://arxiv.org/pdf/2505.23462v1)**

> **作者:** Runyi Li; Bin Chen; Jian Zhang; Radu Timofte
>
> **摘要:** Blind face restoration from low-quality (LQ) images is a challenging task that requires not only high-fidelity image reconstruction but also the preservation of facial identity. While diffusion models like Stable Diffusion have shown promise in generating high-quality (HQ) images, their VAE modules are typically trained only on HQ data, resulting in semantic misalignment when encoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ conditions during the denoising process. Existing approaches often tackle this issue by retraining the VAE encoder, which is computationally expensive and memory-intensive. To address this limitation efficiently, we propose LAFR (Latent Alignment for Face Restoration), a novel codebook-based latent space adapter that aligns the latent distribution of LQ images with that of HQ counterparts, enabling semantically consistent diffusion sampling without altering the original VAE. To further enhance identity preservation, we introduce a multi-level restoration loss that combines constraints from identity embeddings and facial structural priors. Additionally, by leveraging the inherent structural regularity of facial images, we show that lightweight finetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to achieve results comparable to state-of-the-art methods, reduce training time by 70%. Extensive experiments on both synthetic and real-world face restoration benchmarks demonstrate the effectiveness and efficiency of LAFR, achieving high-quality, identity-preserving face reconstruction from severely degraded inputs.
>
---
#### [new 125] Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought
- **分类: cs.CV**

- **简介: 该论文提出Argus模型，针对多模态大模型在视觉核心任务中视觉注意力不足的问题，通过对象中心的视觉接地机制构建视觉推理链，提升多模态推理和目标定位任务表现，在多项基准测试中验证了有效性。（99字）**

- **链接: [http://arxiv.org/pdf/2505.23766v1](http://arxiv.org/pdf/2505.23766v1)**

> **作者:** Yunze Man; De-An Huang; Guilin Liu; Shiwei Sheng; Shilong Liu; Liang-Yan Gui; Jan Kautz; Yu-Xiong Wang; Zhiding Yu
>
> **备注:** CVPR 2025. Project Page: https://yunzeman.github.io/argus/
>
> **摘要:** Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language tasks, yet they often struggle with vision-centric scenarios where precise visual focus is needed for accurate reasoning. In this paper, we introduce Argus to address these limitations with a new visual attention grounding mechanism. Our approach employs object-centric grounding as visual chain-of-thought signals, enabling more effective goal-conditioned visual attention during multimodal reasoning tasks. Evaluations on diverse benchmarks demonstrate that Argus excels in both multimodal reasoning tasks and referring object grounding tasks. Extensive analysis further validates various design choices of Argus, and reveals the effectiveness of explicit language-guided visual region-of-interest engagement in MLLMs, highlighting the importance of advancing multimodal intelligence from a visual-centric perspective. Project page: https://yunzeman.github.io/argus/
>
---
#### [new 126] Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation
- **分类: cs.CV**

- **简介: 该论文属于单目深度估计任务，旨在提升复杂场景（如复杂结构、重叠物体）下的深度预测精度。针对现有方法泛化能力不足的问题，提出BriGeS方法，通过融合几何与语义基础模型，设计Bridging Gate整合深度与分割模型优势，并采用注意力温度缩放技术优化特征关注平衡，仅微调轻量模块以降低资源消耗，实现在多数据集上的性能提升。**

- **链接: [http://arxiv.org/pdf/2505.23400v1](http://arxiv.org/pdf/2505.23400v1)**

> **作者:** Sanggyun Ma; Wonjoon Choi; Jihun Park; Jaeyeul Kim; Seunghun Lee; Jiwan Seo; Sunghoon Im
>
> **摘要:** We present Bridging Geometric and Semantic (BriGeS), an effective method that fuses geometric and semantic information within foundation models to enhance Monocular Depth Estimation (MDE). Central to BriGeS is the Bridging Gate, which integrates the complementary strengths of depth and segmentation foundation models. This integration is further refined by our Attention Temperature Scaling technique. It finely adjusts the focus of the attention mechanisms to prevent over-concentration on specific features, thus ensuring balanced performance across diverse inputs. BriGeS capitalizes on pre-trained foundation models and adopts a strategy that focuses on training only the Bridging Gate. This method significantly reduces resource demands and training time while maintaining the model's ability to generalize effectively. Extensive experiments across multiple challenging datasets demonstrate that BriGeS outperforms state-of-the-art methods in MDE for complex scenes, effectively handling intricate structures and overlapping objects.
>
---
#### [new 127] MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation
- **分类: cs.CV**

- **简介: 该论文属于语音同步手势视频生成任务，解决仅用音频驱动时大动作捕捉不足及细节失真的问题。提出MMGT网络：第一阶段通过SMGA网络由音频生成姿态视频和运动掩码以捕捉关键区域大动作；第二阶段结合MM-HAA注意力机制与稳定扩散模型，提升局部细节控制和纹理精度，实现高质量上半身视频生成。**

- **链接: [http://arxiv.org/pdf/2505.23120v1](http://arxiv.org/pdf/2505.23120v1)**

> **作者:** Siyuan Wang; Jiawei Liu; Wei Wang; Yeying Jin; Jinsong Du; Zhi Han
>
> **摘要:** Co-Speech Gesture Video Generation aims to generate vivid speech videos from audio-driven still images, which is challenging due to the diversity of different parts of the body in terms of amplitude of motion, audio relevance, and detailed features. Relying solely on audio as the control signal often fails to capture large gesture movements in video, leading to more pronounced artifacts and distortions. Existing approaches typically address this issue by introducing additional a priori information, but this can limit the practical application of the task. Specifically, we propose a Motion Mask-Guided Two-Stage Network (MMGT) that uses audio, as well as motion masks and motion features generated from the audio signal to jointly drive the generation of synchronized speech gesture videos. In the first stage, the Spatial Mask-Guided Audio Pose Generation (SMGA) Network generates high-quality pose videos and motion masks from audio, effectively capturing large movements in key regions such as the face and gestures. In the second stage, we integrate the Motion Masked Hierarchical Audio Attention (MM-HAA) into the Stabilized Diffusion Video Generation model, overcoming limitations in fine-grained motion generation and region-specific detail control found in traditional methods. This guarantees high-quality, detailed upper-body video generation with accurate texture and motion details. Evaluations show improved video quality, lip-sync, and gesture. The model and code are available at https://github.com/SIA-IDE/MMGT.
>
---
#### [new 128] TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models
- **分类: cs.CV**

- **简介: 该论文提出TextRegion框架，解决图文模型细节视觉理解不足的问题。通过结合冻结的图文模型与SAM2分割模型，生成文本对齐的区域标记，无需训练即可支持开放词汇分割、指代理解等任务，性能优于现有方法。**

- **链接: [http://arxiv.org/pdf/2505.23769v1](http://arxiv.org/pdf/2505.23769v1)**

> **作者:** Yao Xiao; Qiqian Fu; Heyi Tao; Yuqun Wu; Zhen Zhu; Derek Hoiem
>
> **备注:** Code is available at: https://github.com/avaxiao/TextRegion
>
> **摘要:** Image-text models excel at image-level tasks but struggle with detailed visual understanding. While these models provide strong visual-language alignment, segmentation models like SAM2 offer precise spatial boundaries for objects. To this end, we propose TextRegion, a simple, effective, and training-free framework that combines the strengths of image-text models and SAM2 to generate powerful text-aligned region tokens. These tokens enable detailed visual understanding while preserving open-vocabulary capabilities. They can be directly applied to various downstream tasks, including open-world semantic segmentation, referring expression comprehension, and grounding. We conduct extensive evaluations and consistently achieve superior or competitive performance compared to state-of-the-art training-free methods. Additionally, our framework is compatible with many image-text models, making it highly practical and easily extensible as stronger models emerge. Code is available at: https://github.com/avaxiao/TextRegion.
>
---
#### [new 129] OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data
- **分类: cs.CV; cs.LG**

- **简介: 该论文提出OmniEarth-Bench，解决现有地球科学多模态基准覆盖不全、任务维度少的问题。通过整合六大地球科学领域及跨领域交互，构建含29,779个标注的多层级（感知、推理等）评估体系，实验显示当前模型表现不足，推动地球系统AI发展。**

- **链接: [http://arxiv.org/pdf/2505.23522v1](http://arxiv.org/pdf/2505.23522v1)**

> **作者:** Fengxiang Wang; Mingshuo Chen; Xuming He; YiFan Zhang; Feng Liu; Zijie Guo; Zhenghao Hu; Jiong Wang; Jingyi Xu; Zhangrui Li; Fenghua Ling; Ben Fei; Weijia Li; Long Lan; Wenjing Yang; Wenlong Zhang; Lei Bai
>
> **摘要:** Existing benchmarks for Earth science multimodal learning exhibit critical limitations in systematic coverage of geosystem components and cross-sphere interactions, often constrained to isolated subsystems (only in Human-activities sphere or atmosphere) with limited evaluation dimensions (less than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first comprehensive multimodal benchmark spanning all six Earth science spheres (atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and Human-activities sphere) and cross-spheres with one hundred expert-curated evaluation dimensions. Leveraging observational data from satellite sensors and in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four tiers: perception, general reasoning, scientific knowledge reasoning and chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per sphere to establish authoritative evaluation dimensions and curate relevant observational datasets, 40 crowd-sourcing annotators to assist experts for annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs reveal that even the most advanced models struggle with our benchmarks, where none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets a new standard for geosystem-aware AI, advancing both scientific discovery and practical applications in environmental monitoring and disaster prediction. The dataset, source code, and trained models were released.
>
---
#### [new 130] Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视频外观编辑任务，旨在解决现有文本引导方法用户意图模糊及细节控制不足的问题。提出Zero-to-Hero方法，分两阶段：先以锚帧生成参考图像，再通过帧间对应关系引导注意力机制传播外观，提升大运动物体的时空一致性；后修复成像退化问题。构建新数据集验证，PSNR提升2.6dB。**

- **链接: [http://arxiv.org/pdf/2505.23134v1](http://arxiv.org/pdf/2505.23134v1)**

> **作者:** Tongtong Su; Chengyu Wang; Jun Huang; Dongming Lu
>
> **摘要:** Appearance editing according to user needs is a pivotal task in video editing. Existing text-guided methods often lead to ambiguities regarding user intentions and restrict fine-grained control over editing specific aspects of objects. To overcome these limitations, this paper introduces a novel approach named {Zero-to-Hero}, which focuses on reference-based video editing that disentangles the editing process into two distinct problems. It achieves this by first editing an anchor frame to satisfy user requirements as a reference image and then consistently propagating its appearance across other frames. We leverage correspondence within the original frames to guide the attention mechanism, which is more robust than previously proposed optical flow or temporal modules in memory-friendly video generative models, especially when dealing with objects exhibiting large motions. It offers a solid ZERO-shot initialization that ensures both accuracy and temporal consistency. However, intervention in the attention mechanism results in compounded imaging degradation with over-saturated colors and unknown blurring issues. Starting from Zero-Stage, our Hero-Stage Holistically learns a conditional generative model for vidEo RestOration. To accurately evaluate the consistency of the appearance, we construct a set of videos with multiple appearances using Blender, enabling a fine-grained and deterministic evaluation. Our method outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The project page is at https://github.com/Tonniia/Zero2Hero.
>
---
#### [new 131] MOVi: Training-free Text-conditioned Multi-Object Video Generation
- **分类: cs.CV**

- **简介: 该论文属于文本驱动的多物体视频生成任务，旨在解决现有扩散模型生成多物体视频时物体交互复杂、运动受限及特征混淆问题。提出无训练方法MOVi，利用大语言模型规划物体轨迹并通过噪声重初始化控制运动，优化注意力机制以增强物体特征分离，提升生成精度与动态效果，实现42%的性能提升。**

- **链接: [http://arxiv.org/pdf/2505.22980v1](http://arxiv.org/pdf/2505.22980v1)**

> **作者:** Aimon Rahman; Jiang Liu; Ze Wang; Ximeng Sun; Jialian Wu; Xiaodong Yu; Yusheng Su; Vishal M. Patel; Zicheng Liu; Emad Barsoum
>
> **摘要:** Recent advances in diffusion-based text-to-video (T2V) models have demonstrated remarkable progress, but these models still face challenges in generating videos with multiple objects. Most models struggle with accurately capturing complex object interactions, often treating some objects as static background elements and limiting their movement. In addition, they often fail to generate multiple distinct objects as specified in the prompt, resulting in incorrect generations or mixed features across objects. In this paper, we present a novel training-free approach for multi-object video generation that leverages the open world knowledge of diffusion models and large language models (LLMs). We use an LLM as the ``director'' of object trajectories, and apply the trajectories through noise re-initialization to achieve precise control of realistic movements. We further refine the generation process by manipulating the attention mechanism to better capture object-specific features and motion patterns, and prevent cross-object feature interference. Extensive experiments validate the effectiveness of our training free approach in significantly enhancing the multi-object generation capabilities of existing video diffusion models, resulting in 42% absolute improvement in motion dynamics and object generation accuracy, while also maintaining high fidelity and motion smoothness.
>
---
#### [new 132] Navigating the Accuracy-Size Trade-Off with Flexible Model Merging
- **分类: cs.CV**

- **简介: 该论文属于模型合并与优化任务，旨在解决多任务模型合并后的准确率下降与部署成本间的权衡问题。提出FlexMerge框架，通过逐步合并模型块并灵活控制输出模型大小，在保持高性能的同时平衡准确率与成本，实验显示其有效。**

- **链接: [http://arxiv.org/pdf/2505.23209v1](http://arxiv.org/pdf/2505.23209v1)**

> **作者:** Akash Dhasade; Divyansh Jhunjhunwala; Milos Vujasinovic; Gauri Joshi; Anne-Marie Kermarrec
>
> **摘要:** Model merging has emerged as an efficient method to combine multiple single-task fine-tuned models. The merged model can enjoy multi-task capabilities without expensive training. While promising, merging into a single model often suffers from an accuracy gap with respect to individual fine-tuned models. On the other hand, deploying all individual fine-tuned models incurs high costs. We propose FlexMerge, a novel data-free model merging framework to flexibly generate merged models of varying sizes, spanning the spectrum from a single merged model to retaining all individual fine-tuned models. FlexMerge treats fine-tuned models as collections of sequential blocks and progressively merges them using any existing data-free merging method, halting at a desired size. We systematically explore the accuracy-size trade-off exhibited by different merging algorithms in combination with FlexMerge. Extensive experiments on vision and NLP benchmarks, with up to 30 tasks, reveal that even modestly larger merged models can provide substantial accuracy improvements over a single model. By offering fine-grained control over fused model size, FlexMerge provides a flexible, data-free, and high-performance solution for diverse deployment scenarios.
>
---
#### [new 133] Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation
- **分类: cs.CV**

- **简介: 该论文属于动态肖像动画生成任务，解决音频与骨骼驱动下的高保真唇同步、自然表情及身体运动同步难题。提出直接偏好优化框架，通过人类偏好数据集对齐生成结果，并创新时间运动调节技术，解决时空分辨率不匹配以保留高频运动细节，提升动画质量。**

- **链接: [http://arxiv.org/pdf/2505.23525v1](http://arxiv.org/pdf/2505.23525v1)**

> **作者:** Jiahao Cui; Yan Chen; Mingwang Xu; Hanlin Shang; Yuxuan Chen; Yun Zhan; Zilong Dong; Yao Yao; Jingdong Wang; Siyu Zhu
>
> **摘要:** Generating highly dynamic and photorealistic portrait animations driven by audio and skeletal motion remains challenging due to the need for precise lip synchronization, natural facial expressions, and high-fidelity body motion dynamics. We propose a human-preference-aligned diffusion framework that addresses these challenges through two key innovations. First, we introduce direct preference optimization tailored for human-centric animation, leveraging a curated dataset of human preferences to align generated outputs with perceptual metrics for portrait motion-video alignment and naturalness of expression. Second, the proposed temporal motion modulation resolves spatiotemporal resolution mismatches by reshaping motion conditions into dimensionally aligned latent features through temporal channel redistribution and proportional feature expansion, preserving the fidelity of high-frequency motion details in diffusion-based synthesis. The proposed mechanism is complementary to existing UNet and DiT-based portrait diffusion approaches, and experiments demonstrate obvious improvements in lip-audio synchronization, expression vividness, body motion coherence over baseline methods, alongside notable gains in human preference metrics. Our model and source code can be found at: https://github.com/xyz123xyz456/hallo4.
>
---
#### [new 134] PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views
- **分类: cs.CV; I.2.10; I.4.8; I.5.1**

- **简介: 该论文提出PhysicsNeRF，用于稀疏视角下的物理驱动3D重建。针对传统NeRF在稀疏数据中表现差的问题，通过结合深度排序、RegNeRF一致性、稀疏先验及跨视角对齐四个物理约束扩展NeRF，采用0.67M参数模型，仅用8视图达21.4dB PSNR，分析泛化差距并探讨约束模型的表达与泛化权衡。**

- **链接: [http://arxiv.org/pdf/2505.23481v1](http://arxiv.org/pdf/2505.23481v1)**

> **作者:** Mohamed Rayan Barhdadi; Hasan Kurban; Hussein Alnuweiri
>
> **备注:** 4 pages, 2 figures, 2 tables. Preliminary work. Under review by the Building Physically Plausible World Models Workshop at the 42nd International Conference on Machine Learning (ICML 2025), Vancouver, Canada
>
> **摘要:** PhysicsNeRF is a physically grounded framework for 3D reconstruction from sparse views, extending Neural Radiance Fields with four complementary constraints: depth ranking, RegNeRF-style consistency, sparsity priors, and cross-view alignment. While standard NeRFs fail under sparse supervision, PhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB average PSNR using only 8 views, outperforming prior methods. A generalization gap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental limitations of sparse-view reconstruction. PhysicsNeRF enables physically consistent, generalizable 3D representations for agent interaction and simulation, and clarifies the expressiveness-generalization trade-off in constrained NeRF models.
>
---
#### [new 135] PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于PAN锐化任务，解决传感器差异等导致的跨模态配准偏差问题，提出PAN-Crafter框架。通过模态自适应重建（MARs）和跨模态对齐注意（CM3A）机制，实现PAN与MS图像的结构-纹理双向对齐，提升超分辨率融合效果，优于现有方法且更高效。**

- **链接: [http://arxiv.org/pdf/2505.23367v1](http://arxiv.org/pdf/2505.23367v1)**

> **作者:** Jeonghyeok Do; Sungpyo Kim; Geunhyuk Youk; Jaehyup Lee; Munchurl Kim
>
> **备注:** Please visit our project page https://kaist-viclab.github.io/PAN-Crafter_site
>
> **摘要:** PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with low-resolution multi-spectral (MS) images to generate high-resolution multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused by sensor placement, acquisition timing, and resolution disparity -- induces a fundamental challenge. Conventional deep learning methods assume perfect pixel-wise alignment and rely on per-pixel reconstruction losses, leading to spectral distortion, double edges, and blurring when misalignment is present. To address this, we propose PAN-Crafter, a modality-consistent alignment framework that explicitly mitigates the misalignment gap between PAN and MS modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a single network to jointly reconstruct HRMS and PAN images, leveraging PAN's high-frequency details as auxiliary self-supervision. Additionally, we introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism that bidirectionally aligns MS texture to PAN structure and vice versa, enabling adaptive feature refinement across modalities. Extensive experiments on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the most recent state-of-the-art method in all metrics, even with 50.11$\times$ faster inference time and 0.63$\times$ the memory size. Furthermore, it demonstrates strong generalization performance on unseen satellite datasets, showing its robustness across different conditions.
>
---
#### [new 136] LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers
- **分类: cs.CV**

- **简介: 该论文提出LoRAShop框架，属于无训练需求的多概念图像生成与编辑任务。旨在解决现有方法需重新训练或无法自然融合多元素且破坏全局细节的问题。通过分析扩散模型中特征激活模式，提取概念特异性潜隐掩码，局部调整LoRA权重实现多主体/风格无缝融合，保留场景光照与细节。**

- **链接: [http://arxiv.org/pdf/2505.23758v1](http://arxiv.org/pdf/2505.23758v1)**

> **作者:** Yusuf Dalva; Hidir Yesiltepe; Pinar Yanardag
>
> **备注:** Project Webpage: https://lorashop.github.io/
>
> **摘要:** We introduce LoRAShop, the first framework for multi-concept image editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration.
>
---
#### [new 137] Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation
- **分类: cs.CV**

- **简介: 该论文属于树冠语义分割任务，旨在解决遥感数据中阴影、复杂背景及尺度差异导致的分割难题。提出PerceptiveNet，融合可训练Log-Gabor卷积层与宽感受野骨干网络，通过对比实验、消融研究及混合CNN-Transformer模型验证，显著提升分割精度并实现跨领域泛化。**

- **链接: [http://arxiv.org/pdf/2505.23597v1](http://arxiv.org/pdf/2505.23597v1)**

> **作者:** Georgios Voulgaris
>
> **备注:** Accepted for publication at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) EarthVision
>
> **摘要:** The accurate semantic segmentation of tree crowns within remotely sensed data is crucial for scientific endeavours such as forest management, biodiversity studies, and carbon sequestration quantification. However, precise segmentation remains challenging due to complexities in the forest canopy, including shadows, intricate backgrounds, scale variations, and subtle spectral differences among tree species. Compared to the traditional methods, Deep Learning models improve accuracy by extracting informative and discriminative features, but often fall short in capturing the aforementioned complexities. To address these challenges, we propose PerceptiveNet, a novel model incorporating a Logarithmic Gabor-parameterised convolutional layer with trainable filter parameters, alongside a backbone that extracts salient features while capturing extensive context and spatial information through a wider receptive field. We investigate the impact of Log-Gabor, Gabor, and standard convolutional layers on semantic segmentation performance through extensive experimentation. Additionally, we conduct an ablation study to assess the contributions of individual layers and their combinations to overall model performance, and we evaluate PerceptiveNet as a backbone within a novel hybrid CNN-Transformer model. Our results outperform state-of-the-art models, demonstrating significant performance improvements on a tree crown dataset while generalising across domains, including two benchmark aerial scene semantic segmentation datasets with varying complexities.
>
---
#### [new 138] Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning
- **分类: cs.CV**

- **简介: 该论文属于医学影像分析任务，旨在解决现有AI模型在胸片解读中缺乏临床推理流程的问题。研究构建了模拟放射科医生8阶段诊断流程的多阶段数据集CXRTrek，并提出CXRTrekNet模型，通过整合临床推理步骤提升诊断准确性和泛化能力。**

- **链接: [http://arxiv.org/pdf/2505.23143v1](http://arxiv.org/pdf/2505.23143v1)**

> **作者:** Jinquan Guan; Qi Chen; Lizhou Liang; Yuhang Liu; Vu Minh Hieu Phan; Minh-Son To; Jian Chen; Yutong Xie
>
> **备注:** 10 pages (main text), 18 pages (appendix)
>
> **摘要:** Artificial intelligence (AI)-based chest X-ray (CXR) interpretation assistants have demonstrated significant progress and are increasingly being applied in clinical settings. However, contemporary medical AI models often adhere to a simplistic input-to-output paradigm, directly processing an image and an instruction to generate a result, where the instructions may be integral to the model's architecture. This approach overlooks the modeling of the inherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is typically sequential, where each interpretive stage considers the images, the current task, and the contextual information from previous stages. This oversight leads to several shortcomings, including misalignment with clinical scenarios, contextless reasoning, and untraceable errors. To fill this gap, we construct CXRTrek, a new multi-stage visual question answering (VQA) dataset for CXR interpretation. The dataset is designed to explicitly simulate the diagnostic reasoning process employed by radiologists in real-world clinical settings for the first time. CXRTrek covers 8 sequential diagnostic stages, comprising 428,966 samples and over 11 million question-answer (Q&A) pairs, with an average of 26.29 Q&A pairs per sample. Building on the CXRTrek dataset, we propose a new vision-language large model (VLLM), CXRTrekNet, specifically designed to incorporate the clinical reasoning flow into the VLLM framework. CXRTrekNet effectively models the dependencies between diagnostic stages and captures reasoning patterns within the radiological context. Trained on our dataset, the model consistently outperforms existing medical VLLMs on the CXRTrek benchmarks and demonstrates superior generalization across multiple tasks on five diverse external datasets. The dataset and model can be found in our repository (https://github.com/guanjinquan/CXRTrek).
>
---
#### [new 139] Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于领域增量学习任务，旨在解决参数隔离增量学习（PIDIL）中参数选择不准确的问题，尤其在领域和类别增多时。提出SOYO框架，通过高斯混合压缩器、领域特征重采样器和多级特征融合网络优化领域选择与特征提取，并支持多种参数高效微调方法。实验显示其在多任务中优于现有方法。**

- **链接: [http://arxiv.org/pdf/2505.23744v1](http://arxiv.org/pdf/2505.23744v1)**

> **作者:** Qiang Wang; Xiang Song; Yuhang He; Jizhou Han; Chenhao Ding; Xinyuan Gao; Yihong Gong
>
> **备注:** Accepted at CVPR 2025
>
> **摘要:** Deep neural networks (DNNs) often underperform in real-world, dynamic settings where data distributions change over time. Domain Incremental Learning (DIL) offers a solution by enabling continual model adaptation, with Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce knowledge conflicts. However, existing PIDIL methods struggle with parameter selection accuracy, especially as the number of domains and corresponding classes grows. To address this, we propose SOYO, a lightweight framework that improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior domain data efficiently, while a Multi-level Domain Feature Fusion Network (MDFN) enhances domain feature extraction. Our framework supports multiple Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks such as image classification, object detection, and speech enhancement. Experimental results on six benchmarks demonstrate SOYO's consistent superiority over existing baselines, showcasing its robustness and adaptability in complex, evolving environments. The codes will be released in https://github.com/qwangcv/SOYO.
>
---
#### [new 140] CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement via Simple Image Processing
- **分类: cs.CV**

- **简介: 该论文提出CURVE方法，针对零参考低光图像增强任务，解决生成优质图像与高分辨率下计算效率问题。通过贝塞尔曲线调整色调，结合CLIP驱动的强化学习优化参数，在实验中展现高质量与快速处理优势。**

- **链接: [http://arxiv.org/pdf/2505.23102v1](http://arxiv.org/pdf/2505.23102v1)**

> **作者:** Yuka Ogino; Takahiro Toizumi; Atsushi Ito
>
> **备注:** Accepted to ICIP2025
>
> **摘要:** Low-Light Image Enhancement (LLIE) is crucial for improving both human perception and computer vision tasks. This paper addresses two challenges in zero-reference LLIE: obtaining perceptually 'good' images using the Contrastive Language-Image Pre-Training (CLIP) model and maintaining computational efficiency for high-resolution images. We propose CLIP-Utilized Reinforcement learning-based Visual image Enhancement (CURVE). CURVE employs a simple image processing module which adjusts global image tone based on B\'ezier curve and estimates its processing parameters iteratively. The estimator is trained by reinforcement learning with rewards designed using CLIP text embeddings. Experiments on low-light and multi-exposure datasets demonstrate the performance of CURVE in terms of enhancement quality and processing speed compared to conventional methods.
>
---
#### [new 141] AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views
- **分类: cs.CV**

- **简介: 该论文提出AnySplat，一种前向神经网络，用于无相机参数约束的新型视图合成。针对传统方法依赖相机位姿或计算效率低的问题，其通过单次推理直接预测3D高斯体素（几何与外观）、相机参数，实现无标注多视角数据的实时渲染，在稀疏/密集场景中性能超越现有方法。任务为新型视图合成，核心是无监督、高效处理无约束图像集合。**

- **链接: [http://arxiv.org/pdf/2505.23716v1](http://arxiv.org/pdf/2505.23716v1)**

> **作者:** Lihan Jiang; Yucheng Mao; Linning Xu; Tao Lu; Kerui Ren; Yichen Jin; Xudong Xu; Mulin Yu; Jiangmiao Pang; Feng Zhao; Dahua Lin; Bo Dai
>
> **备注:** Project page: https://city-super.github.io/anysplat/
>
> **摘要:** We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/
>
---
#### [new 142] DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers
- **分类: cs.CV**

- **简介: 该论文属于Vision Transformer（ViT）的参数高效微调任务。针对现有视觉提示调优（VPT）未探索提示与图像token间分布关联的问题，提出DA-VPT框架，利用度量学习从语义数据中学习距离度量，引导提示分布，使其成为图像块与类别token间的语义桥梁，提升下游视觉任务性能。**

- **链接: [http://arxiv.org/pdf/2505.23694v1](http://arxiv.org/pdf/2505.23694v1)**

> **作者:** Li Ren; Chen Chen; Liqiang Wang; Kien Hua
>
> **备注:** CVPR 2025
>
> **摘要:** Visual Prompt Tuning (VPT) has become a promising solution for Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT) models by partially fine-tuning learnable tokens while keeping most model parameters frozen. Recent research has explored modifying the connection structures of the prompts. However, the fundamental correlation and distribution between the prompts and image tokens remain unexplored. In this paper, we leverage metric learning techniques to investigate how the distribution of prompts affects fine-tuning performance. Specifically, we propose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to guide the distributions of the prompts by learning the distance metric from their class-related semantic data. Our method demonstrates that the prompts can serve as an effective bridge to share semantic information between image patches and the class token. We extensively evaluated our approach on popular benchmarks in both recognition and segmentation tasks. The results demonstrate that our approach enables more effective and efficient fine-tuning of ViT models by leveraging semantic information to guide the learning of the prompts, leading to improved performance on various downstream vision tasks.
>
---
#### [new 143] Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss
- **分类: cs.CV**

- **简介: 该论文属于模型校准任务，旨在解决加权风险函数（如焦损、逆焦损）与校准误差的理论关联问题。通过建立加权风险与校准误差的理论联系，提出正则化AURC优化方法，采用SoftRank实现可微AURC，并验证其校准性能优势。**

- **链接: [http://arxiv.org/pdf/2505.23463v1](http://arxiv.org/pdf/2505.23463v1)**

> **作者:** Han Zhou; Sebastian G. Gruber; Teodora Popordanoska; Matthew B. Blaschko
>
> **摘要:** Several variants of reweighted risk functionals, such as focal losss, inverse focal loss, and the Area Under the Risk-Coverage Curve (AURC), have been proposed in the literature and claims have been made in relation to their calibration properties. However, focal loss and inverse focal loss propose vastly different weighting schemes. In this paper, we revisit a broad class of weighted risk functions commonly used in deep learning and establish a principled connection between these reweighting schemes and calibration errors. We show that minimizing calibration error is closely linked to the selective classification paradigm and demonstrate that optimizing a regularized variant of the AURC naturally leads to improved calibration. This regularized AURC shares a similar reweighting strategy with inverse focal loss, lending support to the idea that focal loss is less principled when calibration is a desired outcome. Direct AURC optimization offers greater flexibility through the choice of confidence score functions (CSFs). To enable gradient-based optimization, we introduce a differentiable formulation of the regularized AURC using the SoftRank technique. Empirical evaluations demonstrate that our AURC-based loss achieves competitive class-wise calibration performance across a range of datasets and model architectures.
>
---
#### [new 144] Are MLMs Trapped in the Visual Room?
- **分类: cs.CV**

- **简介: 该论文通过"视觉房间"论证质疑多模态大模型（MLMs）是否真正理解图像。任务为评估MLMs的感知与认知能力差距，针对"感知等于理解"的假设提出挑战。研究构建含924图/100视频的多模态反讽数据集，设计两阶段框架（表面细节检测与反讽推理），实验显示MLMs感知表现良好但理解错误率达16.1%，揭示情感推理等认知缺陷，验证视觉房间假设并提出新评估范式。**

- **链接: [http://arxiv.org/pdf/2505.23272v1](http://arxiv.org/pdf/2505.23272v1)**

> **作者:** Yazhou Zhang; Chunwang Zou; Qimeng Liu; Lu Rong; Ben Yao; Zheng Lian; Qiuchi Li; Peng Zhang; Jing Qin
>
> **摘要:** Can multi-modal large models (MLMs) that can ``see'' an image be said to ``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose the \textbf{Visual Room} argument: a system may process and describe every detail of visual inputs by following algorithmic rules, without genuinely comprehending the underlying intention. This dilemma challenges the prevailing assumption that perceptual mastery implies genuine understanding. In implementation, we introduce a two-tier evaluation framework spanning perception and cognition. The perception component evaluates whether MLMs can accurately capture the surface-level details of visual contents, where the cognitive component examines their ability to infer sarcasm polarity. To support this framework, We further introduce a high-quality multi-modal sarcasm dataset comprising both 924 static images and 100 dynamic videos. All sarcasm labels are annotated by the original authors and verified by independent reviewers to ensure clarity and consistency. We evaluate eight state-of-the-art (SoTA) MLMs. Our results highlight three key findings: (1) MLMs perform well on perception tasks; (2) even with correct perception, models exhibit an average error rate of ~16.1\% in sarcasm understanding, revealing a significant gap between seeing and understanding; (3) error analysis attributes this gap to deficiencies in emotional reasoning, commonsense inference, and context alignment. This work provides empirical grounding for the proposed Visual Room argument and offers a new evaluation paradigm for MLMs.
>
---
#### [new 145] PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation
- **分类: cs.CV; cs.LG**

- **简介: 该论文提出使用PCA预处理提升乳腺超声肿瘤分割模型的跨数据集泛化能力。针对医学超声数据小、多样导致模型跨数据集性能下降的问题，通过保留90%方差的PCA降噪重建数据集。在六个数据集训练U-Net模型，结果显示PCA方法显著提高了召回率（+0.13）和Dice系数（+0.08），降低跨数据集性能下降33%，尤其改善低基线案例，增强医学应用外部有效性。**

- **链接: [http://arxiv.org/pdf/2505.23587v1](http://arxiv.org/pdf/2505.23587v1)**

> **作者:** Christian Schmidt; Heinrich Martin Overhoff
>
> **摘要:** In medical image segmentation, limited external validity remains a critical obstacle when models are deployed across unseen datasets, an issue particularly pronounced in the ultrasound image domain. Existing solutions-such as domain adaptation and GAN-based style transfer-while promising, often fall short in the medical domain where datasets are typically small and diverse. This paper presents a novel application of principal component analysis (PCA) to address this limitation. PCA preprocessing reduces noise and emphasizes essential features by retaining approximately 90\% of the dataset variance. We evaluate our approach across six diverse breast tumor ultrasound datasets comprising 3,983 B-mode images and corresponding expert tumor segmentation masks. For each dataset, a corresponding dimensionality reduced PCA-dataset is created and U-Net-based segmentation models are trained on each of the twelve datasets. Each model trained on an original dataset was inferenced on the remaining five out-of-domain original datasets (baseline results), while each model trained on a PCA dataset was inferenced on five out-of-domain PCA datasets. Our experimental results indicate that using PCA reconstructed datasets, instead of original images, improves the model's recall and Dice scores, particularly for model-dataset pairs where baseline performance was lowest, achieving statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05, $p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p = 0.03$). Our method reduced the decline in recall values due to external validation by $33\%$. These findings underscore the potential of PCA reconstruction as a safeguard to mitigate declines in segmentation performance, especially in challenging cases, with implications for enhancing external validity in real-world medical applications.
>
---
#### [new 146] IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文针对不规则多变量时间序列预测任务，提出VIMTS框架，通过时间线分割特征块、跨通道依赖补全和视觉MAE重建技术，解决缺失数据与多通道信号不齐问题，提升预测精度与少样本学习能力。**

- **链接: [http://arxiv.org/pdf/2505.22815v1](http://arxiv.org/pdf/2505.22815v1)**

> **作者:** Zhangyi Hu; Jiemin Wu; Hua Xu; Mingqian Liao; Ninghui Feng; Bo Gao; Songning Lai; Yutao Yue
>
> **备注:** ICML 2025
>
> **摘要:** Irregular Multivariate Time Series (IMTS) forecasting is challenging due to the unaligned nature of multi-channel signals and the prevalence of extensive missing data. Existing methods struggle to capture reliable temporal patterns from such data due to significant missing values. While pre-trained foundation models show potential for addressing these challenges, they are typically designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel information and its success in RTS forecasting, we propose VIMTS, a framework adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing values, VIMTS first processes IMTS along the timeline into feature patches at equal intervals. These patches are then complemented using learned cross-channel dependencies. Then it leverages visual MAE's capability in handling sparse multichannel data for patch reconstruction, followed by a coarse-to-fine technique to generate precise predictions from focused contexts. In addition, we integrate self-supervised learning for improved IMTS modeling by adapting the visual MAE to IMTS data. Extensive experiments demonstrate VIMTS's superior performance and few-shot capability, advancing the application of visual foundation models in more general time series tasks. Our code is available at https://github.com/WHU-HZY/VIMTS.
>
---
#### [new 147] LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization
- **分类: cs.CV; cs.GR**

- **简介: 该论文提出LayerPeeler，一种分层图像矢量化方法，解决现有工具在处理遮挡区域时导致形状不完整或碎片化的问题。通过自回归分层剥离策略，利用视觉语言模型构建遮挡关系图，指导扩散模型逐层移除顶层非遮挡元素并恢复底层内容，结合局部注意力控制确保精度，同时贡献新数据集，提升矢量图形的路径完整性和结构连贯性。**

- **链接: [http://arxiv.org/pdf/2505.23740v1](http://arxiv.org/pdf/2505.23740v1)**

> **作者:** Ronghuan Wu; Wanchao Su; Jing Liao
>
> **备注:** Project Page: https://layerpeeler.github.io/
>
> **摘要:** Image vectorization is a powerful technique that converts raster images into vector graphics, enabling enhanced flexibility and interactivity. However, popular image vectorization tools struggle with occluded regions, producing incomplete or fragmented shapes that hinder editability. While recent advancements have explored rule-based and data-driven layer-wise image vectorization, these methods face limitations in vectorization quality and flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image vectorization approach that addresses these challenges through a progressive simplification paradigm. The key to LayerPeeler's success lies in its autoregressive peeling strategy: by identifying and removing the topmost non-occluded layers while recovering underlying content, we generate vector graphics with complete paths and coherent layer structures. Our method leverages vision-language models to construct a layer graph that captures occlusion relationships among elements, enabling precise detection and description for non-occluded layers. These descriptive captions are used as editing instructions for a finetuned image diffusion model to remove the identified layers. To ensure accurate removal, we employ localized attention control that precisely guides the model to target regions while faithfully preserving the surrounding content. To support this, we contribute a large-scale dataset specifically designed for layer peeling tasks. Extensive quantitative and qualitative experiments demonstrate that LayerPeeler significantly outperforms existing techniques, producing vectorization results with superior path semantics, geometric regularity, and visual fidelity.
>
---
#### [new 148] cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning
- **分类: cs.CV; cs.LG**

- **简介: 该论文提出多模态CAD重建模型cadrille，解决单一输入模态限制。通过融合点云、图像、文本，采用监督微调与在线强化学习（如GRPO）两阶段训练，首次在CAD任务中应用RL微调，显著提升模型泛化性与性能，在DeepCAD基准中超越单模态方法并创3项SOTA。**

- **链接: [http://arxiv.org/pdf/2505.22914v1](http://arxiv.org/pdf/2505.22914v1)**

> **作者:** Maksim Kolodiazhnyi; Denis Tarasov; Dmitrii Zhemchuzhnikov; Alexander Nikulin; Ilya Zisman; Anna Vorontsova; Anton Konushin; Vladislav Kurenkov; Danila Rukhovich
>
> **摘要:** Computer-Aided Design (CAD) plays a central role in engineering and manufacturing, making it possible to create precise and editable 3D models. Using a variety of sensor or user-provided data as inputs for CAD reconstruction can democratize access to design applications. However, existing methods typically focus on a single input modality, such as point clouds, images, or text, which limits their generalizability and robustness. Leveraging recent advances in vision-language models (VLM), we propose a multi-modal CAD reconstruction model that simultaneously processes all three input modalities. Inspired by large language model (LLM) training paradigms, we adopt a two-stage pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated data, followed by reinforcement learning (RL) fine-tuning using online feedback, obtained programatically. Furthermore, we are the first to explore RL fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such as Group Relative Preference Optimization (GRPO) outperform offline alternatives. In the DeepCAD benchmark, our SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. More importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three challenging datasets, including a real-world one.
>
---
#### [new 149] LADA: Scalable Label-Specific CLIP Adapter for Continual Learning
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于持续学习任务，解决CLIP模型在多任务适配中参数选择错误及灾难性遗忘问题。提出LADA方法，通过添加轻量级标签特定记忆单元到冻结的CLIP图像编码器，结合特征蒸馏保留旧类知识，实现高效增量学习，达最优性能。**

- **链接: [http://arxiv.org/pdf/2505.23271v1](http://arxiv.org/pdf/2505.23271v1)**

> **作者:** Mao-Lin Luo; Zi-Hao Zhou; Tong Wei; Min-Ling Zhang
>
> **备注:** Accepted at ICML 2025
>
> **摘要:** Continual learning with vision-language models like CLIP offers a pathway toward scalable machine learning systems by leveraging its transferable representations. Existing CLIP-based methods adapt the pre-trained image encoder by adding multiple sets of learnable parameters, with each task using a partial set of parameters. This requires selecting the expected parameters for input images during inference, which is prone to error that degrades performance. To address this problem, we introduce LADA (Label-specific ADApter). Instead of partitioning parameters across tasks, LADA appends lightweight, label-specific memory units to the frozen CLIP image encoder, enabling discriminative feature generation by aggregating task-agnostic knowledge. To prevent catastrophic forgetting, LADA employs feature distillation for seen classes, preventing their features from being interfered with by new classes. Positioned after the image encoder, LADA prevents gradient flow to the frozen CLIP parameters, ensuring efficient training. Extensive results show that LADA achieves state-of-the-art performance in continual learning settings. The implementation code is available at https://github.com/MaolinLuo/LADA.
>
---
#### [new 150] MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking
- **分类: cs.HC; cs.CV; 68T10, 68U35; H.5.2; H.1.2; C.2.4; I.5.4**

- **简介: 该论文提出MAC-Gaze方法，解决移动端眼动追踪在用户动态姿势/设备姿态变化中精度下降的问题。通过融合IMU传感器数据与持续学习，结合视觉模型和活动识别，利用聚类决策检测运动状态变化并触发自适应校准，同时采用重放机制防止模型遗忘，提升移动场景下眼动追踪的鲁棒性。**

- **链接: [http://arxiv.org/pdf/2505.22769v1](http://arxiv.org/pdf/2505.22769v1)**

> **作者:** Yaxiong Lei; Mingyue Zhao; Yuheng Wang; Shijing He; Yusuke Sugano; Yafei Wang; Kaixing Zhao; Mohamed Khamis; Juan Ye
>
> **备注:** 24 pages, 7 figures
>
> **摘要:** Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as users naturally change their postures and device orientations. Traditional calibration approaches, like one-off, fail to adapt to these dynamic conditions, leading to degraded performance over time. We present MAC-Gaze, a Motion-Aware continual Calibration approach that leverages smartphone Inertial measurement unit (IMU) sensors and continual learning techniques to automatically detect changes in user motion states and update the gaze tracking model accordingly. Our system integrates a pre-trained visual gaze estimator and an IMU-based activity recognition model with a clustering-based hybrid decision-making mechanism that triggers recalibration when motion patterns deviate significantly from previously encountered states. To enable accumulative learning of new motion conditions while mitigating catastrophic forgetting, we employ replay-based continual learning, allowing the model to maintain performance across previously encountered motion conditions. We evaluate our system through extensive experiments on the publicly available RGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+ images, 800K+ IMU readings), encompassing a wide range of postures under various motion conditions including sitting, standing, lying, and walking. Results demonstrate that our method reduces gaze estimation error by 19.9% on RGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to 1.92 cm) compared to traditional calibration approaches. Our framework provides a robust solution for maintaining gaze estimation accuracy in mobile scenarios.
>
---
#### [new 151] REOrdering Patches Improves Vision Models
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文属于视觉模型优化任务，解决图像序列化时固定补丁顺序导致的性能下降问题。提出REOrder框架，通过信息论压缩评估与Plackett-Luce政策强化学习，学习最优排列。在ImageNet-1K和FMoW上分别提升3.01%和13.35%准确率。**

- **链接: [http://arxiv.org/pdf/2505.23751v1](http://arxiv.org/pdf/2505.23751v1)**

> **作者:** Declan Kutscher; David M. Chan; Yutong Bai; Trevor Darrell; Ritwik Gupta
>
> **摘要:** Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.
>
---
#### [new 152] Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint
- **分类: cs.CL; cs.AI; cs.CV; cs.LG**

- **简介: 该论文评估视觉语言模型（VLM）解决rebus谜题的能力。针对VLM在抽象推理、符号推理及文化语言双关理解上的不足，构建了包含多样英语rebus的基准数据集，分析不同模型表现，发现其虽能处理简单线索，但复杂任务表现欠佳。**

- **链接: [http://arxiv.org/pdf/2505.23759v1](http://arxiv.org/pdf/2505.23759v1)**

> **作者:** Heekyung Lee; Jiaxin Ge; Tsung-Han Wu; Minwoo Kang; Trevor Darrell; David M. Chan
>
> **摘要:** Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues ("head" over "heels"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors.
>
---
#### [new 153] Autoregressive Meta-Actions for Unified Controllable Trajectory Generation
- **分类: cs.RO; cs.CV**

- **简介: 该论文属于自动驾驶可控轨迹生成任务，旨在解决现有方法中固定时间间隔的元动作与实际轨迹时间错配导致的任务不连贯问题。提出将长时段元动作分解为帧级元动作，通过自回归方式逐帧预测元动作并生成轨迹，确保严格对齐，同时采用分阶段预训练提升模型灵活性和稳定性。**

- **链接: [http://arxiv.org/pdf/2505.23612v1](http://arxiv.org/pdf/2505.23612v1)**

> **作者:** Jianbo Zhao; Taiyu Ban; Xiyang Wang; Qibin Zhou; Hangning Zhou; Zhihao Liu; Mu Yang; Lei Liu; Bin Li
>
> **摘要:** Controllable trajectory generation guided by high-level semantic decisions, termed meta-actions, is crucial for autonomous driving systems. A significant limitation of existing frameworks is their reliance on invariant meta-actions assigned over fixed future time intervals, causing temporal misalignment with the actual behavior trajectories. This misalignment leads to irrelevant associations between the prescribed meta-actions and the resulting trajectories, disrupting task coherence and limiting model performance. To address this challenge, we introduce Autoregressive Meta-Actions, an approach integrated into autoregressive trajectory generation frameworks that provides a unified and precise definition for meta-action-conditioned trajectory prediction. Specifically, We decompose traditional long-interval meta-actions into frame-level meta-actions, enabling a sequential interplay between autoregressive meta-action prediction and meta-action-conditioned trajectory generation. This decomposition ensures strict alignment between each trajectory segment and its corresponding meta-action, achieving a consistent and unified task formulation across the entire trajectory span and significantly reducing complexity. Moreover, we propose a staged pre-training process to decouple the learning of basic motion dynamics from the integration of high-level decision control, which offers flexibility, stability, and modularity. Experimental results validate our framework's effectiveness, demonstrating improved trajectory adaptivity and responsiveness to dynamic decision-making scenarios. We provide the video document and dataset, which are available at https://arma-traj.github.io/.
>
---
#### [new 154] MRI Image Generation Based on Text Prompts
- **分类: eess.IV; cs.CV; physics.med-ph**

- **简介: 该论文提出基于文本提示生成MRI图像的方法，解决真实数据获取难题（高成本、样本少、隐私问题）。通过微调Stable Diffusion模型生成多磁场脑部图像（T1/T2/FLAIR），用FID/MS-SSIM验证质量，并通过分类任务证明合成数据可增强医疗AI训练效果。**

- **链接: [http://arxiv.org/pdf/2505.22682v1](http://arxiv.org/pdf/2505.22682v1)**

> **作者:** Xinxian Fan; Mengye Lyu
>
> **摘要:** This study explores the use of text-prompted MRI image generation with the Stable Diffusion (SD) model to address challenges in acquiring real MRI datasets, such as high costs, limited rare case samples, and privacy concerns. The SD model, pre-trained on natural images, was fine-tuned using the 3T fastMRI dataset and the 0.3T M4Raw dataset, with the goal of generating brain T1, T2, and FLAIR images across different magnetic field strengths. The performance of the fine-tuned model was evaluated using quantitative metrics,including Fr\'echet Inception Distance (FID) and Multi-Scale Structural Similarity (MS-SSIM), showing improvements in image quality and semantic consistency with the text prompts. To further evaluate the model's potential, a simple classification task was carried out using a small 0.35T MRI dataset, demonstrating that the synthetic images generated by the fine-tuned SD model can effectively augment training datasets and improve the performance of MRI constrast classification tasks. Overall, our findings suggest that text-prompted MRI image generation is feasible and can serve as a useful tool for medical AI applications.
>
---
#### [new 155] Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion
- **分类: q-bio.TO; cs.AI; cs.CV**

- **简介: 该论文提出名为MAGIC的生理学引导多任务生成网络，解决CT灌注成像依赖对比剂带来的过敏风险及高成本问题。通过融合生理特征与生成模型，将非对比CT图像转化为无对比剂灌注图，经临床数据训练与专家盲测验证，显示其诊断准确性媲美传统方法，实现安全、经济的灌注成像。（99字）**

- **链接: [http://arxiv.org/pdf/2505.22673v1](http://arxiv.org/pdf/2505.22673v1)**

> **作者:** Wasif Khan; Kyle B. See; Simon Kato; Ziqian Huang; Amy Lazarte; Kyle Douglas; Xiangyang Lou; Teng J. Peng; Dhanashree Rajderkar; John Rees; Pina Sanelli; Amita Singh; Ibrahim Tuna; Christina A. Wilson; Ruogu Fang
>
> **备注:** Under Review
>
> **摘要:** Perfusion imaging is extensively utilized to assess hemodynamic status and tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging plays a key role in the early assessment and planning of stroke treatment. While CTP provides essential perfusion parameters to identify abnormal blood flow in the brain, the use of contrast agents in CTP can lead to allergic reactions and adverse side effects, along with costing USD 4.9 billion worldwide in 2022. To address these challenges, we propose a novel deep learning framework called Multitask Automated Generation of Intermodal CT perfusion maps (MAGIC). This framework combines generative artificial intelligence and physiological information to map non-contrast computed tomography (CT) imaging to multiple contrast-free CTP imaging maps. We demonstrate enhanced image fidelity by incorporating physiological characteristics into the loss terms. Our network was trained and validated using CT image data from patients referred for stroke at UF Health and demonstrated robustness to abnormalities in brain perfusion activity. A double-blinded study was conducted involving seven experienced neuroradiologists and vascular neurologists. This study validated MAGIC's visual quality and diagnostic accuracy showing favorable performance compared to clinical perfusion imaging with intravenous contrast injection. Overall, MAGIC holds great promise in revolutionizing healthcare by offering contrast-free, cost-effective, and rapid perfusion imaging.
>
---
#### [new 156] ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging
- **分类: q-bio.NC; cs.AI; cs.CV**

- **简介: 该论文提出ConnectomeDiffuser框架，利用生成式AI从DTI数据自动构建脑网络。旨在解决传统方法存在的主观性强、流程繁琐及无法捕捉复杂拓扑特征的问题。其结合Riemannian几何特征提取、扩散模型生成及GCN分类器，提升脑网络分析的敏感性和疾病诊断准确性，实验显示其在神经退行性疾病研究中优于现有方法。**

- **链接: [http://arxiv.org/pdf/2505.22683v1](http://arxiv.org/pdf/2505.22683v1)**

> **作者:** Xuhang Chen; Michael Kwok-Po Ng; Kim-Fung Tsang; Chi-Man Pun; Shuqiang Wang
>
> **摘要:** Brain network analysis plays a crucial role in diagnosing and monitoring neurodegenerative disorders such as Alzheimer's disease (AD). Existing approaches for constructing structural brain networks from diffusion tensor imaging (DTI) often rely on specialized toolkits that suffer from inherent limitations: operator subjectivity, labor-intensive workflows, and restricted capacity to capture complex topological features and disease-specific biomarkers. To overcome these challenges and advance computational neuroimaging instrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based framework for automated end-to-end brain network construction from DTI. The proposed model combines three key components: (1) a Template Network that extracts topological features from 3D DTI scans using Riemannian geometric principles, (2) a diffusion model that generates comprehensive brain networks with enhanced topological fidelity, and (3) a Graph Convolutional Network classifier that incorporates disease-specific markers to improve diagnostic accuracy. ConnectomeDiffuser demonstrates superior performance by capturing a broader range of structural connectivity and pathology-related information, enabling more sensitive analysis of individual variations in brain networks. Experimental validation on datasets representing two distinct neurodegenerative conditions demonstrates significant performance improvements over other brain network methods. This work contributes to the advancement of instrumentation in the context of neurological disorders, providing clinicians and researchers with a robust, generalizable measurement framework that facilitates more accurate diagnosis, deeper mechanistic understanding, and improved therapeutic monitoring of neurodegenerative diseases such as AD.
>
---
#### [new 157] Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven Facial Animation
- **分类: cs.SD; cs.CV**

- **简介: 该论文属于3D语音驱动面部动画生成任务，旨在解决近音同形音节在自监督音频特征空间的耦合问题，导致唇形生成出现平均效应。提出Wav2Sem模块，通过提取语义特征解耦音频编码，提升面部动画的精确度和自然度。**

- **链接: [http://arxiv.org/pdf/2505.23290v1](http://arxiv.org/pdf/2505.23290v1)**

> **作者:** Hao Li; Ju Dai; Xin Zhao; Feng Zhou; Junjun Pan; Lei Li
>
> **备注:** Accepted to CVPR 2025
>
> **摘要:** In 3D speech-driven facial animation generation, existing methods commonly employ pre-trained self-supervised audio models as encoders. However, due to the prevalence of phonetically similar syllables with distinct lip shapes in language, these near-homophone syllables tend to exhibit significant coupling in self-supervised audio feature spaces, leading to the averaging effect in subsequent lip motion generation. To address this issue, this paper proposes a plug-and-play semantic decorrelation module-Wav2Sem. This module extracts semantic features corresponding to the entire audio sequence, leveraging the added semantic information to decorrelate audio encodings within the feature space, thereby achieving more expressive audio features. Extensive experiments across multiple Speech-driven models indicate that the Wav2Sem module effectively decouples audio features, significantly alleviating the averaging effect of phonetically similar syllables in lip shape generation, thereby enhancing the precision and naturalness of facial animations. Our source code is available at https://github.com/wslh852/Wav2Sem.git.
>
---
#### [new 158] Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文针对子群分布偏移问题，提出多样化原型集成（DPE）方法。通过构建多个关注不同特征与样本的原型分类器组成的集成模型，提升模型在目标数据中的鲁棒性，无需依赖子群标注信息。实验显示其在多个数据集上优于现有方法。**

- **链接: [http://arxiv.org/pdf/2505.23027v1](http://arxiv.org/pdf/2505.23027v1)**

> **作者:** Minh Nguyen Nhat To; Paul F RWilson; Viet Nguyen; Mohamed Harmanani; Michael Cooper; Fahimeh Fooladgar; Purang Abolmaesumi; Parvin Mousavi; Rahul G. Krishnan
>
> **备注:** ICML 2025 Paper
>
> **摘要:** The subpopulationtion shift, characterized by a disparity in subpopulation distributibetween theween the training and target datasets, can significantly degrade the performance of machine learning models. Current solutions to subpopulation shift involve modifying empirical risk minimization with re-weighting strategies to improve generalization. This strategy relies on assumptions about the number and nature of subpopulations and annotations on group membership, which are unavailable for many real-world datasets. Instead, we propose using an ensemble of diverse classifiers to adaptively capture risk associated with subpopulations. Given a feature extractor network, we replace its standard linear classification layer with a mixture of prototypical classifiers, where each member is trained to classify the data while focusing on different features and samples from other members. In empirical evaluation on nine real-world datasets, covering diverse domains and kinds of subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often outperforms the prior state-of-the-art in worst-group accuracy. The code is available at https://github.com/minhto2802/dpe4subpop
>
---
#### [new 159] Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion
- **分类: cs.CR; cs.AI; cs.CV**

- **简介: 该论文提出AdvOF框架，针对服务场景中视觉语言导航系统（VLN），通过生成对抗3D对象攻击其VLM感知模块，解决现有攻击方法不适用于服务场景的问题。工作包括精准对齐目标对象的2D/3D位置、优化对抗物物理属性与VLM感知一致性，实验表明其有效干扰目标任务同时最小化对正常导航的影响。**

- **链接: [http://arxiv.org/pdf/2505.23266v1](http://arxiv.org/pdf/2505.23266v1)**

> **作者:** Chunlong Xie; Jialing He; Shangwei Guo; Jiacheng Wang; Shudong Zhang; Tianwei Zhang; Tao Xiang
>
> **备注:** Under review
>
> **摘要:** We present Adversarial Object Fusion (AdvOF), a novel attack framework targeting vision-and-language navigation (VLN) agents in service-oriented environments by generating adversarial 3D objects. While foundational models like Large Language Models (LLMs) and Vision Language Models (VLMs) have enhanced service-oriented navigation systems through improved perception and decision-making, their integration introduces vulnerabilities in mission-critical service workflows. Existing adversarial attacks fail to address service computing contexts, where reliability and quality-of-service (QoS) are paramount. We utilize AdvOF to investigate and explore the impact of adversarial environments on the VLM-based perception module of VLN agents. In particular, AdvOF first precisely aggregates and aligns the victim object positions in both 2D and 3D space, defining and rendering adversarial objects. Then, we collaboratively optimize the adversarial object with regularization between the adversarial and victim object across physical properties and VLM perceptions. Through assigning importance weights to varying views, the optimization is processed stably and multi-viewedly by iterative fusions from local updates and justifications. Our extensive evaluations demonstrate AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks. This work advances the understanding of service security in VLM-powered navigation systems, providing computational foundations for robust service composition in physical-world deployments.
>
---
#### [new 160] Plug-and-Play Posterior Sampling for Blind Inverse Problems
- **分类: eess.IV; cs.CV**

- **简介: 该论文提出Blind-PnPDM框架，解决盲逆问题（如图像去模糊），目标图像与测量算子均未知。传统方法依赖显式先验或分步参数估计，而该方法用两个扩散模型分别建模图像分布和测量参数，通过交替高斯去噪实现后验采样，实验显示优于现有方法。**

- **链接: [http://arxiv.org/pdf/2505.22923v1](http://arxiv.org/pdf/2505.22923v1)**

> **作者:** Anqi Li; Weijie Gan; Ulugbek S. Kamilov
>
> **备注:** arXiv admin note: text overlap with arXiv:2305.12672
>
> **摘要:** We introduce Blind Plug-and-Play Diffusion Models (Blind-PnPDM) as a novel framework for solving blind inverse problems where both the target image and the measurement operator are unknown. Unlike conventional methods that rely on explicit priors or separate parameter estimation, our approach performs posterior sampling by recasting the problem into an alternating Gaussian denoising scheme. We leverage two diffusion models as learned priors: one to capture the distribution of the target image and another to characterize the parameters of the measurement operator. This PnP integration of diffusion models ensures flexibility and ease of adaptation. Our experiments on blind image deblurring show that Blind-PnPDM outperforms state-of-the-art methods in terms of both quantitative metrics and visual fidelity. Our results highlight the effectiveness of treating blind inverse problems as a sequence of denoising subproblems while harnessing the expressive power of diffusion-based priors.
>
---
#### [new 161] CF-DETR: Coarse-to-Fine Transformer for Real-Time Object Detection
- **分类: eess.SY; cs.CV; cs.SY**

- **简介: 该论文提出CF-DETR方法，解决自动驾驶中DETR模型多任务实时性(R1)与高精度(R2)冲突问题。通过粗到细Transformer架构与NPFP调度框架，动态调整计算资源分配：关键对象优先执行粗粒度推理保障实时性，非关键对象选择性精细处理提升精度，多级批处理优化效率，实现在资源约束下兼顾安全与时效。**

- **链接: [http://arxiv.org/pdf/2505.23317v1](http://arxiv.org/pdf/2505.23317v1)**

> **作者:** Woojin Shin; Donghwa Kang; Byeongyun Park; Brent Byunghoon Kang; Jinkyu Lee; Hyeongboo Baek
>
> **备注:** 12 pages
>
> **摘要:** Detection Transformers (DETR) are increasingly adopted in autonomous vehicle (AV) perception systems due to their superior accuracy over convolutional networks. However, concurrently executing multiple DETR tasks presents significant challenges in meeting firm real-time deadlines (R1) and high accuracy requirements (R2), particularly for safety-critical objects, while navigating the inherent latency-accuracy trade-off under resource constraints. Existing real-time DNN scheduling approaches often treat models generically, failing to leverage Transformer-specific properties for efficient resource allocation. To address these challenges, we propose CF-DETR, an integrated system featuring a novel coarse-to-fine Transformer architecture and a dedicated real-time scheduling framework NPFP**. CF-DETR employs three key strategies (A1: coarse-to-fine inference, A2: selective fine inference, A3: multi-level batch inference) that exploit Transformer properties to dynamically adjust patch granularity and attention scope based on object criticality, aiming to satisfy R2. The NPFP** scheduling framework (A4) orchestrates these adaptive mechanisms A1-A3. It partitions each DETR task into a safety-critical coarse subtask for guaranteed critical object detection within its deadline (ensuring R1), and an optional fine subtask for enhanced overall accuracy (R2), while managing individual and batched execution. Our extensive evaluations on server, GPU-enabled embedded platforms, and actual AV platforms demonstrate that CF-DETR, under an NPFP** policy, successfully meets strict timing guarantees for critical operations and achieves significantly higher overall and critical object detection accuracy compared to existing baselines across diverse AV workloads.
>
---
#### [new 162] Network Inversion for Uncertainty-Aware Out-of-Distribution Detection
- **分类: cs.LG; cs.CV**

- **简介: 该论文提出结合网络逆向与分类器训练的框架，解决OOD检测和不确定性估计问题。通过扩展分类器为n+1类（含"垃圾类"），利用逆向生成样本迭代优化决策边界，将OOD样本推入垃圾类，实现无需外部数据的统一检测与不确定性评估。**

- **链接: [http://arxiv.org/pdf/2505.23448v1](http://arxiv.org/pdf/2505.23448v1)**

> **作者:** Pirzada Suhail; Rehna Afroz; Amit Sethi
>
> **摘要:** Out-of-distribution (OOD) detection and uncertainty estimation (UE) are critical components for building safe machine learning systems, especially in real-world scenarios where unexpected inputs are inevitable. In this work, we propose a novel framework that combines network inversion with classifier training to simultaneously address both OOD detection and uncertainty estimation. For a standard n-class classification task, we extend the classifier to an (n+1)-class model by introducing a "garbage" class, initially populated with random gaussian noise to represent outlier inputs. After each training epoch, we use network inversion to reconstruct input images corresponding to all output classes that initially appear as noisy and incoherent and are therefore excluded to the garbage class for retraining the classifier. This cycle of training, inversion, and exclusion continues iteratively till the inverted samples begin to resemble the in-distribution data more closely, suggesting that the classifier has learned to carve out meaningful decision boundaries while sanitising the class manifolds by pushing OOD content into the garbage class. During inference, this training scheme enables the model to effectively detect and reject OOD samples by classifying them into the garbage class. Furthermore, the confidence scores associated with each prediction can be used to estimate uncertainty for both in-distribution and OOD inputs. Our approach is scalable, interpretable, and does not require access to external OOD datasets or post-hoc calibration techniques while providing a unified solution to the dual challenges of OOD detection and uncertainty estimation.
>
---
#### [new 163] TrackVLA: Embodied Visual Tracking in the Wild
- **分类: cs.RO; cs.CV**

- **简介: 该论文属于具身视觉跟踪任务，旨在解决动态环境中目标追踪的挑战，尤其针对严重遮挡和高场景动态问题。提出TrackVLA模型，通过Vision-Language-Action架构融合目标识别与轨迹规划，共享LLM backbone，结合语言建模头和anchor-based扩散模型。构建含170万样本的EVT-Bench数据集，实验显示其零样本性能达SOTA，且在真实场景下实现10FPS实时追踪。**

- **链接: [http://arxiv.org/pdf/2505.23189v1](http://arxiv.org/pdf/2505.23189v1)**

> **作者:** Shaoan Wang; Jiazhao Zhang; Minghan Li; Jiahang Liu; Anqi Li; Kui Wu; Fangwei Zhong; Junzhi Yu; Zhizheng Zhang; He Wang
>
> **摘要:** Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed. Our project page is: https://pku-epic.github.io/TrackVLA-web.
>
---
#### [new 164] Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model
- **分类: cs.LG; cs.CV**

- **简介: 论文提出Muddit，一种统一离散扩散模型，解决跨模态生成中自回归模型速度慢、非自回归模型泛化弱的问题。通过整合预训练文生图先验与轻量文本解码器，实现高效并行生成，性能优于大型自回归模型。**

- **链接: [http://arxiv.org/pdf/2505.23606v1](http://arxiv.org/pdf/2505.23606v1)**

> **作者:** Qingyu Shi; Jinbin Bai; Zhuoran Zhao; Wenhao Chai; Kaidong Yu; Jianzong Wu; Shuangyong Song; Yunhai Tong; Xiangtai Li; Xuelong Li; Shuicheng Yan
>
> **备注:** The code and model are available at https://github.com/M-E-AGI-Lab/Muddit
>
> **摘要:** Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation.
>
---
#### [new 165] Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis
- **分类: eess.IV; cs.AI; cs.CV**

- **简介: 该论文针对多发性硬化症中罕见环形病灶（PRLs）检测的类别不平衡问题，提出合成数据生成与潜在投影去噪方法，通过生成对抗网络合成定量磁化率图及多通道分割，提升分类器性能并开源数据。**

- **链接: [http://arxiv.org/pdf/2505.23353v1](http://arxiv.org/pdf/2505.23353v1)**

> **作者:** Alexandra G. Roberts; Ha M. Luu; Mert Şişman; Alexey V. Dimov; Ceren Tozlu; Ilhami Kovanlikaya; Susan A. Gauthier; Thanh D. Nguyen; Yi Wang
>
> **备注:** Accepted full paper in Synthetic Data @ CVPR 2025 12 pages, 10 figures
>
> **摘要:** Quantitative susceptibility maps from magnetic resonance images can provide both prognostic and diagnostic information in multiple sclerosis, a neurodegenerative disease characterized by the formation of lesions in white matter brain tissue. In particular, susceptibility maps provide adequate contrast to distinguish between "rim" lesions, surrounded by deposited paramagnetic iron, and "non-rim" lesion types. These paramagnetic rim lesions (PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been devoted to both detection and segmentation of such lesions to monitor longitudinal change. As paramagnetic rim lesions are rare, addressing this problem requires confronting the class imbalance between rim and non-rim lesions. We produce synthetic quantitative susceptibility maps of paramagnetic rim lesions and show that inclusion of such synthetic data improves classifier performance and provide a multi-channel extension to generate accompanying contrasts and probabilistic segmentation maps. We exploit the projection capability of our trained generative network to demonstrate a novel denoising approach that allows us to train on ambiguous rim cases and substantially increase the minority class. We show that both synthetic lesion synthesis and our proposed rim lesion label denoising method best approximate the unseen rim lesion distribution and improve detection in a clinically interpretable manner. We release our code and generated data at https://github.com/agr78/PRLx-GAN upon publication.
>
---
#### [new 166] Number of Clusters in a Dataset: A Regularized K-means Approach
- **分类: cs.LG; cs.CV; 68; I.5.3**

- **简介: 该论文属于聚类分析任务，旨在解决正则化k-means算法中关键超参数λ的选择问题及多解歧义。研究推导了理想簇下λ的理论边界，提出并分析乘法正则化方法，通过实验验证其在簇偏离理想条件下的性能，以减少解的不确定性。**

- **链接: [http://arxiv.org/pdf/2505.22991v1](http://arxiv.org/pdf/2505.22991v1)**

> **作者:** Behzad Kamgar-Parsi; Behrooz Kamgar-Parsi
>
> **备注:** 19 pages, 14 figures. arXiv admin note: substantial text overlap with arXiv:1911.06741
>
> **摘要:** Finding the number of meaningful clusters in an unlabeled dataset is important in many applications. Regularized k-means algorithm is a possible approach frequently used to find the correct number of distinct clusters in datasets. The most common formulation of the regularization function is the additive linear term $\lambda k$, where $k$ is the number of clusters and $\lambda$ a positive coefficient. Currently, there are no principled guidelines for setting a value for the critical hyperparameter $\lambda$. In this paper, we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}. Ideal clusters (defined as $d$-dimensional spheres with identical radii) are close proxies for k-means clusters ($d$-dimensional spherically symmetric distributions with identical standard deviations). Experiments show that the k-means algorithm with additive regularizer often yields multiple solutions. Thus, we also analyze k-means algorithm with multiplicative regularizer. The consensus among k-means solutions with additive and multiplicative regularizations reduces the ambiguity of multiple solutions in certain cases. We also present selected experiments that demonstrate performance of the regularized k-means algorithms as clusters deviate from the ideal assumption.
>
---
#### [new 167] Quality assessment of 3D human animation: Subjective and objective evaluation
- **分类: cs.GR; cs.CV**

- **简介: 该论文提出首个针对非参数化身体模型生成的3D人类动画质量评估方法。任务是解决现有评估指标无法客观衡量此类动画质量的问题。通过构建含用户主观评分的动画数据集，训练线性回归模型预测感知评分，实现90%相关性，超越深度学习基线。**

- **链接: [http://arxiv.org/pdf/2505.23301v1](http://arxiv.org/pdf/2505.23301v1)**

> **作者:** Rim Rekik; Stefanie Wuhrer; Ludovic Hoyet; Katja Zibrek; Anne-Hélène Olivier
>
> **摘要:** Virtual human animations have a wide range of applications in virtual and augmented reality. While automatic generation methods of animated virtual humans have been developed, assessing their quality remains challenging. Recently, approaches introducing task-oriented evaluation metrics have been proposed, leveraging neural network training. However, quality assessment measures for animated virtual humans that are not generated with parametric body models have yet to be developed. In this context, we introduce a first such quality assessment measure leveraging a novel data-driven framework. First, we generate a dataset of virtual human animations together with their corresponding subjective realism evaluation scores collected with a user study. Second, we use the resulting dataset to learn predicting perceptual evaluation scores. Results indicate that training a linear regressor on our dataset results in a correlation of 90%, which outperforms a state of the art deep learning baseline.
>
---
#### [new 168] Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation
- **分类: cs.RO; cs.CV; cs.LG**

- **简介: 该论文提出基于生成扩散模型的离路导航异常检测方法。针对机器人在复杂环境需检测分布外（OOD）异常的问题，通过合成编辑图像移除异常区域，利用模型修改痕迹定位像素级异常。工作包括：提出引导扩散推理新方法，测试时无需微调，结合视觉语言模型在特征空间分析语义编辑，实现精准异常检测。**

- **链接: [http://arxiv.org/pdf/2505.22805v1](http://arxiv.org/pdf/2505.22805v1)**

> **作者:** Siddharth Ancha; Sunshine Jiang; Travis Manderson; Laura Brandt; Yilun Du; Philip R. Osteen; Nicholas Roy
>
> **备注:** Presented at ICRA 2025
>
> **摘要:** In order to navigate safely and reliably in off-road and unstructured environments, robots must detect anomalies that are out-of-distribution (OOD) with respect to the training data. We present an analysis-by-synthesis approach for pixel-wise anomaly detection without making any assumptions about the nature of OOD data. Given an input image, we use a generative diffusion model to synthesize an edited image that removes anomalies while keeping the remaining image unchanged. Then, we formulate anomaly detection as analyzing which image segments were modified by the diffusion model. We propose a novel inference approach for guided diffusion by analyzing the ideal guidance gradient and deriving a principled approximation that bootstraps the diffusion model to predict guidance gradients. Our editing technique is purely test-time that can be integrated into existing workflows without the need for retraining or fine-tuning. Finally, we use a combination of vision-language foundation models to compare pixels in a learned feature space and detect semantically meaningful edits, enabling accurate anomaly detection for off-road navigation. Project website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/
>
---
#### [new 169] Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation
- **分类: cs.LG; cs.CV**

- **简介: 该论文属于多目标领域适应任务，解决量化模型合并时的性能退化问题。提出HDRQ量化方法，通过Hessian和距离正则化，在保持源模型精度的同时优化损失曲面，实现更稳定的模型合并，为量化场景下的多领域适配提供首个解决方案。**

- **链接: [http://arxiv.org/pdf/2505.23651v1](http://arxiv.org/pdf/2505.23651v1)**

> **作者:** Juncheol Shin; Minsang Seok; Seonggon Kim; Eunhyeok Park
>
> **备注:** ICML 2025. Code: https://github.com/ewsn1593/HDRQ
>
> **摘要:** Model merging has emerged as a powerful technique for combining task-specific weights, achieving superior performance in multi-target domain adaptation. However, when applied to practical scenarios, such as quantized models, new challenges arise. In practical scenarios, quantization is often applied to target-specific data, but this process restricts the domain of interest and introduces discretization effects, making model merging highly non-trivial. In this study, we analyze the impact of quantization on model merging through the lens of error barriers. Leveraging these insights, we propose a novel post-training quantization, HDRQ - Hessian and distant regularizing quantization - that is designed to consider model merging for multi-target domain adaptation. Our approach ensures that the quantization process incurs minimal deviation from the source pre-trained model while flattening the loss surface to facilitate smooth model merging. To our knowledge, this is the first study on this challenge, and extensive experiments confirm its effectiveness.
>
---
#### [new 170] Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates
- **分类: cs.CL; cs.AI; cs.CV; cs.LG; cs.SD**

- **简介: 该论文属于多模态模型鲁棒性评估任务，旨在通过文本攻击揭示CLIP等预训练模型的组合性漏洞。提出MAC基准，利用LLM生成欺骗性文本样本评估模型漏洞，并通过自训练方法（含拒绝采样微调和多样性过滤）提升攻击效果与样本多样性，验证了在图像、视频等模态中的有效性。**

- **链接: [http://arxiv.org/pdf/2505.22943v1](http://arxiv.org/pdf/2505.22943v1)**

> **作者:** Jaewoo Ahn; Heeseung Yun; Dayoon Ko; Gunhee Kim
>
> **备注:** ACL 2025 Main. Code is released at https://vision.snu.ac.kr/projects/mac
>
> **摘要:** While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.
>
---
#### [new 171] Test-time augmentation improves efficiency in conformal prediction
- **分类: cs.LG; cs.CV**

- **简介: 该论文属于可信机器学习任务，旨在通过测试时增强（TTA）缩小符合性预测的预测集。针对其结果冗余问题，提出无需模型重训的灵活方法，结合TTA与多种评分策略，实验显示在多数据集上平均减少预测集10%-14%，提升效率并保持置信保证。**

- **链接: [http://arxiv.org/pdf/2505.22764v1](http://arxiv.org/pdf/2505.22764v1)**

> **作者:** Divya Shanmugam; Helen Lu; Swami Sankaranarayanan; John Guttag
>
> **摘要:** A conformal classifier produces a set of predicted classes and provides a probabilistic guarantee that the set includes the true class. Unfortunately, it is often the case that conformal classifiers produce uninformatively large sets. In this work, we show that test-time augmentation (TTA)--a technique that introduces inductive biases during inference--reduces the size of the sets produced by conformal classifiers. Our approach is flexible, computationally efficient, and effective. It can be combined with any conformal score, requires no model retraining, and reduces prediction set sizes by 10%-14% on average. We conduct an evaluation of the approach spanning three datasets, three models, two established conformal scoring methods, different guarantee strengths, and several distribution shifts to show when and why test-time augmentation is a useful addition to the conformal pipeline.
>
---
#### [new 172] ZeroGUI: Automating Online GUI Learning at Zero Human Cost
- **分类: cs.AI; cs.CL; cs.CV**

- **简介: 该论文提出ZeroGUI框架，解决现有GUI代理依赖人工标注及适应性差的问题，通过VLM自动生成任务、评估奖励及两阶段在线强化学习，在零人工成本下提升OSWorld和AndroidLab环境性能。**

- **链接: [http://arxiv.org/pdf/2505.23762v1](http://arxiv.org/pdf/2505.23762v1)**

> **作者:** Chenyu Yang; Shiqian Su; Shi Liu; Xuan Dong; Yue Yu; Weijie Su; Xuehui Wang; Zhaoyang Liu; Jinguo Zhu; Hao Li; Wenhai Wang; Yu Qiao; Xizhou Zhu; Jifeng Dai
>
> **摘要:** The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI.
>
---
#### [new 173] Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization
- **分类: cs.LG; cs.CV**

- **简介: 该论文属于领域泛化任务，旨在解决多源领域泛化（MDG）因依赖多域数据集而难以实际应用的问题。提出PMDG框架，通过风格迁移和数据增强将单源域转化为多个伪域，模拟MDG环境，提升单源场景下的泛化能力，实验验证其效果接近或超越真实多域方法。**

- **链接: [http://arxiv.org/pdf/2505.23173v1](http://arxiv.org/pdf/2505.23173v1)**

> **作者:** Shohei Enomoto
>
> **摘要:** Deep learning models often struggle to maintain performance when deployed on data distributions different from their training data, particularly in real-world applications where environmental conditions frequently change. While Multi-source Domain Generalization (MDG) has shown promise in addressing this challenge by leveraging multiple source domains during training, its practical application is limited by the significant costs and difficulties associated with creating multi-domain datasets. To address this limitation, we propose Pseudo Multi-source Domain Generalization (PMDG), a novel framework that enables the application of sophisticated MDG algorithms in more practical Single-source Domain Generalization (SDG) settings. PMDG generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques, creating a synthetic multi-domain dataset that can be used with existing MDG algorithms. Through extensive experiments with PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze the effectiveness of PMDG across multiple datasets and architectures. Our analysis reveals several key findings, including a positive correlation between MDG and PMDG performance and the potential of pseudo-domains to match or exceed actual multi-domain performance with sufficient data. These comprehensive empirical results provide valuable insights for future research in domain generalization. Our code is available at https://github.com/s-enmt/PseudoDomainBed.
>
---
#### [new 174] Semantics-Aware Human Motion Generation from Audio Instructions
- **分类: cs.SD; cs.CV**

- **简介: 该论文提出基于音频指令生成语义对齐人类动作的新任务，解决现有方法仅关注音频节奏而语义关联弱的问题。通过端到端框架（含掩码生成Transformer和记忆检索注意力模块）处理长音频输入，并扩展数据集（转换文本为对话式音频），实验验证了方法的有效性。**

- **链接: [http://arxiv.org/pdf/2505.23465v1](http://arxiv.org/pdf/2505.23465v1)**

> **作者:** Zi-An Wang; Shihao Zou; Shiyao Yu; Mingyuan Zhang; Chao Dong
>
> **摘要:** Recent advances in interactive technologies have highlighted the prominence of audio signals for semantic encoding. This paper explores a new task, where audio signals are used as conditioning inputs to generate motions that align with the semantics of the audio. Unlike text-based interactions, audio provides a more natural and intuitive communication method. However, existing methods typically focus on matching motions with music or speech rhythms, which often results in a weak connection between the semantics of the audio and generated motions. We propose an end-to-end framework using a masked generative transformer, enhanced by a memory-retrieval attention module to handle sparse and lengthy audio inputs. Additionally, we enrich existing datasets by converting descriptions into conversational style and generating corresponding audio with varied speaker identities. Experiments demonstrate the effectiveness and efficiency of the proposed framework, demonstrating that audio instructions can convey semantics similar to text while providing more practical and user-friendly interactions.
>
---
#### [new 175] DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography
- **分类: eess.IV; cs.AI; cs.CV**

- **简介: 该论文提出DeepMultiConnectome模型，通过深度学习直接从dMRI纤维束追踪预测结构连接组，解决传统方法依赖灰质划分且耗时的问题。采用多任务点云网络，在84和164区域划分上实现高相关性（r>0.98），支持多划分方案，加速大规模研究。**

- **链接: [http://arxiv.org/pdf/2505.22685v1](http://arxiv.org/pdf/2505.22685v1)**

> **作者:** Marcus J. Vroemen; Yuqian Chen; Yui Lo; Tengfei Xu; Weidong Cai; Fan Zhang; Josien P. W. Pluim; Lauren J. O'Donnell
>
> **备注:** 15 pages, 5 figures, 5 tables
>
> **摘要:** Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural connections, but traditional connectome generation is time-consuming and requires gray matter parcellation, posing challenges for large-scale studies. We introduce DeepMultiConnectome, a deep-learning model that predicts structural connectomes directly from tractography, bypassing the need for gray matter parcellation while supporting multiple parcellation schemes. Using a point-cloud-based neural network with multi-task learning, the model classifies streamlines according to their connected regions across two parcellation schemes, sharing a learned representation. We train and validate DeepMultiConnectome on tractography from the Human Connectome Project Young Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter parcellation scheme. DeepMultiConnectome predicts multiple structural connectomes from a whole-brain tractogram containing 3 million streamlines in approximately 40 seconds. DeepMultiConnectome is evaluated by comparing predicted connectomes with traditional connectomes generated using the conventional method of labeling streamlines using a gray matter parcellation. The predicted connectomes are highly correlated with traditionally generated connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region scheme) and largely preserve network properties. A test-retest analysis of DeepMultiConnectome demonstrates reproducibility comparable to traditionally generated connectomes. The predicted connectomes perform similarly to traditionally generated connectomes in predicting age and cognitive function. Overall, DeepMultiConnectome provides a scalable, fast model for generating subject-specific connectomes across multiple parcellation schemes.
>
---
#### [new 176] ZeroSep: Separate Anything in Audio with Zero Training
- **分类: cs.SD; cs.CV**

- **简介: 论文提出ZeroSep，实现零训练音频源分离。针对传统方法依赖大量标注数据及泛化性差问题，利用预训练文本引导音频扩散模型，通过潜空间逆向与文本引导去噪，无需任务训练即支持开放集，性能超越监督方法。**

- **链接: [http://arxiv.org/pdf/2505.23625v1](http://arxiv.org/pdf/2505.23625v1)**

> **作者:** Chao Huang; Yuesheng Ma; Junxuan Huang; Susan Liang; Yunlong Tang; Jing Bi; Wenqiang Liu; Nima Mesgarani; Chenliang Xu
>
> **备注:** Project page: https://wikichao.github.io/ZeroSep/
>
> **摘要:** Audio source separation is fundamental for machines to understand complex acoustic environments and underpins numerous audio applications. Current supervised deep learning approaches, while powerful, are limited by the need for extensive, task-specific labeled data and struggle to generalize to the immense variability and open-set nature of real-world acoustic scenes. Inspired by the success of generative foundation models, we investigate whether pre-trained text-guided audio diffusion models can overcome these limitations. We make a surprising discovery: zero-shot source separation can be achieved purely through a pre-trained text-guided audio diffusion model under the right configuration. Our method, named ZeroSep, works by inverting the mixed audio into the diffusion model's latent space and then using text conditioning to guide the denoising process to recover individual sources. Without any task-specific training or fine-tuning, ZeroSep repurposes the generative diffusion model for a discriminative separation task and inherently supports open-set scenarios through its rich textual priors. ZeroSep is compatible with a variety of pre-trained text-guided audio diffusion backbones and delivers strong separation performance on multiple separation benchmarks, surpassing even supervised methods.
>
---
#### [new 177] Buffer-free Class-Incremental Learning with Out-of-Distribution Detection
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文属于类增量学习任务，解决开放环境下模型需持续学习新类、避免遗忘旧类并检测未知输入的问题。现有方法依赖记忆缓冲，存在隐私和效率问题。本文通过分析并应用后验分布外检测方法，在推理阶段替代缓冲，实验表明其性能优于或匹敌缓冲方法，提升隐私保护与效率。**

- **链接: [http://arxiv.org/pdf/2505.23412v1](http://arxiv.org/pdf/2505.23412v1)**

> **作者:** Srishti Gupta; Daniele Angioni; Maura Pintor; Ambra Demontis; Lea Schönherr; Battista Biggio; Fabio Roli
>
> **摘要:** Class-incremental learning (CIL) poses significant challenges in open-world scenarios, where models must not only learn new classes over time without forgetting previous ones but also handle inputs from unknown classes that a closed-set model would misclassify. Recent works address both issues by (i)~training multi-head models using the task-incremental learning framework, and (ii) predicting the task identity employing out-of-distribution (OOD) detectors. While effective, the latter mainly relies on joint training with a memory buffer of past data, raising concerns around privacy, scalability, and increased training time. In this paper, we present an in-depth analysis of post-hoc OOD detection methods and investigate their potential to eliminate the need for a memory buffer. We uncover that these methods, when applied appropriately at inference time, can serve as a strong substitute for buffer-based OOD detection. We show that this buffer-free approach achieves comparable or superior performance to buffer-based methods both in terms of class-incremental learning and the rejection of unknown samples. Experimental results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings, offering new insights into the design of efficient and privacy-preserving CIL systems for open-world settings.
>
---
#### [new 178] NegVQA: Can Vision Language Models Understand Negation?
- **分类: cs.CL; cs.AI; cs.CV; cs.CY; cs.LG**

- **简介: 该论文属于视觉问答（VQA）任务，旨在评估视觉语言模型（VLMs）对否定的理解能力。针对VLMs在否定场景下的表现缺陷，构建了含7,379个否定问题的NegVQA基准数据集，并发现模型性能显著下降且存在U型规模效应，揭示了VLMs的关键不足。**

- **链接: [http://arxiv.org/pdf/2505.22946v1](http://arxiv.org/pdf/2505.22946v1)**

> **作者:** Yuhui Zhang; Yuchang Su; Yiming Liu; Serena Yeung-Levy
>
> **备注:** Published at ACL 2025 Findings
>
> **摘要:** Negation is a fundamental linguistic phenomenon that can entirely reverse the meaning of a sentence. As vision language models (VLMs) continue to advance and are deployed in high-stakes applications, assessing their ability to comprehend negation becomes essential. To address this, we introduce NegVQA, a visual question answering (VQA) benchmark consisting of 7,379 two-choice questions covering diverse negation scenarios and image-question distributions. We construct NegVQA by leveraging large language models to generate negated versions of questions from existing VQA datasets. Evaluating 20 state-of-the-art VLMs across seven model families, we find that these models struggle significantly with negation, exhibiting a substantial performance drop compared to their responses to the original questions. Furthermore, we uncover a U-shaped scaling trend, where increasing model size initially degrades performance on NegVQA before leading to improvements. Our benchmark reveals critical gaps in VLMs' negation understanding and offers insights into future VLM development. Project page available at https://yuhui-zh15.github.io/NegVQA/.
>
---
#### [new 179] Mobi-$π$: Mobilizing Your Robot Learning Policy
- **分类: cs.RO; cs.CV; cs.LG**

- **简介: 该论文提出Mobi-π框架，解决视觉运动策略在移动机器人新位姿下的泛化问题。任务为"政策动员"，即通过优化机器人基座位姿使其适应预训练策略的视角分布，而非重新训练策略。工作包括提出评估指标、模拟任务、可视化工具及基于3D高斯散射和采样优化的位姿搜索方法，在仿真和现实环境验证有效性。**

- **链接: [http://arxiv.org/pdf/2505.23692v1](http://arxiv.org/pdf/2505.23692v1)**

> **作者:** Jingyun Yang; Isabella Huang; Brandon Vu; Max Bajracharya; Rika Antonova; Jeannette Bohg
>
> **备注:** Project website: https://mobipi.github.io/
>
> **摘要:** Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the policy mobilization problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. Crucially, this problem formulation complements existing efforts to improve manipulation policy robustness to novel viewpoints and remains compatible with them. To study policy mobilization, we introduce the Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, (3) visualization tools for analysis, and (4) several baseline methods. We also propose a novel approach that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes 3D Gaussian Splatting for novel view synthesis, a score function to evaluate pose suitability, and sampling-based optimization to identify optimal robot poses. We show that our approach outperforms baselines in both simulation and real-world environments, demonstrating its effectiveness for policy mobilization.
>
---
## 更新

#### [replaced 001] Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.19854v2](http://arxiv.org/pdf/2505.19854v2)**

> **作者:** Natsuki Takama; Shintaro Ito; Koichi Ito; Hwann-Tzong Chen; Takafumi Aoki
>
> **备注:** Accepted to ICIP 2025
>
> **摘要:** Gaussian Splatting (GS) has gained attention as a fast and effective method for novel view synthesis. It has also been applied to 3D reconstruction using multi-view images and can achieve fast and accurate 3D reconstruction. However, GS assumes that the input contains a large number of multi-view images, and therefore, the reconstruction accuracy significantly decreases when only a limited number of input images are available. One of the main reasons is the insufficient number of 3D points in the sparse point cloud obtained through Structure from Motion (SfM), which results in a poor initialization for optimizing the Gaussian primitives. We propose a new 3D reconstruction method, called Sparse2DGS, to enhance 2DGS in reconstructing objects using only three images. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along with COLMAP MVS to generate highly accurate and dense 3D point clouds, which are then used to initialize 2D Gaussians. Through experiments on the DTU dataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of objects using just three images. The project page is available at https://gsisaoki.github.io/SPARSE2DGS/
>
---
#### [replaced 002] BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.19328v2](http://arxiv.org/pdf/2505.19328v2)**

> **作者:** Manuela González-González; Soufiane Belharbi; Muhammad Osama Zeeshan; Masoumeh Sharafi; Muhammad Haseeb Aslam; Marco Pedersoli; Alessandro Lameiras Koerich; Simon L Bacon; Eric Granger
>
> **备注:** 41 pages, 13 figures, under review
>
> **摘要:** Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can play a critical role in the personalization and effectiveness of digital behaviour change interventions. These subtle and conflicting emotions are manifested by a discord between multiple modalities, such as facial and vocal expressions, and body language. Although experts can be trained to identify A/H, integrating them into digital interventions is costly and less effective. Automatic learning systems provide a cost-effective alternative that can adapt to individual users, and operate seamlessly within real-time, and resource-limited environments. However, there are currently no datasets available for the design of ML models to recognize A/H. This paper introduces a first Behavioural Ambivalence/Hesitancy (BAH) dataset collected for subject-based multimodal recognition of A/H in videos. It contains videos from 224 participants captured across 9 provinces in Canada, with different age, and ethnicity. Through our web platform, we recruited participants to answer 7 questions, some of which were designed to elicit A/H while recording themselves via webcam with microphone. BAH amounts to 1,118 videos for a total duration of 8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp segments to indicate where A/H occurs, and provide frame- and video-level annotations with the A/H cues. Video transcripts and their timestamps are also included, along with cropped and aligned faces in each frame, and a variety of participants meta-data. We include results baselines for BAH at frame- and video-level recognition in multi-modal setups, in addition to zero-shot prediction, and for personalization using unsupervised domain adaptation. The limited performance of baseline models highlights the challenges of recognizing A/H in real-world videos. The data, code, and pretrained weights are available.
>
---
#### [replaced 003] Textured Gaussians for Enhanced 3D Scene Appearance Modeling
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2411.18625v2](http://arxiv.org/pdf/2411.18625v2)**

> **作者:** Brian Chao; Hung-Yu Tseng; Lorenzo Porzi; Chen Gao; Tuotuo Li; Qinbo Li; Ayush Saraf; Jia-Bin Huang; Johannes Kopf; Gordon Wetzstein; Changil Kim
>
> **备注:** Will be presented at CVPR 2025. Project website: https://textured-gaussians.github.io/
>
> **摘要:** 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D reconstruction and rendering technique due to its high-quality results and fast training and rendering time. However, pixels covered by the same Gaussian are always shaded in the same color up to a Gaussian falloff scaling factor. Furthermore, the finest geometric detail any individual Gaussian can represent is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity of individual Gaussian primitives. To address these issues, we draw inspiration from texture and alpha mapping in traditional graphics and integrate it with 3DGS. Specifically, we propose a new generalized Gaussian appearance representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture maps to model spatially varying color and opacity across the extent of each Gaussian. As such, each Gaussian can represent a richer set of texture patterns and geometric structures, instead of just a single color and ellipsoid as in naive Gaussian Splatting. Surprisingly, we found that the expressivity of Gaussians can be greatly improved by using alpha-only texture maps, and further augmenting Gaussians with RGB texture maps achieves the highest expressivity. We validate our method on a wide variety of standard benchmark datasets and our own custom captures at both the object and scene levels. We demonstrate image quality improvements over existing methods while using a similar or lower number of Gaussians.
>
---
#### [replaced 004] BECAME: BayEsian Continual Learning with Adaptive Model MErging
- **分类: cs.LG; cs.CV**

- **链接: [http://arxiv.org/pdf/2504.02666v2](http://arxiv.org/pdf/2504.02666v2)**

> **作者:** Mei Li; Yuxiang Lu; Qinyan Dai; Suizhi Huang; Yue Ding; Hongtao Lu
>
> **备注:** Accepted by ICML 2025
>
> **摘要:** Continual Learning (CL) strives to learn incrementally across tasks while mitigating catastrophic forgetting. A key challenge in CL is balancing stability (retaining prior knowledge) and plasticity (learning new tasks). While representative gradient projection methods ensure stability, they often limit plasticity. Model merging techniques offer promising solutions, but prior methods typically rely on empirical assumptions and carefully selected hyperparameters. In this paper, we explore the potential of model merging to enhance the stability-plasticity trade-off, providing theoretical insights that underscore its benefits. Specifically, we reformulate the merging mechanism using Bayesian continual learning principles and derive a closed-form solution for the optimal merging coefficient that adapts to the diverse characteristics of tasks. To validate our approach, we introduce a two-stage framework named BECAME, which synergizes the expertise of gradient projection and adaptive merging. Extensive experiments show that our approach outperforms state-of-the-art CL methods and existing merging strategies.
>
---
#### [replaced 005] UniViTAR: Unified Vision Transformer with Native Resolution
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.01792v2](http://arxiv.org/pdf/2504.01792v2)**

> **作者:** Limeng Qiao; Yiyang Gan; Bairui Wang; Jie Qin; Shuang Xu; Siqi Yang; Lin Ma
>
> **摘要:** Conventional Vision Transformer simplifies visual modeling by standardizing input resolutions, often disregarding the variability of natural visual data and compromising spatial-contextual fidelity. While preliminary explorations have superficially investigated native resolution modeling, existing approaches still lack systematic analysis from a visual representation perspective. To bridge this gap, we introduce UniViTAR, a family of homogeneous vision foundation models tailored for unified visual modality and native resolution scenario in the era of multimodal. Our framework first conducts architectural upgrades to the vanilla paradigm by integrating multiple advanced components. Building upon these improvements, a progressive training paradigm is introduced, which strategically combines two core mechanisms: (1) resolution curriculum learning, transitioning from fixed-resolution pretraining to native resolution tuning, thereby leveraging ViT's inherent adaptability to variable-length sequences, and (2) visual modality adaptation via inter-batch image-video switching, which balances computational efficiency with enhanced temporal reasoning. In parallel, a hybrid training framework further synergizes sigmoid-based contrastive loss with feature distillation from a frozen teacher model, thereby accelerating early-stage convergence. Finally, trained exclusively on public datasets, externsive experiments across multiple model scales from 0.3B to 1B demonstrate its effectiveness.
>
---
#### [replaced 006] Can We Predict Performance of Large Models across Vision-Language Tasks?
- **分类: cs.CV; cs.CL**

- **链接: [http://arxiv.org/pdf/2410.10112v2](http://arxiv.org/pdf/2410.10112v2)**

> **作者:** Qinyu Zhao; Ming Xu; Kartik Gupta; Akshay Asthana; Liang Zheng; Stephen Gould
>
> **备注:** ICML2025. Project page: https://github.com/Qinyu-Allen-Zhao/CrossPred-LVLM
>
> **摘要:** Evaluating large vision-language models (LVLMs) is very expensive, due to high computational cost and the wide variety of tasks. The good news is that if we already have some observed performance scores, we may be able to infer unknown ones. In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate the performance prediction as a matrix completion task. Specifically, we construct a sparse performance matrix $\boldsymbol{R}$, where each entry $R_{mn}$ represents the performance score of the $m$-th model on the $n$-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can complete the performance matrix, i.e., predict unknown scores. Additionally, we estimate the uncertainty of performance prediction based on MCMC. Practitioners can evaluate their models on untested tasks with higher uncertainty first, which quickly reduces the prediction errors. We further introduce several improvements to enhance PMF for scenarios with sparse observed performance scores. Our experiments demonstrate the accuracy of PMF in predicting unknown scores, the reliability of uncertainty estimates in ordering evaluations, and the effectiveness of our enhancements for handling sparse data.
>
---
#### [replaced 007] Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.19846v2](http://arxiv.org/pdf/2505.19846v2)**

> **作者:** Nagito Saito; Shintaro Ito; Koichi Ito; Takafumi Aoki
>
> **备注:** Accepted to ICIP 2025
>
> **摘要:** Semantic segmentation is a fundamental task in medical image analysis and autonomous driving and has a problem with the high cost of annotating the labels required in training. To address this problem, semantic segmentation methods based on semi-supervised learning with a small number of labeled data have been proposed. For example, one approach is to train a semantic segmentation model using images with annotated labels and pseudo labels. In this approach, the accuracy of the semantic segmentation model depends on the quality of the pseudo labels, and the quality of the pseudo labels depends on the performance of the model to be trained and the amount of data with annotated labels. In this paper, we generate pseudo labels using zero-shot annotation with the Segment Anything Model (SAM) and Contrastive Language-Image Pretraining (CLIP), improve the accuracy of the pseudo labels using the Unified Dual-Stream Perturbations Approach (UniMatch), and use them as enhanced labels to train a semantic segmentation model. The effectiveness of the proposed method is demonstrated through the experiments using the public datasets: PASCAL and MS COCO. The project web page is available at: https://gsisaoki.github.io/ZERO-SHOT-PLG/
>
---
#### [replaced 008] ChatHuman: Chatting about 3D Humans with Tools
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2405.04533v2](http://arxiv.org/pdf/2405.04533v2)**

> **作者:** Jing Lin; Yao Feng; Weiyang Liu; Michael J. Black
>
> **备注:** Project page: https://chathuman.github.io
>
> **摘要:** Numerous methods have been proposed to detect, estimate, and analyze properties of people in images, including 3D pose, shape, contact, human-object interaction, and emotion. While widely applicable in vision and other areas, such methods require expert knowledge to select, use, and interpret the results. To address this, we introduce ChatHuman, a language-driven system that integrates the capabilities of specialized methods into a unified framework. ChatHuman functions as an assistant proficient in utilizing, analyzing, and interacting with tools specific to 3D human tasks, adeptly discussing and resolving related challenges. Built on a Large Language Model (LLM) framework, ChatHuman is trained to autonomously select, apply, and interpret a diverse set of tools in response to user inputs. Our approach overcomes significant hurdles in adapting LLMs to 3D human tasks, including the need for domain-specific knowledge and the ability to interpret complex 3D outputs. The innovations of ChatHuman include leveraging academic publications to instruct the LLM on tool usage, employing a retrieval-augmented generation model to create in-context learning examples for managing new tools, and effectively discriminating between and integrating tool results by transforming specialized 3D outputs into comprehensible formats. Experiments demonstrate that ChatHuman surpasses existing models in both tool selection accuracy and overall performance across various 3D human tasks, and it supports interactive chatting with users. ChatHuman represents a significant step toward consolidating diverse analytical methods into a unified, robust system for 3D human tasks.
>
---
#### [replaced 009] Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?
- **分类: cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2502.11501v2](http://arxiv.org/pdf/2502.11501v2)**

> **作者:** Zichen Wen; Yifeng Gao; Weijia Li; Conghui He; Linfeng Zhang
>
> **备注:** ACL 2025 Findings
>
> **摘要:** Multimodal large language models (MLLMs) have shown remarkable performance for cross-modal understanding and generation, yet still suffer from severe inference costs. Recently, abundant works have been proposed to solve this problem with token pruning, which identifies the redundant tokens in MLLMs and then prunes them to reduce the computation and KV storage costs, leading to significant acceleration without training. While these methods claim efficiency gains, critical questions about their fundamental design and evaluation remain unanswered: Why do many existing approaches underperform even compared to naive random token selection? Are attention-based scoring sufficient for reliably identifying redundant tokens? Is language information really helpful during token pruning? What makes a good trade-off between token importance and duplication? Are current evaluation protocols comprehensive and unbiased? The ignorance of previous research on these problems hinders the long-term development of token pruning. In this paper, we answer these questions one by one, providing insights into the design of future token pruning methods.
>
---
#### [replaced 010] mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus
- **分类: cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2406.08707v2](http://arxiv.org/pdf/2406.08707v2)**

> **作者:** Matthieu Futeral; Armel Zebaze; Pedro Ortiz Suarez; Julien Abadji; Rémi Lacroix; Cordelia Schmid; Rachel Bawden; Benoît Sagot
>
> **备注:** ACL 2025 (Findings)
>
> **摘要:** Multimodal Large Language Models (mLLMs) are trained on a large amount of text-image data. While most mLLMs are trained on caption-like data only, Alayrac et al. (2022) showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities. However, the dataset they used, M3W, is not public and is only in English. There have been attempts to reproduce their results but the released datasets are English-only. In contrast, current multilingual and multimodal datasets are either composed of caption-like only or medium-scale or fully private data. This limits mLLM research for the 7,000 other languages spoken in the world. We therefore introduce mOSCAR, to the best of our knowledge the first large-scale multilingual and multimodal document corpus crawled from the web. It covers 163 languages, 303M documents, 200B tokens and 1.15B images. We carefully conduct a set of filtering and evaluation steps to make sure mOSCAR is sufficiently safe, diverse and of good quality. We additionally train two types of multilingual model to prove the benefits of mOSCAR: (1) a model trained on a subset of mOSCAR and captioning data and (2) a model trained on captioning data only. The model additionally trained on mOSCAR shows a strong boost in few-shot learning performance across various multilingual image-text tasks and benchmarks, confirming previous findings for English-only mLLMs. The dataset is released under the Creative Commons CC BY 4.0 license and can be accessed here: https://huggingface.co/datasets/oscar-corpus/mOSCAR
>
---
#### [replaced 011] FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.21144v2](http://arxiv.org/pdf/2505.21144v2)**

> **作者:** Sergey Karpukhin; Vadim Titov; Andrey Kuznetsov; Aibek Alanov
>
> **备注:** code available at https://github.com/ControlGenAI/FastFace
>
> **摘要:** In latest years plethora of identity-preserving adapters for a personalized generation with diffusion models have been released. Their main disadvantage is that they are dominantly trained jointly with base diffusion models, which suffer from slow multi-step inference. This work aims to tackle the challenge of training-free adaptation of pretrained ID-adapters to diffusion models accelerated via distillation - through careful re-design of classifier-free guidance for few-step stylistic generation and attention manipulation mechanisms in decoupled blocks to improve identity similarity and fidelity, we propose universal FastFace framework. Additionally, we develop a disentangled public evaluation protocol for id-preserving adapters.
>
---
#### [replaced 012] Position: Interactive Generative Video as Next-Generation Game Engine
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.17359v2](http://arxiv.org/pdf/2503.17359v2)**

> **作者:** Jiwen Yu; Yiran Qin; Haoxuan Che; Quande Liu; Xintao Wang; Pengfei Wan; Di Zhang; Xihui Liu
>
> **摘要:** Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.
>
---
#### [replaced 013] Do large language vision models understand 3D shapes?
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.10908v4](http://arxiv.org/pdf/2412.10908v4)**

> **作者:** Sagi Eppel
>
> **摘要:** Large vision language models (LVLM) are the leading A.I approach for achieving a general visual understanding of the world. Models such as GPT, Claude, Gemini, and LLama can use images to understand and analyze complex visual scenes. 3D objects and shapes are the basic building blocks of the world, recognizing them is a fundamental part of human perception. The goal of this work is to test whether LVLMs truly understand 3D shapes by testing the models ability to identify and match objects of the exact same 3D shapes but with different orientations and materials/textures. A large number of test images were created using CGI with a huge number of highly diverse objects, materials, and scenes. The results of this test show that the ability of such models to match 3D shapes is significantly below humans but much higher than random guesses. Suggesting that the models have gained some abstract understanding of 3D shapes but still trail far beyond humans in this task. Mainly it seems that the models can easily identify the same object with a different orientation as well as matching identical 3D shapes of the same orientation but with different materials and textures. However, when both the object material and orientation are changed, all models perform poorly relative to humans. Code and benchmark are available.
>
---
#### [replaced 014] OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics
- **分类: cs.CV; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2505.17473v3](http://arxiv.org/pdf/2505.17473v3)**

> **作者:** Jiangning Zhu; Yuxing Zhou; Zheng Wang; Juntao Yao; Yima Gu; Yuhui Yuan; Shixia Liu
>
> **摘要:** Given the central role of charts in scientific, business, and communication contexts, enhancing the chart understanding capabilities of vision-language models (VLMs) has become increasingly critical. A key limitation of existing VLMs lies in their inaccurate visual grounding of infographic elements, including charts and human-recognizable objects (HROs) such as icons and images. However, chart understanding often requires identifying relevant elements and reasoning over them. To address this limitation, we introduce OrionBench, a benchmark designed to support the development of accurate object detection models for charts and HROs in infographics. It contains 26,250 real and 78,750 synthetic infographics, with over 6.9 million bounding box annotations. These annotations are created by combining the model-in-the-loop and programmatic methods. We demonstrate the usefulness of OrionBench through three applications: 1) constructing a Thinking-with-Boxes scheme to boost the chart understanding performance of VLMs, 2) comparing existing object detection models, and 3) applying the developed detection model to document layout and UI element detection.
>
---
#### [replaced 015] Satellite Imagery and AI: A New Era in Ocean Conservation, from Research to Deployment and Impact (Version. 2.0)
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2312.03207v2](http://arxiv.org/pdf/2312.03207v2)**

> **作者:** Patrick Beukema; Favyen Bastani; Yawen Zheng; Piper Wolters; Henry Herzog; Joe Ferdinando
>
> **备注:** 8 pages, 3 figures, NeurIPS Computational Sustainability 2023 best paper
>
> **摘要:** Illegal, unreported, and unregulated (IUU) fishing poses a global threat to ocean habitats. Publicly available satellite data offered by NASA, the European Space Agency (ESA), and the U.S. Geological Survey (USGS), provide an opportunity to actively monitor this activity. Effectively leveraging satellite data for maritime conservation requires highly reliable machine learning models operating globally with minimal latency. This paper introduces four specialized computer vision models designed for a variety of sensors including Sentinel-1 (synthetic aperture radar), Sentinel-2 (optical imagery), Landsat 8-9 (optical imagery), and Suomi-NPP/NOAA-20/NOAA-21 (nighttime lights). It also presents best practices for developing and deploying global-scale real-time satellite based computer vision. All of the models are open sourced under permissive licenses. These models have all been deployed in Skylight, a real-time maritime monitoring platform, which is provided at no cost to users worldwide.
>
---
#### [replaced 016] 3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics Based Appearance-Medium Decoupling
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.21238v2](http://arxiv.org/pdf/2505.21238v2)**

> **作者:** Jieyu Yuan; Yujun Li; Yuanlin Zhang; Chunle Guo; Xiongxin Tang; Ruixing Wang; Chongyi Li
>
> **摘要:** Novel view synthesis for underwater scene reconstruction presents unique challenges due to complex light-media interactions. Optical scattering and absorption in water body bring inhomogeneous medium attenuation interference that disrupts conventional volume rendering assumptions of uniform propagation medium. While 3D Gaussian Splatting (3DGS) offers real-time rendering capabilities, it struggles with underwater inhomogeneous environments where scattering media introduce artifacts and inconsistent appearance. In this study, we propose a physics-based framework that disentangles object appearance from water medium effects through tailored Gaussian modeling. Our approach introduces appearance embeddings, which are explicit medium representations for backscatter and attenuation, enhancing scene consistency. In addition, we propose a distance-guided optimization strategy that leverages pseudo-depth maps as supervision with depth regularization and scale penalty terms to improve geometric fidelity. By integrating the proposed appearance and medium modeling components via an underwater imaging model, our approach achieves both high-quality novel view synthesis and physically accurate scene restoration. Experiments demonstrate our significant improvements in rendering quality and restoration accuracy over existing methods. The project page is available at https://bilityniu.github.io/3D-UIR.
>
---
#### [replaced 017] All Patches Matter, More Patches Better: Enhance AI-Generated Image Detection via Panoptic Patch Learning
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.01396v2](http://arxiv.org/pdf/2504.01396v2)**

> **作者:** Zheng Yang; Ruoxin Chen; Zhiyuan Yan; Ke-Yue Zhang; Xinghe Fu; Shuang Wu; Xiujun Shu; Taiping Yao; Shouhong Ding; Xi Li
>
> **摘要:** The exponential growth of AI-generated images (AIGIs) underscores the urgent need for robust and generalizable detection methods. In this paper, we establish two key principles for AIGI detection through systematic analysis: (1) All Patches Matter: Unlike conventional image classification where discriminative features concentrate on object-centric regions, each patch in AIGIs inherently contains synthetic artifacts due to the uniform generation process, suggesting that every patch serves as an important artifact source for detection. (2) More Patches Better: Leveraging distributed artifacts across more patches improves detection robustness by capturing complementary forensic evidence and reducing over-reliance on specific patches, thereby enhancing robustness and generalization. However, our counterfactual analysis reveals an undesirable phenomenon: naively trained detectors often exhibit a Few-Patch Bias, discriminating between real and synthetic images based on minority patches. We identify Lazy Learner as the root cause: detectors preferentially learn conspicuous artifacts in limited patches while neglecting broader artifact distributions. To address this bias, we propose the Panoptic Patch Learning (PPL) framework, involving: (1) Random Patch Replacement that randomly substitutes synthetic patches with real counterparts to compel models to identify artifacts in underutilized regions, encouraging the broader use of more patches; (2) Patch-wise Contrastive Learning that enforces consistent discriminative capability across all patches, ensuring uniform utilization of all patches. Extensive experiments across two different settings on several benchmarks verify the effectiveness of our approach.
>
---
#### [replaced 018] ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.02179v2](http://arxiv.org/pdf/2505.02179v2)**

> **作者:** Tao Zhu; Qi Yu; Xinru Dong; Shiyu Li; Yue Liu; Jinlong Jiang; Lei Shu
>
> **备注:** A newly identified systematic error in our data processing pipeline has affected the calculation and reporting of AUC metrics (notably in Tables [1, 2]). This significantly impacts our main experimental results and conclusions, compromising their reliability. To ensure academic rigor and prevent misleading information, this manuscript is withdrawn for thorough correction and re-evaluation
>
> **摘要:** Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance Learning (MIL) suffers from label ambiguity, hindering discriminative feature learning. We propose ProDisc-VAD, an efficient framework tackling this via two synergistic components. The Prototype Interaction Layer (PIL) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M parameters, over 800x fewer than recent ViT-based methods like VadCLIP, demonstrating exceptional efficiency alongside state-of-the-art performance. Code is available at https://github.com/modadundun/ProDisc-VAD.
>
---
#### [replaced 019] An AI System for Continuous Knee Osteoarthritis Severity Grading Using Self-Supervised Anomaly Detection with Limited Data
- **分类: cs.LG; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2407.11500v2](http://arxiv.org/pdf/2407.11500v2)**

> **作者:** Niamh Belton; Aonghus Lawlor; Kathleen M. Curran
>
> **摘要:** The diagnostic accuracy and subjectivity of existing Knee Osteoarthritis (OA) ordinal grading systems has been a subject of on-going debate and concern. Existing automated solutions are trained to emulate these imperfect systems, whilst also being reliant on large annotated databases for fully-supervised training. This work proposes a three stage approach for automated continuous grading of knee OA that is built upon the principles of Anomaly Detection (AD); learning a robust representation of healthy knee X-rays and grading disease severity based on its distance to the centre of normality. In the first stage, SS-FewSOME is proposed, a self-supervised AD technique that learns the 'normal' representation, requiring only examples of healthy subjects and <3% of the labels that existing methods require. In the second stage, this model is used to pseudo label a subset of unlabelled data as 'normal' or 'anomalous', followed by denoising of pseudo labels with CLIP. The final stage involves retraining on labelled and pseudo labelled data using the proposed Dual Centre Representation Learning (DCRL) which learns the centres of two representation spaces; normal and anomalous. Disease severity is then graded based on the distance to the learned centres. The proposed methodology outperforms existing techniques by margins of up to 24% in terms of OA detection and the disease severity scores correlate with the Kellgren-Lawrence grading system at the same level as human expert performance. Code available at https://github.com/niamhbelton/SS-FewSOME_Disease_Severity_Knee_Osteoarthritis.
>
---
#### [replaced 020] Quantifying the Impact of Motion on 2D Gaze Estimation in Real-World Mobile Interactions
- **分类: cs.HC; cs.CV; H.5; I.4**

- **链接: [http://arxiv.org/pdf/2502.10570v2](http://arxiv.org/pdf/2502.10570v2)**

> **作者:** Yaxiong Lei; Yuheng Wang; Fergus Buchanan; Mingyue Zhao; Yusuke Sugano; Shijing He; Mohamed Khamis; Juan Ye
>
> **备注:** 27 pages, 14 figures
>
> **摘要:** Mobile gaze tracking involves inferring a user's gaze point or direction on a mobile device's screen from facial images captured by the device's front camera. While this technology inspires an increasing number of gaze-interaction applications, achieving consistent accuracy remains challenging due to dynamic user-device spatial relationships and varied motion conditions inherent in mobile contexts. This paper provides empirical evidence on how user mobility and behaviour affect mobile gaze tracking accuracy. We conduct two user studies collecting behaviour and gaze data under various motion conditions - from lying to maze navigation - and during different interaction tasks. Quantitative analysis has revealed behavioural regularities among daily tasks and identified head distance, head pose, and device orientation as key factors affecting accuracy, with errors increasing by up to 48.91% in dynamic conditions compared to static ones. These findings highlight the need for more robust, adaptive eye-tracking systems that account for head movements and device deflection to maintain accuracy across diverse mobile contexts.
>
---
#### [replaced 021] Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation
- **分类: cs.CV; cs.CL**

- **链接: [http://arxiv.org/pdf/2505.21549v2](http://arxiv.org/pdf/2505.21549v2)**

> **作者:** Daniel Csizmadia; Andrei Codreanu; Victor Sim; Vighnesh Prabhu; Michael Lu; Kevin Zhu; Sean O'Brien; Vasu Sharma
>
> **摘要:** We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that enhances multimodal image-text retrieval while preserving the original model's strong zero-shot classification capabilities. CLIP models are typically constrained by fixed image resolutions and limited context, which can hinder their effectiveness in retrieval tasks that require fine-grained cross-modal understanding. DCLIP addresses these challenges through a meta teacher-student distillation framework, where a cross-modal transformer teacher is fine-tuned to produce enriched embeddings via bidirectional cross-attention between YOLO-extracted image regions and corresponding textual spans. These semantically and spatially aligned global representations guide the training of a lightweight student model using a hybrid loss that combines contrastive learning and cosine similarity objectives. Despite being trained on only ~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a fraction of CLIP's original dataset-DCLIP significantly improves image-text retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's zero-shot classification performance. These results demonstrate that DCLIP effectively mitigates the trade-off between task specialization and generalization, offering a resource-efficient, domain-adaptive, and detail-sensitive solution for advanced vision-language tasks. Code available at https://anonymous.4open.science/r/DCLIP-B772/README.md.
>
---
#### [replaced 022] FlexEvent: Towards Flexible Event-Frame Object Detection at Varying Operational Frequencies
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2412.06708v2](http://arxiv.org/pdf/2412.06708v2)**

> **作者:** Dongyue Lu; Lingdong Kong; Gim Hee Lee; Camille Simon Chane; Wei Tsang Ooi
>
> **备注:** Preprint; 27 pages, 14 figures, 10 tables; Code at https://flexevent.github.io/
>
> **摘要:** Event cameras offer unparalleled advantages for real-time perception in dynamic environments, thanks to the microsecond-level temporal resolution and asynchronous operation. Existing event detectors, however, are limited by fixed-frequency paradigms and fail to fully exploit the high-temporal resolution and adaptability of event data. To address these limitations, we propose FlexEvent, a novel framework that enables detection at varying frequencies. Our approach consists of two key components: FlexFuse, an adaptive event-frame fusion module that integrates high-frequency event data with rich semantic information from RGB frames, and FlexTune, a frequency-adaptive fine-tuning mechanism that generates frequency-adjusted labels to enhance model generalization across varying operational frequencies. This combination allows our method to detect objects with high accuracy in both fast-moving and static scenarios, while adapting to dynamic environments. Extensive experiments on large-scale event camera datasets demonstrate that our approach surpasses state-of-the-art methods, achieving significant improvements in both standard and high-frequency settings. Notably, our method maintains robust performance when scaling from 20 Hz to 90 Hz and delivers accurate detection up to 180 Hz, proving its effectiveness in extreme conditions. Our framework sets a new benchmark for event-based object detection and paves the way for more adaptable, real-time vision systems. Code is publicly available.
>
---
#### [replaced 023] OSCAR: One-Step Diffusion Codec for Image Compression Across Multiple Bit-rates
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.16091v3](http://arxiv.org/pdf/2505.16091v3)**

> **作者:** Jinpei Guo; Yifei Ji; Zheng Chen; Kai Liu; Min Liu; Wang Rao; Wenbo Li; Yong Guo; Yulun Zhang
>
> **摘要:** Pretrained latent diffusion models have shown strong potential for lossy image compression, owing to their powerful generative priors. Most existing diffusion-based methods reconstruct images by iteratively denoising from random noise, guided by compressed latent representations. While these approaches have achieved high reconstruction quality, their multi-step sampling process incurs substantial computational overhead. Moreover, they typically require training separate models for different compression bit-rates, leading to significant training and storage costs. To address these challenges, we propose a one-step diffusion codec across multiple bit-rates. termed OSCAR. Specifically, our method views compressed latents as noisy variants of the original latents, where the level of distortion depends on the bit-rate. This perspective allows them to be modeled as intermediate states along a diffusion trajectory. By establishing a mapping from the compression bit-rate to a pseudo diffusion timestep, we condition a single generative model to support reconstructions at multiple bit-rates. Meanwhile, we argue that the compressed latents retain rich structural information, thereby making one-step denoising feasible. Thus, OSCAR replaces iterative sampling with a single denoising pass, significantly improving inference efficiency. Extensive experiments demonstrate that OSCAR achieves superior performance in both quantitative and visual quality metrics. The code and models will be released at https://github.com/jp-guo/OSCAR.
>
---
#### [replaced 024] Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.20569v2](http://arxiv.org/pdf/2505.20569v2)**

> **作者:** Jihoon Lee; Min Song
>
> **备注:** ACL 2025 Findings camera-ready version. Code is released at https://github.com/JiHoonLee9898/RVCD
>
> **摘要:** Despite significant advancements in Large Vision-Language Models, Object Hallucination (OH) remains a persistent challenge. Building upon prior studies on contrastive decoding that address this issue without requiring additional model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an advanced method to suppress OH. RVCD leverages both negative and positive images at the logit level, explicitly referencing AI-generated images designed to represent a single concept. Our approach demonstrates substantial improvements over existing decoding-based methods.
>
---
#### [replaced 025] One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks
- **分类: cs.CV; cs.CR**

- **链接: [http://arxiv.org/pdf/2410.22725v4](http://arxiv.org/pdf/2410.22725v4)**

> **作者:** Ji Guo; Wenbo Jiang; Rui Zhang; Guoming Lu; Hongwei Li
>
> **摘要:** Recently, various types of Text-to-Image (T2I) models have emerged (such as DALL-E and Stable Diffusion), and showing their advantages in different aspects. Therefore, some third-party service platforms collect different model interfaces and provide cheaper API services and more flexibility in T2I model selections. However, this also raises a new security concern: Are these third-party services truly offering the models they claim? To answer this question, we first define the concept of T2I model verification, which aims to determine whether a black-box target model is identical to a given white-box reference T2I model. After that, we propose VerifyPrompt, which performs T2I model verification through a special designed verify prompt. Intuitionally, the verify prompt is an adversarial prompt for the target model without transferability for other models. It makes the target model generate a specific image while making other models produce entirely different images. Specifically, VerifyPrompt utilizes the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize the cosine similarity of a prompt's text encoding, generating verify prompts. Finally, by computing the CLIP-text similarity scores between the prompts the generated images, VerifyPrompt can determine whether the target model aligns with the reference model. Experimental results demonstrate that VerifyPrompt consistently achieves over 90\% accuracy across various T2I models, confirming its effectiveness in practical model platforms (such as Hugging Face).
>
---
#### [replaced 026] An unsupervised method for MRI recovery: Deep image prior with structured sparsity
- **分类: eess.IV; cs.CV; cs.LG; eess.SP**

- **链接: [http://arxiv.org/pdf/2501.01482v3](http://arxiv.org/pdf/2501.01482v3)**

> **作者:** Muhammad Ahmad Sultan; Chong Chen; Yingmin Liu; Katarzyna Gil; Karolina Zareba; Rizwan Ahmad
>
> **备注:** Magn Reson Mater Phy (2025)
>
> **摘要:** Objective: To propose and validate an unsupervised MRI reconstruction method that does not require fully sampled k-space data. Materials and Methods: The proposed method, deep image prior with structured sparsity (DISCUS), extends the deep image prior (DIP) by introducing group sparsity to frame-specific code vectors, enabling the discovery of a low-dimensional manifold for capturing temporal variations. \discus was validated using four studies: (I) simulation of a dynamic Shepp-Logan phantom to demonstrate its manifold discovery capabilities, (II) comparison with compressed sensing and DIP-based methods using simulated single-shot late gadolinium enhancement (LGE) image series from six distinct digital cardiac phantoms in terms of normalized mean square error (NMSE) and structural similarity index measure (SSIM), (III) evaluation on retrospectively undersampled single-shot LGE data from eight patients, and (IV) evaluation on prospectively undersampled single-shot LGE data from eight patients, assessed via blind scoring from two expert readers. Results: DISCUS outperformed competing methods, demonstrating superior reconstruction quality in terms of NMSE and SSIM (Studies I--III) and expert reader scoring (Study IV). Discussion: An unsupervised image reconstruction method is presented and validated on simulated and measured data. These developments can benefit applications where acquiring fully sampled data is challenging.
>
---
#### [replaced 027] It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2503.24129v2](http://arxiv.org/pdf/2503.24129v2)**

> **作者:** Dominik Schnaus; Nikita Araslanov; Daniel Cremers
>
> **备注:** Accepted to CVPR 2025, Project page: https://dominik-schnaus.github.io/itsamatch/
>
> **摘要:** The platonic representation hypothesis suggests that vision and language embeddings become more homogeneous as model and dataset sizes increase. In particular, pairwise distances within each modality become more similar. This suggests that as foundation models mature, it may become possible to match vision and language embeddings in a fully unsupervised fashion, i.e. without parallel data. We present the first feasibility study, and investigate conformity of existing vision and language foundation models in the context of unsupervised, or "blind", matching. First, we formulate unsupervised matching as a quadratic assignment problem and introduce a novel heuristic that outperforms previous solvers. We also develop a technique to find optimal matching problems, for which a non-trivial match is very likely. Second, we conduct an extensive study deploying a range of vision and language models on four datasets. Our analysis reveals that for many problem instances, vision and language representations can be indeed matched without supervision. This finding opens up the exciting possibility of embedding semantic knowledge into other modalities virtually annotation-free. As a proof of concept, we showcase an unsupervised classifier, which achieves non-trivial classification accuracy without any image-text annotation.
>
---
#### [replaced 028] SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2412.04383v2](http://arxiv.org/pdf/2412.04383v2)**

> **作者:** Rong Li; Shijie Li; Lingdong Kong; Xulei Yang; Junwei Liang
>
> **备注:** CVPR 2025; 21 pages, 10 figures, 10 tables; Code at https://seeground.github.io/
>
> **摘要:** 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on textual descriptions, essential for applications like augmented reality and robotics. Traditional 3DVG approaches rely on annotated 3D datasets and predefined object categories, limiting scalability and adaptability. To overcome these limitations, we introduce SeeGround, a zero-shot 3DVG framework leveraging 2D Vision-Language Models (VLMs) trained on large-scale 2D data. SeeGround represents 3D scenes as a hybrid of query-aligned rendered images and spatially enriched text descriptions, bridging the gap between 3D data and 2D-VLMs input formats. We propose two modules: the Perspective Adaptation Module, which dynamically selects viewpoints for query-relevant image rendering, and the Fusion Alignment Module, which integrates 2D images with 3D spatial descriptions to enhance object localization. Extensive experiments on ScanRefer and Nr3D demonstrate that our approach outperforms existing zero-shot methods by large margins. Notably, we exceed weakly supervised methods and rival some fully supervised ones, outperforming previous SOTA by 7.7% on ScanRefer and 7.1% on Nr3D, showcasing its effectiveness in complex 3DVG tasks.
>
---
#### [replaced 029] Diffusion Sampling Correction via Approximately 10 Parameters
- **分类: cs.LG; cs.CV**

- **链接: [http://arxiv.org/pdf/2411.06503v3](http://arxiv.org/pdf/2411.06503v3)**

> **作者:** Guangyi Wang; Wei Peng; Lijiang Li; Wenyu Chen; Yuren Cai; Songzhi Su
>
> **备注:** Accepted at ICML 2025
>
> **摘要:** While powerful for generation, Diffusion Probabilistic Models (DPMs) face slow sampling challenges, for which various distillation-based methods have been proposed. However, they typically require significant additional training costs and model parameter storage, limiting their practicality. In this work, we propose PCA-based Adaptive Search (PAS), which optimizes existing solvers for DPMs with minimal additional costs. Specifically, we first employ PCA to obtain a few basis vectors to span the high-dimensional sampling space, which enables us to learn just a set of coordinates to correct the sampling direction; furthermore, based on the observation that the cumulative truncation error exhibits an ``S"-shape, we design an adaptive search strategy that further enhances the sampling efficiency and reduces the number of stored parameters to approximately 10. Extensive experiments demonstrate that PAS can significantly enhance existing fast solvers in a plug-and-play manner with negligible costs. E.g., on CIFAR10, PAS optimizes DDIM's FID from 15.69 to 4.37 (NFE=10) using only 12 parameters and sub-minute training on a single A100 GPU. Code is available at https://github.com/onefly123/PAS.
>
---
#### [replaced 030] Cross-Modal Causal Intervention for Medical Report Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2303.09117v5](http://arxiv.org/pdf/2303.09117v5)**

> **作者:** Weixing Chen; Yang Liu; Ce Wang; Jiarui Zhu; Guanbin Li; Cheng-Lin Liu; Liang Lin
>
> **备注:** Accepted by IEEE TIP 2025, 16 pages, 11 figures, 7 tables
>
> **摘要:** Radiology Report Generation (RRG) is essential for computer-aided diagnosis and medication guidance, which can relieve the heavy burden of radiologists by automatically generating the corresponding radiology reports according to the given radiology image. However, generating accurate lesion descriptions remains challenging due to spurious correlations from visual-linguistic biases and inherent limitations of radiological imaging, such as low resolution and noise interference. To address these issues, we propose a two-stage framework named CrossModal Causal Representation Learning (CMCRL), consisting of the Radiological Cross-modal Alignment and Reconstruction Enhanced (RadCARE) pre-training and the Visual-Linguistic Causal Intervention (VLCI) fine-tuning. In the pre-training stage, RadCARE introduces a degradation-aware masked image restoration strategy tailored for radiological images, which reconstructs high-resolution patches from low-resolution inputs to mitigate noise and detail loss. Combined with a multiway architecture and four adaptive training strategies (e.g., text postfix generation with degraded images and text prefixes), RadCARE establishes robust cross-modal correlations even with incomplete data. In the VLCI phase, we deploy causal front-door intervention through two modules: the Visual Deconfounding Module (VDM) disentangles local-global features without fine-grained annotations, while the Linguistic Deconfounding Module (LDM) eliminates context bias without external terminology databases. Experiments on IU-Xray and MIMIC-CXR show that our CMCRL pipeline significantly outperforms state-of-the-art methods, with ablation studies confirming the necessity of both stages. Code and models are available at https://github.com/WissingChen/CMCRL.
>
---
#### [replaced 031] Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging
- **分类: cs.CV; cs.CL**

- **链接: [http://arxiv.org/pdf/2505.22150v2](http://arxiv.org/pdf/2505.22150v2)**

> **作者:** Runze Xia; Shuo Feng; Renzhi Wang; Congchi Yin; Xuyun Wen; Piji Li
>
> **备注:** CogSci2025
>
> **摘要:** Brain-to-Image reconstruction aims to recover visual stimuli perceived by humans from brain activity. However, the reconstructed visual stimuli often missing details and semantic inconsistencies, which may be attributed to insufficient semantic information. To address this issue, we propose an approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which employs fine-grained text as bridge to improve image reconstruction. FgB2I comprises three key stages: detail enhancement, decoding fine-grained text descriptions, and text-bridged brain-to-image reconstruction. In the detail-enhancement stage, we leverage large vision-language models to generate fine-grained captions for visual stimuli and experimentally validate its importance. We propose three reward metrics (object accuracy, text-image semantic similarity, and image-image semantic similarity) to guide the language model in decoding fine-grained text descriptions from fMRI signals. The fine-grained text descriptions can be integrated into existing reconstruction methods to achieve fine-grained Brain-to-Image reconstruction.
>
---
#### [replaced 032] How We Won the ISLES'24 Challenge by Preprocessing
- **分类: eess.IV; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.18424v2](http://arxiv.org/pdf/2505.18424v2)**

> **作者:** Tianyi Ren; Juampablo E. Heras Rivera; Hitender Oswal; Yutong Pan; William Henry; Sophie Walters; Mehmet Kurt
>
> **摘要:** Stroke is among the top three causes of death worldwide, and accurate identification of stroke lesion boundaries is critical for diagnosis and treatment. Supervised deep learning methods have emerged as the leading solution for stroke lesion segmentation but require large, diverse, and annotated datasets. The ISLES'24 challenge addresses this need by providing longitudinal stroke imaging data, including CT scans taken on arrival to the hospital and follow-up MRI taken 2-9 days from initial arrival, with annotations derived from follow-up MRI. Importantly, models submitted to the ISLES'24 challenge are evaluated using only CT inputs, requiring prediction of lesion progression that may not be visible in CT scans for segmentation. Our winning solution shows that a carefully designed preprocessing pipeline including deep-learning-based skull stripping and custom intensity windowing is beneficial for accurate segmentation. Combined with a standard large residual nnU-Net architecture for segmentation, this approach achieves a mean test Dice of 28.5 with a standard deviation of 21.27.
>
---
#### [replaced 033] Point Cloud Synthesis Using Inner Product Transforms
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2410.18987v3](http://arxiv.org/pdf/2410.18987v3)**

> **作者:** Ernst Röell; Bastian Rieck
>
> **摘要:** Point-cloud synthesis, i.e. the generation of novel point clouds from an input distribution, remains a challenging task, for which numerous complex machine-learning models have been devised. We develop a novel method that encodes geometrical-topological characteristics of point clouds using inner products, leading to a highly-efficient point cloud representation with provable expressivity properties. Integrated into deep learning models, our encoding exhibits high quality in typical tasks like reconstruction, generation, and interpolation, with inference times orders of magnitude faster than existing methods.
>
---
#### [replaced 034] DreamForge: Motion-Aware Autoregressive Video Generation for Multi-View Driving Scenes
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2409.04003v4](http://arxiv.org/pdf/2409.04003v4)**

> **作者:** Jianbiao Mei; Tao Hu; Xuemeng Yang; Licheng Wen; Yu Yang; Tiantian Wei; Yukai Ma; Min Dou; Botian Shi; Yong Liu
>
> **备注:** 15 figures, 11 tables
>
> **摘要:** Recent advances in diffusion models have improved controllable streetscape generation and supported downstream perception and planning tasks. However, challenges remain in accurately modeling driving scenes and generating long videos. To alleviate these issues, we propose DreamForge, an advanced diffusion-based autoregressive video generation model tailored for 3D-controllable long-term generation. To enhance the lane and foreground generation, we introduce perspective guidance and integrate object-wise position encoding to incorporate local 3D correlation and improve foreground object modeling. We also propose motion-aware temporal attention to capture motion cues and appearance changes in videos. By leveraging motion frames and an autoregressive generation paradigm,we can autoregressively generate long videos (over 200 frames) using a model trained in short sequences, achieving superior quality compared to the baseline in 16-frame video evaluations. Finally, we integrate our method with the realistic simulator DriveArena to provide more reliable open-loop and closed-loop evaluations for vision-based driving agents. Project Page: https://pjlab-adg.github.io/DriveArena/dreamforge.
>
---
#### [replaced 035] Differential Coding for Training-Free ANN-to-SNN Conversion
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.00301v2](http://arxiv.org/pdf/2503.00301v2)**

> **作者:** Zihan Huang; Wei Fang; Tong Bu; Peng Xue; Zecheng Hao; Wenxuan Liu; Yuanhong Tang; Zhaofei Yu; Tiejun Huang
>
> **摘要:** Spiking Neural Networks (SNNs) exhibit significant potential due to their low energy consumption. Converting Artificial Neural Networks (ANNs) to SNNs is an efficient way to achieve high-performance SNNs. However, many conversion methods are based on rate coding, which requires numerous spikes and longer time-steps compared to directly trained SNNs, leading to increased energy consumption and latency. This article introduces differential coding for ANN-to-SNN conversion, a novel coding scheme that reduces spike counts and energy consumption by transmitting changes in rate information rather than rates directly, and explores its application across various layers. Additionally, the threshold iteration method is proposed to optimize thresholds based on activation distribution when converting Rectified Linear Units (ReLUs) to spiking neurons. Experimental results on various Convolutional Neural Networks (CNNs) and Transformers demonstrate that the proposed differential coding significantly improves accuracy while reducing energy consumption, particularly when combined with the threshold iteration method, achieving state-of-the-art performance. The source codes of the proposed method are available at https://github.com/h-z-h-cell/ANN-to-SNN-DCGS.
>
---
#### [replaced 036] CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner
- **分类: cs.GR; cs.CV**

- **链接: [http://arxiv.org/pdf/2405.14979v3](http://arxiv.org/pdf/2405.14979v3)**

> **作者:** Weiyu Li; Jiarui Liu; Hongyu Yan; Rui Chen; Yixun Liang; Xuelin Chen; Ping Tan; Xiaoxiao Long
>
> **备注:** HomePage: https://craftsman3d.github.io/, Code: https://github.com/wyysf-98/CraftsMan3D
>
> **摘要:** We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, irregular mesh topologies, noisy surfaces, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling software. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we employ a 3D native diffusion model, which operates on latent space learned from latent set-based 3D representations, to generate coarse geometries with regular mesh topology in seconds. In particular, this process takes as input a text prompt or a reference image and leverages a powerful multi-view (MV) diffusion model to generate multiple views of the coarse geometry, which are fed into our MV-conditioned 3D diffusion model for generating the 3D geometry, significantly improving robustness and generalizability. Following that, a normal-based geometry refiner is used to significantly enhance the surface details. This refinement can be performed automatically, or interactively with user-supplied edits. Extensive experiments demonstrate that our method achieves high efficacy in producing superior-quality 3D assets compared to existing methods. HomePage: https://craftsman3d.github.io/, Code: https://github.com/wyysf-98/CraftsMan
>
---
#### [replaced 037] Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling
- **分类: cs.LG; cs.CV**

- **链接: [http://arxiv.org/pdf/2502.05743v2](http://arxiv.org/pdf/2502.05743v2)**

> **作者:** Xiao Li; Zekai Zhang; Xiang Li; Siyi Chen; Zhihui Zhu; Peng Wang; Qing Qu
>
> **备注:** First two authors contributed equally
>
> **摘要:** Diffusion models, though originally designed for generative tasks, have demonstrated impressive self-supervised representation learning capabilities. A particularly intriguing phenomenon in these models is the emergence of unimodal representation dynamics, where the quality of learned features peaks at an intermediate noise level. In this work, we conduct a comprehensive theoretical and empirical investigation of this phenomenon. Leveraging the inherent low-dimensionality structure of image data, we theoretically demonstrate that the unimodal dynamic emerges when the diffusion model successfully captures the underlying data distribution. The unimodality arises from an interplay between denoising strength and class confidence across noise scales. Empirically, we further show that, in classification tasks, the presence of unimodal dynamics reliably indicates generalization: it emerges when the model generalizes and gradually transitions to a monotonically decreasing curve as the model begins to memorize the training data.
>
---
#### [replaced 038] Tracking Progress Towards Sustainable Development Goal 6 Using Satellite Imagery
- **分类: cs.CV; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2411.19093v2](http://arxiv.org/pdf/2411.19093v2)**

> **作者:** Othmane Echchabi; Aya Lahlou; Nizar Talty; Josh Malcolm Manto; Ka Leung Lam
>
> **摘要:** Clean water and sanitation are essential for health, well-being, and sustainable development, yet significant global disparities persist. Although the United Nations' Sustainable Development Goal (SDG) 6 clearly defines targets for universal access to clean water and sanitation, limitations in data coverage and openness impede accurate tracking of progress in many countries. To bridge these gaps, this study integrates Afrobarometer survey data, satellite imagery from Landsat 8 and Sentinel-2, and advanced deep learning techniques using Meta's self-supervised Distillation with No Labels (DINO) model to develop a modeling framework for evaluating access to piped water and sewage system across diverse African regions. The modeling framework achieved notable accuracy, with over 96% for piped water and 97% for sewage system access classification. When combined with geospatial population data, validation against official statistics from the United Nations Joint Monitoring Program demonstrated high concordance at the national scale (R2 of 0.95 for piped water access and R2 of 0.85 for sewage system access). The national-level estimates can represent SDG Indicators 6.1.1 and 6.2.1. This approach provides policymakers and stakeholders with an effective, scalable, and cost-efficient tool to pinpoint underserved areas requiring targeted intervention. The methodology developed herein can be adapted for assessing other infrastructure-related SDGs, promoting enhanced monitoring and informed decision-making towards achieving global sustainability objectives.
>
---
#### [replaced 039] NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-ID
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.20001v2](http://arxiv.org/pdf/2505.20001v2)**

> **作者:** Shihao Li; Chenglong Li; Aihua Zheng; Andong Lu; Jin Tang; Jixin Ma
>
> **摘要:** Multi-modal object re-identification (ReID) aims to extract identity features across heterogeneous spectral modalities to enable accurate recognition and retrieval in complex real-world scenarios. However, most existing methods rely on implicit feature fusion structures, making it difficult to model fine-grained recognition strategies under varying challenging conditions. Benefiting from the powerful semantic understanding capabilities of Multi-modal Large Language Models (MLLMs), the visual appearance of an object can be effectively translated into descriptive text. In this paper, we propose a reliable multi-modal caption generation method based on attribute confidence, which significantly reduces the unknown recognition rate of MLLMs in multi-modal semantic generation and improves the quality of generated text. Additionally, we propose a novel ReID framework NEXT, the Multi-grained Mixture of Experts via Text-Modulation for Multi-modal Object Re-Identification. Specifically, we decouple the recognition problem into semantic and structural expert branches to separately capture modality-specific appearance and intrinsic structure. For semantic recognition, we propose the Text-Modulated Semantic-sampling Experts (TMSE), which leverages randomly sampled high-quality semantic texts to modulate expert-specific sampling of multi-modal features and mining intra-modality fine-grained semantic cues. Then, to recognize coarse-grained structure features, we propose the Context-Shared Structure-aware Experts (CSSE) that focuses on capturing the holistic object structure across modalities and maintains inter-modality structural consistency through a soft routing mechanism. Finally, we propose the Multi-Modal Feature Aggregation (MMFA), which adopts a unified feature fusion strategy to simply and effectively integrate semantic and structural expert outputs into the final identity representations.
>
---
#### [replaced 040] Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection
- **分类: cs.LG; cs.CL; cs.CV; cs.IR; cs.MM**

- **链接: [http://arxiv.org/pdf/2502.01699v2](http://arxiv.org/pdf/2502.01699v2)**

> **作者:** Tianlin Zhang; En Yu; Yi Shao; Jiande Sun
>
> **备注:** 13 pages, 6 figures
>
> **摘要:** Multimodal fake news detection has garnered significant attention due to its profound implications for social security. While existing approaches have contributed to understanding cross-modal consistency, they often fail to leverage modal-specific representations and explicit discrepant features. To address these limitations, we propose a Multimodal Inverse Attention Network (MIAN), a novel framework that explores intrinsic discriminative features based on news content to advance fake news detection. Specifically, MIAN introduces a hierarchical learning module that captures diverse intra-modal relationships through local-to-global and local-to-local interactions, thereby generating enhanced unimodal representations to improve the identification of fake news at the intra-modal level. Additionally, a cross-modal interaction module employs a co-attention mechanism to establish and model dependencies between the refined unimodal representations, facilitating seamless semantic integration across modalities. To explicitly extract inconsistency features, we propose an inverse attention mechanism that effectively highlights the conflicting patterns and semantic deviations introduced by fake news in both intra- and inter-modality. Extensive experiments on benchmark datasets demonstrate that MIAN significantly outperforms state-of-the-art methods, underscoring its pivotal contribution to advancing social security through enhanced multimodal fake news detection.
>
---
#### [replaced 041] Robustness-enhanced Myoelectric Control with GAN-based Open-set Recognition
- **分类: cs.CV; cs.HC; eess.SP**

- **链接: [http://arxiv.org/pdf/2412.15819v2](http://arxiv.org/pdf/2412.15819v2)**

> **作者:** Cheng Wang; Ziyang Feng; Pin Zhang; Manjiang Cao; Yiming Yuan; Tengfei Chang
>
> **备注:** 11 pages, 14 figures
>
> **摘要:** Electromyography (EMG) signals are widely used in human motion recognition and medical rehabilitation, yet their variability and susceptibility to noise significantly limit the reliability of myoelectric control systems. Existing recognition algorithms often fail to handle unfamiliar actions effectively, leading to system instability and errors. This paper proposes a novel framework based on Generative Adversarial Networks (GANs) to enhance the robustness and usability of myoelectric control systems by enabling open-set recognition. The method incorporates a GAN-based discriminator to identify and reject unknown actions, maintaining system stability by preventing misclassifications. Experimental evaluations on publicly available and self-collected datasets demonstrate a recognition accuracy of 97.6\% for known actions and a 23.6\% improvement in Active Error Rate (AER) after rejecting unknown actions. The proposed approach is computationally efficient and suitable for deployment on edge devices, making it practical for real-world applications.
>
---
#### [replaced 042] Low-Rank Interconnected Adaptation across Layers
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2407.09946v3](http://arxiv.org/pdf/2407.09946v3)**

> **作者:** Yibo Zhong; Jinman Zhao; Yao Zhou
>
> **备注:** Accepted to ACL 2025 (findings, long paper)
>
> **摘要:** Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning (PEFT) method that learns weight updates $\Delta W = AB$ for pretrained weights $W$ through low-rank adapters $A$ and $B$. While LoRA ensures hardware efficiency, its low-rank weight updates limit adaptation performance. In this paper, we propose low-rank interconnected adaptation across layers (Lily), a novel PEFT method that introduces an interconnected framework with locally shared $A$ and globally shared $B$ experts. This structure eliminates redundant per-layer $AB$ pairs, enabling higher-rank $\Delta W$ with equal or fewer parameters. To enhance expressiveness, we use data-dependent routers to determine $A$-$B$ interconnections, preventing $B$ experts from converging to the same behavior and improving representational power across domains. Experiments across modalities, architectures, and model sizes demonstrate Lily's superior performance and efficiency. GitHub: https://github.com/yibozhong/lily
>
---
#### [replaced 043] UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.21336v2](http://arxiv.org/pdf/2504.21336v2)**

> **作者:** Linshan Wu; Yuxiang Nie; Sunan He; Jiaxin Zhuang; Luyang Luo; Neeraj Mahboobani; Varut Vardhanabhuti; Ronald Cheong Kin Chan; Yifan Peng; Pranav Rajpurkar; Hao Chen
>
> **备注:** The first universal foundation model for grounded biomedical image interpretation
>
> **摘要:** The integration of AI-assisted biomedical image analysis into clinical practice demands AI-generated findings that are not only accurate but also interpretable to clinicians. However, existing biomedical AI models generally lack the ability to simultaneously generate diagnostic findings and localize corresponding biomedical objects. This limitation makes it challenging for clinicians to correlate AI-generated findings with visual evidence (e.g., tiny lesions) in images and interpret the results of AI models. To address this challenge, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation, which is capable of generating accurate diagnostic findings and simultaneously segmenting the corresponding biomedical targets. UniBiomed is based on a novel integration of Multi-modal Large Language Model and Segment Anything Model, which can effectively unify diverse biomedical tasks in universal training for advancing grounded interpretation. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, region annotations, and text descriptions across ten biomedical imaging modalities. Extensive validation on 70 internal and 14 external datasets demonstrated the state-of-the-art performance of UniBiomed in diverse biomedical tasks, including image segmentation, disease recognition, region-aware diagnosis, vision question answering, and report generation. In summary, UniBiomed is a powerful and versatile biomedical foundation model, unlocking the untapped grounded interpretation capability for optimizing AI-assisted biomedical image analysis.
>
---
#### [replaced 044] Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.21649v2](http://arxiv.org/pdf/2505.21649v2)**

> **作者:** Keanu Nichols; Nazia Tasnim; Yuting Yan; Nicholas Ikechukwu; Elva Zou; Deepti Ghadiyaram; Bryan A. Plummer
>
> **摘要:** Object orientation understanding represents a fundamental challenge in visual perception critical for applications like robotic manipulation and augmented reality. Current vision-language benchmarks fail to isolate this capability, often conflating it with positional relationships and general scene understanding. We introduce DORI (Discriminative Orientation Reasoning Intelligence), a comprehensive benchmark establishing object orientation perception as a primary evaluation target. DORI assesses four dimensions of orientation comprehension: frontal alignment, rotational transformations, relative directional relationships, and canonical orientation understanding. Through carefully curated tasks from 11 datasets spanning 67 object categories across synthetic and real-world scenarios, DORI provides insights on how multi-modal systems understand object orientations. Our evaluation of 15 state-of-the-art vision-language models reveals critical limitations: even the best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular orientation judgments, with performance deteriorating for tasks requiring reference frame shifts or compound rotations. These findings demonstrate the need for dedicated orientation representation mechanisms, as models show systematic inability to perform precise angular estimations, track orientation changes across viewpoints, and understand compound rotations - suggesting limitations in their internal 3D spatial representations. As the first diagnostic framework specifically designed for orientation awareness in multimodal systems, DORI offers implications for improving robotic control, 3D scene reconstruction, and human-AI interaction in physical environments. DORI data: https://huggingface.co/datasets/appledora/DORI-Benchmark
>
---
#### [replaced 045] Beyond the Permutation Symmetry of Transformers: The Role of Rotation for Model Fusion
- **分类: cs.LG; cs.CV**

- **链接: [http://arxiv.org/pdf/2502.00264v2](http://arxiv.org/pdf/2502.00264v2)**

> **作者:** Binchi Zhang; Zaiyi Zheng; Zhengzhang Chen; Jundong Li
>
> **备注:** ICML 2025
>
> **摘要:** Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on https://github.com/zhengzaiyi/RotationSymmetry.
>
---
#### [replaced 046] A Benchmark and Evaluation for Real-World Out-of-Distribution Detection Using Vision-Language Models
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2501.18463v3](http://arxiv.org/pdf/2501.18463v3)**

> **作者:** Shiho Noda; Atsuyuki Miyai; Qing Yu; Go Irie; Kiyoharu Aizawa
>
> **备注:** Accepted at ICIP2025 Dataset and Benchmark Track
>
> **摘要:** Out-of-distribution (OOD) detection is a task that detects OOD samples during inference to ensure the safety of deployed models. However, conventional benchmarks have reached performance saturation, making it difficult to compare recent OOD detection methods. To address this challenge, we introduce three novel OOD detection benchmarks that enable a deeper understanding of method characteristics and reflect real-world conditions. First, we present ImageNet-X, designed to evaluate performance under challenging semantic shifts. Second, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing robustness to covariate shifts (feature distribution shifts). Finally, we propose Wilds-FS-X, which extends these evaluations to real-world datasets, offering a more comprehensive testbed. Our experiments reveal that recent CLIP-based OOD detection methods struggle to varying degrees across the three proposed benchmarks, and none of them consistently outperforms the others. We hope the community goes beyond specific benchmarks and includes more challenging conditions reflecting real-world scenarios. The code is https://github.com/hoshi23/OOD-X-Benchmarks.
>
---
#### [replaced 047] Stereo Radargrammetry Using Deep Learning from Airborne SAR Images
- **分类: cs.CV; eess.IV**

- **链接: [http://arxiv.org/pdf/2505.20876v3](http://arxiv.org/pdf/2505.20876v3)**

> **作者:** Tatsuya Sasayama; Shintaro Ito; Koichi Ito; Takafumi Aoki
>
> **备注:** 5 pages, 5 figures, conference IGARSS2025
>
> **摘要:** In this paper, we propose a stereo radargrammetry method using deep learning from airborne Synthetic Aperture Radar (SAR) images. Deep learning-based methods are considered to suffer less from geometric image modulation, while there is no public SAR image dataset used to train such methods. We create a SAR image dataset and perform fine-tuning of a deep learning-based image correspondence method. The proposed method suppresses the degradation of image quality by pixel interpolation without ground projection of the SAR image and divides the SAR image into patches for processing, which makes it possible to apply deep learning. Through a set of experiments, we demonstrate that the proposed method exhibits a wider range and more accurate elevation measurements compared to conventional methods. The project web page is available at: https://gsisaoki.github.io/IGARSS2025_sasayama/
>
---
#### [replaced 048] WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.18686v2](http://arxiv.org/pdf/2505.18686v2)**

> **作者:** Yang Liu; Silin Cheng; Xinwei He; Sebastien Ourselin; Lei Tan; Gen Luo
>
> **备注:** Accepted by CVPR2025
>
> **摘要:** Weakly supervised referring expression comprehension(WREC) and segmentation(WRES) aim to learn object grounding based on a given expression using weak supervision signals like image-text pairs. While these tasks have traditionally been modeled separately, we argue that they can benefit from joint learning in a multi-task framework. To this end, we propose WeakMCN, a novel multi-task collaborative network that effectively combines WREC and WRES with a dual-branch architecture. Specifically, the WREC branch is formulated as anchor-based contrastive learning, which also acts as a teacher to supervise the WRES branch. In WeakMCN, we propose two innovative designs to facilitate multi-task collaboration, namely Dynamic Visual Feature Enhancement(DVFE) and Collaborative Consistency Module(CCM). DVFE dynamically combines various pre-trained visual knowledge to meet different task requirements, while CCM promotes cross-task consistency from the perspective of optimization. Extensive experimental results on three popular REC and RES benchmarks, i.e., RefCOCO, RefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN over state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on RefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also validate the strong generalization ability of WeakMCN in both semi-supervised REC and RES settings against existing methods, e.g., +8.94% for semi-REC and +7.71% for semi-RES on 1% RefCOCO. The code is publicly available at https://github.com/MRUIL/WeakMCN.
>
---
#### [replaced 049] LEAVS: An LLM-based Labeler for Abdominal CT Supervision
- **分类: eess.IV; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2503.13330v2](http://arxiv.org/pdf/2503.13330v2)**

> **作者:** Ricardo Bigolin Lanfredi; Yan Zhuang; Mark Finkelstein; Praveen Thoppey Srinivasan Balamuralikrishna; Luke Krembs; Brandon Khoury; Arthi Reddy; Pritam Mukherjee; Neil M. Rofsky; Ronald M. Summers
>
> **备注:** Early acceptance (top 9% of submissions) for MICCAI 2025
>
> **摘要:** Extracting structured labels from radiology reports has been employed to create vision models to simultaneously detect several types of abnormalities. However, existing works focus mainly on the chest region. Few works have been investigated on abdominal radiology reports due to more complex anatomy and a wider range of pathologies in the abdomen. We propose LEAVS (Large language model Extractor for Abdominal Vision Supervision). This labeler can annotate the certainty of presence and the urgency of seven types of abnormalities for nine abdominal organs on CT radiology reports. To ensure broad coverage, we chose abnormalities that encompass most of the finding types from CT reports. Our approach employs a specialized chain-of-thought prompting strategy for a locally-run LLM using sentence extraction and multiple-choice questions in a tree-based decision system. We demonstrate that the LLM can extract several abnormality types across abdominal organs with an average F1 score of 0.89, significantly outperforming competing labelers and humans. Additionally, we show that extraction of urgency labels achieved performance comparable to human annotations. Finally, we demonstrate that the abnormality labels contain valuable information for training a single vision model that classifies several organs as normal or abnormal. We release our code and structured annotations for a public CT dataset containing over 1,000 CT volumes.
>
---
#### [replaced 050] SIGHT: Synthesizing Image-Text Conditioned and Geometry-Guided 3D Hand-Object Trajectories
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.22869v3](http://arxiv.org/pdf/2503.22869v3)**

> **作者:** Alexey Gavryushin; Alexandros Delitzas; Luc Van Gool; Marc Pollefeys; Kaichun Mo; Xi Wang
>
> **摘要:** When humans grasp an object, they naturally form trajectories in their minds to manipulate it for specific tasks. Modeling hand-object interaction priors holds significant potential to advance robotic and embodied AI systems in learning to operate effectively within the physical world. We introduce SIGHT, a novel task focused on generating realistic and physically plausible 3D hand-object interaction trajectories from a single image and a brief language-based task description. Prior work on hand-object trajectory generation typically relies on textual input that lacks explicit grounding to the target object, or assumes access to 3D object meshes, which are often considerably more difficult to obtain than 2D images. We propose SIGHT-Fusion, a novel diffusion-based image-text conditioned generative model that tackles this task by retrieving the most similar 3D object mesh from a database and enforcing geometric hand-object interaction constraints via a novel inference-time diffusion guidance. We benchmark our model on the HOI4D and H2O datasets, adapting relevant baselines for this novel task. Experiments demonstrate our superior performance in the diversity and quality of generated trajectories, as well as in hand-object interaction geometry metrics.
>
---
#### [replaced 051] Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.22622v3](http://arxiv.org/pdf/2503.22622v3)**

> **作者:** Jangho Park; Taesung Kwon; Jong Chul Ye
>
> **备注:** project page: https://zero4dvid.github.io/
>
> **摘要:** Recently, multi-view or 4D video generation has emerged as a significant research topic. Nonetheless, recent approaches to 4D generation still struggle with fundamental limitations, as they primarily rely on harnessing multiple video diffusion models with additional training or compute-intensive training of a full 4D diffusion model with limited real-world 4D data and large computational costs. To address these challenges, here we propose the first training-free 4D video generation method that leverages the off-the-shelf video diffusion models to generate multi-view videos from a single input video. Our approach consists of two key steps: (1) By designating the edge frames in the spatio-temporal sampling grid as key frames, we first synthesize them using a video diffusion model, leveraging a depth-based warping technique for guidance. This approach ensures structural consistency across the generated frames, preserving spatial and temporal coherence. (2) We then interpolate the remaining frames using a video diffusion model, constructing a fully populated and temporally coherent sampling grid while preserving spatial and temporal consistency. Through this approach, we extend a single video into a multi-view video along novel camera trajectories while maintaining spatio-temporal consistency. Our method is training-free and fully utilizes an off-the-shelf video diffusion model, offering a practical and effective solution for multi-view video generation.
>
---
#### [replaced 052] Theorem-Validated Reverse Chain-of-Thought Problem Generation for Geometric Reasoning
- **分类: cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2410.17885v3](http://arxiv.org/pdf/2410.17885v3)**

> **作者:** Linger Deng; Linghao Zhu; Yuliang Liu; Yu Wang; Qunyi Xie; Jingjing Wu; Gang Zhang; Yingying Zhu; Xiang Bai
>
> **摘要:** Large Multimodal Models (LMMs) face limitations in geometric reasoning due to insufficient Chain of Thought (CoT) image-text training data. While existing approaches leverage template-based or LLM-assisted methods for geometric CoT data creation, they often face challenges in achieving both diversity and precision. To bridge this gap, we introduce a two-stage Theorem-Validated Reverse Chain-of-Thought Reasoning Synthesis (TR-CoT) framework. The first stage, TR-Engine, synthesizes theorem-grounded geometric diagrams with structured descriptions and properties. The second stage, TR-Reasoner, employs reverse reasoning to iteratively refine question-answer pairs by cross-validating geometric properties and description fragments. Our approach expands theorem-type coverage, corrects long-standing misunderstandings, and enhances geometric reasoning. Fine-grained CoT improves theorem understanding and increases logical consistency by 24.5%. Our best models surpass the baselines in MathVista and GeoQA by 10.1% and 4.7%, outperforming advanced closed-source models like GPT-4o.
>
---
#### [replaced 053] Demystifying Catastrophic Forgetting in Two-Stage Incremental Object Detector
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2502.05540v3](http://arxiv.org/pdf/2502.05540v3)**

> **作者:** Qirui Wu; Shizhou Zhang; De Cheng; Yinghui Xing; Di Xu; Peng Wang; Yanning Zhang
>
> **备注:** Accepted in ICML2025
>
> **摘要:** Catastrophic forgetting is a critical chanllenge for incremental object detection (IOD). Most existing methods treat the detector monolithically, relying on instance replay or knowledge distillation without analyzing component-specific forgetting. Through dissection of Faster R-CNN, we reveal a key insight: Catastrophic forgetting is predominantly localized to the RoI Head classifier, while regressors retain robustness across incremental stages. This finding challenges conventional assumptions, motivating us to develop a framework termed NSGP-RePRE. Regional Prototype Replay (RePRE) mitigates classifier forgetting via replay of two types of prototypes: coarse prototypes represent class-wise semantic centers of RoI features, while fine-grained prototypes model intra-class variations. Null Space Gradient Projection (NSGP) is further introduced to eliminate prototype-feature misalignment by updating the feature extractor in directions orthogonal to subspace of old inputs via gradient projection, aligning RePRE with incremental learning dynamics. Our simple yet effective design allows NSGP-RePRE to achieve state-of-the-art performance on the Pascal VOC and MS COCO datasets under various settings. Our work not only advances IOD methodology but also provide pivotal insights for catastrophic forgetting mitigation in IOD. Code is available at \href{https://github.com/fanrena/NSGP-RePRE}{https://github.com/fanrena/NSGP-RePRE} .
>
---
#### [replaced 054] Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.17702v2](http://arxiv.org/pdf/2505.17702v2)**

> **作者:** Xueyang Li; Jiahao Li; Yu Song; Yunzhong Lou; Xiangdong Zhou
>
> **摘要:** The advent of Computer-Aided Design (CAD) generative modeling will significantly transform the design of industrial products. The recent research endeavor has extended into the realm of Large Language Models (LLMs). In contrast to fine-tuning methods, training-free approaches typically utilize the advanced closed-source LLMs, thereby offering enhanced flexibility and efficiency in the development of AI agents for generating CAD parametric models. However, the substantial cost and limitations of local deployment of the top-tier closed-source LLMs pose challenges in practical applications. The Seek-CAD is the pioneer exploration of locally deployed open-source inference LLM DeepSeek-R1 for CAD parametric model generation with a training-free methodology. This study is the first investigation to incorporate both visual and Chain-of-Thought (CoT) feedback within the self-refinement mechanism for generating CAD models. Specifically, the initial generated parametric CAD model is rendered into a sequence of step-wise perspective images, which are subsequently processed by a Vision Language Model (VLM) alongside the corresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation. Then, the feedback is utilized by DeepSeek-R1 to refine the initial generated model for the next round of generation. Moreover, we present an innovative 3D CAD model dataset structured around the SSR (Sketch, Sketch-based feature, and Refinements) triple design paradigm. This dataset encompasses a wide range of CAD commands, thereby aligning effectively with industrial application requirements and proving suitable for the generation of LLMs. Extensive experiments validate the effectiveness of Seek-CAD under various metrics.
>
---
#### [replaced 055] Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.21535v2](http://arxiv.org/pdf/2505.21535v2)**

> **作者:** Yuxin Ren; Maxwell D Collins; Miao Hu; Huanrui Yang
>
> **备注:** 12 pages main paper + 6 pages appendix, 14 figures
>
> **摘要:** While transformers excel across vision and language pretraining tasks, their reliance on attention mechanisms poses challenges for inference efficiency, especially on edge and embedded accelerators with limited parallelism and memory bandwidth. Hinted by the observed redundancy of attention at inference time, we hypothesize that though the model learns complicated token dependency through pretraining, the inference-time sequence-to-sequence mapping in each attention layer is actually ''simple'' enough to be represented with a much cheaper function. In this work, we explore FAR, a Function-preserving Attention Replacement framework that replaces all attention blocks in pretrained transformers with learnable sequence-to-sequence modules, exemplified by an LSTM. FAR optimize a multi-head LSTM architecture with a block-wise distillation objective and a global structural pruning framework to achieve a family of efficient LSTM-based models from pretrained transformers. We validate FAR on the DeiT vision transformer family and demonstrate that it matches the accuracy of the original models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships and the token-to-token correlation learned in the transformer's attention module.
>
---
#### [replaced 056] CVOCSemRPL: Class-Variance Optimized Clustering, Semantic Information Injection and Restricted Pseudo Labeling based Improved Semi-Supervised Few-Shot Learning
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2501.14401v2](http://arxiv.org/pdf/2501.14401v2)**

> **作者:** Souvik Maji; Rhythm Baghel; Pratik Mazumder
>
> **摘要:** Few-shot learning has been extensively explored to address problems where the amount of labeled samples is very limited for some classes. In the semi-supervised few-shot learning setting, substantial quantities of unlabeled samples are available. Such unlabeled samples are generally cheaper to obtain and can be used to improve the few-shot learning performance of the model. Some of the recent methods for this setting rely on clustering to generate pseudo-labels for the unlabeled samples. Since the effectiveness of clustering heavily influences the labeling of the unlabeled samples, it can significantly affect the few-shot learning performance. In this paper, we focus on improving the representation learned by the model in order to improve the clustering and, consequently, the model performance. We propose an approach for semi-supervised few-shot learning that performs a class-variance optimized clustering coupled with a cluster separation tuner in order to improve the effectiveness of clustering the labeled and unlabeled samples in this setting. It also optimizes the clustering-based pseudo-labeling process using a restricted pseudo-labeling approach and performs semantic information injection in order to improve the semi-supervised few-shot learning performance of the model. We experimentally demonstrate that our proposed approach significantly outperforms recent state-of-the-art methods on the benchmark datasets.
>
---
#### [replaced 057] RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.17502v2](http://arxiv.org/pdf/2504.17502v2)**

> **作者:** Aviv Slobodkin; Hagai Taitelbaum; Yonatan Bitton; Brian Gordon; Michal Sokolik; Nitzan Bitton Guetta; Almog Gueta; Royi Rassin; Dani Lischinski; Idan Szpektor
>
> **摘要:** Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability - ranging from enhanced personalization in image generation to consistent character representation in video rendering - progress in this field is limited by the lack of reliable automatic evaluation. Existing methods either assess only one aspect of the task (i.e., textual alignment or subject preservation), misalign with human judgments, or rely on costly API-based evaluation. To address this gap, we introduce RefVNLI, a cost-effective metric that evaluates both textual alignment and subject preservation in a single run. Trained on a large-scale dataset derived from video-reasoning benchmarks and image perturbations, RefVNLI outperforms or statistically matches existing baselines across multiple benchmarks and subject categories (e.g., \emph{Animal}, \emph{Object}), achieving up to 6.4-point gains in textual alignment and 5.9-point gains in subject preservation.
>
---
#### [replaced 058] X2-DFD: A framework for eXplainable and eXtendable Deepfake Detection
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2410.06126v4](http://arxiv.org/pdf/2410.06126v4)**

> **作者:** Yize Chen; Zhiyuan Yan; Guangliang Cheng; Kangran Zhao; Siwei Lyu; Baoyuan Wu
>
> **摘要:** This paper proposes X2-DFD, an eXplainable and eXtendable framework based on multimodal large-language models (MLLMs) for deepfake detection, consisting of three key stages. The first stage, Model Feature Assessment, systematically evaluates the detectability of forgery-related features for the MLLM, generating a prioritized ranking of features based on their intrinsic importance to the model. The second stage, Explainable Dataset Construction, consists of two key modules: Strong Feature Strengthening, which is designed to enhance the model's existing detection and explanation capabilities by reinforcing its well-learned features, and Weak Feature Supplementing, which addresses gaps by integrating specific feature detectors (e.g., low-level artifact analyzers) to compensate for the MLLM's limitations. The third stage, Fine-tuning and Inference, involves fine-tuning the MLLM on the constructed dataset and deploying it for final detection and explanation. By integrating these three stages, our approach enhances the MLLM's strengths while supplementing its weaknesses, ultimately improving both the detectability and explainability. Extensive experiments and ablations, followed by a comprehensive human study, validate the improved performance of our approach compared to the original MLLMs. More encouragingly, our framework is designed to be plug-and-play, allowing it to seamlessly integrate with future more advanced MLLMs and specific feature detectors, leading to continual improvement and extension to face the challenges of rapidly evolving deepfakes.
>
---
#### [replaced 059] Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.14359v3](http://arxiv.org/pdf/2505.14359v3)**

> **作者:** Ruoxin Chen; Junwei Xi; Zhiyuan Yan; Ke-Yue Zhang; Shuang Wu; Jingyi Xie; Xu Chen; Lei Xu; Isabel Guan; Taiping Yao; Shouhong Ding
>
> **备注:** 12 Pages, 9 figures
>
> **摘要:** Existing detectors are often trained on biased datasets, leading to the possibility of overfitting on non-causal image attributes that are spuriously correlated with real/synthetic labels. While these biased features enhance performance on the training data, they result in substantial performance degradation when applied to unbiased datasets. One common solution is to perform dataset alignment through generative reconstruction, matching the semantic content between real and synthetic images. However, we revisit this approach and show that pixel-level alignment alone is insufficient. The reconstructed images still suffer from frequency-level misalignment, which can perpetuate spurious correlations. To illustrate, we observe that reconstruction models tend to restore the high-frequency details lost in real images (possibly due to JPEG compression), inadvertently creating a frequency-level misalignment, where synthetic images appear to have richer high-frequency content than real ones. This misalignment leads to models associating high-frequency features with synthetic labels, further reinforcing biased cues. To resolve this, we propose Dual Data Alignment (DDA), which aligns both the pixel and frequency domains. Moreover, we introduce two new test sets: DDA-COCO, containing DDA-aligned synthetic images for testing detector performance on the most aligned dataset, and EvalGEN, featuring the latest generative models for assessing detectors under new generative architectures such as visual auto-regressive generators. Finally, our extensive evaluations demonstrate that a detector trained exclusively on DDA-aligned MSCOCO could improve across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on in-the-wild benchmarks, highlighting the improved generalizability of unbiased detectors.
>
---
#### [replaced 060] Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?
- **分类: cs.LG; cs.CV; 68T07; I.2.6; I.5.1; I.5.5; I.5.4; I.4.10**

- **链接: [http://arxiv.org/pdf/2503.10632v2](http://arxiv.org/pdf/2503.10632v2)**

> **作者:** Subhajit Maity; Killian Hitsman; Xin Li; Aritra Dutta
>
> **备注:** Preprint, Appendix included
>
> **摘要:** Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep networks, including advanced architectures such as vision Transformers (ViTs). This work asks whether a similar replacement in the attention can bring benefits. In this paper, we design the first learnable attention called Kolmogorov-Arnold Attention (KArAt) for ViTs that can operate on any basis, ranging from Fourier, Wavelets, Splines, to Rational Functions. However, learnable activations in attention cause a memory explosion. To remedy this, we propose a modular version of KArAt that uses a low-rank approximation. By adopting the Fourier basis, Fourier-KArAt and its variants, in some cases, outperform their traditional softmax counterparts, or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We also deploy Fourier KArAt to ConViT and Swin-Transformer, and use it in detection and segmentation with ViT-Det. We dissect these architectures' performance by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and transferability to other datasets. KArAt's learnable activation shows a better attention score across all ViTs, indicating better token-to-token interactions, contributing to better inference. Still, its generalizability does not scale with larger ViTs. However, many factors, including the present computing interface, affect the performance of parameter- and memory-heavy KArAts. We note that the goal of this paper is not to produce efficient attention or challenge the traditional activations; by designing KArAt, we are the first to show that attention can be learned and encourage researchers to explore KArAt in conjunction with more advanced architectures.
>
---
#### [replaced 061] Nexus: An Omni-Perceptive And -Interactive Model for Language, Audio, And Vision
- **分类: cs.MM; cs.CV; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2503.01879v3](http://arxiv.org/pdf/2503.01879v3)**

> **作者:** Che Liu; Yingji Zhang; Dong Zhang; Weijie Zhang; Chenggong Gong; Haohan Li; Yu Lu; Shilin Zhou; Yue Lu; Ziliang Gan; Ziao Wang; Junwei Liao; Haipang Wu; Ji Liu; André Freitas; Qifan Wang; Zenglin Xu; Rongjuncheng Zhang; Yong Dai
>
> **摘要:** This work proposes an industry-level omni-modal large language model (LLM) pipeline that integrates auditory, visual, and linguistic modalities to overcome challenges such as limited tri-modal datasets, high computational costs, and complex feature alignments. Our pipeline consists of three main components: First, a modular framework enabling flexible configuration of various encoder-LLM-decoder architectures. Second, a lightweight training strategy that pre-trains audio-language alignment on the state-of-the-art vision-language model Qwen2.5-VL, thus avoiding the costly pre-training of vision-specific modalities. Third, an audio synthesis pipeline that generates high-quality audio-text data from diverse real-world scenarios, supporting applications such as Automatic Speech Recognition and Speech-to-Speech chat. To this end, we introduce an industry-level omni-modal LLM, Nexus. Extensive experiments validate the efficacy of our pipeline, yielding the following key findings:(1) In the visual understanding task, Nexus exhibits superior performance compared with its backbone model - Qwen2.5-VL-7B, validating the efficiency of our training strategy. (2) Within the English Spoken Question-Answering task, the model achieves better accuracy than the same-period competitor (i.e, MiniCPM-o2.6-7B) in the LLaMA Q. benchmark. (3) In our real-world ASR testset, Nexus achieves outstanding performance, indicating its robustness in real scenarios. (4) In the Speech-to-Text Translation task, our model outperforms Qwen2-Audio-Instruct-7B. (5) In the Text-to-Speech task, based on pretrained vocoder (e.g., Fishspeech1.4 or CosyVoice2.0), Nexus is comparable to its backbone vocoder on Seed-TTS benchmark. (6) An in-depth analysis of tri-modal alignment reveals that incorporating the audio modality enhances representational alignment between vision and language.
>
---
#### [replaced 062] Erasing Concepts, Steering Generations: A Comprehensive Survey of Concept Suppression
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.19398v2](http://arxiv.org/pdf/2505.19398v2)**

> **作者:** Yiwei Xie; Ping Liu; Zheng Zhang
>
> **摘要:** Text-to-Image (T2I) models have demonstrated impressive capabilities in generating high-quality and diverse visual content from natural language prompts. However, uncontrolled reproduction of sensitive, copyrighted, or harmful imagery poses serious ethical, legal, and safety challenges. To address these concerns, the concept erasure paradigm has emerged as a promising direction, enabling the selective removal of specific semantic concepts from generative models while preserving their overall utility. This survey provides a comprehensive overview and in-depth synthesis of concept erasure techniques in T2I diffusion models. We systematically categorize existing approaches along three key dimensions: intervention level, which identifies specific model components targeted for concept removal; optimization structure, referring to the algorithmic strategies employed to achieve suppression; and semantic scope, concerning the complexity and nature of the concepts addressed. This multi-dimensional taxonomy enables clear, structured comparisons across diverse methodologies, highlighting fundamental trade-offs between erasure specificity, generalization, and computational complexity. We further discuss current evaluation benchmarks, standardized metrics, and practical datasets, emphasizing gaps that limit comprehensive assessment, particularly regarding robustness and practical effectiveness. Finally, we outline major challenges and promising future directions, including disentanglement of concept representations, adaptive and incremental erasure strategies, adversarial robustness, and new generative architectures. This survey aims to guide researchers toward safer, more ethically aligned generative models, providing foundational knowledge and actionable recommendations to advance responsible development in generative AI.
>
---
#### [replaced 063] RingMo-Aerial: An Aerial Remote Sensing Foundation Model With Affine Transformation Contrastive Learning
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2409.13366v3](http://arxiv.org/pdf/2409.13366v3)**

> **作者:** Wenhui Diao; Haichen Yu; Kaiyue Kang; Tong Ling; Di Liu; Yingchao Feng; Hanbo Bi; Libo Ren; Xuexue Li; Yongqiang Mao; Xian Sun
>
> **摘要:** Aerial Remote Sensing (ARS) vision tasks pose significant challenges due to the unique characteristics of their viewing angles. Existing research has primarily focused on algorithms for specific tasks, which have limited applicability in a broad range of ARS vision applications. This paper proposes the RingMo-Aerial model, aiming to fill the gap in foundation model research in the field of ARS vision. By introducing the Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) mechanism and an affine transformation-based contrastive learning pre-training method, the model's detection capability for small targets is enhanced and optimized for the tilted viewing angles characteristic of ARS. Furthermore, the ARS-Adapter, an efficient parameter fine-tuning method, is proposed to improve the model's adaptability and effectiveness in various ARS vision tasks. Experimental results demonstrate that RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This indicates the practicality and efficacy of RingMo-Aerial in enhancing the performance of ARS vision tasks.
>
---
#### [replaced 064] Non-rigid Motion Correction for MRI Reconstruction via Coarse-To-Fine Diffusion Models
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.15057v2](http://arxiv.org/pdf/2505.15057v2)**

> **作者:** Frederic Wang; Jonathan I. Tamir
>
> **备注:** ICIP 2025
>
> **摘要:** Magnetic Resonance Imaging (MRI) is highly susceptible to motion artifacts due to the extended acquisition times required for k-space sampling. These artifacts can compromise diagnostic utility, particularly for dynamic imaging. We propose a novel alternating minimization framework that leverages a bespoke diffusion model to jointly reconstruct and correct non-rigid motion-corrupted k-space data. The diffusion model uses a coarse-to-fine denoising strategy to capture large overall motion and reconstruct the lower frequencies of the image first, providing a better inductive bias for motion estimation than that of standard diffusion models. We demonstrate the performance of our approach on both real-world cine cardiac MRI datasets and complex simulated rigid and non-rigid deformations, even when each motion state is undersampled by a factor of 64x. Additionally, our method is agnostic to sampling patterns, anatomical variations, and MRI scanning protocols, as long as some low frequency components are sampled during each motion state.
>
---
#### [replaced 065] Diffusion Classifiers Understand Compositionality, but Conditions Apply
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.17955v2](http://arxiv.org/pdf/2505.17955v2)**

> **作者:** Yujin Jeong; Arnas Uselis; Seong Joon Oh; Anna Rohrbach
>
> **摘要:** Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at https://github.com/eugene6923/Diffusion-Classifiers-Compositionality.
>
---
#### [replaced 066] The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.22499v2](http://arxiv.org/pdf/2505.22499v2)**

> **作者:** Aixuan Li; Mochu Xiang; Jing Zhang; Yuchao Dai
>
> **摘要:** 3D object detection is a critical component in autonomous driving systems. It allows real-time recognition and detection of vehicles, pedestrians and obstacles under varying environmental conditions. Among existing methods, 3D object detection in the Bird's Eye View (BEV) has emerged as the mainstream framework. To guarantee a safe, robust and trustworthy 3D object detection, 3D adversarial attacks are investigated, where attacks are placed in 3D environments to evaluate the model performance, e.g. putting a film on a car, clothing a pedestrian. The vulnerability of 3D object detection models to 3D adversarial attacks serves as an important indicator to evaluate the robustness of the model against perturbations. To investigate this vulnerability, we generate non-invasive 3D adversarial objects tailored for real-world attack scenarios. Our method verifies the existence of universal adversarial objects that are spatially consistent across time and camera views. Specifically, we employ differentiable rendering techniques to accurately model the spatial relationship between adversarial objects and the target vehicle. Furthermore, we introduce an occlusion-aware module to enhance visual consistency and realism under different viewpoints. To maintain attack effectiveness across multiple frames, we design a BEV spatial feature-guided optimization strategy. Experimental results demonstrate that our approach can reliably suppress vehicle predictions from state-of-the-art 3D object detectors, serving as an important tool to test robustness of 3D object detection models before deployment. Moreover, the generated adversarial objects exhibit strong generalization capabilities, retaining its effectiveness at various positions and distances in the scene.
>
---
#### [replaced 067] Exploring Disentangled and Controllable Human Image Synthesis: From End-to-End to Stage-by-Stage
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.19486v3](http://arxiv.org/pdf/2503.19486v3)**

> **作者:** Zhengwentai Sun; Chenghong Li; Hongjie Liao; Xihe Yang; Keru Zheng; Heyuan Li; Yihao Zhi; Shuliang Ning; Shuguang Cui; Xiaoguang Han
>
> **备注:** The authors are performing a major restructuring of the paper's methodology and experiments. To avoid citation of an outdated or incomplete version, we prefer to withdraw this version. A revised version may be posted in the future
>
> **摘要:** Achieving fine-grained controllability in human image synthesis is a long-standing challenge in computer vision. Existing methods primarily focus on either facial synthesis or near-frontal body generation, with limited ability to simultaneously control key factors such as viewpoint, pose, clothing, and identity in a disentangled manner. In this paper, we introduce a new disentangled and controllable human synthesis task, which explicitly separates and manipulates these four factors within a unified framework. We first develop an end-to-end generative model trained on MVHumanNet for factor disentanglement. However, the domain gap between MVHumanNet and in-the-wild data produces unsatisfactory results, motivating the exploration of virtual try-on (VTON) dataset as a potential solution. Through experiments, we observe that simply incorporating the VTON dataset as additional data to train the end-to-end model degrades performance, primarily due to the inconsistency in data forms between the two datasets, which disrupts the disentanglement process. To better leverage both datasets, we propose a stage-by-stage framework that decomposes human image generation into three sequential steps: clothed A-pose generation, back-view synthesis, and pose and view control. This structured pipeline enables better dataset utilization at different stages, significantly improving controllability and generalization, especially for in-the-wild scenarios. Extensive experiments demonstrate that our stage-by-stage approach outperforms end-to-end models in both visual fidelity and disentanglement quality, offering a scalable solution for real-world tasks. Additional demos are available on the project page: https://taited.github.io/discohuman-project/.
>
---
#### [replaced 068] Safeguarding AI in Medical Imaging: Post-Hoc Out-of-Distribution Detection with Normalizing Flows
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2502.11638v2](http://arxiv.org/pdf/2502.11638v2)**

> **作者:** Dariush Lotfi; Mohammad-Ali Nikouei Mahani; Mohamad Koohi-Moghadam; Kyongtae Ty Bae
>
> **摘要:** In AI-driven medical imaging, the failure to detect out-of-distribution (OOD) data poses a severe risk to clinical reliability, potentially leading to critical diagnostic errors. Current OOD detection methods often demand impractical retraining or modifications to pre-trained models, hindering their adoption in regulated clinical environments. To address this challenge, we propose a post-hoc normalizing flow-based approach that seamlessly integrates with existing pre-trained models without altering their weights. Our evaluation used a novel in-house built dataset, MedOOD, meticulously curated to simulate clinically relevant distributional shifts, alongside the MedMNIST benchmark dataset. On our in-house MedOOD dataset, our method achieved an AUROC of 84.61%, outperforming state-of-the-art methods like ViM (80.65%) and MDS (80.87%). Similarly, on MedMNIST, it reached an exceptional AUROC of 93.8%, surpassing leading approaches such as ViM (88.08%) and ReAct (87.05%). This superior performance, coupled with its post-hoc integration capability, positions our method as a vital safeguard for enhancing safety in medical imaging workflows. The model and code to build OOD datasets are publicly accessible at https://github.com/dlotfi/MedOODFlow.
>
---
#### [replaced 069] YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction
- **分类: cs.CV; q-bio.QM**

- **链接: [http://arxiv.org/pdf/2505.22250v2](http://arxiv.org/pdf/2505.22250v2)**

> **作者:** Mingzhuang Wang; Yvyang Li; Xiyang Zhang; Fei Tan; Qi Shi; Guotao Zhang; Siqi Chen; Yufei Liu; Lei Lei; Ming Zhou; Qiang Lin; Hongqiang Yang
>
> **摘要:** Coral reefs, crucial for sustaining marine biodiversity and ecological processes (e.g., nutrient cycling, habitat provision), face escalating threats, underscoring the need for efficient monitoring. Coral reef ecological monitoring faces dual challenges of low efficiency in manual analysis and insufficient segmentation accuracy in complex underwater scenarios. This study develops the YH-MINER system, establishing an intelligent framework centered on the Multimodal Large Model (MLLM) for "object detection-semantic segmentation-prior input". The system uses the object detection module (mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the segment module to complete pixel-level segmentation in low-light and densely occluded scenarios. The segmentation masks and finetuned classification instructions are fed into the Qwen2-VL-based multimodal model as prior inputs, achieving a genus-level classification accuracy of 88% and simultaneously extracting core ecological metrics. Meanwhile, the system retains the scalability of the multimodal model through standardized interfaces, laying a foundation for future integration into multimodal agent-based underwater robots and supporting the full-process automation of "image acquisition-prior generation-real-time analysis".
>
---
#### [replaced 070] GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2505.22421v2](http://arxiv.org/pdf/2505.22421v2)**

> **作者:** Anthony Chen; Wenzhao Zheng; Yida Wang; Xueyang Zhang; Kun Zhan; Peng Jia; Kurt Keutzer; Shanghang Zhang
>
> **备注:** code will be released at https://github.com/antonioo-c/GeoDrive
>
> **摘要:** Recent advancements in world models have revolutionized dynamic environment simulation, allowing systems to foresee future states and assess potential actions. In autonomous driving, these capabilities help vehicles anticipate the behavior of other road users, perform risk-aware planning, accelerate training in simulation, and adapt to novel scenarios, thereby enhancing safety and reliability. Current approaches exhibit deficiencies in maintaining robust 3D geometric consistency or accumulating artifacts during occlusion handling, both critical for reliable safety assessment in autonomous navigation tasks. To address this, we introduce GeoDrive, which explicitly integrates robust 3D geometry conditions into driving world models to enhance spatial understanding and action controllability. Specifically, we first extract a 3D representation from the input frame and then obtain its 2D rendering based on the user-specified ego-car trajectory. To enable dynamic modeling, we propose a dynamic editing module during training to enhance the renderings by editing the positions of the vehicles. Extensive experiments demonstrate that our method significantly outperforms existing models in both action accuracy and 3D spatial awareness, leading to more realistic, adaptable, and reliable scene modeling for safer autonomous driving. Additionally, our model can generalize to novel trajectories and offers interactive scene editing capabilities, such as object editing and object trajectory control.
>
---
#### [replaced 071] QMamba: On First Exploration of Vision Mamba for Image Quality Assessment
- **分类: cs.CV; eess.IV**

- **链接: [http://arxiv.org/pdf/2406.09546v2](http://arxiv.org/pdf/2406.09546v2)**

> **作者:** Fengbin Guan; Xin Li; Zihao Yu; Yiting Lu; Zhibo Chen
>
> **备注:** Accepted by ICML 2025
>
> **摘要:** In this work, we take the first exploration of the recently popular foundation model, i.e., State Space Model/Mamba, in image quality assessment (IQA), aiming at observing and excavating the perception potential in vision Mamba. A series of works on Mamba has shown its significant potential in various fields, e.g., segmentation and classification. However, the perception capability of Mamba remains under-explored. Consequently, we propose QMamba by revisiting and adapting the Mamba model for three crucial IQA tasks, i.e., task-specific, universal, and transferable IQA, which reveals its clear advantages over existing foundational models, e.g., Swin Transformer, ViT, and CNNs, in terms of perception and computational cost. To improve the transferability of QMamba, we propose the StylePrompt tuning paradigm, where lightweight mean and variance prompts are injected to assist task-adaptive transfer learning of pre-trained QMamba for different downstream IQA tasks. Compared with existing prompt tuning strategies, our StylePrompt enables better perceptual transfer with lower computational cost. Extensive experiments on multiple synthetic, authentic IQA datasets, and cross IQA datasets demonstrate the effectiveness of our proposed QMamba. The code will be available at: https://github.com/bingo-G/QMamba.git
>
---
#### [replaced 072] Rethinking Positive Pairs in Contrastive Learning
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2410.18200v2](http://arxiv.org/pdf/2410.18200v2)**

> **作者:** Jiantao Wu; Sara Atito; Zhenhua Feng; Shentong Mo; Josef Kitler; Muhammad Awais
>
> **摘要:** The training methods in AI do involve semantically distinct pairs of samples. However, their role typically is to enhance the between class separability. The actual notion of similarity is normally learned from semantically identical pairs. This paper presents SimLAP: a simple framework for learning visual representation from arbitrary pairs. SimLAP explores the possibility of learning similarity from semantically distinct sample pairs. The approach is motivated by the observation that for any pair of classes there exists a subspace in which semantically distinct samples exhibit similarity. This phenomenon can be exploited for a novel method of learning, which optimises the similarity of an arbitrary pair of samples, while simultaneously learning the enabling subspace. The feasibility of the approach will be demonstrated experimentally and its merits discussed.
>
---
#### [replaced 073] From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.12821v4](http://arxiv.org/pdf/2503.12821v4)**

> **作者:** Mingyang Song; Xiaoye Qu; Jiawei Zhou; Yu Cheng
>
> **备注:** Accepted by CVPR 2025. Project Page: https://vlmlt.github.io/
>
> **摘要:** Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an $\textbf{A}$daptive $\textbf{D}$ata $\textbf{R}$efinement Framework ($\textbf{ADR}$), which consists of two stages: $\textbf{D}$ata $\textbf{R}$ebalancing ($\textbf{DR}$) and $\textbf{D}$ata $\textbf{S}$ynthesis ($\textbf{DS}$). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.
>
---
#### [replaced 074] Weight Space Representation Learning on Diverse NeRF Architectures
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2502.09623v2](http://arxiv.org/pdf/2502.09623v2)**

> **作者:** Francesco Ballerini; Pierluigi Zama Ramirez; Samuele Salti; Luigi Di Stefano
>
> **备注:** v2: added third NeRF architecture. Under review
>
> **摘要:** Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for representing 3D objects and scenes by encoding shape and appearance information into the weights of a neural network. Recent studies have demonstrated that these weights can be used as input for frameworks designed to address deep learning tasks; however, such frameworks require NeRFs to adhere to a specific, predefined architecture. In this paper, we introduce the first framework capable of processing NeRFs with diverse architectures and performing inference on architectures unseen at training time. We achieve this by training a Graph Meta-Network within an unsupervised representation learning framework, and show that a contrastive objective is conducive to obtaining an architecture-agnostic latent space. In experiments conducted across 13 NeRF architectures belonging to three families (MLPs, tri-planes, and, for the first time, hash tables), our approach demonstrates robust performance in classification and retrieval tasks involving multiple architectures, even unseen at training time, while also exceeding the results of existing frameworks limited to single architectures.
>
---
#### [replaced 075] Minute-Long Videos with Dual Parallelisms
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.21070v2](http://arxiv.org/pdf/2505.21070v2)**

> **作者:** Zeqing Wang; Bowen Zheng; Xingyi Yang; Zhenxiong Tan; Yuecong Xu; Xinchao Wang
>
> **备注:** The code is available at https://github.com/DualParal-Project/DualParal
>
> **摘要:** Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54$\times$ lower latency and 1.48$\times$ lower memory cost on 8$\times$RTX 4090 GPUs.
>
---
#### [replaced 076] Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation
- **分类: cs.CV; cs.AI; cs.CL; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.21956v2](http://arxiv.org/pdf/2505.21956v2)**

> **作者:** Mengdan Zhu; Senhao Cheng; Guangji Bai; Yifei Zhang; Liang Zhao
>
> **摘要:** Text-to-image generation increasingly demands access to domain-specific, fine-grained, and rapidly evolving knowledge that pretrained models cannot fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to address this by retrieving globally relevant images, but they fail when no single image contains all desired elements from a complex user query. We propose Cross-modal RAG, a novel framework that decomposes both queries and images into sub-dimensional components, enabling subquery-aware retrieval and generation. Our method introduces a hybrid retrieval strategy - combining a sub-dimensional sparse retriever with a dense retriever - to identify a Pareto-optimal set of images, each contributing complementary aspects of the query. During generation, a multimodal large language model is guided to selectively condition on relevant visual features aligned to specific subqueries, ensuring subquery-aware image synthesis. Extensive experiments on MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal RAG significantly outperforms existing baselines in both retrieval and generation quality, while maintaining high efficiency.
>
---
#### [replaced 077] SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2307.07333v3](http://arxiv.org/pdf/2307.07333v3)**

> **作者:** Zhili Ng; Haozhe Wang; Zhengshen Zhang; Francis Tay Eng Hock; Marcelo H. Ang Jr
>
> **备注:** Camera-ready version for SynData4CV Workshop @ CVPR 2025. 18 Pages, 11 figures
>
> **摘要:** In this work, we present SynTable, a unified and flexible Python-based dataset generator built using NVIDIA's Isaac Sim Replicator Composer for generating high-quality synthetic datasets for unseen object amodal instance segmentation of cluttered tabletop scenes. Our dataset generation tool can render complex 3D scenes containing object meshes, materials, textures, lighting, and backgrounds. Metadata, such as modal and amodal instance segmentation masks, object amodal RGBA instances, occlusion masks, depth maps, bounding boxes, and material properties can be automatically generated to annotate the scene according to the users' requirements. Our tool eliminates the need for manual labeling in the dataset generation process while ensuring the quality and accuracy of the dataset. In this work, we discuss our design goals, framework architecture, and the performance of our tool. We demonstrate the use of a sample dataset generated using SynTable for training a state-of-the-art model, UOAIS-Net. Our state-of-the-art results show significantly improved performance in Sim-to-Real transfer when evaluated on the OSD-Amodal dataset. We offer this tool as an open-source, easy-to-use, photorealistic dataset generator for advancing research in deep learning and synthetic data generation. The links to our source code, demonstration video, and sample dataset can be found in the supplementary materials.
>
---
#### [replaced 078] GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning
- **分类: cs.CV; cs.CL**

- **链接: [http://arxiv.org/pdf/2505.21863v2](http://arxiv.org/pdf/2505.21863v2)**

> **作者:** Shikhhar Siingh; Abhinav Rawat; Chitta Baral; Vivek Gupta
>
> **摘要:** Publicly significant images from events hold valuable contextual information, crucial for journalism and education. However, existing methods often struggle to extract this relevance accurately. To address this, we introduce GETReason (Geospatial Event Temporal Reasoning), a framework that moves beyond surface-level image descriptions to infer deeper contextual meaning. We propose that extracting global event, temporal, and geospatial information enhances understanding of an image's significance. Additionally, we introduce GREAT (Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric for evaluating reasoning-based image understanding. Our layered multi-agent approach, assessed using a reasoning-weighted metric, demonstrates that meaningful insights can be inferred, effectively linking images to their broader event context.
>
---
#### [replaced 079] GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats
- **分类: cs.RO; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.10923v2](http://arxiv.org/pdf/2505.10923v2)**

> **作者:** Simeon Adebola; Shuangyu Xie; Chung Min Kim; Justin Kerr; Bart M. van Marrewijk; Mieke van Vlaardingen; Tim van Daalen; E. N. van Loo; Jose Luis Susa Rincon; Eugen Solowjow; Rick van de Zedde; Ken Goldberg
>
> **摘要:** Accurate temporal reconstructions of plant growth are essential for plant phenotyping and breeding, yet remain challenging due to complex geometries, occlusions, and non-rigid deformations of plants. We present a novel framework for building temporal digital twins of plants by combining 3D Gaussian Splatting with a robust sample alignment pipeline. Our method begins by reconstructing Gaussian Splats from multi-view camera data, then leverages a two-stage registration approach: coarse alignment through feature-based matching and Fast Global Registration, followed by fine alignment with Iterative Closest Point. This pipeline yields a consistent 4D model of plant development in discrete time steps. We evaluate the approach on data from the Netherlands Plant Eco-phenotyping Center, demonstrating detailed temporal reconstructions of Sequoia and Quinoa species. Videos and Images can be seen at https://berkeleyautomation.github.io/GrowSplat/
>
---
#### [replaced 080] CellFlux: Simulating Cellular Morphology Changes via Flow Matching
- **分类: q-bio.QM; cs.CV; cs.LG; q-bio.BM; q-bio.CB**

- **链接: [http://arxiv.org/pdf/2502.09775v2](http://arxiv.org/pdf/2502.09775v2)**

> **作者:** Yuhui Zhang; Yuchang Su; Chenyu Wang; Tianhong Li; Zoe Wefers; Jeffrey Nirschl; James Burgess; Daisy Ding; Alejandro Lozano; Emma Lundberg; Serena Yeung-Levy
>
> **备注:** Published at ICML 2025
>
> **摘要:** Building a virtual cell capable of accurately simulating cellular behaviors in silico has long been a dream in computational biology. We introduce CellFlux, an image-generative model that simulates cellular morphology changes induced by chemical and genetic perturbations using flow matching. Unlike prior methods, CellFlux models distribution-wise transformations from unperturbed to perturbed cell states, effectively distinguishing actual perturbation effects from experimental artifacts such as batch effects -- a major challenge in biological data. Evaluated on chemical (BBBC021), genetic (RxRx1), and combined perturbation (JUMP) datasets, CellFlux generates biologically meaningful cell images that faithfully capture perturbation-specific morphological changes, achieving a 35% improvement in FID scores and a 12% increase in mode-of-action prediction accuracy over existing methods. Additionally, CellFlux enables continuous interpolation between cellular states, providing a potential tool for studying perturbation dynamics. These capabilities mark a significant step toward realizing virtual cell modeling for biomedical research. Project page: https://yuhui-zh15.github.io/CellFlux/.
>
---
#### [replaced 081] NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks
- **分类: cs.LG; cs.CV; 68T07**

- **链接: [http://arxiv.org/pdf/2401.13330v2](http://arxiv.org/pdf/2401.13330v2)**

> **作者:** Matteo Gambella; Jary Pomponi; Simone Scardapane; Manuel Roveri
>
> **备注:** 14 pages, 5 figures
>
> **摘要:** Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS), the first NAS framework for the design of optimal EENNs satisfying constraints on the accuracy and the number of Multiply and Accumulate (MAC) operations performed by the EENNs at inference time. In particular, this provides the joint design of backbone and EECs to select a set of admissible (i.e., respecting the constraints) Pareto Optimal Solutions in terms of best tradeoff between the accuracy and number of MACs. The results show that the models designed by NACHOS are competitive with the state-of-the-art EENNs. Additionally, this work investigates the effectiveness of two novel regularization terms designed for the optimization of the auxiliary classifiers of the EENN
>
---
#### [replaced 082] Audio Visual Segmentation Through Text Embeddings
- **分类: cs.CV; cs.AI; cs.MM**

- **链接: [http://arxiv.org/pdf/2502.16359v2](http://arxiv.org/pdf/2502.16359v2)**

> **作者:** Kyungbok Lee; You Zhang; Zhiyao Duan
>
> **摘要:** The goal of Audio-Visual Segmentation (AVS) is to localize and segment the sounding source objects from video frames. Research on AVS suffers from data scarcity due to the high cost of fine-grained manual annotations. Recent works attempt to overcome the challenge of limited data by leveraging the vision foundation model, Segment Anything Model (SAM), prompting it with audio to enhance its ability to segment sounding source objects. While this approach alleviates the model's burden on understanding visual modality by utilizing knowledge of pre-trained SAM, it does not address the fundamental challenge of learning audio-visual correspondence with limited data. To address this limitation, we propose \textbf{AV2T-SAM}, a novel framework that bridges audio features with the text embedding space of pre-trained text-prompted SAM. Our method leverages multimodal correspondence learned from rich text-image paired datasets to enhance audio-visual alignment. Furthermore, we introduce a novel feature, $\mathbf{\textit{\textbf{f}}_{CLIP} \odot \textit{\textbf{f}}_{CLAP}}$, which emphasizes shared semantics of audio and visual modalities while filtering irrelevant noise. Our approach outperforms existing methods on the AVSBench dataset by effectively utilizing pre-trained segmentation models and cross-modal semantic alignment. The source code is released at https://github.com/bok-bok/AV2T-SAM.
>
---
#### [replaced 083] UDHF2-Net: Uncertainty-diffusion-model-based High-Frequency TransFormer Network for Remotely Sensed Imagery Interpretation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2406.16129v3](http://arxiv.org/pdf/2406.16129v3)**

> **作者:** Pengfei Zhang; Chang Li; Yongjun Zhang; Rongjun Qin; Kyle Gao; Jonathan Li
>
> **摘要:** Remotely sensed imagery interpretation (RSII) faces the three major problems: (1) objective representation of spatial distribution patterns; (2) edge uncertainty problem caused by downsampling encoder and intrinsic edge noises (e.g., mixed pixel and edge occlusion etc.); and (3) false detection problem caused by geometric registration error in change detection. To solve the aforementioned problems, uncertainty-diffusion-model-based high-Frequency TransFormer network (UDHF2-Net) is the first to be proposed, whose superiorities are as follows: (1) a spatially-stationary-and-non-stationary high-frequency connection paradigm (SHCP) is proposed to enhance the interaction of spatially frequency-wise stationary and non-stationary features to yield high-fidelity edge extraction result. Inspired by HRFormer, SHCP proposes high-frequency-wise stream to replace high-resolution-wise stream in HRFormer through the whole encoder-decoder process with parallel frequency-wise high-to-low streams, so it improves the edge extraction accuracy by continuously remaining high-frequency information; (2) a mask-and-geo-knowledge-based uncertainty diffusion module (MUDM), which is a self-supervised learning strategy, is proposed to improve the edge accuracy of extraction and change detection by gradually removing the simulated spectrum noises based on geo-knowledge and the generated diffused spectrum noises; (3) a frequency-wise semi-pseudo-Siamese UDHF2-Net is the first to be proposed to balance accuracy and complexity for change detection. Besides the aforementioned spectrum noises in semantic segmentation, MUDM is also a self-supervised learning strategy to effectively reduce the edge false change detection from the generated imagery with geometric registration error.
>
---
#### [replaced 084] Benchmarking YOLOv8 for Optimal Crack Detection in Civil Infrastructure
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2501.06922v2](http://arxiv.org/pdf/2501.06922v2)**

> **作者:** Woubishet Zewdu Taffese; Ritesh Sharma; Mohammad Hossein Afsharmovahed; Gunasekaran Manogaran; Genda Chen
>
> **备注:** We would like to extend/modify this work and make changes for resubmission to a different place. Hence we would like to withdraw the paper
>
> **摘要:** Ensuring the structural integrity and safety of bridges is crucial for the reliability of transportation networks and public safety. Traditional crack detection methods are increasingly being supplemented or replaced by advanced artificial intelligence (AI) techniques. However, most of the models rely on two-stage target detection algorithms, which pose concerns for real-time applications due to their lower speed. While models such as YOLO (You Only Look Once) have emerged as transformative tools due to their remarkable speed and accuracy. However, the potential of the latest YOLOv8 framework in this domain remains underexplored. This study bridges that gap by rigorously evaluating YOLOv8's performance across five model scales (nano, small, medium, large, and extra-large) using a high-quality Roboflow dataset. A comprehensive hyperparameter optimization was performed, testing six state-of-the-art optimizers-Stochastic Gradient Descent, Adaptive Moment Estimation, Adam with Decoupled Weight Decay, Root Mean Square Propagation, Rectified Adam, and Nesterov-accelerated Adam. Results revealed that YOLOv8, optimized with Stochastic Gradient Descent, delivered exceptional accuracy and speed, setting a new benchmark for real-time crack detection. Beyond its immediate application, this research positions YOLOv8 as a foundational approach for integrating advanced computer vision techniques into infrastructure monitoring. By enabling more reliable and proactive maintenance of aging bridge networks, this work paves the way for safer, more efficient transportation systems worldwide.
>
---
#### [replaced 085] Circumventing shortcuts in audio-visual deepfake detection datasets with unsupervised learning
- **分类: cs.CV; cs.LG; cs.SD; eess.AS; eess.IV**

- **链接: [http://arxiv.org/pdf/2412.00175v3](http://arxiv.org/pdf/2412.00175v3)**

> **作者:** Stefan Smeu; Dragos-Alexandru Boldisor; Dan Oneata; Elisabeta Oneata
>
> **备注:** Accepted as a highlight paper at the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025
>
> **摘要:** Good datasets are essential for developing and benchmarking any machine learning system. Their importance is even more extreme for safety critical applications such as deepfake detection - the focus of this paper. Here we reveal that two of the most widely used audio-video deepfake datasets suffer from a previously unidentified spurious feature: the leading silence. Fake videos start with a very brief moment of silence and based on this feature alone, we can separate the real and fake samples almost perfectly. As such, previous audio-only and audio-video models exploit the presence of silence in the fake videos and consequently perform worse when the leading silence is removed. To circumvent latching on such unwanted artifact and possibly other unrevealed ones we propose a shift from supervised to unsupervised learning by training models exclusively on real data. We show that by aligning self-supervised audio-video representations we remove the risk of relying on dataset-specific biases and improve robustness in deepfake detection.
>
---
#### [replaced 086] Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from Skin Surface
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.22511v2](http://arxiv.org/pdf/2505.22511v2)**

> **作者:** Siyeop Yoon; Yujin Oh; Pengfei Jin; Sifan Song; Matthew Tivnan; Dufan Wu; Xiang Li; Quanzheng Li
>
> **备注:** Neurips 2025 submitted
>
> **摘要:** We present Surf2CT, a novel cascaded flow matching framework that synthesizes full 3D computed tomography (CT) volumes of the human torso from external surface scans and simple demographic data (age, sex, height, weight). This is the first approach capable of generating realistic volumetric internal anatomy images solely based on external body shape and demographics, without any internal imaging. Surf2CT proceeds through three sequential stages: (1) Surface Completion, reconstructing a complete signed distance function (SDF) from partial torso scans using conditional 3D flow matching; (2) Coarse CT Synthesis, generating a low-resolution CT volume from the completed SDF and demographic information; and (3) CT Super-Resolution, refining the coarse volume into a high-resolution CT via a patch-wise conditional flow model. Each stage utilizes a 3D-adapted EDM2 backbone trained via flow matching. We trained our model on a combined dataset of 3,198 torso CT scans (approximately 1.13 million axial slices) sourced from Massachusetts General Hospital (MGH) and the AutoPET challenge. Evaluation on 700 paired torso surface-CT cases demonstrated strong anatomical fidelity: organ volumes exhibited small mean percentage differences (range from -11.1% to 4.4%), and muscle/fat body composition metrics matched ground truth with strong correlation (range from 0.67 to 0.96). Lung localization had minimal bias (mean difference -2.5 mm), and surface completion significantly improved metrics (Chamfer distance: from 521.8 mm to 2.7 mm; Intersection-over-Union: from 0.87 to 0.98). Surf2CT establishes a new paradigm for non-invasive internal anatomical imaging using only external data, opening opportunities for home-based healthcare, preventive medicine, and personalized clinical assessments without the risks associated with conventional imaging techniques.
>
---
#### [replaced 087] Dataset Distillation of 3D Point Clouds via Distribution Matching
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.22154v2](http://arxiv.org/pdf/2503.22154v2)**

> **作者:** Jae-Young Yim; Dongwook Kim; Jae-Young Sim
>
> **摘要:** Large-scale datasets are usually required to train deep neural networks, but it increases the computational complexity hindering the practical applications. Recently, dataset distillation for images and texts has been attracting a lot of attention, that reduces the original dataset to a synthetic dataset to alleviate the computational burden of training while preserving essential task-relevant information. However, the dataset distillation for 3D point clouds remains largely unexplored, as the point clouds exhibit fundamentally different characteristics from that of images, making the dataset distillation more challenging. In this paper, we propose a distribution matching-based distillation framework for 3D point clouds that jointly optimizes the geometric structures as well as the orientations of the synthetic 3D objects. To address the semantic misalignment caused by unordered indexing of points, we introduce a Semantically Aligned Distribution Matching loss computed on the sorted features in each channel. Moreover, to address the rotation variation, we jointly learn the optimal rotation angles while updating the synthetic dataset to better align with the original feature distribution. Extensive experiments on widely used benchmark datasets demonstrate that the proposed method consistently outperforms existing dataset distillation methods, achieving superior accuracy and strong cross-architecture generalization.
>
---
#### [replaced 088] SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.22461v2](http://arxiv.org/pdf/2505.22461v2)**

> **作者:** Qiucheng Yu; Yuan Xie; Xin Tan
>
> **摘要:** 3D occupancy prediction has attracted much attention in the field of autonomous driving due to its powerful geometric perception and object recognition capabilities. However, existing methods have not explored the most essential distribution patterns of voxels, resulting in unsatisfactory results. This paper first explores the inter-class distribution and geometric distribution of voxels, thereby solving the long-tail problem caused by the inter-class distribution and the poor performance caused by the geometric distribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail Occupancy), which uses sparse head-tail voxel construction to accurately identify and balance key voxels in the head and tail classes, while using decoupled learning to reduce the model's bias towards the dominant (head) category and enhance the focus on the tail class. Experiments show that significant improvements have been made on multiple baselines: SHTOcc reduces GPU memory usage by 42.2%, increases inference speed by 58.6%, and improves accuracy by about 7%, verifying its effectiveness and efficiency. The code is available at https://github.com/ge95net/SHTOcc
>
---
#### [replaced 089] ReferDINO-Plus: 2nd Solution for 4th PVUW MeViS Challenge at CVPR 2025
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.23509v2](http://arxiv.org/pdf/2503.23509v2)**

> **作者:** Tianming Liang; Haichao Jiang; Wei-Shi Zheng; Jian-Fang Hu
>
> **摘要:** Referring Video Object Segmentation (RVOS) aims to segment target objects throughout a video based on a text description. This task has attracted increasing attention in the field of computer vision due to its promising applications in video editing and human-agent interaction. Recently, ReferDINO has demonstrated promising performance in this task by adapting object-level vision-language knowledge from pretrained foundational image models. In this report, we further enhance its capabilities by incorporating the advantages of SAM2 in mask quality and object consistency. In addition, to effectively balance performance between single-object and multi-object scenarios, we introduce a conditional mask fusion strategy that adaptively fuses the masks from ReferDINO and SAM2. Our solution, termed ReferDINO-Plus, achieves 60.43 \(\mathcal{J}\&\mathcal{F}\) on MeViS test set, securing 2nd place in the MeViS PVUW challenge at CVPR 2025. The code is available at: https://github.com/iSEE-Laboratory/ReferDINO-Plus.
>
---
#### [replaced 090] PanopticNeRF-360: Panoramic 3D-to-2D Label Transfer in Urban Scenes
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2309.10815v3](http://arxiv.org/pdf/2309.10815v3)**

> **作者:** Xiao Fu; Shangzhan Zhang; Tianrun Chen; Yichong Lu; Xiaowei Zhou; Andreas Geiger; Yiyi Liao
>
> **备注:** Project page: http://fuxiao0719.github.io/projects/panopticnerf360/ Code: https://github.com/fuxiao0719/PanopticNeRF/tree/panopticnerf360 (Minor Revision). arXiv admin note: text overlap with arXiv:2203.15224
>
> **摘要:** Training perception systems for self-driving cars requires substantial 2D annotations that are labor-intensive to manual label. While existing datasets provide rich annotations on pre-recorded sequences, they fall short in labeling rarely encountered viewpoints, potentially hampering the generalization ability for perception models. In this paper, we present PanopticNeRF-360, a novel approach that combines coarse 3D annotations with noisy 2D semantic cues to generate high-quality panoptic labels and images from any viewpoint. Our key insight lies in exploiting the complementarity of 3D and 2D priors to mutually enhance geometry and semantics. Specifically, we propose to leverage coarse 3D bounding primitives and noisy 2D semantic and instance predictions to guide geometry optimization, by encouraging predicted labels to match panoptic pseudo ground truth. Simultaneously, the improved geometry assists in filtering 3D&2D annotation noise by fusing semantics in 3D space via a learned semantic field. To further enhance appearance, we combine MLP and hash grids to yield hybrid scene features, striking a balance between high-frequency appearance and contiguous semantics. Our experiments demonstrate PanopticNeRF-360's state-of-the-art performance over label transfer methods on the challenging urban scenes of the KITTI-360 dataset. Moreover, PanopticNeRF-360 enables omnidirectional rendering of high-fidelity, multi-view and spatiotemporally consistent appearance, semantic and instance labels. We make our code and data available at https://github.com/fuxiao0719/PanopticNeRF
>
---
#### [replaced 091] ZooplanktonBench: A Geo-Aware Zooplankton Recognition and Classification Dataset from Marine Observations
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.18477v3](http://arxiv.org/pdf/2505.18477v3)**

> **作者:** Fukun Liu; Adam T. Greer; Gengchen Mai; Jin Sun
>
> **备注:** Accepted to KDD 2025 Datasets and Benchmarks Track
>
> **摘要:** Plankton are small drifting organisms found throughout the world's oceans and can be indicators of ocean health. One component of this plankton community is the zooplankton, which includes gelatinous animals and crustaceans (e.g. shrimp), as well as the early life stages (i.e., eggs and larvae) of many commercially important fishes. Being able to monitor zooplankton abundances accurately and understand how populations change in relation to ocean conditions is invaluable to marine science research, with important implications for future marine seafood productivity. While new imaging technologies generate massive amounts of video data of zooplankton, analyzing them using general-purpose computer vision tools turns out to be highly challenging due to the high similarity in appearance between the zooplankton and its background (e.g., marine snow). In this work, we present the ZooplanktonBench, a benchmark dataset containing images and videos of zooplankton associated with rich geospatial metadata (e.g., geographic coordinates, depth, etc.) in various water ecosystems. ZooplanktonBench defines a collection of tasks to detect, classify, and track zooplankton in challenging settings, including highly cluttered environments, living vs non-living classification, objects with similar shapes, and relatively small objects. Our dataset presents unique challenges and opportunities for state-of-the-art computer vision systems to evolve and improve visual understanding in dynamic environments characterized by significant variation and the need for geo-awareness. The code and settings described in this paper can be found on our website: https://lfk118.github.io/ZooplanktonBench_Webpage.
>
---
#### [replaced 092] Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.16512v2](http://arxiv.org/pdf/2505.16512v2)**

> **作者:** Jiaxin Liu; Jia Wang; Saihui Hou; Min Ren; Huijia Wu; Zhaofeng He
>
> **摘要:** In recent years, the rapid development of deepfake technology has given rise to an emerging and serious threat to public security: diffusion-based digital human generation. Unlike traditional face manipulation methods, such models can generate highly realistic videos with consistency through multimodal control signals. Their flexibility and covertness pose severe challenges to existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the new large-scale multimodal digital human forgery dataset based on diffusion models. Employing five of the latest digital human generation methods and the voice cloning methods, we systematically produce a dataset comprising 60,000 videos (8.4 million frames), covering multiple nationalities, skin tones, genders, and real-world scenarios, significantly enhancing data diversity and realism. User studies demonstrate that participants misclassify forged videos as real in 68% of tests, and existing detection models exhibit a large drop in performance on DigiFakeAV, highlighting the challenge of the dataset. To address this problem, we propose DigiShield, an effective detection baseline based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of videos and the semantic-acoustic features of audio, DigiShield achieves state-of-the-art (SOTA) performance on the DigiFakeAV and shows strong generalization on other datasets.
>
---
#### [replaced 093] Compositional Scene Understanding through Inverse Generative Modeling
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.21780v2](http://arxiv.org/pdf/2505.21780v2)**

> **作者:** Yanbo Wang; Justin Dauwels; Yilun Du
>
> **备注:** ICML 2025, Webpage: https://energy-based-model.github.io/compositional-inference
>
> **摘要:** Generative models have demonstrated remarkable abilities in generating high-fidelity visual content. In this work, we explore how generative models can further be used not only to synthesize visual content but also to understand the properties of a scene given a natural image. We formulate scene understanding as an inverse generative modeling problem, where we seek to find conditional parameters of a visual generative model to best fit a given natural image. To enable this procedure to infer scene structure from images substantially different than those seen during training, we further propose to build this visual generative model compositionally from smaller models over pieces of a scene. We illustrate how this procedure enables us to infer the set of objects in a scene, enabling robust generalization to new test scenes with an increased number of objects of new shapes. We further illustrate how this enables us to infer global scene factors, likewise enabling robust generalization to new scenes. Finally, we illustrate how this approach can be directly applied to existing pretrained text-to-image generative models for zero-shot multi-object perception. Code and visualizations are at https://energy-based-model.github.io/compositional-inference.
>
---
#### [replaced 094] Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis
- **分类: cs.MM; cs.CV; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2411.17690v2](http://arxiv.org/pdf/2411.17690v2)**

> **作者:** Akshita Gupta; Tatiana Likhomanenko; Karren Dai Yang; Richard He Bai; Zakaria Aldeneh; Navdeep Jaitly
>
> **摘要:** The rapid progress of foundation models and large language models (LLMs) has fueled significantly improvement in the capabilities of machine learning systems that benefit from mutlimodal input data. However, existing multimodal models are predominantly built on top of pre-trained LLMs, which can limit accurate modeling of temporal dependencies across other modalities and thus limit the model's ability to jointly process and leverage multimodal inputs. To specifically investigate the alignment of text, video, and speech modalities in LLM-style (decoder-only) models, we consider a simplified multimodal generation task, Video-Text to Speech (VTTS): speech generation conditioned on both its corresponding text and video of talking people. The ultimate goal is to generate speech that not only follows the text but also aligns temporally with the video and is consistent with the facial expressions. In this paper, we first introduce Visatronic, a unified multimodal decoder-only transformer model that adopts an LLM-style architecture to embed visual, textual, and speech inputs into a shared subspace, treating all modalities as temporally aligned token streams. Next, we carefully explore different token mixing strategies to understand the best way to propagate information from the steps where video and text conditioning is input to the steps where the audio is generated. We extensively evaluate Visatronic on the challenging VoxCeleb2 dataset and demonstrate zero-shot generalization to LRS3, where Visatronic, trained on VoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only on LRS3, which report a 21.4% WER. Additionally, we propose a new objective metric, TimeSync, specifically designed to measure phoneme-level temporal alignment between generated and reference speech, further ensuring synchronization quality. Demo: https://apple.github.io/visatronic-demo/
>
---
#### [replaced 095] Calibrating Undisciplined Over-Smoothing in Transformer for Weakly Supervised Semantic Segmentation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2305.03112v2](http://arxiv.org/pdf/2305.03112v2)**

> **作者:** Lechao Cheng; Zerun Liu; Jingxuan He; Chaowei Fang; Dingwen Zhang; Meng Wang
>
> **摘要:** Weakly supervised semantic segmentation (WSSS) has recently attracted considerable attention because it requires fewer annotations than fully supervised approaches, making it especially promising for large-scale image segmentation tasks. Although many vision transformer-based methods leverage self-attention affinity matrices to refine Class Activation Maps (CAMs), they often treat each layer's affinity equally and thus introduce considerable background noise at deeper layers, where attention tends to converge excessively on certain tokens (i.e., over-smoothing). We observe that this deep-level attention naturally converges on a subset of tokens, yet unregulated query-key affinity can generate unpredictable activation patterns (undisciplined over-smoothing), adversely affecting CAM accuracy. To address these limitations, we propose an Adaptive Re-Activation Mechanism (AReAM), which exploits shallow-level affinity to guide deeper-layer convergence in an entropy-aware manner, thereby suppressing background noise and re-activating crucial semantic regions in the CAMs. Experiments on two commonly used datasets demonstrate that AReAM substantially improves segmentation performance compared with existing WSSS methods, reducing noise while sharpening focus on relevant semantic regions. Overall, this work underscores the importance of controlling deep-level attention to mitigate undisciplined over-smoothing, introduces an entropy-aware mechanism that harmonizes shallow and deep-level affinities, and provides a refined approach to enhance transformer-based WSSS accuracy by re-activating CAMs.
>
---
#### [replaced 096] Interpreting the linear structure of vision-language model embedding spaces
- **分类: cs.CV; cs.MM**

- **链接: [http://arxiv.org/pdf/2504.11695v2](http://arxiv.org/pdf/2504.11695v2)**

> **作者:** Isabel Papadimitriou; Huangyuan Su; Thomas Fel; Sham Kakade; Stephanie Gil
>
> **摘要:** Vision-language models encode images and text in a joint space, minimizing the distance between corresponding image and text pairs. How are language and images organized in this joint space, and how do the models encode meaning and modality? To investigate this, we train and release sparse autoencoders (SAEs) on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2, and AIMv2). SAEs approximate model embeddings as sparse linear combinations of learned directions, or "concepts". We find that, compared to other methods of linear feature learning, SAEs are better at reconstructing the real embeddings, while also able to retain the most sparsity. Retraining SAEs with different seeds or different data diet leads to two findings: the rare, specific concepts captured by the SAEs are liable to change drastically, but we also show that the key commonly-activating concepts extracted by SAEs are remarkably stable across runs. Interestingly, while most concepts are strongly unimodal in activation, we find they are not merely encoding modality per se. Many lie close to - but not entirely within - the subspace defining modality, suggesting that they encode cross-modal semantics despite their unimodal usage. To quantify this bridging behavior, we introduce the Bridge Score, a metric that identifies concept pairs which are both co-activated across aligned image-text inputs and geometrically aligned in the shared space. This reveals that even unimodal concepts can collaborate to support cross-modal integration. We release interactive demos of the SAEs for all models, allowing researchers to explore the organization of the concept spaces. Overall, our findings uncover a sparse linear structure within VLM embedding spaces that is shaped by modality, yet stitched together through latent bridges-offering new insight into how multimodal meaning is constructed.
>
---
#### [replaced 097] Aligning Text to Image in Diffusion Models is Easier Than You Think
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2503.08250v4](http://arxiv.org/pdf/2503.08250v4)**

> **作者:** Jaa-Yeon Lee; Byunghee Cha; Jeongsol Kim; Jong Chul Ye
>
> **摘要:** While recent advancements in generative modeling have significantly improved text-image alignment, some residual misalignment between text and image representations still remains. Some approaches address this issue by fine-tuning models in terms of preference optimization, etc., which require tailored datasets. Orthogonal to these methods, we revisit the challenge from the perspective of representation alignment-an approach that has gained popularity with the success of REPresentation Alignment (REPA). We first argue that conventional text-to-image (T2I) diffusion models, typically trained on paired image and text data (i.e., positive pairs) by minimizing score matching or flow matching losses, is suboptimal from the standpoint of representation alignment. Instead, a better alignment can be achieved through contrastive learning that leverages existing dataset as both positive and negative pairs. To enable efficient alignment with pretrained models, we propose SoftREPA- a lightweight contrastive fine-tuning strategy that leverages soft text tokens for representation alignment. This approach improves alignment with minimal computational overhead by adding fewer than 1M trainable parameters to the pretrained model. Our theoretical analysis demonstrates that our method explicitly increases the mutual information between text and image representations, leading to enhanced semantic consistency. Experimental results across text-to-image generation and text-guided image editing tasks validate the effectiveness of our approach in improving the semantic consistency of T2I generative models.
>
---
#### [replaced 098] Agentic Knowledgeable Self-awareness
- **分类: cs.CL; cs.AI; cs.CV; cs.LG; cs.MA**

- **链接: [http://arxiv.org/pdf/2504.03553v2](http://arxiv.org/pdf/2504.03553v2)**

> **作者:** Shuofei Qiao; Zhisong Qiu; Baochang Ren; Xiaobin Wang; Xiangyuan Ru; Ningyu Zhang; Xiang Chen; Yong Jiang; Pengjun Xie; Fei Huang; Huajun Chen
>
> **备注:** ACL 2025
>
> **摘要:** Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.
>
---
#### [replaced 099] Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.13169v2](http://arxiv.org/pdf/2504.13169v2)**

> **作者:** Tsung-Han Wu; Heekyung Lee; Jiaxin Ge; Joseph E. Gonzalez; Trevor Darrell; David M. Chan
>
> **备注:** Preprint. Project Page: https://reverse-vlm.github.io
>
> **摘要:** Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 34% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io.
>
---
#### [replaced 100] BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro QR Codes
- **分类: cs.CV; cs.CL; cs.MM**

- **链接: [http://arxiv.org/pdf/2404.03161v3](http://arxiv.org/pdf/2404.03161v3)**

> **作者:** Tomohiro Nishimoto; Taichi Nishimura; Koki Yamamoto; Keisuke Shirai; Hirotaka Kameko; Yuto Haneji; Tomoya Yoshida; Keiya Kajimura; Taiyu Cui; Chihiro Nishiwaki; Eriko Daikoku; Natsuko Okuda; Fumihito Ono; Shinsuke Mori
>
> **备注:** ICIP2025
>
> **摘要:** This paper introduces BioVL-QR, a biochemical vision-and-language dataset comprising 23 egocentric experiment videos, corresponding protocols, and vision-and-language alignments. A major challenge in understanding biochemical videos is detecting equipment, reagents, and containers because of the cluttered environment and indistinguishable objects. Previous studies assumed manual object annotation, which is costly and time-consuming. To address the issue, we focus on Micro QR Codes. However, detecting objects using only Micro QR Codes is still difficult due to blur and occlusion caused by object manipulation. To overcome this, we propose an object labeling method combining a Micro QR Code detector with an off-the-shelf hand object detector. As an application of the method and BioVL-QR, we tackled the task of localizing the procedural steps in an instructional video. The experimental results show that using Micro QR Codes and our method improves biochemical video understanding. Data and code are available through https://nishi10mo.github.io/BioVL-QR/
>
---
#### [replaced 101] M3Bench: Benchmarking Whole-body Motion Generation for Mobile Manipulation in 3D Scenes
- **分类: cs.RO; cs.AI; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2410.06678v3](http://arxiv.org/pdf/2410.06678v3)**

> **作者:** Zeyu Zhang; Sixu Yan; Muzhi Han; Zaijin Wang; Xinggang Wang; Song-Chun Zhu; Hangxin Liu
>
> **备注:** This paper has been accepted by IEEE Robotics and Automation Letters 2025 (RA-L)
>
> **摘要:** We propose M3Bench, a new benchmark for whole-body motion generation in mobile manipulation tasks. Given a 3D scene context, M3Bench requires an embodied agent to reason about its configuration, environmental constraints, and task objectives to generate coordinated whole-body motion trajectories for object rearrangement. M3Bench features 30,000 object rearrangement tasks across 119 diverse scenes, providing expert demonstrations generated by our newly developed M3BenchMaker, an automatic data generation tool that produces whole-body motion trajectories from high-level task instructions using only basic scene and robot information. Our benchmark includes various task splits to evaluate generalization across different dimensions and leverages realistic physics simulation for trajectory assessment. Extensive evaluation analysis reveals that state-of-the-art models struggle with coordinating base-arm motion while adhering to environmental and task-specific constraints, underscoring the need for new models to bridge this gap. By releasing M3Bench and M3BenchMaker we aim to advance robotics research toward more adaptive and capable mobile manipulation in diverse, real-world environments.
>
---
#### [replaced 102] A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data
- **分类: stat.ML; cs.CV; cs.LG; stat.ME**

- **链接: [http://arxiv.org/pdf/2505.20688v2](http://arxiv.org/pdf/2505.20688v2)**

> **作者:** Taehyo Kim; Qiran Jia; Mony J. de Leon; Hai Shu
>
> **摘要:** False discovery rate (FDR) control methods are essential for voxel-wise multiple testing in neuroimaging data analysis, where hundreds of thousands or even millions of tests are conducted to detect brain regions associated with disease-related changes. Classical FDR control methods (e.g., BH, q-value, and LocalFDR) assume independence among tests and often lead to high false non-discovery rates (FNR). Although various spatial FDR control methods have been developed to improve power, they still fall short of jointly addressing three major challenges in neuroimaging applications: capturing complex spatial dependencies, maintaining low variability in both false discovery proportion (FDP) and false non-discovery proportion (FNP) across replications, and achieving computational scalability for high-resolution data. To address these challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR control method for voxel-wise multiple testing. It integrates the local index of significance (LIS)-based testing procedure with a novel fully connected hidden Markov random field (fcHMRF) designed to model complex spatial structures using a parsimonious parameterization. We develop an efficient expectation-maximization algorithm incorporating mean-field approximation, the Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and permutohedral lattice filtering, reducing the time complexity from quadratic to linear in the number of tests. Extensive simulations demonstrate that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability in FDP and FNP, and a higher number of true positives compared to existing methods. Applied to an FDG-PET dataset from the Alzheimer's Disease Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain regions and offers notable advantages in computational efficiency.
>
---
#### [replaced 103] Information Entropy Guided Height-aware Histogram for Quantization-friendly Pillar Feature Encoder
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2405.18734v5](http://arxiv.org/pdf/2405.18734v5)**

> **作者:** Sifan Zhou; Zhihang Yuan; Dawei Yang; Ziyu Zhao; Xing Hu; Yuguang Shi; Xiaobo Lu; Qiang Wu
>
> **摘要:** Real-time and high-performance 3D object detection plays a critical role in autonomous driving and robotics. Recent pillar-based 3D object detectors have gained significant attention due to their compact representation and low computational overhead, making them suitable for onboard deployment and quantization. However, existing pillar-based detectors still suffer from information loss along height dimension and large numerical distribution difference during pillar feature encoding (PFE), which severely limits their performance and quantization potential. To address above issue, we first unveil the importance of different input information during PFE and identify the height dimension as a key factor in enhancing 3D detection performance. Motivated by this observation, we propose a height-aware pillar feature encoder, called PillarHist. Specifically, PillarHist statistics the discrete distribution of points at different heights within one pillar with the information entropy guidance. This simple yet effective design greatly preserves the information along the height dimension while significantly reducing the computation overhead of the PFE. Meanwhile, PillarHist also constrains the arithmetic distribution of PFE input to a stable range, making it quantization-friendly. Notably, PillarHist operates exclusively within the PFE stage to enhance performance, enabling seamless integration into existing pillar-based methods without introducing complex operations. Extensive experiments show the effectiveness of PillarHist in terms of both efficiency and performance.
>
---
#### [replaced 104] SafeCFG: Controlling Harmful Features with Dynamic Safe Guidance for Safe Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.16039v2](http://arxiv.org/pdf/2412.16039v2)**

> **作者:** Jiadong Pan; Liang Li; Hongcheng Gao; Zheng-Jun Zha; Qingming Huang; Jiebo Luo
>
> **摘要:** Diffusion models (DMs) have demonstrated exceptional performance in text-to-image tasks, leading to their widespread use. With the introduction of classifier-free guidance (CFG), the quality of images generated by DMs is significantly improved. However, one can use DMs to generate more harmful images by maliciously guiding the image generation process through CFG. Existing safe alignment methods aim to mitigate the risk of generating harmful images but often reduce the quality of clean image generation. To address this issue, we propose SafeCFG to adaptively control harmful features with dynamic safe guidance by modulating the CFG generation process. It dynamically guides the CFG generation process based on the harmfulness of the prompts, inducing significant deviations only in harmful CFG generations, achieving high quality and safety generation. SafeCFG can simultaneously modulate different harmful CFG generation processes, so it could eliminate harmful elements while preserving high-quality generation. Additionally, SafeCFG provides the ability to detect image harmfulness, allowing unsupervised safe alignment on DMs without pre-defined clean or harmful labels. Experimental results show that images generated by SafeCFG achieve both high quality and safety, and safe DMs trained in our unsupervised manner also exhibit good safety performance.
>
---
#### [replaced 105] RefCut: Interactive Segmentation with Reference Guidance
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.17820v2](http://arxiv.org/pdf/2503.17820v2)**

> **作者:** Zheng Lin; Nan Zhou; Chen-Xi Du; Deng-Ping Fan; Shi-Min Hu
>
> **摘要:** Interactive segmentation aims to segment the specified target on the image with positive and negative clicks from users. Interactive ambiguity is a crucial issue in this field, which refers to the possibility of multiple compliant outcomes with the same clicks, such as selecting a part of an object versus the entire object, a single object versus a combination of multiple objects, and so on. The existing methods cannot provide intuitive guidance to the model, which leads to unstable output results and makes it difficult to meet the large-scale and efficient annotation requirements for specific targets in some scenarios. To bridge this gap, we introduce RefCut, a reference-based interactive segmentation framework designed to address part ambiguity and object ambiguity in segmenting specific targets. Users only need to provide a reference image and corresponding reference masks, and the model will be optimized based on them, which greatly reduces the interactive burden on users when annotating a large number of such targets. In addition, to enrich these two kinds of ambiguous data, we propose a new Target Disassembly Dataset which contains two subsets of part disassembly and object disassembly for evaluation. In the combination evaluation of multiple datasets, our RefCut achieved state-of-the-art performance. Extensive experiments and visualized results demonstrate that RefCut advances the field of intuitive and controllable interactive segmentation. Our code will be publicly available and the demo video is in https://www.lin-zheng.com/refcut.
>
---
#### [replaced 106] Minimal Sufficient Views: A DNN model making predictions with more evidence has higher accuracy
- **分类: cs.LG; cs.AI; cs.CV; stat.ML**

- **链接: [http://arxiv.org/pdf/2402.01095v2](http://arxiv.org/pdf/2402.01095v2)**

> **作者:** Keisuke Kawano; Takuro Kutsuna; Keisuke Sano
>
> **备注:** 24 pages
>
> **摘要:** Deep neural networks (DNNs) exhibit high performance in image recognition; however, the reasons for their strong generalization abilities remain unclear. A plausible hypothesis is that DNNs achieve robust and accurate predictions by identifying multiple pieces of evidence from images. Thus, to test this hypothesis, this study proposed minimal sufficient views (MSVs). MSVs is defined as a set of minimal regions within an input image that are sufficient to preserve the prediction of DNNs, thus representing the evidence discovered by the DNN. We empirically demonstrated a strong correlation between the number of MSVs (i.e., the number of pieces of evidence) and the generalization performance of the DNN models. Remarkably, this correlation was found to hold within a single DNN as well as between different DNNs, including convolutional and transformer models. This suggested that a DNN model that makes its prediction based on more evidence has a higher generalization performance. We proposed a metric based on MSVs for DNN model selection that did not require label information. Consequently, we empirically showed that the proposed metric was less dependent on the degree of overfitting, rendering it a more reliable indicator of model performance than existing metrics, such as average confidence.
>
---
#### [replaced 107] Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.15484v3](http://arxiv.org/pdf/2412.15484v3)**

> **作者:** Saehyung Lee; Seunghyun Yoon; Trung Bui; Jing Shi; Sungroh Yoon
>
> **备注:** ICML 2025
>
> **摘要:** Multimodal large language models (MLLMs) excel at generating highly detailed captions but often produce hallucinations. Our analysis reveals that existing hallucination detection methods struggle with detailed captions. We attribute this to the increasing reliance of MLLMs on their generated text, rather than the input image, as the sequence length grows. To address this issue, we propose a multiagent approach that leverages LLM-MLLM collaboration to correct given captions. Additionally, we introduce an evaluation framework and a benchmark dataset to facilitate the systematic analysis of detailed captions. Our experiments demonstrate that our proposed evaluation method better aligns with human judgments of factuality than existing metrics and that existing approaches to improve the MLLM factuality may fall short in hyper-detailed image captioning tasks. In contrast, our proposed method significantly enhances the factual accuracy of captions, even improving those generated by GPT-4V. Finally, we highlight a limitation of VQA-centric benchmarking by demonstrating that an MLLM's performance on VQA benchmarks may not correlate with its ability to generate detailed image captions.
>
---
#### [replaced 108] FreSca: Scaling in Frequency Space Enhances Diffusion Models
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.02154v3](http://arxiv.org/pdf/2504.02154v3)**

> **作者:** Chao Huang; Susan Liang; Yunlong Tang; Jing Bi; Li Ma; Yapeng Tian; Chenliang Xu
>
> **备注:** Project page: https://wikichao.github.io/FreSca/
>
> **摘要:** Latent diffusion models (LDMs) have achieved remarkable success in a variety of image tasks, yet achieving fine-grained, disentangled control over global structures versus fine details remains challenging. This paper explores frequency-based control within latent diffusion models. We first systematically analyze frequency characteristics across pixel space, VAE latent space, and internal LDM representations. This reveals that the "noise difference" term, derived from classifier-free guidance at each step t, is a uniquely effective and semantically rich target for manipulation. Building on this insight, we introduce FreSca, a novel and plug-and-play framework that decomposes noise difference into low- and high-frequency components and applies independent scaling factors to them via spatial or energy-based cutoffs. Essentially, FreSca operates without any model retraining or architectural change, offering model- and task-agnostic control. We demonstrate its versatility and effectiveness in improving generation quality and structural emphasis on multiple architectures (e.g., SD3, SDXL) and across applications including image generation, editing, depth estimation, and video synthesis, thereby unlocking a new dimension of expressive control within LDMs.
>
---
#### [replaced 109] CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.21904v2](http://arxiv.org/pdf/2505.21904v2)**

> **作者:** Pardis Taghavi; Tian Liu; Renjie Li; Reza Langari; Zhengzhong Tu
>
> **摘要:** Instance segmentation demands costly per-pixel annotations and large models. We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework that compresses pretrained vision foundation models (VFM) into compact experts using limited labeled and abundant unlabeled data. CAST unfolds in three stages: (1) domain adaptation of the VFM teacher(s) via self-training with contrastive pixel calibration, (2) distillation into a compact student via a unified multi-objective loss that couples standard supervision and pseudo-labels with our instance-aware pixel-wise contrastive term, and (3) fine-tuning on labeled data to remove residual pseudo-label bias. Central to CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask and class scores to mine informative negatives and enforce clear inter-instance margins. By maintaining this contrastive signal across both adaptation and distillation, we align teacher and student embeddings and fully leverage unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs. 15.2) and outperforms state-of-the-art semi-supervised approaches.
>
---
#### [replaced 110] Epsilon-VAE: Denoising as Visual Decoding
- **分类: cs.CV; cs.AI; eess.IV**

- **链接: [http://arxiv.org/pdf/2410.04081v4](http://arxiv.org/pdf/2410.04081v4)**

> **作者:** Long Zhao; Sanghyun Woo; Ziyu Wan; Yandong Li; Han Zhang; Boqing Gong; Hartwig Adam; Xuhui Jia; Ting Liu
>
> **备注:** Accepted to ICML 2025. v2: added comparisons to SD-VAE and more visual results; v3: minor change to title; v4: camera-ready version
>
> **摘要:** In generative modeling, tokenization simplifies complex data into compact, structured representations, creating a more efficient, learnable space. For high-dimensional visual data, it reduces redundancy and emphasizes key features for high-quality generation. Current visual tokenization methods rely on a traditional autoencoder framework, where the encoder compresses data into latent representations, and the decoder reconstructs the original input. In this work, we offer a new perspective by proposing denoising as decoding, shifting from single-step reconstruction to iterative refinement. Specifically, we replace the decoder with a diffusion process that iteratively refines noise to recover the original image, guided by the latents provided by the encoder. We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID), comparing it to state-of-the-art autoencoding approaches. By adopting iterative reconstruction through diffusion, our autoencoder, namely Epsilon-VAE, achieves high reconstruction quality, which in turn enhances downstream generation quality by 22% at the same compression rates or provides 2.3x inference speedup through increasing compression rates. We hope this work offers new insights into integrating iterative generation and autoencoding for improved compression and generation.
>
---
#### [replaced 111] Generalizable Representation Learning for fMRI-based Neurological Disorder Identification
- **分类: eess.IV; cs.CE; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2412.16197v2](http://arxiv.org/pdf/2412.16197v2)**

> **作者:** Wenhui Cui; Haleh Akrami; Anand A. Joshi; Richard M. Leahy
>
> **备注:** Accepted by TMLR
>
> **摘要:** Despite the impressive advances achieved using deep learning for functional brain activity analysis, the heterogeneity of functional patterns and the scarcity of imaging data still pose challenges in tasks such as identifying neurological disorders. For functional Magnetic Resonance Imaging (fMRI), while data may be abundantly available from healthy controls, clinical data is often scarce, especially for rare diseases, limiting the ability of models to identify clinically-relevant features. We overcome this limitation by introducing a novel representation learning strategy integrating meta-learning with self-supervised learning to improve the generalization from normal to clinical features. This approach enables generalization to challenging clinical tasks featuring scarce training data. We achieve this by leveraging self-supervised learning on the control dataset to focus on inherent features that are not limited to a particular supervised task and incorporating meta-learning to improve the generalization across domains. To explore the generalizability of the learned representations to unseen clinical applications, we apply the model to four distinct clinical datasets featuring scarce and heterogeneous data for neurological disorder classification. Results demonstrate the superiority of our representation learning strategy on diverse clinically-relevant tasks. Code is publicly available at https://github.com/wenhui0206/MeTSK/tree/main
>
---
#### [replaced 112] VideoRAG: Retrieval-Augmented Generation over Video Corpus
- **分类: cs.CV; cs.AI; cs.CL; cs.IR; cs.LG**

- **链接: [http://arxiv.org/pdf/2501.05874v3](http://arxiv.org/pdf/2501.05874v3)**

> **作者:** Soyeong Jeong; Kangsan Kim; Jinheon Baek; Sung Ju Hwang
>
> **备注:** ACL Findings 2025
>
> **摘要:** Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the factual accuracy of models by retrieving external knowledge relevant to queries and incorporating it into the generation process. However, existing approaches primarily focus on text, with some recent advancements considering images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing contextual details more effectively than any other modality. While very recent studies explore the use of videos in response generation, they either predefine query-associated videos without retrieval or convert videos into textual descriptions losing multimodal richness. To tackle these, we introduce VideoRAG, a framework that not only dynamically retrieves videos based on their relevance with queries but also utilizes both visual and textual information. The operation of VideoRAG is powered by recent Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and the seamless integration of retrieved videos jointly with queries for response generation. Also, inspired by that the context size of LVLMs may not be sufficient to process all frames in extremely long videos and not all frames are equally important, we introduce a video frame selection mechanism to extract the most informative subset of frames, along with a strategy to extract textual information from videos (as it can aid the understanding of video content) when their subtitles are not available. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines. Code is available at https://github.com/starsuzi/VideoRAG.
>
---
#### [replaced 113] RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.22535v2](http://arxiv.org/pdf/2505.22535v2)**

> **作者:** Mohamad Hakam Shams Eddin; Yikui Zhang; Stefan Kollet; Juergen Gall
>
> **备注:** Main paper 10 pages, Appendix 53 pages
>
> **摘要:** Recent deep learning approaches for river discharge forecasting have improved the accuracy and efficiency in flood forecasting, enabling more reliable early warning systems for risk management. Nevertheless, existing deep learning approaches in hydrology remain largely confined to local-scale applications and do not leverage the inherent spatial connections of bodies of water. Thus, there is a strong need for new deep learning methodologies that are capable of modeling spatio-temporal relations to improve river discharge and flood forecasting for scientific and operational applications. To address this, we present RiverMamba, a novel deep learning model that is pretrained with long-term reanalysis data and that can forecast global river discharge and floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high relevance in early warning. To achieve this, RiverMamba leverages efficient Mamba blocks that enable the model to capture global-scale channel network routing and enhance its forecast capability for longer lead times. The forecast blocks integrate ECMWF HRES meteorological forecasts, while accounting for their inaccuracies through spatio-temporal modeling. Our analysis demonstrates that RiverMamba delivers reliable predictions of river discharge, including extreme floods across return periods and lead times, surpassing both operational AI- and physics-based models.
>
---
#### [replaced 114] ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.21117v2](http://arxiv.org/pdf/2505.21117v2)**

> **作者:** Adeela Islam; Stefano Fiorini; Stuart James; Pietro Morerio; Alessio Del Bue
>
> **摘要:** The task of reassembly is a significant challenge across multiple domains, including archaeology, genomics, and molecular docking, requiring the precise placement and orientation of elements to reconstruct an original structure. In this work, we address key limitations in state-of-the-art Deep Learning methods for reassembly, namely i) scalability; ii) multimodality; and iii) real-world applicability: beyond square or simple geometric shapes, realistic and complex erosion, or other real-world problems. We propose ReassembleNet, a method that reduces complexity by representing each input piece as a set of contour keypoints and learning to select the most informative ones by Graph Neural Networks pooling inspired techniques. ReassembleNet effectively lowers computational complexity while enabling the integration of features from multiple modalities, including both geometric and texture data. Further enhanced through pretraining on a semi-synthetic dataset. We then apply diffusion-based pose estimation to recover the original structure. We improve on prior methods by 55% and 86% for RMSE Rotation and Translation, respectively.
>
---
#### [replaced 115] BrainMRDiff: A Diffusion Model for Anatomically Consistent Brain MRI Synthesis
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2504.04532v2](http://arxiv.org/pdf/2504.04532v2)**

> **作者:** Moinak Bhattacharya; Saumya Gupta; Annie Singh; Chao Chen; Gagandeep Singh; Prateek Prasanna
>
> **摘要:** Accurate brain tumor diagnosis relies on the assessment of multiple Magnetic Resonance Imaging (MRI) sequences. However, in clinical practice, the acquisition of certain sequences may be affected by factors like motion artifacts or contrast agent contraindications, leading to suboptimal outcome, such as poor image quality. This can then affect image interpretation by radiologists. Synthesizing high quality MRI sequences has thus become a critical research focus. Though recent advancements in controllable generative AI have facilitated the synthesis of diagnostic quality MRI, ensuring anatomical accuracy remains a significant challenge. Preserving critical structural relationships between different anatomical regions is essential, as even minor structural or topological inconsistencies can compromise diagnostic validity. In this work, we propose BrainMRDiff, a novel topology-preserving, anatomy-guided diffusion model for synthesizing brain MRI, leveraging brain and tumor anatomies as conditioning inputs. To achieve this, we introduce two key modules: Tumor+Structure Aggregation (TSA) and Topology-Guided Anatomy Preservation (TGAP). TSA integrates diverse anatomical structures with tumor information, forming a comprehensive conditioning mechanism for the diffusion process. TGAP enforces topological consistency during reverse denoising diffusion process; both these modules ensure that the generated image respects anatomical integrity. Experimental results demonstrate that BrainMRDiff surpasses existing baselines, achieving performance improvements of 23.33% on the BraTS-AG dataset and 33.33% on the BraTS-Met dataset. Code will be made publicly available soon.
>
---
#### [replaced 116] SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.07934v2](http://arxiv.org/pdf/2504.07934v2)**

> **作者:** Xiyao Wang; Zhengyuan Yang; Chao Feng; Hongjin Lu; Linjie Li; Chung-Ching Lin; Kevin Lin; Furong Huang; Lijuan Wang
>
> **备注:** 27 pages, 5 figures
>
> **摘要:** We introduce ThinkLite-VL, a family of visual reasoning models that achieve state-of-the-art (SoTA) performance using an order of magnitude fewer training samples, relying purely on reinforcement fine-tuning (RFT) self-improvement without any knowledge distillation. Our central insight is that sample difficulty critically influences RFT effectiveness: appropriately challenging examples can drive substantial reasoning improvements, even in low-data regimes. However, quantifying sample difficulty in a reliable and scalable manner remains non-trivial. To address this, we repurpose Monte Carlo Tree Search (MCTS) to measure sample difficulty via the number of reasoning iterations a vision-language model (VLM) requires to solve each instance. This MCTS-based selection procedure identifies samples that induce deeper reasoning while remaining solvable, allowing us to filter a high-quality subset from 70k open-source examples spanning math, natural image understanding, and chart comprehension. Using this approach, we select just 11k challenging samples for RFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The resulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly outperform their respective base models across eight visual reasoning benchmarks. In particular, ThinkLite-VL-7B improves the average performance of Qwen2.5-VL-7B-Instruct by 7\% and surpasses all existing 7B-level models, as well as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a new SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA frontier, achieving an accuracy of 79.7 on MathVista and an average benchmark improvement of 4.42 over the open-source SOTA. These results demonstrate that MCTS-guided difficulty filtering provides a scalable and effective path toward data-efficient self-improvement in multimodal reasoning.
>
---
#### [replaced 117] ReDDiT: Rehashing Noise for Discrete Visual Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.19656v2](http://arxiv.org/pdf/2505.19656v2)**

> **作者:** Tianren Ma; Xiaosong Zhang; Boyu Yang; Junlan Feng; Qixiang Ye
>
> **备注:** Preprint. Check out our project page at github.com/martian422/ReDDiT
>
> **摘要:** Discrete diffusion models are gaining traction in the visual generative area for their efficiency and compatibility. However, the pioneered attempts still fall behind the continuous counterparts, which we attribute to the noise (absorbing state) design and sampling heuristics. In this study, we propose the rehashing noise framework for discrete diffusion transformer, termed ReDDiT, to extend absorbing states and improve expressive capacity of discrete diffusion models. ReDDiT enriches the potential paths that latent variables can traverse during training with randomized multi-index corruption. The derived rehash sampler, which reverses the randomized absorbing paths, guarantees the diversity and low discrepancy of the generation process. These reformulations lead to more consistent and competitive generation quality, mitigating the need for heavily tuned randomness. Experiments show that ReDDiT significantly outperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with the continuous counterparts with higher efficiency.
>
---
#### [replaced 118] PolyPose: Localizing Deformable Anatomy in 3D from Sparse 2D X-ray Images using Polyrigid Transforms
- **分类: cs.CV; physics.med-ph**

- **链接: [http://arxiv.org/pdf/2505.19256v2](http://arxiv.org/pdf/2505.19256v2)**

> **作者:** Vivek Gopalakrishnan; Neel Dey; Polina Golland
>
> **备注:** Code available at https://github.com/eigenvivek/polypose
>
> **摘要:** Determining the 3D pose of a patient from a limited set of 2D X-ray images is a critical task in interventional settings. While preoperative volumetric imaging (e.g., CT and MRI) provides precise 3D localization and visualization of anatomical targets, these modalities cannot be acquired during procedures, where fast 2D imaging (X-ray) is used instead. To integrate volumetric guidance into intraoperative procedures, we present PolyPose, a simple and robust method for deformable 2D/3D registration. PolyPose parameterizes complex 3D deformation fields as a composition of rigid transforms, leveraging the biological constraint that individual bones do not bend in typical motion. Unlike existing methods that either assume no inter-joint movement or fail outright in this under-determined setting, our polyrigid formulation enforces anatomically plausible priors that respect the piecewise rigid nature of human movement. This approach eliminates the need for expensive deformation regularizers that require patient- and procedure-specific hyperparameter optimization. Across extensive experiments on diverse datasets from orthopedic surgery and radiotherapy, we show that this strong inductive bias enables PolyPose to successfully align the patient's preoperative volume to as few as two X-ray images, thereby providing crucial 3D guidance in challenging sparse-view and limited-angle settings where current registration methods fail.
>
---
#### [replaced 119] Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion
- **分类: cs.CV; eess.IV**

- **链接: [http://arxiv.org/pdf/2504.10974v3](http://arxiv.org/pdf/2504.10974v3)**

> **作者:** Zhisheng Zhang; Peng Zhang; Fengxiang Wang; Liangli Ma; Fuchun Sun
>
> **摘要:** Enhancing forward-looking sonar images is critical for accurate underwater target detection. Current deep learning methods mainly rely on supervised training with simulated data, but the difficulty in obtaining high-quality real-world paired data limits their practical use and generalization. Although self-supervised approaches from remote sensing partially alleviate data shortages, they neglect the cross-modal degradation gap between sonar and remote sensing images. Directly transferring pretrained weights often leads to overly smooth sonar images, detail loss, and insufficient brightness. To address this, we propose a feature-space transformation that maps sonar images from the pixel domain to a robust feature domain, effectively bridging the degradation gap. Additionally, our self-supervised multi-frame fusion strategy leverages complementary inter-frame information to naturally remove speckle noise and enhance target-region brightness. Experiments on three self-collected real-world forward-looking sonar datasets show that our method significantly outperforms existing approaches, effectively suppressing noise, preserving detailed edges, and substantially improving brightness, demonstrating strong potential for underwater target detection applications.
>
---
