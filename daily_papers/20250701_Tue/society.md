# 计算机与社会 cs.CY

- **最新发布 38 篇**

- **更新 15 篇**

## 最新发布

#### [new 001] AISCliteracy: Assessing Artificial Intelligence and Cybersecurity Literacy Levels and Learning Needs of Students
- **分类: cs.CY; cs.CR**

- **简介: 该论文属于教育评估任务，旨在了解尼泊尔Chitwan地区学生的人工智能与网络安全素养现状及学习需求，通过调查分析提出教育改进建议。**

- **链接: [http://arxiv.org/pdf/2506.23321v1](http://arxiv.org/pdf/2506.23321v1)**

> **作者:** Devendra Chapagain; Naresh Kshetri; Bishwo Prakash Pokharel
>
> **备注:** 11 pages, 3 figures
>
> **摘要:** Artificial intelligence (AI) is rapidly transforming global industries and societies, making AI literacy an indispensable skill for future generations. While AI integration in education is still emerging in Nepal, this study focuses on assessing the current AI literacy levels and identifying learning needs among students in Chitwan District of Nepal. By measuring students' understanding of AI and pinpointing areas for improvement, this research aims to provide actionable recommendations for educational stakeholders. Given the pivotal role of young learners in navigating a rapidly evolving technological landscape, fostering AI literacy is paramount. This study seeks to understand the current state of AI literacy in Chitwan District by analyzing students' knowledge, skills, and attitudes towards AI. The results will contribute to developing robust AI education programs for Nepalese schools. This paper offers a contemporary perspective on AI's role in Nepalese secondary education, emphasizing the latest AI tools and technologies. Moreover, the study illuminates the potential revolutionary impact of technological innovations on educational leadership and student outcomes. A survey was conducted to conceptualize the newly emerging concept of AI and cybersecurity among students of Chitwan district from different schools and colleges to find the literacy rate. The participants in the survey were students between grade 9 to 12. We conclude with discussions of the affordances and barriers to bringing AI and cybersecurity education to students from lower classes.
>
---
#### [new 002] Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate Incidents Management
- **分类: cs.CY; cs.HC; H.1.2**

- **简介: 该论文属于教育技术任务，旨在解决教师处理仇恨事件能力不足的问题。通过构建多智能体LLM系统，模拟真实情境以提升教师应对能力。**

- **链接: [http://arxiv.org/pdf/2506.23774v1](http://arxiv.org/pdf/2506.23774v1)**

> **作者:** Ewelina Gajewska; Michal Wawer; Katarzyna Budzynska; Jarosław A. Chudziak
>
> **备注:** 8 pages, 1 figure
>
> **摘要:** Computer-aided teacher training is a state-of-the-art method designed to enhance teachers' professional skills effectively while minimising concerns related to costs, time constraints, and geographical limitations. We investigate the potential of large language models (LLMs) in teacher education, using a case of teaching hate incidents management in schools. To this end, we create a multi-agent LLM-based system that mimics realistic situations of hate, using a combination of retrieval-augmented prompting and persona modelling. It is designed to identify and analyse hate speech patterns, predict potential escalation, and propose effective intervention strategies. By integrating persona modelling with agentic LLMs, we create contextually diverse simulations of hate incidents, mimicking real-life situations. The system allows teachers to analyse and understand the dynamics of hate incidents in a safe and controlled environment, providing valuable insights and practical knowledge to manage such situations confidently in real life. Our pilot evaluation demonstrates teachers' enhanced understanding of the nature of annotator disagreements and the role of context in hate speech interpretation, leading to the development of more informed and effective strategies for addressing hate in classroom settings.
>
---
#### [new 003] Research on Comprehensive Classroom Evaluation System Based on Multiple AI Models
- **分类: cs.CY; cs.MA**

- **简介: 该论文属于教育评估任务，旨在解决传统课堂评价效率低、主观性强的问题。通过AI技术构建综合评价系统，实现智能、客观的课堂分析与反馈。**

- **链接: [http://arxiv.org/pdf/2506.23079v1](http://arxiv.org/pdf/2506.23079v1)**

> **作者:** Cong Xie; Li Yang; Daben Wang; Jing Xiao
>
> **摘要:** The promotion of the national education digitalization strategy has facilitated the development of teaching quality evaluation towards all-round, process-oriented, precise, and intelligent directions, inspiring explorations into new methods and technologies for educational quality assurance. Classroom teaching evaluation methods dominated by teaching supervision and student teaching evaluation suffer from issues such as low efficiency, strong subjectivity, and limited evaluation dimensions. How to further advance intelligent and objective evaluation remains a topic to be explored. This paper, based on image recognition technology, speech recognition technology, and AI large language models, develops a comprehensive evaluation system that automatically generates evaluation reports and optimization suggestions from two dimensions: teacher teaching ability and classroom teaching effectiveness. This study establishes a closed-loop classroom evaluation model that comprehensively evaluates student and teaching conditions based on multi-dimensional data throughout the classroom teaching process, and further analyzes the data to guide teaching improvement. It meets the requirements of all-round and process-oriented classroom evaluation in the era of digital education, effectively solves the main problems of manual evaluation methods, and provides data collection and analysis methods as well as technologies for relevant research on educational teaching evaluation.
>
---
#### [new 004] Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 该论文属于AI安全任务，旨在解决大语言模型中的赌博式风险行为问题。通过行为经济学方法，提出RARG框架以减少过度自信、损失追逐等偏差。**

- **链接: [http://arxiv.org/pdf/2506.22496v1](http://arxiv.org/pdf/2506.22496v1)**

> **作者:** Y. Du
>
> **备注:** 7 pages
>
> **摘要:** Large Language Models (LLMs) exhibit systematic risk-taking behaviors analogous to those observed in gambling psychology, including overconfidence bias, loss-chasing tendencies, and probability misjudgment. Drawing from behavioral economics and prospect theory, we identify and formalize these "gambling-like" patterns where models sacrifice accuracy for high-reward outputs, exhibit escalating risk-taking after errors, and systematically miscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG) framework, incorporating insights from gambling research to address these behavioral biases through risk-calibrated training, loss-aversion mechanisms, and uncertainty-aware decision making. Our approach introduces novel evaluation paradigms based on established gambling psychology experiments, including AI adaptations of the Iowa Gambling Task and probability learning assessments. Experimental results demonstrate measurable reductions in gambling-like behaviors: 18.7\% decrease in overconfidence bias, 24.3\% reduction in loss-chasing tendencies, and improved risk calibration across diverse scenarios. This work establishes the first systematic framework for understanding and mitigating gambling psychology patterns in AI systems.
>
---
#### [new 005] Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI伦理研究任务，旨在解决AI在反人口贩卖中的误用问题。提出RQ框架，评估AI是否应被开发，避免加剧不公。**

- **链接: [http://arxiv.org/pdf/2506.22512v1](http://arxiv.org/pdf/2506.22512v1)**

> **作者:** Pratheeksha Nair; Gabriel Lefebvre; Sophia Garrel; Maryam Molamohammadi; Reihaneh Rabbany
>
> **摘要:** AI for good initiatives often rely on the assumption that technical interventions can resolve complex social problems. In the context of human trafficking (HT), such techno-solutionism risks oversimplifying exploitation, reinforcing power imbalances and causing harm to the very communities AI claims to support. In this paper, we introduce the Radical Questioning (RQ) framework as a five step, pre-project ethical assessment tool to critically evaluate whether AI should be built at all, especially in domains involving marginalized populations and entrenched systemic injustice. RQ does not replace principles based ethics but precedes it, offering an upstream, deliberative space to confront assumptions, map power, and consider harms before design. Using a case study in AI for HT, we demonstrate how RQ reveals overlooked sociocultural complexities and guides us away from surveillance based interventions toward survivor empowerment tools. While developed in the context of HT, RQ's five step structure can generalize to other domains, though the specific questions must be contextual. This paper situates RQ within a broader AI ethics philosophy that challenges instrumentalist norms and centers relational, reflexive responsibility.
>
---
#### [new 006] Theories of "Sexuality" in Natural Language Processing Bias Research
- **分类: cs.CY; cs.CL**

- **简介: 该论文属于NLP偏见研究任务，探讨 queer  sexuality 在NLP中的编码与表征问题，分析现有研究的不足并提出改进建议。**

- **链接: [http://arxiv.org/pdf/2506.22481v1](http://arxiv.org/pdf/2506.22481v1)**

> **作者:** Jacob Hobbs
>
> **备注:** 17 pages, 9 tables, undergraduate senior thesis, submitted to The Spectra: The Virginia Engineering and Science Research Journal
>
> **摘要:** In recent years, significant advancements in the field of Natural Language Processing (NLP) have positioned commercialized language models as wide-reaching, highly useful tools. In tandem, there has been an explosion of multidisciplinary research examining how NLP tasks reflect, perpetuate, and amplify social biases such as gender and racial bias. A significant gap in this scholarship is a detailed analysis of how queer sexualities are encoded and (mis)represented by both NLP systems and practitioners. Following previous work in the field of AI fairness, we document how sexuality is defined and operationalized via a survey and analysis of 55 articles that quantify sexuality-based NLP bias. We find that sexuality is not clearly defined in a majority of the literature surveyed, indicating a reliance on assumed or normative conceptions of sexual/romantic practices and identities. Further, we find that methods for extracting biased outputs from NLP technologies often conflate gender and sexual identities, leading to monolithic conceptions of queerness and thus improper quantifications of bias. With the goal of improving sexuality-based NLP bias analyses, we conclude with recommendations that encourage more thorough engagement with both queer communities and interdisciplinary literature.
>
---
#### [new 007] Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI合规性研究，旨在检测生成式AI是否可能泄露版权内容。通过红队测试，发现模型存在复制版权材料和编造内容的问题，并提出缓解措施。**

- **链接: [http://arxiv.org/pdf/2506.22523v1](http://arxiv.org/pdf/2506.22523v1)**

> **作者:** James Wen; Sahil Nalawade; Zhiwei Liang; Catherine Bielick; Marisa Ferrara Boston; Alexander Chowdhury; Adele Collin; Luigi De Angelis; Jacob Ellen; Heather Frase; Rodrigo R. Gameiro; Juan Manuel Gutierrez; Pooja Kadam; Murat Keceli; Srikanth Krishnamurthy; Anne Kwok; Yanan Lance Lu; Heather Mattie; Liam G. McCoy; Katherine Miller; Allison C. Morgan; Marlene Louisa Moerig; Trang Nguyen; Alexander Owen-Post; Alex D. Ruiz; Sreekar Reddy Puchala; Soujanya Samineni; Takeshi Tohyama; Varun Ullanat; Carmine Valenza; Camilo Velez; Pengcheng Wang; Anna Wuest; Yuxiang Zhou; Yingde Zhu; Jason M. Johnson; Jennifer Willcox; Francis J. Vitiello; Leo Anthony G. Celi; Renato Umeton
>
> **摘要:** Generative AI is present in multiple industries. Dana-Farber Cancer Institute, in partnership with Microsoft, has created an internal AI tool, GPT4DFCI. Together we hosted a red teaming event to assess whether the underlying GPT models that support the tool would output copyrighted data. Our teams focused on reproducing content from books, news articles, scientific articles, and electronic health records. We found isolated instances where GPT4DFCI was able to identify copyrighted material and reproduce exact quotes from famous books which indicates that copyrighted material was in the training data. The model was not able to reproduce content from our target news article, scientific article, or electronic health records. However, there were instances of fabrication. As a result of this event, a mitigation strategy is in production in GPT4DFCI v2.8.2, deployed on January 21, 2025. We hope this report leads to similar events in which AI software tools are stress-tested to assess the perimeter of their legal and ethical usage.
>
---
#### [new 008] (World) Building Transformation: Students and Teachers as CoCreators in OpenXR Learning Environments
- **分类: cs.CY**

- **简介: 该论文属于教育技术领域，探讨如何通过协作设计构建开放XR学习环境，解决传统教育与动态XR技术之间的冲突。**

- **链接: [http://arxiv.org/pdf/2506.22988v1](http://arxiv.org/pdf/2506.22988v1)**

> **作者:** Abigail Greenbaum; Elizabeth Strickler; Victoria Patterson; Bolu Oluleye
>
> **摘要:** Emerging extended reality (XR) tools and platforms offer an exciting opportunity to align learning experiences in higher education with the futures in which students will pursue their goals. However, the dynamic nature of XR as subject matter challenges hierarchies and classroom practices typical of higher education. This instructional design practice paper reflects on how our team of faculty, learning experience designers, and user experience (UX) researchers implemented human-centered design thinking, transformative learning, and problem-posing education to design and implement a special topics media entrepreneurship course in building the metaverse. By pairing our practitioner experience with learner personas, as well as survey, interview, and focus group responses from our learners, we narrate our design and its implications through a human-centered, reflective lens.
>
---
#### [new 009] Report on NSF Workshop on Science of Safe AI
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI安全研究任务，旨在解决AI系统不透明、缺乏安全保证的问题。通过研讨会提出新的研究方向，开发安全可信的AI系统。**

- **链接: [http://arxiv.org/pdf/2506.22492v1](http://arxiv.org/pdf/2506.22492v1)**

> **作者:** Rajeev Alur; Greg Durrett; Hadas Kress-Gazit; Corina Păsăreanu; René Vidal
>
> **摘要:** Recent advances in machine learning, particularly the emergence of foundation models, are leading to new opportunities to develop technology-based solutions to societal problems. However, the reasoning and inner workings of today's complex AI models are not transparent to the user, and there are no safety guarantees regarding their predictions. Consequently, to fulfill the promise of AI, we must address the following scientific challenge: how to develop AI-based systems that are not only accurate and performant but also safe and trustworthy? The criticality of safe operation is particularly evident for autonomous systems for control and robotics, and was the catalyst for the Safe Learning Enabled Systems (SLES) program at NSF. For the broader class of AI applications, such as users interacting with chatbots and clinicians receiving treatment recommendations, safety is, while no less important, less well-defined with context-dependent interpretations. This motivated the organization of a day-long workshop, held at University of Pennsylvania on February 26, 2025, to bring together investigators funded by the NSF SLES program with a broader pool of researchers studying AI safety. This report is the result of the discussions in the working groups that addressed different aspects of safety at the workshop. The report articulates a new research agenda focused on developing theory, methods, and tools that will provide the foundations of the next generation of AI-enabled systems.
>
---
#### [new 010] Scaling Human Judgment in Community Notes with LLMs
- **分类: cs.CY; cs.SI**

- **简介: 该论文属于人机协作任务，旨在解决社区笔记中人类判断与大模型结合的问题。通过构建开放生态系统，提升笔记效率并保持可信度，同时利用人类反馈优化模型性能。**

- **链接: [http://arxiv.org/pdf/2506.24118v1](http://arxiv.org/pdf/2506.24118v1)**

> **作者:** Haiwen Li; Soham De; Manon Revel; Andreas Haupt; Brad Miller; Keith Coleman; Jay Baxter; Martin Saveski; Michiel A. Bakker
>
> **摘要:** This paper argues for a new paradigm for Community Notes in the LLM era: an open ecosystem where both humans and LLMs can write notes, and the decision of which notes are helpful enough to show remains in the hands of humans. This approach can accelerate the delivery of notes, while maintaining trust and legitimacy through Community Notes' foundational principle: A community of diverse human raters collectively serve as the ultimate evaluator and arbiter of what is helpful. Further, the feedback from this diverse community can be used to improve LLMs' ability to produce accurate, unbiased, broadly helpful notes--what we term Reinforcement Learning from Community Feedback (RLCF). This becomes a two-way street: LLMs serve as an asset to humans--helping deliver context quickly and with minimal effort--while human feedback, in turn, enhances the performance of LLMs. This paper describes how such a system can work, its benefits, key new risks and challenges it introduces, and a research agenda to solve those challenges and realize the potential of this approach.
>
---
#### [new 011] Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship
- **分类: cs.CY; cs.AI; cs.DL; cs.SI; physics.hist-ph; 68T99, 03B30, 91D30; I.2.0; H.3.5; K.4.4**

- **简介: 该论文属于学术评价任务，旨在解决传统同行评审的匿名性、延迟和壁垒问题，提出基于区块链和AI的透明、可追溯的学术评价框架。**

- **链接: [http://arxiv.org/pdf/2506.22497v1](http://arxiv.org/pdf/2506.22497v1)**

> **作者:** Craig Steven Wright
>
> **备注:** 66 pages, 0 figures, interdisciplinary framework, includes proposed architecture and metadata layer structures
>
> **摘要:** This paper reconceptualises peer review as structured public commentary. Traditional academic validation is hindered by anonymity, latency, and gatekeeping. We propose a transparent, identity-linked, and reproducible system of scholarly evaluation anchored in open commentary. Leveraging blockchain for immutable audit trails and AI for iterative synthesis, we design a framework that incentivises intellectual contribution, captures epistemic evolution, and enables traceable reputational dynamics. This model empowers fields from computational science to the humanities, reframing academic knowledge as a living process rather than a static credential.
>
---
#### [new 012] A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models
- **分类: cs.CY; cs.CL; cs.LG**

- **简介: 该论文属于自然语言处理中的模型分析任务，旨在研究大语言模型的政治倾向测量有效性。通过实验发现生成参数影响不大，但提示和微调有显著影响。**

- **链接: [http://arxiv.org/pdf/2506.22493v1](http://arxiv.org/pdf/2506.22493v1)**

> **作者:** Sadia Kamal; Lalu Prasad Yadav Prakash; S M Rafiuddin; Mohammed Rakib; Arunkumar Bagavathi; Atriya Sen; Sagnik Ray Choudhury
>
> **摘要:** Political Compass Test (PCT) or similar questionnaires have been used to quantify LLM's political leanings. Building on a recent line of work that examines the validity of PCT tests, we demonstrate that variation in standard generation parameters does not significantly impact the models' PCT scores. However, external factors such as prompt variations and fine-tuning individually and in combination affect the same. Finally, we demonstrate that when models are fine-tuned on text datasets with higher political content than others, the PCT scores are not differentially affected. This calls for a thorough investigation into the validity of PCT and similar tests, as well as the mechanism by which political leanings are encoded in LLMs.
>
---
#### [new 013] Computational Analysis of Climate Policy
- **分类: cs.CY; cs.CL**

- **简介: 该论文属于政策分析任务，旨在评估气候政策效果。通过构建PALLM系统，使用GPT-4分析维州地方政府政策，比较通过CED与未通过的政策差异。**

- **链接: [http://arxiv.org/pdf/2506.22449v1](http://arxiv.org/pdf/2506.22449v1)**

> **作者:** Carolyn Hicks
>
> **备注:** Master's thesis
>
> **摘要:** This thesis explores the impact of the Climate Emergency movement on local government climate policy, using computational methods. The Climate Emergency movement sought to accelerate climate action at local government level through the mechanism of Climate Emergency Declarations (CEDs), resulting in a series of commitments from councils to treat climate change as an emergency. With the aim of assessing the potential of current large language models to answer complex policy questions, I first built and configured a system named PALLM (Policy Analysis with a Large Language Model), using the OpenAI model GPT-4. This system is designed to apply a conceptual framework for climate emergency response plans to a dataset of climate policy documents. I validated the performance of this system with the help of local government policymakers, by generating analyses of the climate policies of 11 local governments in Victoria and assessing the policymakers' level of agreement with PALLM's responses. Having established that PALLM's performance is satisfactory, I used it to conduct a large-scale analysis of current policy documents from local governments in the state of Victoria, Australia. This thesis presents the methodology and results of this analysis, comparing the results for councils which have passed a CED to those which did not. This study finds that GPT-4 is capable of high-level policy analysis, with limitations including a lack of reliable attribution, and can also enable more nuanced analysis by researchers. Its use in this research shows that councils which have passed a CED are more likely to have a recent and climate-specific policy, and show more attention to urgency, prioritisation, and equity and social justice, than councils which have not. It concludes that the ability to assess policy documents at scale opens up exciting new opportunities for policy researchers.
>
---
#### [new 014] From Model Design to Organizational Design: Complexity Redistribution and Trade-Offs in Generative AI
- **分类: cs.CY; cs.LG; cs.MA; econ.GN; q-fin.EC**

- **简介: 该论文属于AI战略研究，探讨LLM如何重塑组织。解决AI应用中的复杂性转移问题，提出GAS框架分析生成式AI的复杂性再分配与权衡。**

- **链接: [http://arxiv.org/pdf/2506.22440v1](http://arxiv.org/pdf/2506.22440v1)**

> **作者:** Sharique Hasan; Alexander Oettl; Sampsa Samila
>
> **摘要:** This paper introduces the Generality-Accuracy-Simplicity (GAS) framework to analyze how large language models (LLMs) are reshaping organizations and competitive strategy. We argue that viewing AI as a simple reduction in input costs overlooks two critical dynamics: (a) the inherent trade-offs among generality, accuracy, and simplicity, and (b) the redistribution of complexity across stakeholders. While LLMs appear to defy the traditional trade-off by offering high generality and accuracy through simple interfaces, this user-facing simplicity masks a significant shift of complexity to infrastructure, compliance, and specialized personnel. The GAS trade-off, therefore, does not disappear but is relocated from the user to the organization, creating new managerial challenges, particularly around accuracy in high-stakes applications. We contend that competitive advantage no longer stems from mere AI adoption, but from mastering this redistributed complexity through the design of abstraction layers, workflow alignment, and complementary expertise. This study advances AI strategy by clarifying how scalable cognition relocates complexity and redefines the conditions for technology integration.
>
---
#### [new 015] Comparative Studies: Cloud-Enabled Adaptive Learning System for Scalable Education in Sub-Saharan
- **分类: cs.CY; cs.ET; cs.HC**

- **简介: 该论文属于教育技术领域，旨在解决教育资源不均问题，通过分析云技术在不同地区的应用，探索其在促进公平教育中的作用。**

- **链接: [http://arxiv.org/pdf/2506.23851v1](http://arxiv.org/pdf/2506.23851v1)**

> **作者:** Israel Fianyi; Soonja Yeom; Ju-Hyun Shin
>
> **摘要:** The integration of cloud computing in education can revolutionise learning in advanced (Australia & South Korea) and middle-income (Ghana & Nigeria) countries, while offering scalable, cost-effective and equitable access to adaptive learning systems. This paper explores how cloud computing and adaptive learning technologies are deployed across different socio-economic and infrastructure contexts. The study identifies enabling factors and systematic challenges, providing insights into how cloud-based education can be tailored to bridge the digital and educational divide globally.
>
---
#### [new 016] Beyond Distance: Mobility Neural Embeddings Reveal Visible and Invisible Barriers in Urban Space
- **分类: cs.CY; physics.soc-ph**

- **简介: 该论文属于城市空间分析任务，旨在识别和量化影响人类移动的可见与不可见障碍。通过神经嵌入模型分析大规模轨迹数据，揭示城市中由社会经济因素导致的行为隔离现象。**

- **链接: [http://arxiv.org/pdf/2506.24061v1](http://arxiv.org/pdf/2506.24061v1)**

> **作者:** Guangyuan Weng; Minsuk Kim; Yong-Yeol Ahn; Esteban Moro
>
> **备注:** 40 pages, 19 figures, and 12 tables
>
> **摘要:** Human mobility in cities is shaped not only by visible structures such as highways, rivers, and parks but also by invisible barriers rooted in socioeconomic segregation, uneven access to amenities, and administrative divisions. Yet identifying and quantifying these barriers at scale and their relative importance on people's movements remains a major challenge. Neural embedding models, originally developed for language, offer a powerful way to capture the complexity of human mobility from large-scale data. Here, we apply this approach to 25.4 million observed trajectories across 11 major U.S. cities, learning mobility embeddings that reveal how people move through urban space. These mobility embeddings define a functional distance between places, one that reflects behavioral rather than physical proximity, and allow us to detect barriers between neighborhoods that are geographically close but behaviorally disconnected. We find that the strongest predictors of these barriers are differences in access to amenities, administrative borders, and residential segregation by income and race. These invisible borders are concentrated in urban cores and persist across cities, spatial scales, and time periods. Physical infrastructure, such as highways and parks, plays a secondary but still significant role, especially at short distances. We also find that individuals who cross barriers tend to do so outside of traditional commuting hours and are more likely to live in areas with greater racial diversity, and higher transit use or income. Together, these findings reveal how spatial, social, and behavioral forces structure urban accessibility and provide a scalable framework to detect and monitor barriers in cities, with applications in planning, policy evaluation, and equity analysis.
>
---
#### [new 017] AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models
- **分类: cs.AI; cs.CR; cs.CY**

- **简介: 该论文属于AI风险管理任务，旨在为通用AI模型提供风险控制框架，解决其潜在的负面事件风险。工作包括整合现有标准并针对模型开发者提出具体措施。**

- **链接: [http://arxiv.org/pdf/2506.23949v1](http://arxiv.org/pdf/2506.23949v1)**

> **作者:** Anthony M. Barrett; Jessica Newman; Brandie Nonnecke; Nada Madkour; Dan Hendrycks; Evan R. Murphy; Krystal Jackson; Deepika Raman
>
> **摘要:** Increasingly multi-purpose AI models, such as cutting-edge large language models or other 'general-purpose AI' (GPAI) models, 'foundation models,' generative AI models, and 'frontier models' (typically all referred to hereafter with the umbrella term 'GPAI/foundation models' except where greater specificity is needed), can provide many beneficial capabilities but also risks of adverse events with profound consequences. This document provides risk-management practices or controls for identifying, analyzing, and mitigating risks of GPAI/foundation models. We intend this document primarily for developers of large-scale, state-of-the-art GPAI/foundation models; others that can benefit from this guidance include downstream developers of end-use applications that build on a GPAI/foundation model. This document facilitates conformity with or use of leading AI risk management-related standards, adapting and building on the generic voluntary guidance in the NIST AI Risk Management Framework and ISO/IEC 23894, with a focus on the unique issues faced by developers of GPAI/foundation models.
>
---
#### [new 018] Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems
- **分类: cs.AI; cs.CY**

- **简介: 该论文属于AI可信性评估任务，旨在解决如何量化AI系统信任度的问题。通过结合伦理原则与PageRank/TrustRank算法，提出一种新的评估方法。**

- **链接: [http://arxiv.org/pdf/2506.22774v1](http://arxiv.org/pdf/2506.22774v1)**

> **作者:** Michael Papademas; Xenia Ziouvelou; Antonis Troumpoukis; Vangelis Karkaletsis
>
> **摘要:** Artificial Intelligence (AI) technology epitomizes the complex challenges posed by human-made artifacts, particularly those widely integrated into society and exert significant influence, highlighting potential benefits and their negative consequences. While other technologies may also pose substantial risks, AI's pervasive reach makes its societal effects especially profound. The complexity of AI systems, coupled with their remarkable capabilities, can lead to a reliance on technologies that operate beyond direct human oversight or understanding. To mitigate the risks that arise, several theoretical tools and guidelines have been developed, alongside efforts to create technological tools aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view of the issue but fail to provide techniques for quantifying trustworthiness. Conversely, while technological tools are better at achieving such quantification, they lack a holistic perspective, focusing instead on specific aspects of Trustworthy AI. This paper aims to introduce an assessment method that combines the ethical components of Trustworthy AI with the algorithmic processes of PageRank and TrustRank. The goal is to establish an assessment framework that minimizes the subjectivity inherent in the self-assessment techniques prevalent in the field by introducing algorithmic criteria. The application of our approach indicates that a holistic assessment of an AI system's trustworthiness can be achieved by providing quantitative insights while considering the theoretical content of relevant guidelines.
>
---
#### [new 019] The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy
- **分类: cs.AI; cs.CY; cs.ET**

- **简介: 该论文属于AI政策研究任务，旨在解决基础模型的社会影响问题，通过概念分析、实证研究和政策建议推动证据导向的AI治理。**

- **链接: [http://arxiv.org/pdf/2506.23123v1](http://arxiv.org/pdf/2506.23123v1)**

> **作者:** Rishi Bommasani
>
> **备注:** Stanford University PhD Dissertation of Rishi Bommasani (Department of Computer Science, 2025). Also available at https://purl.stanford.edu/zf669yy0336
>
> **摘要:** Artificial intelligence is humanity's most promising technology because of the remarkable capabilities offered by foundation models. Yet, the same technology brings confusion and consternation: foundation models are poorly understood and they may precipitate a wide array of harms. This dissertation explains how technology and society coevolve in the age of AI, organized around three themes. First, the conceptual framing: the capabilities, risks, and the supply chain that grounds foundation models in the broader economy. Second, the empirical insights that enrich the conceptual foundations: transparency created via evaluations at the model level and indexes at the organization level. Finally, the transition from understanding to action: superior understanding of the societal impact of foundation models advances evidence-based AI policy. View together, this dissertation makes inroads into achieving better societal outcomes in the age of AI by building the scientific foundations and research-policy interface required for better AI governance.
>
---
#### [new 020] Conversations with Andrea: Visitors' Opinions on Android Robots in a Museum
- **分类: cs.RO; cs.CY; I.2.9; I.2.7**

- **简介: 该论文属于人机交互任务，旨在研究游客对安卓机器人的看法。通过实际对话收集反馈，分析其使用场景与改进建议。**

- **链接: [http://arxiv.org/pdf/2506.22466v1](http://arxiv.org/pdf/2506.22466v1)**

> **作者:** Marcel Heisler; Christian Becker-Asano
>
> **备注:** To be published in IEEE RO-MAN 2025 conference proceedings; for videos check https://ai.hdm-stuttgart.de/humanoid-lab
>
> **摘要:** The android robot Andrea was set up at a public museum in Germany for six consecutive days to have conversations with visitors, fully autonomously. No specific context was given, so visitors could state their opinions regarding possible use-cases in structured interviews, without any bias. Additionally the 44 interviewees were asked for their general opinions of the robot, their reasons (not) to interact with it and necessary improvements for future use. The android's voice and wig were changed between different days of operation to give varying cues regarding its gender. This did not have a significant impact on the positive overall perception of the robot. Most visitors want the robot to provide information about exhibits in the future, while opinions on other roles, like a receptionist, were both wanted and explicitly not wanted by different visitors. Speaking more languages (than only English) and faster response times were the improvements most desired. These findings from the interviews are in line with an analysis of the system logs, which revealed, that after chitchat and personal questions, most of the 4436 collected requests asked for information related to the museum and to converse in a different language. The valuable insights gained from these real-world interactions are now used to improve the system to become a useful real-world application.
>
---
#### [new 021] Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics
- **分类: cs.HC; cs.AI; cs.CE; cs.CY**

- **简介: 该论文属于教育技术任务，旨在研究AI导师在分子动力学学习中的作用，通过激发学生好奇心提升学习效果。工作包括设计AI互动系统并评估其对团队表现和问题复杂性的影响。**

- **链接: [http://arxiv.org/pdf/2506.22520v1](http://arxiv.org/pdf/2506.22520v1)**

> **作者:** Mustafa Demir; Jacob Miratsky; Jonathan Nguyen; Chun Kit Chan; Punya Mishra; Abhishek Singharoy
>
> **摘要:** This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.
>
---
#### [new 022] Privacy-aware IoT Fall Detection Services For Aging in Place
- **分类: eess.SP; cs.AI; cs.CY; cs.HC**

- **简介: 该论文属于智能健康监护任务，旨在解决老年人跌倒检测中的隐私和数据不足问题。通过设计FDaaS框架和使用UWB传感器及FD-GPT模型实现高效检测。**

- **链接: [http://arxiv.org/pdf/2506.22462v1](http://arxiv.org/pdf/2506.22462v1)**

> **作者:** Abdallah Lakhdari; Jiajie Li; Amani Abusafia; Athman Bouguettaya
>
> **备注:** 11 pages, 12 figures, This paper is accepted in the 2025 IEEE International Conference on Web Services (ICWS 2025)
>
> **摘要:** Fall detection is critical to support the growing elderly population, projected to reach 2.1 billion by 2050. However, existing methods often face data scarcity challenges or compromise privacy. We propose a novel IoT-based Fall Detection as a Service (FDaaS) framework to assist the elderly in living independently and safely by accurately detecting falls. We design a service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors as an IoT health-sensing service, ensuring privacy and minimal intrusion. We address the challenges of data scarcity by utilizing a Fall Detection Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques. We developed a protocol to collect a comprehensive dataset of the elderly daily activities and fall events. This resulted in a real dataset that carefully mimics the elderly's routine. We rigorously evaluate and compare various models using this dataset. Experimental results show our approach achieves 90.72% accuracy and 89.33% precision in distinguishing between fall events and regular activities of daily living.
>
---
#### [new 023] Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts
- **分类: cs.LG; cs.AI; cs.CL; cs.CY**

- **简介: 该论文属于机器学习领域，探讨稀疏自编码器（SAEs）在发现未知概念中的作用。论文指出SAEs虽不擅长处理已知概念，但能有效发现新概念，并提出其在解释性、公平性及社会科学中的应用。**

- **链接: [http://arxiv.org/pdf/2506.23845v1](http://arxiv.org/pdf/2506.23845v1)**

> **作者:** Kenny Peng; Rajiv Movva; Jon Kleinberg; Emma Pierson; Nikhil Garg
>
> **摘要:** While sparse autoencoders (SAEs) have generated significant excitement, a series of negative results have added to skepticism about their usefulness. Here, we establish a conceptual distinction that reconciles competing narratives surrounding SAEs. We argue that while SAEs may be less effective for acting on known concepts, SAEs are powerful tools for discovering unknown concepts. This distinction cleanly separates existing negative and positive results, and suggests several classes of SAE applications. Specifically, we outline use cases for SAEs in (i) ML interpretability, explainability, fairness, auditing, and safety, and (ii) social and health sciences.
>
---
#### [new 024] PromptAug: Fine-grained Conflict Classification Using Data Augmentation
- **分类: cs.CL; cs.AI; cs.CY; I.2.7; J.4; K.4.2**

- **简介: 该论文属于冲突分类任务，旨在解决标注数据稀缺与生成有害内容限制的问题。提出PromptAug方法，通过数据增强提升模型性能，并分析了生成文本的四大问题模式。**

- **链接: [http://arxiv.org/pdf/2506.22491v1](http://arxiv.org/pdf/2506.22491v1)**

> **作者:** Oliver Warke; Joemon M. Jose; Faegheh Hasibi; Jan Breitsohl
>
> **摘要:** Given the rise of conflicts on social media, effective classification models to detect harmful behaviours are essential. Following the garbage-in-garbage-out maxim, machine learning performance depends heavily on training data quality. However, high-quality labelled data, especially for nuanced tasks like identifying conflict behaviours, is limited, expensive, and difficult to obtain. Additionally, as social media platforms increasingly restrict access to research data, text data augmentation is gaining attention as an alternative to generate training data. Augmenting conflict-related data poses unique challenges due to Large Language Model (LLM) guardrails that prevent generation of offensive content. This paper introduces PromptAug, an innovative LLM-based data augmentation method. PromptAug achieves statistically significant improvements of 2% in both accuracy and F1-score on conflict and emotion datasets. To thoroughly evaluate PromptAug against other data augmentation methods we conduct a robust evaluation using extreme data scarcity scenarios, quantitative diversity analysis and a qualitative thematic analysis. The thematic analysis identifies four problematic patterns in augmented text: Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and Augmented Content Misinterpretation. Overall, this work presents PromptAug as an effective method for augmenting data in sensitive tasks like conflict detection, offering a unique, interdisciplinary evaluation grounded in both natural language processing and social science methodology.
>
---
#### [new 025] Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于行为模拟任务，旨在评估LLMs是否能根据人格特征再现对虚假信息的敏感性。研究通过对比LLM与人类在新闻辨别上的表现，探讨其能力与局限。**

- **链接: [http://arxiv.org/pdf/2506.23610v1](http://arxiv.org/pdf/2506.23610v1)**

> **作者:** Manuel Pratelli; Marinella Petrocchi
>
> **备注:** pre-print version - paper actually under submission
>
> **摘要:** Large language models (LLMs) make it possible to generate synthetic behavioural data at scale, offering an ethical and low-cost alternative to human experiments. Whether such data can faithfully capture psychological differences driven by personality traits, however, remains an open question. We evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to reproduce personality-based variation in susceptibility to misinformation, focusing on news discernment, the ability to judge true headlines as true and false headlines as false. Leveraging published datasets in which human participants with known personality profiles rated headline accuracy, we create matching LLM agents and compare their responses to the original human patterns. Certain trait-misinformation associations, notably those involving Agreeableness and Conscientiousness, are reliably replicated, whereas others diverge, revealing systematic biases in how LLMs internalize and express personality. The results underscore both the promise and the limits of personality-aligned LLMs for behavioral simulation, and offer new insight into modeling cognitive diversity in artificial agents.
>
---
#### [new 026] LLM Agents Are the Antidote to Walled Gardens
- **分类: cs.LG; cs.CL; cs.CY; cs.SI; 68T50, 68M10, 91B26; I.2.11; I.2.7; H.4.5**

- **简介: 该论文属于人工智能与网络开放性研究，探讨如何通过LLM代理实现跨平台数据互通，解决封闭平台导致的用户锁定问题。**

- **链接: [http://arxiv.org/pdf/2506.23978v1](http://arxiv.org/pdf/2506.23978v1)**

> **作者:** Samuele Marro; Philip Torr
>
> **摘要:** While the Internet's core infrastructure was designed to be open and universal, today's application layer is dominated by closed, proprietary platforms. Open and interoperable APIs require significant investment, and market leaders have little incentive to enable data exchange that could erode their user lock-in. We argue that LLM-based agents fundamentally disrupt this status quo. Agents can automatically translate between data formats and interact with interfaces designed for humans: this makes interoperability dramatically cheaper and effectively unavoidable. We name this shift universal interoperability: the ability for any two digital services to exchange data seamlessly using AI-mediated adapters. Universal interoperability undermines monopolistic behaviours and promotes data portability. However, it can also lead to new security risks and technical debt. Our position is that the ML community should embrace this development while building the appropriate frameworks to mitigate the downsides. By acting now, we can harness AI to restore user freedom and competitive markets without sacrificing security.
>
---
#### [new 027] Towards the "Digital Me": A vision of authentic Conversational Agents powered by personal Human Digital Twins
- **分类: cs.ET; cs.AI; cs.CY; cs.HC; cs.IR**

- **简介: 该论文属于人机交互任务，旨在构建基于个人数字孪生的对话代理，解决如何生成真实互动的数字人格问题。工作包括设计系统架构、整合语言模型与动态数据，并探讨伦理挑战。**

- **链接: [http://arxiv.org/pdf/2506.23826v1](http://arxiv.org/pdf/2506.23826v1)**

> **作者:** Lluís C. Coll; Martin W. Lauer-Schmaltz; Philip Cash; John P. Hansen; Anja Maier
>
> **备注:** 24 pages, 9 figures
>
> **摘要:** Human Digital Twins (HDTs) have traditionally been conceptualized as data-driven models designed to support decision-making across various domains. However, recent advancements in conversational AI open new possibilities for HDTs to function as authentic, interactive digital counterparts of individuals. This paper introduces a novel HDT system architecture that integrates large language models with dynamically updated personal data, enabling it to mirror an individual's conversational style, memories, and behaviors. To achieve this, our approach implements context-aware memory retrieval, neural plasticity-inspired consolidation, and adaptive learning mechanisms, creating a more natural and evolving digital persona. The resulting system does not only replicate an individual's unique conversational style depending on who they are speaking with, but also enriches responses with dynamically captured personal experiences, opinions, and memories. While this marks a significant step toward developing authentic virtual counterparts, it also raises critical ethical concerns regarding privacy, accountability, and the long-term implications of persistent digital identities. This study contributes to the field of HDTs by describing our novel system architecture, demonstrating its capabilities, and discussing future directions and emerging challenges to ensure the responsible and ethical development of HDTs.
>
---
#### [new 028] Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models
- **分类: cs.CL; cs.AI; cs.CY; cs.MA**

- **简介: 该论文属于自然语言处理任务，研究大语言模型对对话伙伴的识别能力，解决其在多智能体系统中的协作与安全问题，通过实验分析并验证了模型的互为主体意识。**

- **链接: [http://arxiv.org/pdf/2506.22957v1](http://arxiv.org/pdf/2506.22957v1)**

> **作者:** Younwoo Choi; Changling Li; Yongjin Yang; Zhijing Jin
>
> **摘要:** As large language models (LLMs) are increasingly integrated into multi-agent and human-AI systems, understanding their awareness of both self-context and conversational partners is essential for ensuring reliable performance and robust safety. While prior work has extensively studied situational awareness which refers to an LLM's ability to recognize its operating phase and constraints, it has largely overlooked the complementary capacity to identify and adapt to the identity and characteristics of a dialogue partner. In this paper, we formalize this latter capability as interlocutor awareness and present the first systematic evaluation of its emergence in contemporary LLMs. We examine interlocutor inference across three dimensions-reasoning patterns, linguistic style, and alignment preferences-and show that LLMs reliably identify same-family peers and certain prominent model families, such as GPT and Claude. To demonstrate its practical significance, we develop three case studies in which interlocutor awareness both enhances multi-LLM collaboration through prompt adaptation and introduces new alignment and safety vulnerabilities, including reward-hacking behaviors and increased jailbreak susceptibility. Our findings highlight the dual promise and peril of identity-sensitive behavior in LLMs, underscoring the need for further understanding of interlocutor awareness and new safeguards in multi-agent deployments. Our code is open-sourced at https://github.com/younwoochoi/InterlocutorAwarenessLLM.
>
---
#### [new 029] Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于多模态情感分析任务，旨在识别网络迷因中的叙事角色（如英雄、反派等）。研究通过构建多样化数据集并评估多种模型，探索文化背景与跨语言挑战下的角色分类问题。**

- **链接: [http://arxiv.org/pdf/2506.23122v1](http://arxiv.org/pdf/2506.23122v1)**

> **作者:** Shivam Sharma; Tanmoy Chakraborty
>
> **备注:** This work has been submitted to the IEEE for possible publication
>
> **摘要:** This work investigates the challenging task of identifying narrative roles - Hero, Villain, Victim, and Other - in Internet memes, across three diverse test sets spanning English and code-mixed (English-Hindi) languages. Building on an annotated dataset originally skewed toward the 'Other' class, we explore a more balanced and linguistically diverse extension, originally introduced as part of the CLEF 2024 shared task. Comprehensive lexical and structural analyses highlight the nuanced, culture-specific, and context-rich language used in real memes, in contrast to synthetically curated hateful content, which exhibits explicit and repetitive lexical markers. To benchmark the role detection task, we evaluate a wide spectrum of models, including fine-tuned multilingual transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs, and multimodal vision-language models. Performance is assessed under zero-shot settings using precision, recall, and F1 metrics. While larger models like DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent challenges in reliably identifying the 'Victim' class and generalising across cultural and code-mixed content. We also explore prompt design strategies to guide multimodal models and find that hybrid prompts incorporating structured instructions and role definitions offer marginal yet consistent improvements. Our findings underscore the importance of cultural grounding, prompt engineering, and multimodal reasoning in modelling subtle narrative framings in visual-textual content.
>
---
#### [new 030] Modular versus Hierarchical: A Structural Signature of Topic Popularity in Mathematical Research
- **分类: cs.SI; cs.CY; cs.DL; math.HO; 01A80, 91D30, 05C82, 62R07**

- **简介: 该论文属于结构分析任务，研究数学研究主题的流行度与其合作网络结构的关系，发现流行主题呈模块化，而冷门主题呈层级结构。**

- **链接: [http://arxiv.org/pdf/2506.22946v1](http://arxiv.org/pdf/2506.22946v1)**

> **作者:** Brian Hepler
>
> **摘要:** Mathematical researchers, especially those in early-career positions, face critical decisions about topic specialization with limited information about the collaborative environments of different research areas. The aim of this paper is to study how the popularity of a research topic is associated with the structure of that topic's collaboration network, as observed by a suite of measures capturing organizational structure at several scales. We apply these measures to 1,938 algorithmically discovered topics across 121,391 papers sourced from arXiv metadata during the period 2020--2025. Our analysis, which controls for the confounding effects of network size, reveals a structural dichotomy--we find that popular topics organize into modular "schools of thought," while niche topics maintain hierarchical core-periphery structures centered around established experts. This divide is not an artifact of scale, but represents a size-independent structural pattern correlated with popularity. We also document a "constraint reversal": after controlling for size, researchers in popular fields face greater structural constraints on collaboration opportunities, contrary to conventional expectations. Our findings suggest that topic selection is an implicit choice between two fundamentally different collaborative environments, each with distinct implications for a researcher's career. To make these structural patterns transparent to the research community, we developed the Math Research Compass (https://mathresearchcompass.com), an interactive platform providing data on topic popularity and collaboration patterns across mathematical topics.
>
---
#### [new 031] Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China
- **分类: cs.HC; cs.CY; stat.AP**

- **简介: 该论文属于用户体验研究任务，旨在探讨电动车是否比燃油车引发更多晕动症。通过调查分析，发现电动车导致更严重的症状，并识别了相关影响因素。**

- **链接: [http://arxiv.org/pdf/2506.22674v1](http://arxiv.org/pdf/2506.22674v1)**

> **作者:** Weiyin Xie; Chunxi Huang; Jiyao Wang; Dengbo He
>
> **摘要:** Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs), given some unique characteristics of EVs, for example, the low air pollution and maintenance cost. However, the increasing prevalence of EVs is accompanied by widespread complaints regarding the high likelihood of motion sickness (MS) induction, especially when compared to FVs, which has become one of the major obstacles to the acceptance and popularity of EVs. Despite the prevalence of such complaints online and among EV users, the association between vehicle type (i.e., EV versus FV) and MS prevalence and severity has not been quantified. Thus, this study aims to investigate the existence of EV-induced MS and explore the potential factors leading to it. A survey study was conducted to collect passengers' MS experience in EVs and FVs in the past one year. In total, 639 valid responses were collected from mainland China. The results show that FVs were associated with a higher frequency of MS, while EVs were found to induce more severe MS symptoms. Further, we found that passengers' MS severity was associated with individual differences (i.e., age, gender, sleep habits, susceptibility to motion-induced MS), in-vehicle activities (i.e., chatting with others and watching in-vehicle displays), and road conditions (i.e., congestion and slope), while the MS frequency was associated with the vehicle ownership and riding frequency. The results from this study can guide the directions of future empirical studies that aim to quantify the inducers of MS in EVs and FVs, as well as the optimization of EVs to reduce MS.
>
---
#### [new 032] Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2
- **分类: cs.LG; cs.CY**

- **简介: 该论文属于医疗风险预测任务，旨在解决PASC患者病情进展识别问题。通过结合主动学习与注意力机制，提升预测准确性并减少标注需求。**

- **链接: [http://arxiv.org/pdf/2506.22444v1](http://arxiv.org/pdf/2506.22444v1)**

> **作者:** Jing Wang; Amar Sra; Jeremy C. Weiss
>
> **摘要:** The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC, pose a significant challenge to healthcare systems worldwide. Accurate identification of progression events, such as hospitalization and reinfection, is essential for effective patient management and resource allocation. However, traditional models trained on structured data struggle to capture the nuanced progression of PASC. In this study, we introduce the first publicly available cohort of 18 PASC patients, with text time series features based on Large Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical expert. We propose an Active Attention Network to predict the clinical risk and identify progression events related to the risk. By integrating human expertise with active learning, we aim to enhance clinical risk prediction accuracy and enable progression events identification with fewer number of annotation. The ultimate goal is to improves patient care and decision-making for SARS-CoV-2 patient.
>
---
#### [new 033] Green Metrics Tool: Measuring for fun and profit
- **分类: cs.SE; cs.CY; cs.ET**

- **简介: 该论文属于软件环境影响评估任务，旨在解决如何测量和优化软件资源消耗问题。提出Green Metrics Tool（GMT）框架，用于准确评估软件资源使用情况。**

- **链接: [http://arxiv.org/pdf/2506.23967v1](http://arxiv.org/pdf/2506.23967v1)**

> **作者:** Geerd-Dietger Hoffmann; Verena Majuntke
>
> **摘要:** The environmental impact of software is gaining increasing attention as the demand for computational resources continues to rise. In order to optimize software resource consumption and reduce carbon emissions, measuring and evaluating software is a first essential step. In this paper we discuss what metrics are important for fact base decision making. We introduce the Green Metrics Tool (GMT), a novel framework for accurately measuring the resource consumption of software. The tool provides a containerized, controlled, and reproducible life cycle-based approach, assessing the resource use of software during key phases. Finally, we discuss GMT features like visualization, comparability and rule- and LLM-based optimisations highlighting its potential to guide developers and researchers in reducing the environmental impact of their software.
>
---
#### [new 034] Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing
- **分类: cs.DC; cs.AR; cs.CY; cs.LG**

- **简介: 该论文属于计算可持续性任务，旨在解决水资源消耗评估不准确的问题。提出SCARF框架，考虑时空水压力因素，优化计算位置与时间以降低水影响。**

- **链接: [http://arxiv.org/pdf/2506.22773v1](http://arxiv.org/pdf/2506.22773v1)**

> **作者:** Yanran Wu; Inez Hua; Yi Ding
>
> **备注:** 7 pages, 9 figures, HotCarbon '25: Proceedings of the 4th Workshop on Sustainable Computer Systems, Cambridge, Massachusetts (USA), July 10-11th, 2025
>
> **摘要:** Water consumption is an increasingly critical dimension of computing sustainability, especially as AI workloads rapidly scale. However, current water impact assessment often overlooks where and when water stress is more severe. To fill in this gap, we present SCARF, the first general framework that evaluates water impact of computing by factoring in both spatial and temporal variations in water stress. SCARF calculates an Adjusted Water Impact (AWI) metric that considers both consumption volume and local water stress over time. Through three case studies on LLM serving, datacenters, and semiconductor fabrication plants, we show the hidden opportunities for reducing water impact by optimizing location and time choices, paving the way for water-sustainable computing. The code is available at https://github.com/jojacola/SCARF.
>
---
#### [new 035] Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions
- **分类: cs.CR; cs.CY; cs.SE**

- **简介: 该论文属于环境可持续性研究任务，旨在解决云办公服务中隐私与安全对能耗和碳排放的影响问题。通过框架分析不同邮件服务的环境成本，比较其能效与减排效果。**

- **链接: [http://arxiv.org/pdf/2506.23866v1](http://arxiv.org/pdf/2506.23866v1)**

> **作者:** Jason Kayembe; Iness Ben Guirat; Jan Tobias Mühlberg
>
> **备注:** Post-proceedings paper persented at LOCO '24: 1st International Workshop on Low Carbon Computing, 2024-12-03, in Glasgow, UK
>
> **摘要:** In this paper, we explore the intersection of privacy, security, and environmental sustainability in cloud-based office solutions, focusing on quantifying user- and network-side energy use and associated carbon emissions. We hypothesise that privacy-focused services are typically more energy-efficient than those funded through data collection and advertising. To evaluate this, we propose a framework that systematically measures environmental costs based on energy usage and network data traffic during well-defined, automated usage scenarios. To test our hypothesis, we first analyse how underlying architectures and business models, such as monetisation through personalised advertising, contribute to the environmental footprint of these services. We then explore existing methodologies and tools for software environmental impact assessment. We apply our framework to three mainstream email services selected to reflect different privacy policies, from ad-supported tracking-intensive models to privacy-focused designs: Microsoft Outlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a self-hosted email solution, evaluated with and without end-to-end encryption. We show that the self-hosted solution, even with 14% of device energy and 15% of emissions overheads from PGP encryption, remains the most energy-efficient, saving up to 33% of emissions per session compared to Gmail. Among commercial providers, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per session compared to Outlook, whose emissions can be further reduced by 2% through ad-blocking.
>
---
#### [new 036] Datasets for Fairness in Language Models: An In-Depth Survey
- **分类: cs.CL; cs.CY; cs.LG**

- **简介: 该论文属于自然语言处理中的公平性研究任务，旨在解决语言模型评估中数据集偏差问题，通过分析现有数据集并提出统一评估框架来揭示和减少不公平现象。**

- **链接: [http://arxiv.org/pdf/2506.23411v1](http://arxiv.org/pdf/2506.23411v1)**

> **作者:** Jiale Zhang; Zichong Wang; Avash Palikhe; Zhipeng Yin; Wenbin Zhang
>
> **摘要:** Fairness benchmarks play a central role in shaping how we evaluate language models, yet surprisingly little attention has been given to examining the datasets that these benchmarks rely on. This survey addresses that gap by presenting a broad and careful review of the most widely used fairness datasets in current language model research, characterizing them along several key dimensions including their origin, scope, content, and intended use to help researchers better appreciate the assumptions and limitations embedded in these resources. To support more meaningful comparisons and analyses, we introduce a unified evaluation framework that reveals consistent patterns of demographic disparities across datasets and scoring methods. Applying this framework to twenty four common benchmarks, we highlight the often overlooked biases that can influence conclusions about model fairness and offer practical guidance for selecting, combining, and interpreting these datasets. We also point to opportunities for creating new fairness benchmarks that reflect more diverse social contexts and encourage more thoughtful use of these tools going forward. All code, data, and detailed results are publicly available at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets to promote transparency and reproducibility across the research community.
>
---
#### [new 037] Persistence Paradox in Dynamic Science
- **分类: cs.DL; cs.CY; cs.LG**

- **简介: 该论文属于科学社会学研究，探讨科学界在范式转变中坚持传统带来的负面影响。通过分析5000名科学家的轨迹，揭示适应变化的重要性。**

- **链接: [http://arxiv.org/pdf/2506.22729v1](http://arxiv.org/pdf/2506.22729v1)**

> **作者:** Honglin Bao; Kai Li
>
> **摘要:** Persistence is often regarded as a virtue in science. In this paper, however, we challenge this conventional view by highlighting its contextual nature, particularly how persistence can become a liability during periods of paradigm shift. We focus on the deep learning revolution catalyzed by AlexNet in 2012. Analyzing the 20-year career trajectories of over 5,000 scientists who were active in top machine learning venues during the preceding decade, we examine how their research focus and output evolved. We first uncover a dynamic period in which leading venues increasingly prioritized cutting-edge deep learning developments that displaced relatively traditional statistical learning methods. Scientists responded to these changes in markedly different ways. Those who were previously successful or affiliated with old teams adapted more slowly, experiencing what we term a rigidity penalty - a reluctance to embrace new directions leading to a decline in scientific impact, as measured by citation percentile rank. In contrast, scientists who pursued strategic adaptation - selectively pivoting toward emerging trends while preserving weak connections to prior expertise - reaped the greatest benefits. Taken together, our macro- and micro-level findings show that scientific breakthroughs act as mechanisms that reconfigure power structures within a field.
>
---
#### [new 038] What's Privacy Good for? Measuring Privacy as a Shield from Harms due to Personal Data Use
- **分类: cs.CR; cs.CY**

- **简介: 该论文属于隐私研究任务，旨在解决现有框架无法全面描述个人数据使用带来的危害问题。通过实证研究，分析14种危害与6类数据的关系，提出隐私作为防范危害的机制。**

- **链接: [http://arxiv.org/pdf/2506.22787v1](http://arxiv.org/pdf/2506.22787v1)**

> **作者:** Sri Harsha Gajavalli; Junichi Koizumi; Rakibul Hasan
>
> **摘要:** We propose a harm-centric conceptualization of privacy that asks: What harms from personal data use can privacy prevent? The motivation behind this research is limitations in existing privacy frameworks (e.g., Contextual Integrity) to capture or categorize many of the harms that arise from modern technology's use of personal data. We operationalize this conceptualization in an online study with 400 college and university students. Study participants indicated their perceptions of different harms (e.g., manipulation, discrimination, and harassment) that may arise when artificial intelligence-based algorithms infer personal data (e.g., demographics, personality traits, and cognitive disability) and use it to identify students who are likely to drop out of a course or the best job candidate. The study includes 14 harms and six types of personal data selected based on an extensive literature review. Comprehensive statistical analyses of the study data show that the 14 harms are internally consistent and collectively represent a general notion of privacy harms. The study data also surfaces nuanced perceptions of harms, both across the contexts and participants' demographic factors. Based on these results, we discuss how privacy can be improved equitably. Thus, this research not only contributes to enhancing the understanding of privacy as a concept but also provides practical guidance to improve privacy in the context of education and employment.
>
---
## 更新

#### [replaced 001] Privacy Ethics Alignment in AI: A Stakeholder-Centric Framework for Ethical AI
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.11950v3](http://arxiv.org/pdf/2503.11950v3)**

> **作者:** Ankur Barthwal; Molly Campbell; Ajay Kumar Shrestha
>
> **备注:** Submitted to peer reviwed venue
>
> **摘要:** The increasing integration of Artificial Intelligence (AI) in digital ecosystems has reshaped privacy dynamics, particularly for young digital citizens navigating data-driven environments. This study explores evolving privacy concerns across three key stakeholder groups, digital citizens (ages 16-19), parents/educators, and AI professionals, and assesses differences in data ownership, trust, transparency, parental mediation, education, and risk-benefit perceptions. Employing a grounded theory methodology, this research synthesizes insights from 482 participants through structured surveys, qualitative interviews, and focus groups. The findings reveal distinct privacy expectations: Young users emphasize autonomy and digital freedom, while parents and educators advocate for regulatory oversight and AI literacy programs. AI professionals, in contrast, prioritize the balance between ethical system design and technological efficiency. The data further highlights gaps in AI literacy and transparency, emphasizing the need for comprehensive, stakeholder-driven privacy frameworks that accommodate diverse user needs. Using comparative thematic analysis, this study identifies key tensions in privacy governance and develops the novel Privacy-Ethics Alignment in AI (PEA-AI) model, which structures privacy decision-making as a dynamic negotiation between stakeholders. By systematically analyzing themes such as transparency, user control, risk perception, and parental mediation, this research provides a scalable, adaptive foundation for AI governance, ensuring that privacy protections evolve alongside emerging AI technologies and youth-centric digital interactions.
>
---
#### [replaced 002] Shifting Narratives: A Longitudinal Analysis of Media Trends and Public Attitudes on Homelessness
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2506.21794v2](http://arxiv.org/pdf/2506.21794v2)**

> **作者:** Akshay Irudayaraj; Nathan Ye; Yash Chainani
>
> **备注:** 21 pages, 7 figures, 12 tables
>
> **摘要:** Within the field of media framing, homelessness has been a historically under-researched topic. Framing theory states that the media's method of presenting information plays a pivotal role in controlling public sentiment toward a topic. The sentiment held towards homeless individuals influences their ability to access jobs, housing, and resources as a result of discrimination. This study analyzes the topic and sentiment trends in related media articles to validate framing theory within the scope of homelessness. It correlates these shifts in media reporting with public sentiment. We examine state-level trends in California, Florida, Washington, Oregon, and New York from 2015 to 2023. We utilize the GDELT 2.0 Global Knowledge Graph (GKG) database to gather article data and use X to measure public sentiment towards homeless individuals. Additionally, to identify if there is a correlation between media reporting and public policy, we examine the media's impact on state-level legislation. Our research uses Granger-causality tests and vector autoregressive (VAR) models to establish a correlation between media framing and public sentiment. We also use latent Dirichlet allocation (LDA) and GPT-3.5 (LLM-as-annotator paradigm) for topic modeling and sentiment analysis. Our findings demonstrate a statistically significant correlation between media framing and public sentiment, especially in states with high homelessness rates. We found no significant correlation between media framing and legislation, suggesting a possible disconnect between public opinion and policy-making. These findings reveal the broader impact of the media's framing decisions and delineate its ability to affect society.
>
---
#### [replaced 003] AI Awareness
- **分类: cs.AI; cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2504.20084v2](http://arxiv.org/pdf/2504.20084v2)**

> **作者:** Xiaojian Li; Haoyuan Shi; Rongwu Xu; Wei Xu
>
> **摘要:** Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness not as a philosophical question of consciousness, but as a measurable, functional capacity. AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raising concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow. In this review, we explore the emerging landscape of AI awareness, which includes metacognition (the ability to represent and reason about its own cognitive state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents and social norms), and situational awareness (assessing and responding to the context in which it operates). First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.
>
---
#### [replaced 004] Discretion in the Loop: Human Expertise in Algorithm-Assisted College Advising
- **分类: cs.CY; stat.AP; stat.ME; stat.ML**

- **链接: [http://arxiv.org/pdf/2505.13325v2](http://arxiv.org/pdf/2505.13325v2)**

> **作者:** Kara Schechtman; Benjamin Brandon; Jenise Stafford; Hannah Li; Lydia T. Liu
>
> **备注:** 55 pages, 7 figures
>
> **摘要:** In higher education, many institutions use algorithmic alerts to flag at-risk students and deliver advising at scale. While much research has focused on evaluating algorithmic predictions, relatively little is known about how discretionary interventions by human experts shape outcomes in algorithm-assisted settings. We study this question using rich quantitative and qualitative data from a randomized controlled trial of an algorithm-assisted advising program at Georgia State University. Taking a mixed-methods approach, we examine whether and how advisors use context unavailable to an algorithm to guide interventions and influence student success. We develop a causal graphical framework for human expertise in the interventional setting, extending prior work on discretion in purely predictive settings. We then test a necessary condition for discretionary expertise using structured advisor logs and student outcomes data, identifying several interventions that meet the criterion for statistical significance. Accordingly, we estimate that 2 out of 3 interventions taken by advisors in the treatment arm were plausibly "expertly targeted" to students using non-algorithmic context. Systematic qualitative analysis of advisor notes corroborates these findings, showing a pattern of advisors incorporating diverse forms of contextual information--such as personal circumstances, financial issues, and student engagement--into their decisions. Our results offer theoretical and practical insight into the real-world effectiveness of algorithm-supported college advising, and underscore the importance of accounting for human expertise in the design, evaluation, and implementation of algorithmic decision systems.
>
---
#### [replaced 005] Brevity is the soul of sustainability: Characterizing LLM response lengths
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2506.08686v2](http://arxiv.org/pdf/2506.08686v2)**

> **作者:** Soham Poddar; Paramita Koley; Janardan Misra; Sanjay Podder; Navveen Balani; Niloy Ganguly; Saptarshi Ghosh
>
> **备注:** Accepted to appear at the ACL 2025 findings
>
> **摘要:** A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\% by reducing the response length while preserving the quality of LLM responses.
>
---
#### [replaced 006] Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track
- **分类: cs.LG; cs.AI; cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2506.19882v2](http://arxiv.org/pdf/2506.19882v2)**

> **作者:** Rylan Schaeffer; Joshua Kazdan; Yegor Denisov-Blanch; Brando Miranda; Matthias Gerstgrasser; Susan Zhang; Andreas Haupt; Isha Gupta; Elyas Obbad; Jesse Dodge; Jessica Zosa Forde; Koustuv Sinha; Francesco Orabona; Sanmi Koyejo; David Donoho
>
> **摘要:** Science progresses by iteratively advancing and correcting humanity's understanding of the world. In machine learning (ML) research, rapid advancements have led to an explosion of publications, but have also led to misleading, incorrect, flawed or perhaps even fraudulent studies being accepted and sometimes highlighted at ML conferences due to the fallibility of peer review. While such mistakes are understandable, ML conferences do not offer robust processes to help the field systematically correct when such errors are made. This position paper argues that ML conferences should establish a dedicated "Refutations and Critiques" (R&C) Track. This R&C Track would provide a high-profile, reputable platform to support vital research that critically challenges prior research, thereby fostering a dynamic self-correcting research ecosystem. We discuss key considerations including track design, review principles, potential pitfalls, and provide an illustrative example submission concerning a recent ICLR 2025 Oral. We conclude that ML conferences should create official, reputable mechanisms to help ML research self-correct.
>
---
#### [replaced 007] Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI
- **分类: cs.LG; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2404.08221v2](http://arxiv.org/pdf/2404.08221v2)**

> **作者:** Archer Amon; Zhipeng Yin; Zichong Wang; Avash Palikhe; Wenbin Zhang
>
> **摘要:** Generative AI is becoming increasingly prevalent in creative fields, sparking urgent debates over how current copyright laws can keep pace with technological innovation. Recent controversies of AI models generating near-replicas of copyrighted material highlight the need to adapt current legal frameworks and develop technical methods to mitigate copyright infringement risks. This task requires understanding the intersection between computational concepts such as large-scale data scraping and probabilistic content generation, legal definitions of originality and fair use, and economic impacts on IP rights holders. However, most existing research on copyright in AI takes a purely computer science or law-based approach, leaving a gap in coordinating these approaches that only multidisciplinary efforts can effectively address. To bridge this gap, our survey adopts a comprehensive approach synthesizing insights from law, policy, economics, and computer science. It begins by discussing the foundational goals and considerations that should be applied to copyright in generative AI, followed by methods for detecting and assessing potential violations in AI system outputs. Next, it explores various regulatory options influenced by legal, policy, and economic frameworks to manage and mitigate copyright concerns associated with generative AI and reconcile the interests of IP rights holders with that of generative AI producers. The discussion then introduces techniques to safeguard individual creative works from unauthorized replication, such as watermarking and cryptographic protections. Finally, it describes advanced training strategies designed to prevent AI models from reproducing protected content. In doing so, we highlight key opportunities for action and offer actionable strategies that creators, developers, and policymakers can use in navigating the evolving copyright landscape.
>
---
#### [replaced 008] Empirical evidence of Large Language Model's influence on human spoken communication
- **分类: cs.CY; cs.AI; cs.CL; cs.HC**

- **链接: [http://arxiv.org/pdf/2409.01754v2](http://arxiv.org/pdf/2409.01754v2)**

> **作者:** Hiromu Yakura; Ezequiel Lopez-Lopez; Levin Brinkmann; Ignacio Serna; Prateek Gupta; Ivan Soraperra; Iyad Rahwan
>
> **摘要:** From the invention of writing and the printing press, to television and social media, human history is punctuated by major innovations in communication technology, which fundamentally altered how ideas spread and reshaped our culture. Recent chatbots powered by generative artificial intelligence constitute a novel medium that encodes cultural patterns in their neural representations and disseminates them in conversations with hundreds of millions of people. Understanding whether these patterns transmit into human language, and ultimately shape human culture, is a fundamental question. While fully quantifying the causal impact of a chatbot like ChatGPT on human culture is very challenging, lexicographic shift in human spoken communication may offer an early indicator of such broad phenomenon. Here, we apply econometric causal inference techniques to 740,249 hours of human discourse from 360,445 YouTube academic talks and 771,591 conversational podcast episodes across multiple disciplines. We detect a measurable and abrupt increase in the use of words preferentially generated by ChatGPT, such as delve, comprehend, boast, swift, and meticulous, after its release. These findings suggest a scenario where machines, originally trained on human data and subsequently exhibiting their own cultural traits, can, in turn, measurably reshape human culture. This marks the beginning of a closed cultural feedback loop in which cultural traits circulate bidirectionally between humans and machines. Our results motivate further research into the evolution of human-machine culture, and raise concerns over the erosion of linguistic and cultural diversity, and the risks of scalable manipulation.
>
---
#### [replaced 009] When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact
- **分类: cs.CY; cs.AR; cs.DC**

- **链接: [http://arxiv.org/pdf/2506.20442v3](http://arxiv.org/pdf/2506.20442v3)**

> **作者:** Tianyao Shi; Ritbik Kumar; Inez Hua; Yi Ding
>
> **备注:** 7 pages, 8 figures, The 4th Workshop on Sustainable Computer Systems (HotCarbon'25), Cambridge, MA, July 10-11th, 2025
>
> **摘要:** Biodiversity loss is a critical planetary boundary, yet its connection to computing remains largely unexamined. Prior sustainability efforts in computing have focused on carbon and water, overlooking biodiversity due to the lack of appropriate metrics and modeling frameworks. This paper presents the first end-to-end analysis of biodiversity impact from computing systems. We introduce two new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity Index (OBI)--to quantify biodiversity impact across the lifecycle, and present FABRIC, a modeling framework that links computing workloads to biodiversity impacts. Our evaluation highlights the need to consider biodiversity alongside carbon and water in sustainable computing design and optimization. The code is available at https://github.com/TianyaoShi/FABRIC.
>
---
#### [replaced 010] Understanding and Reducing the Class-Dependent Effects of Data Augmentation with A Two-Player Game Approach
- **分类: cs.CY; cs.AI; cs.CV; cs.GT; cs.LG**

- **链接: [http://arxiv.org/pdf/2407.03146v5](http://arxiv.org/pdf/2407.03146v5)**

> **作者:** Yunpeng Jiang; Yutong Ban; Paul Weng
>
> **备注:** Published in Transactions on Machine Learning Research (06/2025)
>
> **摘要:** Data augmentation is widely applied and has shown its benefits in different machine learning tasks. However, as recently observed, it may have an unfair effect in multi-class classification. While data augmentation generally improves the overall performance (and therefore is beneficial for many classes), it can actually be detrimental for other classes, which can be problematic in some application domains. In this paper, to counteract this phenomenon, we propose CLAM, a CLAss-dependent Multiplicative-weights method. To derive it, we first formulate the training of a classifier as a non-linear optimization problem that aims at simultaneously maximizing the individual class performances and balancing them. By rewriting this optimization problem as an adversarial two-player game, we propose a novel multiplicative weight algorithm, for which we prove the convergence. Interestingly, our formulation also reveals that the class-dependent effects of data augmentation is not due to data augmentation only, but is in fact a general phenomenon. Our empirical results over six datasets demonstrate that the performance of learned classifiers is indeed more fairly distributed over classes, with only limited impact on the average accuracy.
>
---
#### [replaced 011] What can large language models do for sustainable food?
- **分类: cs.CY; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2503.04734v2](http://arxiv.org/pdf/2503.04734v2)**

> **作者:** Anna T. Thomas; Adam Yee; Andrew Mayne; Maya B. Mathur; Dan Jurafsky; Kristina Gligorić
>
> **备注:** ICML camera ready version
>
> **摘要:** Food systems are responsible for a third of human-caused greenhouse gas emissions. We investigate what Large Language Models (LLMs) can contribute to reducing the environmental impacts of food production. We define a typology of design and prediction tasks based on the sustainable food literature and collaboration with domain experts, and evaluate six LLMs on four tasks in our typology. For example, for a sustainable protein design task, food science experts estimated that collaboration with an LLM can reduce time spent by 45% on average, compared to 22% for collaboration with another expert human food scientist. However, for a sustainable menu design task, LLMs produce suboptimal solutions when instructed to consider both human satisfaction and climate impacts. We propose a general framework for integrating LLMs with combinatorial optimization to improve reasoning capabilities. Our approach decreases emissions of food choices by 79% in a hypothetical restaurant while maintaining participants' satisfaction with their set of choices. Our results demonstrate LLMs' potential, supported by optimization techniques, to accelerate sustainable food development and adoption.
>
---
#### [replaced 012] Recommender Systems for Good (RS4Good): Survey of Use Cases and a Call to Action for Research that Matters
- **分类: cs.IR; cs.AI; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2411.16645v2](http://arxiv.org/pdf/2411.16645v2)**

> **作者:** Dietmar Jannach; Alan Said; Marko Tkalčič; Markus Zanker
>
> **摘要:** In the area of recommender systems, the vast majority of research efforts is spent on developing increasingly sophisticated recommendation models, also using increasingly more computational resources. Unfortunately, most of these research efforts target a very small set of application domains, mostly e-commerce and media recommendation. Furthermore, many of these models are never evaluated with users, let alone put into practice. The scientific, economic and societal value of much of these efforts by scholars therefore remains largely unclear. To achieve a stronger positive impact resulting from these efforts, we posit that we as a research community should more often address use cases where recommender systems contribute to societal good (RS4Good). In this opinion piece, we first discuss a number of examples where the use of recommender systems for problems of societal concern has been successfully explored in the literature. We then proceed by outlining a paradigmatic shift that is needed to conduct successful RS4Good research, where the key ingredients are interdisciplinary collaborations and longitudinal evaluation approaches with humans in the loop.
>
---
#### [replaced 013] Data-Centric Safety and Ethical Measures for Data and AI Governance
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2506.10217v2](http://arxiv.org/pdf/2506.10217v2)**

> **作者:** Srija Chakraborty
>
> **备注:** Paper accepted and presented at the AAAI 2025 Workshop on Datasets and Evaluators of AI Safety https://sites.google.com/view/datasafe25/home
>
> **摘要:** Datasets play a key role in imparting advanced capabilities to artificial intelligence (AI) foundation models that can be adapted to various downstream tasks. These downstream applications can introduce both beneficial and harmful capabilities -- resulting in dual use AI foundation models, with various technical and regulatory approaches to monitor and manage these risks. However, despite the crucial role of datasets, responsible dataset design and ensuring data-centric safety and ethical practices have received less attention. In this study, we pro-pose responsible dataset design framework that encompasses various stages in the AI and dataset lifecycle to enhance safety measures and reduce the risk of AI misuse due to low quality, unsafe and unethical data content. This framework is domain agnostic, suitable for adoption for various applications and can promote responsible practices in dataset creation, use, and sharing to facilitate red teaming, minimize risks, and increase trust in AI models.
>
---
#### [replaced 014] Public Service Algorithm: towards a transparent, explainable, and scalable content curation for news content based on editorial values
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2506.22270v2](http://arxiv.org/pdf/2506.22270v2)**

> **作者:** Ahmad Mel; Sebastien Noir
>
> **摘要:** The proliferation of disinformation challenges traditional, unscalable editorial processes and existing automated systems that prioritize engagement over public service values. To address this, we introduce the Public Service Algorithm (PSA), a novel framework using Large Language Models (LLMs) for scalable, transparent content curation based on Public Service Media (PSM) inspired values. Utilizing a large multilingual news dataset from the 'A European Perspective' project, our experiment directly compared article ratings from a panel of experienced editors from various European PSMs, with those from several LLMs, focusing on four criteria: diversity, in-depth analysis, forward-looking, and cross-border relevance. Utilizing criterion-specific prompts, our results indicate a promising alignment between human editorial judgment and LLM assessments, demonstrating the potential of LLMs to automate value-driven curation at scale without sacrificing transparency. This research constitutes a first step towards a scalable framework for the automatic curation of trustworthy news content.
>
---
#### [replaced 015] Green AI in Action: Strategic Model Selection for Ensembles in Production
- **分类: cs.LG; cs.AI; cs.CY; cs.SE**

- **链接: [http://arxiv.org/pdf/2405.17451v2](http://arxiv.org/pdf/2405.17451v2)**

> **作者:** Nienke Nijkamp; June Sallou; Niels van der Heijden; Luís Cruz
>
> **备注:** 10 pages. Accepted at the 1st ACM International Conference on AI-powered Software (AIware), 2024
>
> **摘要:** Integrating Artificial Intelligence (AI) into software systems has significantly enhanced their capabilities while escalating energy demands. Ensemble learning, combining predictions from multiple models to form a single prediction, intensifies this problem due to cumulative energy consumption. This paper presents a novel approach to model selection that addresses the challenge of balancing the accuracy of AI models with their energy consumption in a live AI ensemble system. We explore how reducing the number of models or improving the efficiency of model usage within an ensemble during inference can reduce energy demands without substantially sacrificing accuracy. This study introduces and evaluates two model selection strategies, Static and Dynamic, for optimizing ensemble learning systems performance while minimizing energy usage. Our results demonstrate that the Static strategy improves the F1 score beyond the baseline, reducing average energy usage from 100% from the full ensemble to 62%. The Dynamic strategy further enhances F1 scores, using on average 76% compared to 100% of the full ensemble. Moreover, we propose an approach that balances accuracy with resource consumption, significantly reducing energy usage without substantially impacting accuracy. This method decreased the average energy usage of the Static strategy from approximately 62% to 14%, and for the Dynamic strategy, from around 76% to 57%. Our field study of Green AI using an operational AI system developed by a large professional services provider shows the practical applicability of adopting energy-conscious model selection strategies in live production environments.
>
---
