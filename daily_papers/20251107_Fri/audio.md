# 音频 cs.SD;  eess.AS

- **最新发布 5 篇**

- **更新 3 篇**

## 最新发布

#### [new 001] PromptSep: Generative Audio Separation via Multimodal Prompting
- **分类: cs.SD; eess.AS**

- **简介: PromptSep提出一种多模态生成音频分离框架，支持文本与语音模仿双模态引导，实现音频提取与删除，解决传统方法操作受限、文本描述不直观问题，提升分离效果。**

- **链接: [http://arxiv.org/pdf/2511.04623v1](http://arxiv.org/pdf/2511.04623v1)**

> **作者:** Yutong Wen; Ke Chen; Prem Seetharaman; Oriol Nieto; Jiaqi Su; Rithesh Kumar; Minje Kim; Paris Smaragdis; Zeyu Jin; Justin Salamon
>
> **备注:** Submitted to ICASSP 2026
>
> **摘要:** Recent breakthroughs in language-queried audio source separation (LASS) have shown that generative models can achieve higher separation audio quality than traditional masking-based approaches. However, two key limitations restrict their practical use: (1) users often require operations beyond separation, such as sound removal; and (2) relying solely on text prompts can be unintuitive for specifying sound sources. In this paper, we propose PromptSep to extend LASS into a broader framework for general-purpose sound separation. PromptSep leverages a conditional diffusion model enhanced with elaborated data simulation to enable both audio extraction and sound removal. To move beyond text-only queries, we incorporate vocal imitation as an additional and more intuitive conditioning modality for our model, by incorporating Sketch2Sound as a data augmentation strategy. Both objective and subjective evaluations on multiple benchmarks demonstrate that PromptSep achieves state-of-the-art performance in sound removal and vocal-imitation-guided source separation, while maintaining competitive results on language-queried source separation.
>
---
#### [new 002] MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers
- **分类: cs.SD; cs.AI; cs.LG; cs.MM; eess.AS**

- **简介: MusRec提出首个零样本文本到音乐编辑模型，基于修正流与扩散变换器，实现对真实音乐的高效精准编辑，解决现有模型需重训练、仅限合成音乐及提示要求苛刻等问题。**

- **链接: [http://arxiv.org/pdf/2511.04376v1](http://arxiv.org/pdf/2511.04376v1)**

> **作者:** Ali Boudaghi; Hadi Zare
>
> **摘要:** Music editing has emerged as an important and practical area of artificial intelligence, with applications ranging from video game and film music production to personalizing existing tracks according to user preferences. However, existing models face significant limitations, such as being restricted to editing synthesized music generated by their own models, requiring highly precise prompts, or necessitating task-specific retraining, thus lacking true zero-shot capability. Leveraging recent advances in rectified flow and diffusion transformers, we introduce MusRec, the first zero-shot text-to-music editing model capable of performing diverse editing tasks on real-world music efficiently and effectively. Experimental results demonstrate that our approach outperforms existing methods in preserving musical content, structural consistency, and editing fidelity, establishing a strong foundation for controllable music editing in real-world scenarios.
>
---
#### [new 003] MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation
- **分类: cs.SD; cs.CL; cs.MM**

- **简介: 论文提出MIDI-LLM，将大语言模型适配为文本到MIDI音乐生成模型，通过扩展词汇表并采用两阶段训练，实现高质量、高控制力的多轨MIDI生成，同时借助vLLM加速推理，优于现有Text2midi模型。**

- **链接: [http://arxiv.org/pdf/2511.03942v1](http://arxiv.org/pdf/2511.03942v1)**

> **作者:** Shih-Lun Wu; Yoon Kim; Cheng-Zhi Anna Huang
>
> **备注:** To appear at NeurIPS 2025 Workshop on AI for Music
>
> **摘要:** We present MIDI-LLM, an LLM for generating multitrack MIDI music from free-form text prompts. Our approach expands a text LLM's vocabulary to include MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI abilities. By preserving the original LLM's parameter structure, we can directly leverage the vLLM library for accelerated inference. Experiments show that MIDI-LLM achieves higher quality, better text control, and faster inference compared to the recent Text2midi model. Live demo at https://midi-llm-demo.vercel.app.
>
---
#### [new 004] CardioPHON: Quality assessment and self-supervised pretraining for screening of cardiac function based on phonocardiogram recordings
- **分类: eess.AS**

- **简介: CardioPHON提出一种基于心音图的自监督预训练模型，用于心功能异常筛查，兼具质量评估与分类功能，首次公开预训练模型，显著提升诊断准确性，单模态性能居榜首。**

- **链接: [http://arxiv.org/pdf/2511.04533v1](http://arxiv.org/pdf/2511.04533v1)**

> **作者:** Vladimir Despotovic; Peter Pocta; Andrej Zgank
>
> **摘要:** Remote monitoring of cardiovascular diseases plays an essential role in early detection of abnormal cardiac function, enabling timely intervention, improved preventive care, and personalized patient treatment. Abnormalities in the heart sounds can be detected automatically via computer-assisted decision support systems, and used as the first-line screening tool for detection of cardiovascular problems, or for monitoring the effects of treatments and interventions. We propose in this paper CardioPHON, an integrated heart sound quality assessment and classification tool that can be used for screening of abnormal cardiac function from phonocardiogram recordings. The model is pretrained in a self-supervised fashion on a collection of six small- and mid-sized heart sound datasets, enables automatic removal of low quality recordings to ensure that subtle sounds of heart abnormalities are not misdiagnosed, and provides a state-of-the-art performance for the heart sound classification task. The multimodal model that combines audio and socio-demographic features demonstrated superior performance, achieving the best ranking on the official leaderboard of the 2022 George B. Moody PhysioNet heart sound challenge, whereas the unimodal model, that is based only on phonocardiogram recordings, holds the first position among the unimodal approaches (a total rank 4), surpassing the models utilizing multiple modalities. CardioPHON is the first publicly released pretrained model in the domain of heart sound recordings, facilitating the development of data-efficient artificial intelligence models that can generalize to various downstream tasks in cardiovascular diagnostics.
>
---
#### [new 005] CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese
- **分类: cs.CL; cs.SD**

- **简介: CantoASR提出一种ASR与LALM协作框架，解决低资源粤语语音识别中声调与韵律建模难题，通过强制对齐、LoRA微调Whisper和Qwen-Audio指令微调，显著降低字错误率。**

- **链接: [http://arxiv.org/pdf/2511.04139v1](http://arxiv.org/pdf/2511.04139v1)**

> **作者:** Dazhong Chen; Yi-Cheng Lin; Yuchen Huang; Ziwei Gong; Di Jiang; Zeying Xie; Yi R.; Fung
>
> **摘要:** Automatic speech recognition (ASR) is critical for language accessibility, yet low-resource Cantonese remains challenging due to limited annotated data, six lexical tones, tone sandhi, and accent variation. Existing ASR models, such as Whisper, often suffer from high word error rates. Large audio-language models (LALMs), in contrast, can leverage broader contextual reasoning but still require explicit tonal and prosodic acoustic cues. We introduce CantoASR, a collaborative ASR-LALM error correction framework that integrates forced alignment for acoustic feature extraction, a LoRA-finetuned Whisper for improved tone discrimination, and an instruction-tuned Qwen-Audio for prosody-aware correction. Evaluations on spontaneous Cantonese data show substantial CER gains over Whisper-Large-V3. These findings suggest that integrating acoustic cues with LALM reasoning provides a scalable strategy for low-resource tonal and dialectal ASR.
>
---
## 更新

#### [replaced 001] Back to Ear: Perceptually Driven High Fidelity Music Reconstruction
- **分类: cs.SD; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.14912v2](http://arxiv.org/pdf/2509.14912v2)**

> **作者:** Kangdi Wang; Zhiyue Wu; Dinghao Zhou; Rui Lin; Junyu Dai; Tao Jiang
>
> **备注:** Check the Code here: https://github.com/Eps-Acoustic-Revolution-Lab/EAR_VAE and Model Weights here: https://huggingface.co/earlab/EAR_VAE
>
> **摘要:** Variational Autoencoders (VAEs) are essential for large-scale audio tasks like diffusion-based generation. However, existing open-source models often neglect auditory perceptual aspects during training, leading to weaknesses in phase accuracy and stereophonic spatial representation. To address these challenges, we propose {\epsilon}ar-VAE, an open-source music signal reconstruction model that rethinks and optimizes the VAE training paradigm. Our contributions are threefold: (i) A K-weighting perceptual filter applied prior to loss calculation to align the objective with auditory perception. (ii) Two novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss using its derivatives--Instantaneous Frequency and Group Delay--for precision. (iii) A new spectral supervision paradigm where magnitude is supervised by all four Mid/Side/Left/Right components, while phase is supervised only by the LR components. Experiments show {\epsilon}ar-VAE at 44.1kHz substantially outperforms leading open-source models across diverse metrics, showing particular strength in reconstructing high-frequency harmonics and the spatial characteristics.
>
---
#### [replaced 002] Uncertainty Quantification in Melody Estimation using Histogram Representation
- **分类: eess.AS**

- **链接: [http://arxiv.org/pdf/2505.05156v2](http://arxiv.org/pdf/2505.05156v2)**

> **作者:** Kavya Ranjan Saxena; Vipul Arora
>
> **摘要:** Confidence estimation can improve the reliability of melody estimation by indicating which predictions are likely incorrect. The existing classification-based approach provides confidence for predicted pitch classes but fails to capture the magnitude of deviation from the ground truth. To address this limitation, we reformulate melody estimation as a regression problem and propose a novel approach to estimate uncertainty directly from the histogram representation of the pitch values, which correlates well with the deviation between the prediction and the ground-truth. We design three methods to model pitch on a continuous support range of histogram, which introduces the challenge of handling the discontinuity of unvoiced from the voiced pitch values. The first two methods address the abrupt discontinuity by mapping the pitch values to a continuous range, while the third adopts a fully Bayesian formulation, which models voicing detection as a classification and voiced pitch estimation as a regression task. Experimental results demonstrate that regression-based formulations yield more reliable uncertainty estimates compared to classification-based approaches in identifying incorrect pitch predictions. Comparing the proposed methods with a state-of-the-art regression model, it is observed that the Bayesian method performs the best at estimating both the melody and its associated uncertainty.
>
---
#### [replaced 003] dCoNNear: An Artifact-Free Neural Network Architecture for Closed-loop Audio Signal Processing
- **分类: eess.AS; cs.SD**

- **链接: [http://arxiv.org/pdf/2501.04116v3](http://arxiv.org/pdf/2501.04116v3)**

> **作者:** Chuan Wen; Guy Torfs; Sarah Verhulst
>
> **备注:** Published in IEEE Transactions on Audio, Speech and Language Processing, vol. 33, pp. 4414-4429, 2025
>
> **摘要:** Recent advances in deep neural networks (DNNs) have significantly improved various audio processing applications, including speech enhancement, synthesis, and hearing-aid algorithms. DNN-based closed-loop systems have gained popularity in these applications due to their robust performance and ability to adapt to diverse conditions. Despite their effectiveness, current DNN-based closed-loop systems often suffer from sound quality degradation caused by artifacts introduced by suboptimal sampling methods. To address this challenge, we introduce dCoNNear, a novel DNN architecture designed for seamless integration into closed-loop frameworks. This architecture specifically aims to prevent the generation of spurious artifacts-most notably tonal and aliasing artifacts arising from non-ideal sampling layers. We demonstrate the effectiveness of dCoNNear through a proof-of-principle example within a closed-loop framework that employs biophysically realistic models of auditory processing for both normal and hearing-impaired profiles to design personalized hearing-aid algorithms. We further validate the broader applicability and artifact-free performance of dCoNNear through speech-enhancement experiments, confirming its ability to improve perceptual sound quality without introducing architecture-induced artifacts. Our results show that dCoNNear not only accurately simulates all processing stages of existing non-DNN biophysical models but also significantly improves sound quality by eliminating audible artifacts in both hearing-aid and speech-enhancement applications. This study offers a robust, perceptually transparent closed-loop processing framework for high-fidelity audio applications.
>
---
