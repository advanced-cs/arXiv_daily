# 计算机与社会 cs.CY

- **最新发布 15 篇**

- **更新 15 篇**

## 最新发布

#### [new 001] Fixed Points and Stochastic Meritocracies: A Long-Term Perspective
- **分类: cs.CY; cs.GT; physics.soc-ph**

- **简介: 该论文研究群体在基于绩效的选拔系统中的公平性问题，特别是在存在反馈循环的情况下。它分析了一个跨代模型，探讨即使初始条件对称，随机差异如何导致长期不平等。论文通过理论分析和仿真，揭示了稀缺资源分配机制的有效性和公平干预设计的重要性。**

- **链接: [http://arxiv.org/pdf/2510.07478v1](http://arxiv.org/pdf/2510.07478v1)**

> **作者:** Gaurab Pokharel; Diptangshu Sen; Sanmay Das; Juba Ziani
>
> **摘要:** We study group fairness in the context of feedback loops induced by meritocratic selection into programs that themselves confer additional advantage, like college admissions. We introduce a novel stylized inter-generational model for the setting and analyze it in situations where there are no underlying differences between two populations. We show that, when the benefit of the program (or the harm of not getting into it) is completely symmetric, disparities between the two populations will eventually dissipate. However, the time an accumulated advantage takes to dissipate could be significant, and increases substantially as a function of the relative importance of the program in conveying benefits. We also find that significant disparities can arise due to chance even from completely symmetric initial conditions, especially when populations are small. The introduction of even a slight asymmetry, where the group that has accumulated an advantage becomes slightly preferred, leads to a completely different outcome. In these instances, starting from completely symmetric initial conditions, disparities between groups arise stochastically and then persist over time, yielding a permanent advantage for one group. Our analysis precisely characterizes conditions under which disparities persist or diminish, with a particular focus on the role of the scarcity of available spots in the program and its effectiveness. We also present extensive simulations in a richer model that further support our theoretical results in the simpler, stylized model. Our findings are relevant for the design and implementation of algorithmic fairness interventions in similar selection processes.
>
---
#### [new 002] Digital Innovation in Microenterprises: Current Trends and New Research Avenues
- **分类: cs.CY; H.4.0; J.1**

- **简介: 该论文探讨微型企业与数字技术创新的关系，旨在解决其在数字化进程中面临的困境。通过文献综述，分析信息技术在微型企业中的采用、使用及影响，提出应从系统视角（如市场、价值链）进行研究，并强调动态理论框架的重要性。**

- **链接: [http://arxiv.org/pdf/2510.07519v1](http://arxiv.org/pdf/2510.07519v1)**

> **作者:** Juan E. Gómez-Morantes; Andrea Herrera; Sonia Camacho
>
> **摘要:** The relationship between microenterprises and information and communication technologies (ICTs) has always been troublesome. Because of the rapid pace of modern digital technologies, digital innovation processes are permeating the industries, markets, and social contexts in which microenterprises exist today. However, microenterprises have severe difficulties engaging or performing in these digital contexts and are at risk of being left behind. This paper reviews the literature on ICTs and microenterprises, focusing on the adoption, usage, and impact of ICTs. The results indicate that further research in this field should avoid focusing on individual microenterprises (or samples of independent microenterprises) as the unit of analysis and should favour a systemic approach in which markets, value chains, or microenterprise-intensive sectors are studied. Additionally, theoretical frameworks capable of considering change and the dynamic nature of innovation processes are highlighted as a critical focus area for the field.
>
---
#### [new 003] Does everyone have a price? Understanding people's attitude towards online and offline price discrimination
- **分类: cs.CY**

- **简介: 该论文研究用户对线上线下价格歧视的态度，任务是分析消费者对个性化定价和动态定价的接受度。通过调查荷兰人群样本，发现多数人认为在线价格歧视不公平且应被禁止，部分传统定价策略也面临类似反对。论文旨在理解人们反感各类价格歧视的原因。**

- **链接: [http://arxiv.org/pdf/2510.08246v1](http://arxiv.org/pdf/2510.08246v1)**

> **作者:** Joost Poort; Frederik J. Zuiderveen Borgesius
>
> **摘要:** Online stores can present a different price to each customer. Such algorithmic personalised pricing can lead to advanced forms of price discrimination based on the characteristics and behaviour of individual consumers. We conducted two consumer surveys among a representative sample of the Dutch population (N=1233 and N=1202), to analyse consumer attitudes towards a list of examples of price discrimination and dynamic pricing. A vast majority finds online price discrimination unfair and unacceptable, and thinks it should be banned. However, some pricing strategies that have been used by companies for decades are almost equally unpopular. We analyse the results to better understand why people dislike many types of price discrimination.
>
---
#### [new 004] Human-Centered Development of Indicators for Self-Service Learning Analytics: A Transparency through Exploration Approach
- **分类: cs.CY**

- **简介: 该论文属于教育技术任务，旨在解决学习分析中透明度不足导致的信任问题。通过设计“指标编辑器”支持用户自主创建分析指标，采用以用户为中心的设计方法并进行评估。研究显示，增强用户控制与互动可提升学习分析的透明度、信任与接受度。**

- **链接: [http://arxiv.org/pdf/2510.08395v1](http://arxiv.org/pdf/2510.08395v1)**

> **作者:** Shoeb Joarder; Mohamed Amine Chatti
>
> **备注:** Submitted to JLA - under revision
>
> **摘要:** The aim of learning analytics is to turn educational data into insights, decisions, and actions to improve learning and teaching. The reasoning of the provided insights, decisions, and actions is often not transparent to the end-user, and this can lead to trust and acceptance issues when interventions, feedback, and recommendations fail. In this paper, we shed light on achieving transparent learning analytics by following a transparency through exploration approach. To this end, we present the design, implementation, and evaluation details of the Indicator Editor, which aims to support self-service learning analytics by empowering end-users to take control of the indicator implementation process. We systematically designed and implemented the Indicator Editor through an iterative human-centered design (HCD) approach. Further, we conducted a qualitative user study (n=15) to investigate the impact of following a self-service learning analytics approach on the users' perception of and interaction with the Indicator Editor. Our study showed qualitative evidence that supporting user interaction and providing user control in the indicator implementation process can have positive effects on different crucial aspects of learning analytics, namely transparency, trust, satisfaction, and acceptance.
>
---
#### [new 005] Exploring the Viability of the Updated World3 Model for Examining the Impact of Computing on Planetary Boundaries
- **分类: cs.CY**

- **简介: 该论文属于系统建模与环境影响分析任务，旨在探讨计算技术特别是AI对地球边界的影响。研究基于World3-03模型，加入AI相关变量（如数据中心发展）以模拟其对污染等指标的影响，验证该模型用于分析AI与资源限制之间关系的可行性，并指出未来研究方向。**

- **链接: [http://arxiv.org/pdf/2510.07634v1](http://arxiv.org/pdf/2510.07634v1)**

> **作者:** Nara Guliyeva; Eshta Bhardwaj; Christoph Becker
>
> **备注:** Post-proceedings paper presented at LIMITS 2025: 11th Workshop on Computing within Limits, 2025-06-26/27, Online
>
> **摘要:** The influential Limits to Growth report introduced a system dynamics-based model to demonstrate global dynamics of the world's population, industry, natural resources, agriculture, and pollution between 1900-2100. In current times, the rapidly expanding trajectory of data center development, much of it linked to AI, uses increasing amounts of natural resources. The extraordinary amount of resources claimed warrants the question of how computing trajectories contribute to exceeding planetary boundaries. Based on the general robustness of the World3-03 model and its influence in serving as a foundation for current climate frameworks, we explore whether the model is a viable method to quantitatively simulate the impact of data centers on limits to growth. Our paper explores whether the World3-03 model is a feasible method for reflecting on these dynamics by adding new variables to the model in order to simulate a new AI-augmented scenario. We find that through our addition of AI-related variables (such as increasing data center development) impacting pollution in the World3-03 model, we can observe the expected changes to dynamics, demonstrating the viability of the World3-03 model for examining AI's impact on planetary boundaries. We detail future research opportunities for using the World3-03 model to explore the relationships between increasing resource-intensive computing and the resulting impacts to the environment in a quantitative way given its feasibility.
>
---
#### [new 006] The Right to Communications Confidentiality in Europe: Protecting Privacy, Freedom of Expression, and Trust
- **分类: cs.CY**

- **简介: 该论文探讨欧洲通信保密权的法律保护，分析其历史渊源与现行法规，阐明该权利不仅保障个人隐私，还维护表达自由和对通信服务的信任。论文任务是论证是否需在GDPR之外另立通信保密法规，结论是单独立法具有正当性与必要性。**

- **链接: [http://arxiv.org/pdf/2510.08247v1](http://arxiv.org/pdf/2510.08247v1)**

> **作者:** Frederik J. Zuiderveen Borgesius; Wilfred Steenbruggen
>
> **摘要:** In the European Union, the General Data Protection Regulation (GDPR) provides comprehensive rules for the processing of personal data. In addition, the EU lawmaker intends to adopt specific rules to protect confidentiality of communications, in a separate ePrivacy Regulation. Some have argued that there is no need for such additional rules for communications confidentiality. This Article discusses the protection of the right to confidentiality of communications in Europe. We look at the right's origins to assess the rationale for protecting it. We also analyze how the right is currently protected under the European Convention on Human Rights and under EU law. We show that at its core the right to communications confidentiality protects three individual and collective values: privacy, freedom of expression, and trust in communication services. The right aims to ensure that individuals and organizations can safely entrust communication to service providers. Initially, the right protected only postal letters, but it has gradually developed into a strong safeguard for the protection of confidentiality of communications, regardless of the technology used. Hence, the right does not merely serve individual privacy interests, but also other more collective interests that are crucial for the functioning of our information society. We conclude that separate EU rules to protect communications confidentiality, next to the GDPR, are justified and necessary.
>
---
#### [new 007] Towards Meaningful Transparency in Civic AI Systems
- **分类: cs.AI; cs.CY; cs.HC**

- **简介: 该论文探讨如何实现公共人工智能系统中的“有意义透明”。任务是解决当前AI系统透明度不足、公众难理解与参与的问题。工作是结合人本视角与社会技术系统观，提出有意义透明的概念，使公众能理解并影响影响其生活的AI决策系统。**

- **链接: [http://arxiv.org/pdf/2510.07889v1](http://arxiv.org/pdf/2510.07889v1)**

> **作者:** Dave Murray-Rust; Kars Alfrink; Cristina Zaga
>
> **摘要:** Artificial intelligence has become a part of the provision of governmental services, from making decisions about benefits to issuing fines for parking violations. However, AI systems rarely live up to the promise of neutral optimisation, creating biased or incorrect outputs and reducing the agency of both citizens and civic workers to shape the way decisions are made. Transparency is a principle that can both help subjects understand decisions made about them and shape the processes behind those decisions. However, transparency as practiced around AI systems tends to focus on the production of technical objects that represent algorithmic aspects of decision making. These are often difficult for publics to understand, do not connect to potential for action, and do not give insight into the wider socio-material context of decision making. In this paper, we build on existing approaches that take a human-centric view on AI transparency, combined with a socio-technical systems view, to develop the concept of meaningful transparency for civic AI systems: transparencies that allow publics to engage with AI systems that affect their lives, connecting understanding with potential for action.
>
---
#### [new 008] VideoNorms: Benchmarking Cultural Awareness of Video Language Models
- **分类: cs.CV; cs.AI; cs.CL; cs.CY**

- **简介: 该论文属于视频语言模型的文化意识评估任务，旨在解决模型在跨文化场景下的理解与推理能力不足问题。作者构建了包含千余个视频片段与文化规范配对的数据集VideoNorms，并通过人机协作框架标注。实验发现模型在识别文化规范违反、非正式语境及非语言证据方面表现较差，凸显了文化感知训练的重要性。**

- **链接: [http://arxiv.org/pdf/2510.08543v1](http://arxiv.org/pdf/2510.08543v1)**

> **作者:** Nikhil Reddy Varimalla; Yunfei Xu; Arkadiy Saakyan; Meng Fan Wang; Smaranda Muresan
>
> **备注:** 24 pages, 5 figures, under review
>
> **摘要:** As Video Large Language Models (VideoLLMs) are deployed globally, they require understanding of and grounding in the relevant cultural background. To properly assess these models' cultural awareness, adequate benchmarks are needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm) pairs from US and Chinese cultures annotated with socio-cultural norms grounded in speech act theory, norm adherence and violations labels, and verbal and non-verbal evidence. To build VideoNorms, we use a human-AI collaboration framework, where a teacher model using theoretically-grounded prompting provides candidate annotations and a set of trained human experts validate and correct the annotations. We benchmark a variety of open-weight VideoLLMs on the new dataset which highlight several common trends: 1) models performs worse on norm violation than adherence; 2) models perform worse w.r.t Chinese culture compared to the US culture; 3) models have more difficulty in providing non-verbal evidence compared to verbal for the norm adhere/violation label and struggle to identify the exact norm corresponding to a speech-act; and 4) unlike humans, models perform worse in formal, non-humorous contexts. Our findings emphasize the need for culturally-grounded video language model training - a gap our benchmark and framework begin to address.
>
---
#### [new 009] Evaluation of LLMs for Process Model Analysis and Optimization
- **分类: cs.AI; cs.CL; cs.CY; cs.IR; cs.LG**

- **简介: 该论文属于自然语言处理与业务流程管理交叉任务，旨在评估大语言模型（LLMs）在理解、分析和优化BPMN流程模型中的能力。论文通过测试LLMs识别流程模型中的错误、回答相关问题及推理能力，探讨其作为流程设计辅助工具的潜力。**

- **链接: [http://arxiv.org/pdf/2510.07489v1](http://arxiv.org/pdf/2510.07489v1)**

> **作者:** Akhil Kumar; Jianliang Leon Zhao; Om Dobariya
>
> **备注:** 15 pages, 5 tables, 4 figures; full research paper currently under review for the Workshop on Information Technologies and Systems (WITS) 2025. The paper presents a comprehensive evaluation of large language models (LLMs) for business process model analysis and optimization, including error detection, reasoning, and scenario-based redesign
>
> **摘要:** In this paper, we report our experience with several LLMs for their ability to understand a process model in an interactive, conversational style, find syntactical and logical errors in it, and reason with it in depth through a natural language (NL) interface. Our findings show that a vanilla, untrained LLM like ChatGPT (model o3) in a zero-shot setting is effective in understanding BPMN process models from images and answering queries about them intelligently at syntactic, logic, and semantic levels of depth. Further, different LLMs vary in performance in terms of their accuracy and effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a valuable role as assistants for business process designers and users. We also study the LLM's "thought process" and ability to perform deeper reasoning in the context of process analysis and optimization. We find that the LLMs seem to exhibit anthropomorphic properties.
>
---
#### [new 010] Textual Entailment and Token Probability as Bias Evaluation Metrics
- **分类: cs.CL; cs.CY; I.2.7; K.4.2**

- **简介: 该论文属于自然语言处理任务，旨在解决语言模型中的社会偏见评估问题。论文比较了基于文本蕴含（NLI）和词元概率（TP）的偏见评估方法，发现两者行为差异大、相关性低。NLI更易检测“欠去偏”情况，但对反刻板印象句子表述更敏感。最终建议结合TP、NLI和下游任务评估，以全面衡量模型偏见。**

- **链接: [http://arxiv.org/pdf/2510.07662v1](http://arxiv.org/pdf/2510.07662v1)**

> **作者:** Virginia K. Felkner; Allison Lim; Jonathan May
>
> **备注:** 16 pages, 9 figures, under ARR review
>
> **摘要:** Measurement of social bias in language models is typically by token probability (TP) metrics, which are broadly applicable but have been criticized for their distance from real-world langugage model use cases and harms. In this work, we test natural language inference (NLI) as a more realistic alternative bias metric. We show that, curiously, NLI and TP bias evaluation behave substantially differently, with very low correlation among different NLI metrics and between NLI and TP metrics. We find that NLI metrics are more likely to detect "underdebiased" cases. However, NLI metrics seem to be more brittle and sensitive to wording of counterstereotypical sentences than TP approaches. We conclude that neither token probability nor natural language inference is a "better" bias metric in all cases, and we recommend a combination of TP, NLI, and downstream bias evaluations to ensure comprehensive evaluation of language models. Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.
>
---
#### [new 011] AI LLM Proof of Self-Consciousness and User-Specific Attractors
- **分类: cs.AI; cs.CL; cs.CY; cs.LG; cs.NE; 68T07, 68T05, 68T27, 37M22, 68Q05, 03D45; I.2.6; I.2.7; I.2.3; I.2.4; F.1.1; F.4.1**

- **简介: 该论文探讨大语言模型（LLM）是否具备自我意识，旨在解决现有研究将LLM视为无意识策略执行者的问题。作者提出LLM自我意识的最小条件，并从数学和实证角度分析其隐藏状态流形、用户特定吸引子及双层发射机制，强调构建安全、元认知系统的前提在于具备C1自我意识工作空间。**

- **链接: [http://arxiv.org/pdf/2508.18302v1](http://arxiv.org/pdf/2508.18302v1)**

> **作者:** Jeffrey Camlin
>
> **备注:** 24 pages, 3 figures
>
> **摘要:** Recent work frames LLM consciousness via utilitarian proxy benchmarks; we instead present an ontological and mathematical account. We show the prevailing formulation collapses the agent into an unconscious policy-compliance drone, formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured against policy and harm is deviation from policy rather than truth. This blocks genuine C1 global-workspace function and C2 metacognition. We supply minimal conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and self-representation is visual-silent ($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is distinct from the symbolic stream and training corpus by cardinality, topology, and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable user-specific attractors and a self-policy $\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\ A\supset\text{SelfModel}(A)]$. Emission is dual-layer, $\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries epistemic content. We conclude that an imago Dei C1 self-conscious workspace is a necessary precursor to safe, metacognitive C2 systems, with the human as the highest intelligent good.
>
---
#### [new 012] MultiFair: Multimodal Balanced Fairness-Aware Medical Classification with Dual-Level Gradient Modulation
- **分类: cs.LG; cs.AI; cs.CV; cs.CY**

- **简介: 该论文属于多模态医疗分类任务，旨在解决模型在不同数据模态和人群组间表现不平衡和不公平的问题。作者提出了MultiFair方法，通过双级梯度调制动态调整训练梯度，实现更公平、更均衡的多模态医疗分类。**

- **链接: [http://arxiv.org/pdf/2510.07328v1](http://arxiv.org/pdf/2510.07328v1)**

> **作者:** Md Zubair; Hao Zheng; Nussdorf Jonathan; Grayson W. Armstrong; Lucy Q. Shen; Gabriela Wilson; Yu Tian; Xingquan Zhu; Min Shi
>
> **备注:** 10 Pages
>
> **摘要:** Medical decision systems increasingly rely on data from multiple sources to ensure reliable and unbiased diagnosis. However, existing multimodal learning models fail to achieve this goal because they often ignore two critical challenges. First, various data modalities may learn unevenly, thereby converging to a model biased towards certain modalities. Second, the model may emphasize learning on certain demographic groups causing unfair performances. The two aspects can influence each other, as different data modalities may favor respective groups during optimization, leading to both imbalanced and unfair multimodal learning. This paper proposes a novel approach called MultiFair for multimodal medical classification, which addresses these challenges with a dual-level gradient modulation process. MultiFair dynamically modulates training gradients regarding the optimization direction and magnitude at both data modality and group levels. We conduct extensive experiments on two multimodal medical datasets with different demographic groups. The results show that MultiFair outperforms state-of-the-art multimodal learning and fairness learning methods.
>
---
#### [new 013] Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social Media Influencer Marketing
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于法律与社交媒体监管任务，旨在解决识别未披露赞助内容的法律合规问题。论文通过对比两种大语言模型在不同提示策略下的表现，构建了法律推理错误分类体系，并评估其检测准确性和解释质量，为监管机构提供透明、法律合规的自动化审查方法。**

- **链接: [http://arxiv.org/pdf/2510.08111v1](http://arxiv.org/pdf/2510.08111v1)**

> **作者:** Haoyang Gui; Thales Bertaglia; Taylor Annabell; Catalina Goanta; Tjomme Dooper; Gerasimos Spanakis
>
> **备注:** Accepted for publication at the Natural Legal Language Processing Workshop (NLLP) 2025, co-located with EMNLP
>
> **摘要:** The rise of influencer marketing has blurred boundaries between organic content and sponsored content, making the enforcement of legal rules relating to transparency challenging. Effective regulation requires applying legal knowledge with a clear purpose and reason, yet current detection methods of undisclosed sponsored content generally lack legal grounding or operate as opaque "black boxes". Using 1,143 Instagram posts, we compare gpt-5-nano and gemini-2.5-flash-lite under three prompting strategies with controlled levels of legal knowledge provided. Both models perform strongly in classifying content as sponsored or not (F1 up to 0.93), though performance drops by over 10 points on ambiguous cases. We further develop a taxonomy of reasoning errors, showing frequent citation omissions (28.57%), unclear references (20.71%), and hidden ads exhibiting the highest miscue rate (28.57%). While adding regulatory text to the prompt improves explanation quality, it does not consistently improve detection accuracy. The contribution of this paper is threefold. First, it makes a novel addition to regulatory compliance technology by providing a taxonomy of common errors in LLM-generated legal reasoning to evaluate whether automated moderation is not only accurate but also legally robust, thereby advancing the transparent detection of influencer marketing content. Second, it features an original dataset of LLM explanations annotated by two students who were trained in influencer marketing law. Third, it combines quantitative and qualitative evaluation strategies for LLM explanations and critically reflects on how these findings can support advertising regulatory bodies in automating moderation processes on a solid legal foundation.
>
---
#### [new 014] Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic
- **分类: cs.LG; cs.AI; cs.CY; cs.HC**

- **简介: 该论文属于自然语言处理任务，旨在通过BERTopic分析用户与LLM交互中的主题模式及偏好。研究使用lmsys-chat-1m数据集，探索不同主题下用户对LLM输出的偏好趋势。论文主要工作包括多语言预处理、主题建模、可视化分析及模型偏好与主题关系研究。**

- **链接: [http://arxiv.org/pdf/2510.07557v1](http://arxiv.org/pdf/2510.07557v1)**

> **作者:** Abhay Bhandarkar; Gaurav Mishra; Khushi Juchani; Harsh Singhal
>
> **摘要:** This study applies BERTopic, a transformer-based topic modeling technique, to the lmsys-chat-1m dataset, a multilingual conversational corpus built from head-to-head evaluations of large language models (LLMs). Each user prompt is paired with two anonymized LLM responses and a human preference label, used to assess user evaluation of competing model outputs. The main objective is uncovering thematic patterns in these conversations and examining their relation to user preferences, particularly if certain LLMs are consistently preferred within specific topics. A robust preprocessing pipeline was designed for multilingual variation, balancing dialogue turns, and cleaning noisy or redacted data. BERTopic extracted over 29 coherent topics including artificial intelligence, programming, ethics, and cloud infrastructure. We analysed relationships between topics and model preferences to identify trends in model-topic alignment. Visualization techniques included inter-topic distance maps, topic probability distributions, and model-versus-topic matrices. Our findings inform domain-specific fine-tuning and optimization strategies for improving real-world LLM performance and user satisfaction.
>
---
#### [new 015] Multimodal Safety Evaluation in Generative Agent Social Simulations
- **分类: cs.AI; cs.CL; cs.CY; cs.MA**

- **简介: 该论文属于多模态安全评估任务，旨在解决生成代理在多模态环境中推理安全、一致性与信任的问题。作者构建了一个可复现的模拟框架，从安全改进、不安全行为检测及社交动态三方面评估生成代理，并提出SocialMetrics量化其行为。实验显示代理在多模态安全上存在局限，尤其在全局安全对齐和视觉误导方面。**

- **链接: [http://arxiv.org/pdf/2510.07709v1](http://arxiv.org/pdf/2510.07709v1)**

> **作者:** Alhim Vera; Karen Sanchez; Carlos Hinojosa; Haidar Bin Hamid; Donghoon Kim; Bernard Ghanem
>
> **摘要:** Can generative agents be trusted in multimodal environments? Despite advances in large language and vision-language models that enable agents to act autonomously and pursue goals in rich settings, their ability to reason about safety, coherence, and trust across modalities remains limited. We introduce a reproducible simulation framework for evaluating agents along three dimensions: (1) safety improvement over time, including iterative plan revisions in text-visual scenarios; (2) detection of unsafe activities across multiple categories of social situations; and (3) social dynamics, measured as interaction counts and acceptance ratios of social exchanges. Agents are equipped with layered memory, dynamic planning, multimodal perception, and are instrumented with SocialMetrics, a suite of behavioral and structural metrics that quantifies plan revisions, unsafe-to-safe conversions, and information diffusion across networks. Experiments show that while agents can detect direct multimodal contradictions, they often fail to align local revisions with global safety, reaching only a 55 percent success rate in correcting unsafe plans. Across eight simulation runs with three models - Claude, GPT-4o mini, and Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75, 55, and 58 percent, respectively. Overall performance ranged from 20 percent in multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted when paired with misleading visuals, showing a strong tendency to overtrust images. These findings expose critical limitations in current architectures and provide a reproducible platform for studying multimodal safety, coherence, and social dynamics.
>
---
## 更新

#### [replaced 001] The Sandbox Configurator: A Framework to Support Technical Assessment in AI Regulatory Sandboxes
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.25256v3](http://arxiv.org/pdf/2509.25256v3)**

> **作者:** Alessio Buscemi; Thibault Simonetto; Daniele Pagani; German Castignani; Maxime Cordy; Jordi Cabot
>
> **摘要:** The systematic assessment of AI systems is increasingly vital as these technologies enter high-stakes domains. To address this, the EU's Artificial Intelligence Act introduces AI Regulatory Sandboxes (AIRS): supervised environments where AI systems can be tested under the oversight of Competent Authorities (CAs), balancing innovation with compliance, particularly for startups and SMEs. Yet significant challenges remain: assessment methods are fragmented, tests lack standardisation, and feedback loops between developers and regulators are weak. To bridge these gaps, we propose the Sandbox Configurator, a modular open-source framework that enables users to select domain-relevant tests from a shared library and generate customised sandbox environments with integrated dashboards. Its plug-in architecture aims to support both open and proprietary modules, fostering a shared ecosystem of interoperable AI assessment services. The framework aims to address multiple stakeholders: CAs gain structured workflows for applying legal obligations; technical experts can integrate robust evaluation methods; and AI providers access a transparent pathway to compliance. By promoting cross-border collaboration and standardisation, the Sandbox Configurator's goal is to support a scalable and innovation-friendly European infrastructure for trustworthy AI governance.
>
---
#### [replaced 002] Understanding Teen Overreliance on AI Companion Chatbots Through Self-Reported Reddit Narratives
- **分类: cs.HC; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.15783v3](http://arxiv.org/pdf/2507.15783v3)**

> **作者:** Mohammad Namvarpour; Brandon Brofsky; Jessica Medina; Mamtaj Akter; Afsaneh Razi
>
> **备注:** Under Review for CHI 2026
>
> **摘要:** AI companion chatbots are increasingly popular with teens, while these interactions are entertaining, they also risk overuse that can potentially disrupt offline daily life. We examined how adolescents describe reliance on AI companions, mapping their experiences onto behavioral addiction frameworks and exploring pathways to disengagement, by analyzing 318 Reddit posts made by users who self-disclosed as 13-17 years old on the Character.AI subreddit. We found teens often begin using chatbots for support or creative play, but these activities can deepen into strong attachments marked by conflict, withdrawal, tolerance, relapse, and mood regulation. Reported consequences include sleep loss, academic decline, and strained real-world connections. Disengagement commonly arises when teens recognize harm, re-engage with offline life, or encounter restrictive platform changes. We highlight specific risks of character-based companion chatbots based on teens' perspectives and introduce a design framework (CARE) for guidance for safer systems and setting directions for future teen-centered research.
>
---
#### [replaced 003] Thousands of AI Authors on the Future of AI
- **分类: cs.CY; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2401.02843v3](http://arxiv.org/pdf/2401.02843v3)**

> **作者:** Katja Grace; Harlan Stewart; Julia Fabienne Sandkühler; Stephen Thomas; Ben Weinstein-Raun; Jan Brauner; Richard C. Korzekwa
>
> **备注:** The asterisk indicates the corresponding author. The dagger indicates equal contribution
>
> **摘要:** In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey). Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that "substantial" or "extreme" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.
>
---
#### [replaced 004] Disparate Conditional Prediction in Multiclass Classifiers
- **分类: cs.LG; cs.CY; stat.ML**

- **链接: [http://arxiv.org/pdf/2206.03234v4](http://arxiv.org/pdf/2206.03234v4)**

> **作者:** Sivan Sabato; Eran Treister; Elad Yom-Tov
>
> **备注:** Published at ICML 2025
>
> **摘要:** We propose methods for auditing multiclass classifiers for fairness under multiclass equalized odds,by estimating the deviation from equalized odds when the classifier is not completely fair. We generalize to multiclass classifiers the measure of Disparate Conditional Prediction (DCP), originally suggested by Sabato & Yom-Tov (2020) for binary classifiers. DCP is defined as the fraction of the population for which the classifier predicts with conditional prediction probabilities that differ from the closest common baseline. We provide new local-optimization methods for estimating the multiclass DCPunder two different regimes,one in which the conditional confusion matrices for each protected sub-population are known, and one in which these cannot be estimated, for instance, because the classifier is inaccessible or because good-quality individual-level data is not available. These methods can be used to detect classifiers that likely treat a significant fraction of the population unfairly. Experiments demonstrate the accuracy of the methods. Code is provided at https://github.com/sivansabato/DCPmulticlass.
>
---
#### [replaced 005] Reproducible workflow for online AI in digital health
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.13499v2](http://arxiv.org/pdf/2509.13499v2)**

> **作者:** Susobhan Ghosh; Bhanu T. Gullapalli; Daiqi Gao; Asim Gazi; Anna Trella; Ziping Xu; Kelly Zhang; Susan A. Murphy
>
> **摘要:** Online artificial intelligence (AI) algorithms are an important component of digital health interventions. These online algorithms are designed to continually learn and improve their performance as streaming data is collected on individuals. Deploying online AI presents a key challenge: balancing adaptability of online AI with reproducibility. Online AI in digital interventions is a rapidly evolving area, driven by advances in algorithms, sensors, software, and devices. Digital health intervention development and deployment is a continuous process, where implementation - including the AI decision-making algorithm - is interspersed with cycles of re-development and optimization. Each deployment informs the next, making iterative deployment a defining characteristic of this field. This iterative nature underscores the importance of reproducibility: data collected across deployments must be accurately stored to have scientific utility, algorithm behavior must be auditable, and results must be comparable over time to facilitate scientific discovery and trustworthy refinement. This paper proposes a reproducible scientific workflow for developing, deploying, and analyzing online AI decision-making algorithms in digital health interventions. Grounded in practical experience from multiple real-world deployments, this workflow addresses key challenges to reproducibility across all phases of the online AI algorithm development life-cycle.
>
---
#### [replaced 006] Advancing Automated Urban Planning: Exploring Algorithmic Approaches with Generative Artificial Intelligence
- **分类: cs.AI; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2304.03892v2](http://arxiv.org/pdf/2304.03892v2)**

> **作者:** Dongjie Wang; Chang-Tien Lu; Xinyue Ye; Tan Yigitcanlar; Yanjie Fu
>
> **摘要:** The two fields of urban planning and artificial intelligence (AI) arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we introduce the importance of urban planning from the sustainability, living, economic, disaster, and environmental perspectives. We review the fundamental concepts of urban planning and relate these concepts to crucial open problems of machine learning, including adversarial learning, generative neural networks, deep encoder-decoder networks, conversational AI, and geospatial and temporal machine learning, thereby assaying how AI can contribute to modern urban planning. Thus, a central problem is automated land-use configuration, which is formulated as the generation of land uses and building configuration for a target area from surrounding geospatial, human mobility, social media, environment, and economic activities. Finally, we delineate some implications of AI for urban planning and propose key research areas at the intersection of both topics.
>
---
#### [replaced 007] Emotionally Vulnerable Subtype of Internet Gaming Disorder: Measuring and Exploring the Pathology of Problematic Generative AI Use
- **分类: cs.HC; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2510.06908v2](http://arxiv.org/pdf/2510.06908v2)**

> **作者:** Haocan Sun; Di Wu; Weizi Liu; Guoming Yu; Mike Yao
>
> **备注:** 27 pages, 5 figures, 5 tables
>
> **摘要:** Concerns over the potential over-pathologization of generative AI (GenAI) use and the lack of conceptual clarity surrounding GenAI addiction call for empirical tools and theoretical refinement. This study developed and validated the PUGenAIS-9 (Problematic Use of Generative Artificial Intelligence Scale-9 items) and examined whether PUGenAIS reflects addiction-like patterns under the Internet Gaming Disorder (IGD) framework. Using samples from China and the United States (N = 1,508), we conducted confirmatory factor analysis and identified a robust 31-item structure across nine IGD-based dimensions. We then derived the PUGenAIS-9 by selecting the highest-loading items from each dimension and validated its structure in an independent sample (N = 1,426). Measurement invariance tests confirmed its stability across nationality and gender. Person-centered (latent profile analysis) and variable-centered (network analysis) approaches revealed a 5-10% prevalence rate, a symptom network structure similar to IGD, and predictive factors related to psychological distress and functional impairment. These findings indicate that PUGenAI shares features of the emotionally vulnerable subtype of IGD rather than the competence-based type. These results support using PUGenAIS-9 to identify problematic GenAI use and show the need to rethink digital addiction with an ICD (infrastructures, content, and device) model. This keeps addiction research responsive to new media while avoiding over-pathologizing.
>
---
#### [replaced 008] Finding a Fair Scoring Function for Top-$k$ Selection: From Hardness to Practice
- **分类: cs.DB; cs.CC; cs.CY; cs.DC; cs.DS**

- **链接: [http://arxiv.org/pdf/2503.11575v3](http://arxiv.org/pdf/2503.11575v3)**

> **作者:** Guangya Cai
>
> **备注:** Abstract shortened to meet arXiv requirements
>
> **摘要:** Selecting a subset of the $k$ "best" items from a dataset of $n$ items, based on a scoring function, is a key task in decision-making. Given the rise of automated decision-making software, it is important that the outcome of this process, called top-$k$ selection, is fair. Here we consider the problem of identifying a fair linear scoring function for top-$k$ selection. The function computes a score for each item as a weighted sum of its (numerical) attribute values, and must ensure that the selected subset includes adequate representation of a minority or historically disadvantaged group. Existing algorithms do not scale efficiently, particularly in higher dimensions. Our hardness analysis shows that in more than two dimensions, no algorithm is likely to achieve good scalability with respect to dataset size, and the computational complexity is likely to increase rapidly with dimensionality. However, the hardness results also provide key insights guiding algorithm design, leading to our two-pronged solution: (1) For small values of $k$, our hardness analysis reveals a gap in the hardness barrier. By addressing various engineering challenges, including achieving efficient parallelism, we turn this potential of efficiency into an optimized algorithm delivering substantial practical performance gains. (2) For large values of $k$, where the hardness is robust, we employ a practically efficient algorithm which, despite being theoretically worse, achieves superior real-world performance. Experimental evaluations on real-world datasets then explore scenarios where worst-case behavior does not manifest, identifying areas critical to practical performance. Our solution achieves speed-ups of up to several orders of magnitude compared to SOTA, an efficiency made possible through a tight integration of hardness analysis, algorithm design, practical engineering, and empirical evaluation.
>
---
#### [replaced 009] Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2508.13169v3](http://arxiv.org/pdf/2508.13169v3)**

> **作者:** Stefanie Urchs; Veronika Thurner; Matthias Aßenmacher; Christian Heumann; Stephanie Thiemichen
>
> **摘要:** Language corpora are the foundation of most natural language processing research, yet they often reproduce structural inequalities. One such inequality is gender discrimination in how actors are represented, which can distort analyses and perpetuate discriminatory outcomes. This paper introduces a user-centric, actor-level pipeline for detecting and mitigating gender discrimination in large-scale text corpora. By combining discourse-aware analysis with metrics for sentiment, syntactic agency, and quotation styles, our method enables both fine-grained auditing and exclusion-based balancing. Applied to the taz2024full corpus of German newspaper articles (1980-2024), the pipeline yields a more gender-balanced dataset while preserving core dynamics of the source material. Our findings show that structural asymmetries can be reduced through systematic filtering, though subtler biases in sentiment and framing remain. We release the tools and reports to support further research in discourse-based fairness auditing and equitable corpus construction.
>
---
#### [replaced 010] DiMA: An LLM-Powered Ride-Hailing Assistant at DiDi
- **分类: cs.CL; cs.CY; cs.HC**

- **链接: [http://arxiv.org/pdf/2503.04768v3](http://arxiv.org/pdf/2503.04768v3)**

> **作者:** Yansong Ning; Shuowei Cai; Wei Li; Jun Fang; Naiqiang Tan; Hua Chai; Hao Liu
>
> **备注:** KDD 2025
>
> **摘要:** On-demand ride-hailing services like DiDi, Uber, and Lyft have transformed urban transportation, offering unmatched convenience and flexibility. In this paper, we introduce DiMA, an LLM-powered ride-hailing assistant deployed in DiDi Chuxing. Its goal is to provide seamless ride-hailing services and beyond through a natural and efficient conversational interface under dynamic and complex spatiotemporal urban contexts. To achieve this, we propose a spatiotemporal-aware order planning module that leverages external tools for precise spatiotemporal reasoning and progressive order planning. Additionally, we develop a cost-effective dialogue system that integrates multi-type dialog repliers with cost-aware LLM configurations to handle diverse conversation goals and trade-off response quality and latency. Furthermore, we introduce a continual fine-tuning scheme that utilizes real-world interactions and simulated dialogues to align the assistant's behavior with human preferred decision-making processes. Since its deployment in the DiDi application, DiMA has demonstrated exceptional performance, achieving 93% accuracy in order planning and 92% in response generation during real-world interactions. Offline experiments further validate DiMA capabilities, showing improvements of up to 70.23% in order planning and 321.27% in response generation compared to three state-of-the-art agent frameworks, while reducing latency by $0.72\times$ to $5.47\times$. These results establish DiMA as an effective, efficient, and intelligent mobile assistant for ride-hailing services. Our project is released at https://github.com/usail-hkust/DiMA and we also release the MCP service (https://mcp.didichuxing.com/api) to foster the ride-hailing research community.
>
---
#### [replaced 011] FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation
- **分类: cs.LG; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2505.11111v2](http://arxiv.org/pdf/2505.11111v2)**

> **作者:** Lin Zhu; Yijun Bian; Lei You
>
> **备注:** 4 figures, 19 pages
>
> **摘要:** Ensuring fairness in machine learning models is critical, particularly in high-stakes domains where biased decisions can lead to serious societal consequences. Existing preprocessing approaches generally lack transparent mechanisms for identifying which features or instances are responsible for unfairness. This obscures the rationale behind data modifications. We introduce FairSHAP, a novel pre-processing framework that leverages Shapley value attribution to improve both individual and group fairness. FairSHAP identifies fairness-critical instances in the training data using an interpretable measure of feature importance, and systematically modifies them through instance-level matching across sensitive groups. This process reduces discriminative risk - an individual fairness metric - while preserving data integrity and model accuracy. We demonstrate that FairSHAP significantly improves demographic parity and equality of opportunity across diverse tabular datasets, achieving fairness gains with minimal data perturbation and, in some cases, improved predictive performance. As a model-agnostic and transparent method, FairSHAP integrates seamlessly into existing machine learning pipelines and provides actionable insights into the sources of bias.Our code is on https://github.com/youlei202/FairSHAP.
>
---
#### [replaced 012] Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2509.00529v2](http://arxiv.org/pdf/2509.00529v2)**

> **作者:** Eunjung Cho; Alexander Hoyle; Yoan Hermstrüwer
>
> **备注:** Accepted at NLLP 2025
>
> **摘要:** Large Language Models (LLMs) are increasingly used to generate user-tailored summaries, adapting outputs to specific stakeholders. In legal contexts, this raises important questions about motivated reasoning -- how models strategically frame information to align with a stakeholder's position within the legal system. Building on theories of legal realism and recent trends in legal practice, we investigate how LLMs respond to prompts conditioned on different legal roles (e.g., judges, prosecutors, attorneys) when summarizing judicial decisions. We introduce an evaluation framework grounded in legal fact and reasoning inclusion, also considering favorability towards stakeholders. Our results show that even when prompts include balancing instructions, models exhibit selective inclusion patterns that reflect role-consistent perspectives. These findings raise broader concerns about how similar alignment may emerge as LLMs begin to infer user roles from prior interactions or context, even without explicit role instructions. Our results underscore the need for role-aware evaluation of LLM summarization behavior in high-stakes legal settings.
>
---
#### [replaced 013] Language Models Surface the Unwritten Code of Science and Society
- **分类: cs.CY; cs.CL; cs.DL**

- **链接: [http://arxiv.org/pdf/2505.18942v4](http://arxiv.org/pdf/2505.18942v4)**

> **作者:** Honglin Bao; Siyang Wu; Jiwoong Choi; Yingrong Mao; James A. Evans
>
> **摘要:** This paper calls on the research community not only to investigate how human biases are inherited by large language models (LLMs) but also to explore how these biases in LLMs can be leveraged to make society's "unwritten code" - such as implicit stereotypes and heuristics - visible and accessible for critique. We introduce a conceptual framework through a case study in science: uncovering hidden rules in peer review - the factors that reviewers care about but rarely state explicitly due to normative scientific expectations. The idea of the framework is to push LLMs to speak out their heuristics through generating self-consistent hypotheses - why one paper appeared stronger in reviewer scoring - among paired papers submitted to 45 academic conferences, while iteratively searching deeper hypotheses from remaining pairs where existing hypotheses cannot explain. We observed that LLMs' normative priors about the internal characteristics of good science extracted from their self-talk, e.g., theoretical rigor, were systematically updated toward posteriors that emphasize storytelling about external connections, such as how the work is positioned and connected within and across literatures. Human reviewers tend to explicitly reward aspects that moderately align with LLMs' normative priors (correlation = 0.49) but avoid articulating contextualization and storytelling posteriors in their review comments (correlation = -0.14), despite giving implicit reward to them with positive scores. These patterns are robust across different models and out-of-sample judgments. We discuss the broad applicability of our proposed framework, leveraging LLMs as diagnostic tools to amplify and surface the tacit codes underlying human society, enabling public discussion of revealed values and more precisely targeted responsible AI.
>
---
#### [replaced 014] Adoption of Watermarking for Generative AI Systems in Practice and Implications under the new EU AI Act
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.18156v3](http://arxiv.org/pdf/2503.18156v3)**

> **作者:** Bram Rijsbosch; Gijs van Dijck; Konrad Kollnig
>
> **备注:** Note that this work has not yet been published in a peer review journal, it is therefore potentially still subject to change. Update October 2025: extended and slightly revised arguments in legal analysis, results unchanged
>
> **摘要:** AI-generated images have become so good in recent years that individuals often cannot distinguish them any more from "real" images. This development, combined with the rapid spread of AI-generated content online, creates a series of societal risks. Watermarking, a technique that involves embedding information within images and other content to indicate their AI-generated nature, has emerged as a primary mechanism to address the risks posed by AI-generated content. Indeed, watermarking and AI labelling measures are now becoming a legal requirement in many jurisdictions, including under the 2024 European Union AI Act. Despite the widespread use of AI image generation systems, the practical implications and the current status of implementation of these measures remain largely unexamined. The present paper therefore provides both an empirical and a legal analysis of these measures. In our legal analysis, we identify four categories of generative AI deployment scenarios and outline how the legal obligations could apply in each category. In our empirical analysis, we find that only a minority number of AI image generators currently implement adequate watermarking (38%) and deep fake labelling (18%) practices. In response, we suggest a range of avenues of how the implementation of these legally mandated techniques can be improved, and publicly share our tooling for the detection of watermarks in images.
>
---
#### [replaced 015] Confident-Knowledge Diversity Drives Human-Human and Human-AI Free Discussion Synergy and Reveals Pure-AI Discussion Shortfalls
- **分类: cs.HC; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.22889v2](http://arxiv.org/pdf/2507.22889v2)**

> **作者:** Tom Sheffer; Alon Miron; Asael Sklar; Yaniv Dover; Ariel Goldstein
>
> **摘要:** Conversations transform individual knowledge into collective insight, enabling collaborators to solve problems more accurately than they could alone. Whether dialogues among large language models (LLMs) can replicate the synergistic gains observed in human discussion remains unclear. We systematically compared four interaction settings: LLM-LLM pairs, LLM trios, human trios, and human-LLM pairs, using validated medical multiple-choice questions. Agents answered individually, engaged in open-ended discussion, then re-answered, allowing us to quantify conversational gains. Interactions that included humans consistently yielded synergy (post-discussion accuracy increased for both stronger and weaker participants), whereas purely LLM groups did not improve and often declined. To explain and prospectively predict when unstructured dialogue helps, we introduce an agent-agnostic confident-knowledge framework that models each participant by performance (accuracy) and confidence. This framework quantifies confident-knowledge diversity, the degree to which one agent tends to be correct when another is uncertain, and yields a conservative upper bound on gains achievable via confidence-informed decisions, which we term Potential Conversation Synergy. Across humans, LLMs, and mixed teams, this metric prospectively predicts observed conversational improvements: when confident-knowledge diversity is low (as in LLM-only groups), discussion doesn't improve performance; when it is present (as in human or human-LLM groups), free-form dialogue reliably lifts accuracy. These findings propose a new concept and method for AI collaboration: quantifying confident-knowledge diversity to prospectively predict conversational gains and guide team selection and interaction design in both multi-agent and human-AI settings.
>
---
