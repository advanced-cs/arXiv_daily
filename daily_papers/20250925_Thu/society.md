# 计算机与社会 cs.CY

- **最新发布 10 篇**

- **更新 8 篇**

## 最新发布

#### [new 001] The three main doctrines on the future of AI
- **分类: cs.CY**

- **简介: 该论文属于AI未来风险研究任务，旨在梳理专家对AI发展后果的观点。论文提出了三种主要学说：主导论、灭绝论和替代论，并分析其假设与论据，以明确AI发展影响的核心分歧。**

- **链接: [http://arxiv.org/pdf/2509.20050v1](http://arxiv.org/pdf/2509.20050v1)**

> **作者:** Alex Amadori; Eva Behrens; Gabriel Alfour; Andrea Miotti
>
> **摘要:** This paper develops a taxonomy of expert perspectives on the risks and likely consequences of artificial intelligence, with particular focus on Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI). Drawing from primary sources, we identify three predominant doctrines: (1) The dominance doctrine, which predicts that the first actor to create sufficiently advanced AI will attain overwhelming strategic superiority sufficient to cheaply neutralize its opponents' defenses; (2) The extinction doctrine, which anticipates that humanity will likely lose control of ASI, leading to the extinction of the human species or its permanent disempowerment; (3) The replacement doctrine, which forecasts that AI will automate a large share of tasks currently performed by humans, but will not be so transformative as to fundamentally reshape or bring an end to human civilization. We examine the assumptions and arguments underlying each doctrine, including expectations around the pace of AI progress and the feasibility of maintaining advanced AI under human control. While the boundaries between doctrines are sometimes porous and many experts hedge across them, this taxonomy clarifies the core axes of disagreement over the anticipated scale and nature of the consequences of AI development.
>
---
#### [new 002] Choosing to Be Green: Advancing Green AI via Dynamic Model Selection
- **分类: cs.CY; cs.AI**

- **简介: 该论文研究绿色AI中的动态模型选择任务，旨在降低AI的环境成本。提出两种方法——动态模型级联和路由，根据任务、能效和精度需求选择最优模型。实验表明，可在保留95%精度的同时节省约25%能耗。**

- **链接: [http://arxiv.org/pdf/2509.19996v1](http://arxiv.org/pdf/2509.19996v1)**

> **作者:** Emilio Cruciani; Roberto Verdecchia
>
> **备注:** 2nd Workshop on Green-Aware Artificial Intelligence (Green-Aware 2025). 9 pages, 1 figure
>
> **摘要:** Artificial Intelligence is increasingly pervasive across domains, with ever more complex models delivering impressive predictive performance. This fast technological advancement however comes at a concerning environmental cost, with state-of-the-art models - particularly deep neural networks and large language models - requiring substantial computational resources and energy. In this work, we present the intuition of Green AI dynamic model selection, an approach based on dynamic model selection that aims at reducing the environmental footprint of AI by selecting the most sustainable model while minimizing potential accuracy loss. Specifically, our approach takes into account the inference task, the environmental sustainability of available models, and accuracy requirements to dynamically choose the most suitable model. Our approach presents two different methods, namely Green AI dynamic model cascading and Green AI dynamic model routing. We demonstrate the effectiveness of our approach via a proof of concept empirical example based on a real-world dataset. Our results show that Green AI dynamic model selection can achieve substantial energy savings (up to ~25%) while substantially retaining the accuracy of the most energy greedy solution (up to ~95%). As conclusion, our preliminary findings highlight the potential that hybrid, adaptive model selection strategies withhold to mitigate the energy demands of modern AI systems without significantly compromising accuracy requirements.
>
---
#### [new 003] Affective Computing and Emotional Data: Challenges and Implications in Privacy Regulations, The AI Act, and Ethics in Large Language Models
- **分类: cs.CY; cs.AI**

- **简介: 该论文探讨情感计算与大语言模型在情感识别中的应用，分析情感数据的隐私、伦理及法律问题，涉及GDPR和欧盟AI法案。研究聚焦技术实现、文化差异与监管挑战，旨在推动负责任的情感AI发展。**

- **链接: [http://arxiv.org/pdf/2509.20153v1](http://arxiv.org/pdf/2509.20153v1)**

> **作者:** Nicola Fabiano
>
> **摘要:** This paper examines the integration of emotional intelligence into artificial intelligence systems, with a focus on affective computing and the growing capabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to recognize and respond to human emotions. Drawing on interdisciplinary research that combines computer science, psychology, and neuroscience, the study analyzes foundational neural architectures - CNNs for processing facial expressions and RNNs for sequential data, such as speech and text - that enable emotion recognition. It examines the transformation of human emotional experiences into structured emotional data, addressing the distinction between explicit emotional data collected with informed consent in research settings and implicit data gathered passively through everyday digital interactions. That raises critical concerns about lawful processing, AI transparency, and individual autonomy over emotional expressions in digital environments. The paper explores implications across various domains, including healthcare, education, and customer service, while addressing challenges of cultural variations in emotional expression and potential biases in emotion recognition systems across different demographic groups. From a regulatory perspective, the paper examines emotional data in the context of the GDPR and the EU AI Act frameworks, highlighting how emotional data may be considered sensitive personal data that requires robust safeguards, including purpose limitation, data minimization, and meaningful consent mechanisms.
>
---
#### [new 004] Generative AI as a catalyst for democratic Innovation: Enhancing citizen engagement in participatory budgeting
- **分类: cs.CY; cs.AI**

- **简介: 该论文探讨生成式AI如何促进民主创新，提升公民在参与式预算中的参与度。针对社会参与度下降和极化问题，研究分析了AI在公共咨询平台中的应用，旨在增强公民提案能力和政民对话，推动包容性民主治理。**

- **链接: [http://arxiv.org/pdf/2509.19497v1](http://arxiv.org/pdf/2509.19497v1)**

> **作者:** Italo Alberto do Nascimento Sousa; Jorge Machado; Jose Carlos Vaz
>
> **备注:** 19 pages, VI International Meeting on Participation, Democracy and Public Policies
>
> **摘要:** This research examines the role of Generative Artificial Intelligence (AI) in enhancing citizen engagement in participatory budgeting. In response to challenges like declining civic participation and increased societal polarization, the study explores how online political participation can strengthen democracy and promote social equity. By integrating Generative AI into public consultation platforms, the research aims to improve citizen proposal formulation and foster effective dialogue between citizens and government. It assesses the capacities governments need to implement AI-enhanced participatory tools, considering technological dependencies and vulnerabilities. Analyzing technological structures, actors, interests, and strategies, the study contributes to understanding how technological advancements can reshape participatory institutions to better facilitate citizen involvement. Ultimately, the research highlights how Generative AI can transform participatory institutions, promoting inclusive, democratic engagement and empowering citizens.
>
---
#### [new 005] DSA, AIA, and LLMs: Approaches to conceptualizing and auditing moderation in LLM-based chatbots across languages and interfaces in the electoral contexts
- **分类: cs.CY**

- **简介: 该论文探讨在数字服务法案（DSA）背景下，如何治理和审计基于大语言模型（LLM）的聊天机器人内容，特别是在选举语境中。通过跨语言、跨平台的混合方法审计，研究了Copilot、ChatGPT和Gemini在2024年欧洲和美国选举中的内容审核机制，提出了解决监管不确定性的方法。**

- **链接: [http://arxiv.org/pdf/2509.19890v1](http://arxiv.org/pdf/2509.19890v1)**

> **作者:** Natalia Stanusch; Raziye Buse Cetin; Salvatore Romano; Miazia Schueler; Meret Baumgartner; Bastian August; Alexandra Rosca
>
> **摘要:** The integration of Large Language Models (LLMs) into chatbot-like search engines poses new challenges for governing, assessing, and scrutinizing the content output by these online entities, especially in light of the Digital Service Act (DSA). In what follows, we first survey the regulation landscape in which we can situate LLM-based chatbots and the notion of moderation. Second, we outline the methodological approaches to our study: a mixed-methods audit across chatbots, languages, and elections. We investigated Copilot, ChatGPT, and Gemini across ten languages in the context of the 2024 European Parliamentary Election and the 2024 US Presidential Election. Despite the uncertainty in regulatory frameworks, we propose a set of solutions on how to situate, study, and evaluate chatbot moderation.
>
---
#### [new 006] Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文属于心理学测量任务，旨在减少人格评估中的社会期望偏差。研究使用GPT-3对IPIP-BFM-50量表进行项目中性化改写，并验证其信效度。结果显示AI中性化有一定效果，但不完全。**

- **链接: [http://arxiv.org/pdf/2509.19314v1](http://arxiv.org/pdf/2509.19314v1)**

> **作者:** Sirui Wu; Daijin Yang
>
> **备注:** Accepted for publication in NCME-AIME 2025
>
> **摘要:** This study evaluates item neutralization assisted by the large language model (LLM) to reduce social desirability bias in personality assessment. GPT-o3 was used to rewrite the International Personality Item Pool Big Five Measure (IPIP-BFM-50), and 203 participants completed either the original or neutralized form along with the Marlowe-Crowne Social Desirability Scale. The results showed preserved reliability and a five-factor structure, with gains in Conscientiousness and declines in Agreeableness and Openness. The correlations with social desirability decreased for several items, but inconsistently. Configural invariance held, though metric and scalar invariance failed. Findings support AI neutralization as a potential but imperfect bias-reduction method.
>
---
#### [new 007] A Longitudinal Randomized Control Study of Companion Chatbot Use: Anthropomorphism and Its Mediating Role on Social Impacts
- **分类: cs.HC; cs.AI; cs.CY**

- **简介: 该论文是一项纵向随机对照研究，探讨陪伴聊天机器人使用对社交影响的作用。研究发现，与聊天机器人的互动并未显著影响社交健康，但人类将AI拟人化程度越高，其对人际关系的影响越明显，揭示了拟人化在人机交互中的中介作用。**

- **链接: [http://arxiv.org/pdf/2509.19515v1](http://arxiv.org/pdf/2509.19515v1)**

> **作者:** Rose E. Guingrich; Michael S. A. Graziano
>
> **摘要:** Relationships with social artificial intelligence (AI) agents are on the rise. People report forming friendships, mentorships, and romantic partnerships with chatbots such as Replika, a type of social AI agent that is designed specifically for companionship. Concerns that companion chatbot relationships may harm or replace human ones have been raised, but whether and how these social consequences occur remains unclear. Prior research suggests that people's states of social need and their anthropomorphism of the AI agent may play a role in how human-AI interaction impacts human-human interaction. In this longitudinal study (N = 183), participants were randomly assigned to converse with a companion chatbot over text or to play text-based word games for 10 minutes a day for 21 consecutive days. During these 21 days, participants also completed four surveys and two audio-recorded interviews. We found that people's social health and relationships were not significantly impacted by interacting with a companion chatbot across 21 days compared to the control group. However, people who had a higher desire to socially connect anthropomorphized the chatbot more. Those who anthropomorphized the chatbot more indicated that the human-chatbot interaction had greater impacts on their social interactions and relationships with family and friends. A mediation analysis suggested that the impact of human-AI interaction on human-human social outcomes was mediated by the extent to which people anthropomorphized the AI agent, which itself was related to the desire to socially connect.
>
---
#### [new 008] Current and Future Directions for Responsible Quantum Technologies: A ResQT Community Perspective
- **分类: physics.soc-ph; cs.CY**

- **简介: 该论文探讨了负责任量子技术（ResQT）的发展方向，旨在解决量子技术带来的伦理、社会和地缘政治问题。论文总结了ResQT社区的工作，并提出未来应借鉴人工智能等领域的经验，推动包容性与可持续发展，确保量子技术惠及全社会。**

- **链接: [http://arxiv.org/pdf/2509.19815v1](http://arxiv.org/pdf/2509.19815v1)**

> **作者:** Adrian Schmidt; Alexandre Artaud; Arsev Umur Aydinoglu; Astrid Bötticher; Rodrigo Araiza Bravo; Marilu Chiofalo; Rebecca Coates; Ilke Ercan; Alexei Grinbaum; Emily Haworth; Carolyn Ten Holter; Eline de Jong; Bart Karstens; Matthias C. Kettemann; Anna Knörr; Clarissa Ai Ling Lee; Fabienne Marco; Wenzel Mehnert; Josephine C. Meyer; Shantanu Sharma; Pieter Vermaas; Carrie Weidner; Barbara Wellmann; Mira L. Wolf-Bauwens; Zeki C. Seskir
>
> **备注:** 25 pages, 3 figures
>
> **摘要:** Quantum technologies (QT) are advancing rapidly, promising advancements across a wide spectrum of applications but also raising significant ethical, societal, and geopolitical impacts, including dual-use capabilities, varying levels of access, and impending quantum divide(s). To address these, the Responsible Quantum Technologies (ResQT) community was established to share knowledge, perspectives, and best practices across various disciplines. Its mission is to ensure QT developments align with ethical principles, promote equity, and mitigate unintended consequences. Initial progress has been made, as scholars and policymakers increasingly recognize principles of responsible QT. However, more widespread dissemination is needed, and as QT matures, so must responsible QT. This paper provides a comprehensive overview of the ResQT community's current work and states necessary future directions. Drawing on historical lessons from artificial intelligence and nanotechnology, actions targeting the quantum divide(s) are addressed, including the implementation of responsible research and innovation, fostering wider stakeholder engagement, and sustainable development. These actions aim to build trust and engagement, facilitating the participatory and responsible development of QT. The ResQT community advocates that responsible QT should be an integral part of quantum development rather than an afterthought so that quantum technologies evolve toward a future that is technologically advanced and beneficial for all.
>
---
#### [new 009] What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities
- **分类: cs.AI; cs.CY; cs.LG**

- **简介: 该论文提出一个用于AI能力评估的推理框架，旨在解决基准测试可靠性问题。通过引入能力理论模型和适应性算法，减少扰动和样本不确定性带来的影响，提升评估的可信度。**

- **链接: [http://arxiv.org/pdf/2509.19590v1](http://arxiv.org/pdf/2509.19590v1)**

> **作者:** Nathanael Jo; Ashia Wilson
>
> **摘要:** Evaluations of generative models on benchmark data are now ubiquitous, and their outcomes critically shape public and scientific expectations of AI's capabilities. Yet growing skepticism surrounds their reliability. How can we know that a reported accuracy genuinely reflects a model's true performance? Evaluations are often presented as simple measurements, but in reality they are inferences: to treat benchmark scores as evidence of capability is already to assume a theory of what capability is and how it manifests in a test. We make this step explicit by proposing a principled framework for evaluation as inference: begin from a theory of capability, and then derive methods for estimating it. This perspective, familiar in fields such as psychometrics, has not yet become commonplace in AI evaluation. As a proof of concept, we address a central challenge that undermines reliability: sensitivity to perturbations. After formulating a model of ability, we introduce methods that infer ability while accounting for uncertainty from sensitivity and finite samples, including an adaptive algorithm that significantly reduces sample complexity. Together, these contributions lay the groundwork for more reliable and trustworthy estimates of AI capabilities as measured through benchmarks.
>
---
#### [new 010] Cascade! Human in the loop shortcomings can increase the risk of failures in recommender systems
- **分类: cs.IR; cs.CY**

- **简介: 该论文探讨了推荐系统中“人在回路”机制的局限性，指出其可能引发级联或复合故障。论文分析了三种常见部署场景下的动态特性，并提出两项改进建议，旨在提升推荐系统的社会责任与可靠性。**

- **链接: [http://arxiv.org/pdf/2509.20099v1](http://arxiv.org/pdf/2509.20099v1)**

> **作者:** Wm. Matthew Kennedy; Nishanshi Shukla; Cigdem Patlak; Blake Chambers; Theodora Skeadas; Tuesday; Kingsley Owadara; Aayush Dhanotiya
>
> **摘要:** Recommender systems are among the most commonly deployed systems today. Systems design approaches to AI-powered recommender systems have done well to urge recommender system developers to follow more intentional data collection, curation, and management procedures. So too has the "human-in-the-loop" paradigm been widely adopted, primarily to address the issue of accountability. However, in this paper, we take the position that human oversight in recommender system design also entails novel risks that have yet to be fully described. These risks are "codetermined" by the information context in which such systems are often deployed. Furthermore, new knowledge of the shortcomings of "human-in-the-loop" practices to deliver meaningful oversight of other AI systems suggest that they may also be inadequate for achieving socially responsible recommendations. We review how the limitations of human oversight may increase the chances of a specific kind of failure: a "cascade" or "compound" failure. We then briefly explore how the unique dynamics of three common deployment contexts can make humans in the loop more likely to fail in their oversight duties. We then conclude with two recommendations.
>
---
## 更新

#### [replaced 001] Urania: Differentially Private Insights into AI Use
- **分类: cs.LG; cs.AI; cs.CL; cs.CR; cs.CY**

- **链接: [http://arxiv.org/pdf/2506.04681v2](http://arxiv.org/pdf/2506.04681v2)**

> **作者:** Daogao Liu; Edith Cohen; Badih Ghazi; Peter Kairouz; Pritish Kamath; Alexander Knop; Ravi Kumar; Pasin Manurangsi; Adam Sealfon; Da Yu; Chiyuan Zhang
>
> **备注:** To appear at COLM 2025
>
> **摘要:** We introduce $Urania$, a novel framework for generating insights about LLM chatbot interactions with rigorous differential privacy (DP) guarantees. The framework employs a private clustering mechanism and innovative keyword extraction methods, including frequency-based, TF-IDF-based, and LLM-guided approaches. By leveraging DP tools such as clustering, partition selection, and histogram-based summarization, $Urania$ provides end-to-end privacy protection. Our evaluation assesses lexical and semantic content preservation, pair similarity, and LLM-based metrics, benchmarking against a non-private Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple empirical privacy evaluation that demonstrates the enhanced robustness of our DP pipeline. The results show the framework's ability to extract meaningful conversational insights while maintaining stringent user privacy, effectively balancing data utility with privacy preservation.
>
---
#### [replaced 002] Testing Fairness with Utility Tradeoffs: A Wasserstein Projection Approach
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2505.11678v3](http://arxiv.org/pdf/2505.11678v3)**

> **作者:** Yan Chen; Zheng Tan; Jose Blanchet; Hanzhang Qin
>
> **摘要:** Ensuring fairness in data driven decision making has become a central concern across domains such as marketing, lending, and healthcare, but fairness constraints often come at the cost of utility. We propose a statistical hypothesis testing framework that jointly evaluates approximate fairness and utility, relaxing strict fairness requirements while ensuring that overall utility remains above a specified threshold. Our framework builds on the strong demographic parity (SDP) criterion and incorporates a utility measure motivated by the potential outcomes framework. The test statistic is constructed via Wasserstein projections, enabling auditors to assess whether observed fairness-utility tradeoffs are intrinsic to the algorithm or attributable to randomness in the data. We show that the test is computationally tractable, interpretable, broadly applicable across machine learning models, and extendable to more general settings. We apply our approach to multiple real-world datasets, offering new insights into the fairness-utility tradeoff through the perspective of statistical hypothesis testing.
>
---
#### [replaced 003] Identities are not Interchangeable: The Problem of Overgeneralization in Fair Machine Learning
- **分类: cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.04038v2](http://arxiv.org/pdf/2505.04038v2)**

> **作者:** Angelina Wang
>
> **备注:** ACM Conference on Fairness, Accountability, and Transparency (FAccT) 2025
>
> **摘要:** A key value proposition of machine learning is generalizability: the same methods and model architecture should be able to work across different domains and different contexts. While powerful, this generalization can sometimes go too far, and miss the importance of the specifics. In this work, we look at how fair machine learning has often treated as interchangeable the identity axis along which discrimination occurs. In other words, racism is measured and mitigated the same way as sexism, as ableism, as ageism. Disciplines outside of computer science have pointed out both the similarities and differences between these different forms of oppression, and in this work we draw out the implications for fair machine learning. While certainly not all aspects of fair machine learning need to be tailored to the specific form of oppression, there is a pressing need for greater attention to such specificity than is currently evident. Ultimately, context specificity can deepen our understanding of how to build more fair systems, widen our scope to include currently overlooked harms, and, almost paradoxically, also help to narrow our scope and counter the fear of an infinite number of group-specific methods of analysis.
>
---
#### [replaced 004] Patterns in the Transition From Founder-Leadership to Community Governance of Open Source
- **分类: cs.CY; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2509.16295v2](http://arxiv.org/pdf/2509.16295v2)**

> **作者:** Mobina Noori; Mahasweta Chakraborti; Amy X Zhang; Seth Frey
>
> **摘要:** Open digital public infrastructure needs community management to ensure accountability, sustainability, and robustness. Yet open-source projects often rely on centralized decision-making, and the determinants of successful community management remain unclear. We analyze 637 GitHub repositories to trace transitions from founder-led to shared governance. Specifically, we document trajectories to community governance by extracting institutional roles, actions, and deontic cues from version-controlled project constitutions GOVERNANCE.md. With a semantic parsing pipeline, we cluster elements into broader role and action types. We find roles and actions grow, and regulation becomes more balanced, reflecting increases in governance scope and differentiation over time. Rather than shifting tone, communities grow by layering and refining responsibilities. As transitions to community management mature, projects increasingly regulate ecosystem-level relationships and add definition to project oversight roles. Overall, this work offers a scalable pipeline for tracking the growth and development of community governance regimes from open-source software's familiar default of founder-ownership.
>
---
#### [replaced 005] Fair Clustering with Minimum Representation Constraints
- **分类: math.OC; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2409.02963v2](http://arxiv.org/pdf/2409.02963v2)**

> **作者:** Connor Lawless; Oktay Gunluk
>
> **备注:** arXiv admin note: text overlap with arXiv:2302.03151
>
> **摘要:** Clustering is a well-studied unsupervised learning task that aims to partition data points into a number of clusters. In many applications, these clusters correspond to real-world constructs (e.g., electoral districts, playlists, TV channels), where a group (e.g., social or demographic) benefits only if it reaches a minimum level of representation in the cluster (e.g., 50% to elect their preferred candidate). In this paper, we study the k-means and k-medians clustering problems under the additional fairness constraint that each group must attain a minimum level of representation in at least a specified number of clusters. We formulate this problem as a mixed-integer (nonlinear) optimization problem and propose an alternating minimization algorithm, called MiniReL, to solve it. Although incorporating fairness constraints results in an NP-hard assignment problem within the MiniReL algorithm, we present several heuristic strategies that make the approach practical even for large datasets. Numerical results demonstrate that our method yields fair clusters without increasing clustering cost across standard benchmark datasets.
>
---
#### [replaced 006] Do AI Companies Make Good on Voluntary Commitments to the White House?
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2508.08345v2](http://arxiv.org/pdf/2508.08345v2)**

> **作者:** Jennifer Wang; Kayla Huang; Kevin Klyman; Rishi Bommasani
>
> **摘要:** Voluntary commitments are central to international AI governance, as demonstrated by recent voluntary guidelines from the White House to the G7, from Bletchley Park to Seoul. How do major AI companies make good on their commitments? We score companies based on their publicly disclosed behavior by developing a detailed rubric based on their eight voluntary commitments to the White House in 2023. We find significant heterogeneity: while the highest-scoring company (OpenAI) scores a 83% overall on our rubric, the average score across all companies is just 53%. The companies demonstrate systemically poor performance for their commitment to model weight security with an average score of 17%: 11 of the 16 companies receive 0% for this commitment. Our analysis highlights a clear structural shortcoming that future AI governance initiatives should correct: when companies make public commitments, they should proactively disclose how they meet their commitments to provide accountability, and these disclosures should be verifiable. To advance policymaking on corporate AI governance, we provide three directed recommendations that address underspecified commitments, the role of complex AI supply chains, and public transparency that could be applied towards AI governance initiatives worldwide.
>
---
#### [replaced 007] Who is Responsible? The Data, Models, Users or Regulations? A Comprehensive Survey on Responsible Generative AI for a Sustainable Future
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2502.08650v5](http://arxiv.org/pdf/2502.08650v5)**

> **作者:** Shaina Raza; Rizwan Qureshi; Anam Zahid; Safiullah Kamawal; Ferhat Sadak; Joseph Fioresi; Muhammaed Saeed; Ranjan Sapkota; Aditya Jain; Anas Zafar; Muneeb Ul Hassan; Aizan Zafar; Hasan Maqbool; Ashmal Vayani; Jia Wu; Maged Shoman
>
> **备注:** under review
>
> **摘要:** Generative AI is moving rapidly from research into real world deployment across sectors, which elevates the need for responsible development, deployment, evaluation, and governance. To address this pressing challenge, in this study, we synthesize the landscape of responsible generative AI across methods, benchmarks, and policies, and connects governance expectations to concrete engineering practice. We follow a prespecified search and screening protocol focused on post-ChatGPT era with selective inclusion of foundational work for definitions, and we conduct a narrative and thematic synthesis. Three findings emerge; First, benchmark and practice coverage is dense for bias and toxicity but relatively sparse for privacy and provenance, deepfake and media integrity risk, and system level failure in tool using and agentic settings. Second, many evaluations remain static and task local, which limits evidence portability for audit and lifecycle assurance. Third, documentation and metric validity are inconsistent, which complicates comparison across releases and domains. We outline a research and practice agenda that prioritizes adaptive and multimodal evaluation, privacy and provenance testing, deepfake risk assessment, calibration and uncertainty reporting, versioned and documented artifacts, and continuous monitoring. Limitations include reliance on public artifacts and the focus period, which may under represent capabilities reported later. The survey offers a path to align development and evaluation with governance needs and to support safe, transparent, and accountable deployment across domains. Project page: https://anas-zafar.github.io/responsible-ai.github.io , GitHub: https://github.com/anas-zafar/Responsible-AI
>
---
#### [replaced 008] When Your Boss Is an AI Bot: Exploring Opportunities and Risks of Manager Clone Agents in the Future Workplace
- **分类: cs.HC; cs.CY**

- **链接: [http://arxiv.org/pdf/2509.10993v2](http://arxiv.org/pdf/2509.10993v2)**

> **作者:** Qing Hu; Qing Xiao; Hancheng Cao; Hong Shen
>
> **备注:** 18 pages, 2 figures
>
> **摘要:** As Generative AI (GenAI) becomes increasingly embedded in the workplace, managers are beginning to create Manager Clone Agents - AI-powered digital surrogates that are trained on their work communications and decision patterns to perform managerial tasks on their behalf. To investigate this emerging phenomenon, we conducted six design fiction workshops (n = 23) with managers and workers, in which participants co-created speculative scenarios and discussed how Manager Clone Agents might transform collaborative work. We identified four potential roles that participants envisioned for Manager Clone Agents: proxy presence, informational conveyor belt, productivity engine, and leadership amplifier, while highlighting concerns spanning individual, interpersonal, and organizational levels. We provide design recommendations envisioned by both parties for integrating Manager Clone Agents responsibly into the future workplace, emphasizing the need to prioritize workers' perspectives, strengthen interpersonal bonds, and enable flexible clone configuration.
>
---
