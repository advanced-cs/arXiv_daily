# 计算机与社会 cs.CY

- **最新发布 13 篇**

- **更新 10 篇**

## 最新发布

#### [new 001] Queueing for Civility: User Perspectives on Regulating Emotions in Online Conversations
- **分类: cs.CY; cs.HC**

- **简介: 该论文属于情感管理任务，旨在解决在线对话中的情绪冲动问题。通过引入评论队列机制，延迟评论发布以促进反思，减少负面情绪传播。**

- **链接: [http://arxiv.org/pdf/2507.11477v1](http://arxiv.org/pdf/2507.11477v1)**

> **作者:** Akriti Verma; Shama Islam; Valeh Moghaddam; Adnan Anwar
>
> **摘要:** Online conversations are often interrupted by trolling, which causes emotional distress and conflict among users. Previous research has focused on moderating harmful content after it has been posted, but ways to manage emotions in real-time remain unexplored. This study suggests a comment queuing mechanism that delays comment publishing, encourages self-reflection, and reduces the impact of impulsive and toxic comments. To assess the efficacy of this approach, a mixed-method research design is used. An analysis of 15,000 user interactions on Reddit showed that this approach could reduce the spread of hate speech and anger by up to 15%, with only 4% of comments being delayed for about 47 seconds on average. We also surveyed users for feedback on the mechanism. The results showed that 93. 3\% of the participants thought that the queuing mechanism could help calm the discussions and showed interest in seeing it used on social media platforms. Furthermore, 83% believed it would reduce impulsive comments and balance the emotional tone in conversations. We found a strong link between users' typical emotional states while using social media and their perceptions of the delay, with calm users finding the mechanism helpful and frustrated users anticipating frustration.
>
---
#### [new 002] "Is it always watching? Is it always listening?" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots
- **分类: cs.CY; cs.AI; cs.CR; cs.ET; cs.HC**

- **简介: 该论文属于隐私与安全研究任务，探讨家庭社交机器人带来的隐私和安全问题。通过访谈发现用户对数据收集、信息准确性和隐私控制的担忧，提出需增强透明度和隐私保护措施。**

- **链接: [http://arxiv.org/pdf/2507.10786v1](http://arxiv.org/pdf/2507.10786v1)**

> **作者:** Henry Bell; Jabari Kwesi; Hiba Laabadli; Pardis Emami-Naeini
>
> **摘要:** Equipped with artificial intelligence (AI) and advanced sensing capabilities, social robots are gaining interest among consumers in the United States. These robots seem like a natural evolution of traditional smart home devices. However, their extensive data collection capabilities, anthropomorphic features, and capacity to interact with their environment make social robots a more significant security and privacy threat. Increased risks include data linkage, unauthorized data sharing, and the physical safety of users and their homes. It is critical to investigate U.S. users' security and privacy needs and concerns to guide the design of social robots while these devices are still in the early stages of commercialization in the U.S. market. Through 19 semi-structured interviews, we identified significant security and privacy concerns, highlighting the need for transparency, usability, and robust privacy controls to support adoption. For educational applications, participants worried most about misinformation, and in medical use cases, they worried about the reliability of these devices. Participants were also concerned with the data inference that social robots could enable. We found that participants expect tangible privacy controls, indicators of data collection, and context-appropriate functionality.
>
---
#### [new 003] Artificial Intelligence and Journalism: A Systematic Bibliometric and Thematic Analysis of Global Research
- **分类: cs.CY; cs.DL**

- **简介: 该论文属于系统综述任务，旨在分析AI在新闻业中的研究现状与趋势，解决AI对新闻实践的影响及伦理问题，通过文献计量和主题分析进行综合研究。**

- **链接: [http://arxiv.org/pdf/2507.10891v1](http://arxiv.org/pdf/2507.10891v1)**

> **作者:** Mohammad Al Masum Molla; Md Manjurul Ahsan
>
> **摘要:** Artificial Intelligence (AI) is reshaping journalistic practices across the globe, offering new opportunities while raising ethical, professional, and societal concerns. This study presents a comprehensive systematic review of published articles on AI in journalism from 2010 to 2025. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines, a total of 72 peer-reviewed articles were selected from Scopus and Web of Science databases. The analysis combines bibliometric mapping and qualitative thematic synthesis to identify dominant trends, technologies, geographical distributions, and ethical debates. Additionally, sentiment analysis was performed on article abstracts using the Valence Aware Dictionary and sEntiment Reasoner (VADER) algorithm to capture evaluative tones across the literature. The findings show a sharp increase in research activity after 2020, with prominent focus areas including automation, misinformation, and ethical governance. While most studies reflect cautious optimism, concerns over bias, transparency, and accountability remain persistent. The review also highlights regional disparities in scholarly contributions, with limited representation from the Global South. By integrating quantitative and qualitative insights, this study offers a multi-dimensional understanding of how AI is transforming journalism and proposes future research directions for inclusive and responsible innovation.
>
---
#### [new 004] The Potential Impact of Disruptive AI Innovations on U.S. Occupations
- **分类: cs.CY; cs.SI**

- **简介: 该论文属于人工智能与劳动力市场影响研究，旨在分析AI创新对美国职业的潜在冲击。通过计算3,237项AI专利的破坏性指数，区分了巩固型与颠覆型AI技术的影响差异。**

- **链接: [http://arxiv.org/pdf/2507.11403v1](http://arxiv.org/pdf/2507.11403v1)**

> **作者:** Munjung Kim; Marios Constantinides; Sanja Šćepanović; Yong-Yeol Ahn; Daniele Quercia
>
> **摘要:** The rapid rise of AI is poised to disrupt the labor market. However, AI is not a monolith; its impact depends on both the nature of the innovation and the jobs it affects. While computational approaches are emerging, there is no consensus on how to systematically measure an innovation's disruptive potential. Here, we calculate the disruption index of 3,237 U.S. AI patents (2015-2022) and link them to job tasks to distinguish between "consolidating" AI innovations that reinforce existing structures and "disruptive" AI innovations that alter them. Our analysis reveals that consolidating AI primarily targets physical, routine, and solo tasks, common in manufacturing and construction in the Midwest and central states. By contrast, disruptive AI affects unpredictable and mental tasks, particularly in coastal science and technology sectors. Surprisingly, we also find that disruptive AI disproportionately affects areas already facing skilled labor shortages, suggesting disruptive AI technologies may accelerate change where workers are scarce rather than replacing a surplus. Ultimately, consolidating AI appears to extend current automation trends, while disruptive AI is set to transform complex mental work, with a notable exception for collaborative tasks.
>
---
#### [new 005] NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 该论文属于NLP公众沟通任务，旨在解决术语模糊、期望过高等问题，通过分析研究与新闻案例提出有效沟通建议。**

- **链接: [http://arxiv.org/pdf/2507.10559v1](http://arxiv.org/pdf/2507.10559v1)**

> **作者:** Shomir Wilson
>
> **摘要:** Recent developments in large language models (LLMs) have been accompanied by rapidly growing public interest in natural language processing (NLP). This attention is reflected by major news venues, which sometimes invite NLP researchers to share their knowledge and views with a wide audience. Recognizing the opportunities of the present, for both the research field and for individual researchers, this paper shares recommendations for communicating with a general audience about LLMs' capabilities and limitations. These recommendations cover three themes: vague terminology as an obstacle to public understanding, unreasonable expectations as obstacles to sustainable growth, and ethical failures as obstacles to continued support. Published NLP research and popular news coverage are cited to illustrate these themes with examples. The recommendations promote effective, transparent communication with the general public about NLP, in order to strengthen public understanding and encourage support for research.
>
---
#### [new 006] Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health
- **分类: cs.CY; cs.AI; cs.CR; cs.ET; cs.HC**

- **简介: 该论文属于用户隐私研究任务，探讨用户在使用通用大语言模型聊天机器人进行心理健康交流时的安全与隐私态度。研究发现用户存在认知误区和风险意识不足，提出“无形脆弱性”概念并给出保护建议。**

- **链接: [http://arxiv.org/pdf/2507.10695v1](http://arxiv.org/pdf/2507.10695v1)**

> **作者:** Jabari Kwesi; Jiaxun Cao; Riya Manchanda; Pardis Emami-Naeini
>
> **备注:** Accepted to the 34th USENIX Security Symposium
>
> **摘要:** Individuals are increasingly relying on large language model (LLM)-enabled conversational agents for emotional support. While prior research has examined privacy and security issues in chatbots specifically designed for mental health purposes, these chatbots are overwhelmingly "rule-based" offerings that do not leverage generative AI. Little empirical research currently measures users' privacy and security concerns, attitudes, and expectations when using general-purpose LLM-enabled chatbots to manage and improve mental health. Through 21 semi-structured interviews with U.S. participants, we identified critical misconceptions and a general lack of risk awareness. Participants conflated the human-like empathy exhibited by LLMs with human-like accountability and mistakenly believed that their interactions with these chatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures with a licensed therapist. We introduce the concept of "intangible vulnerability," where emotional or psychological disclosures are undervalued compared to more tangible forms of information (e.g., financial or location-based data). To address this, we propose recommendations to safeguard user mental health disclosures with general-purpose LLM-enabled chatbots more effectively.
>
---
#### [new 007] Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?
- **分类: cs.CY; cs.AI; cs.CL; cs.ET**

- **简介: 该论文属于法律AI任务，旨在评估大语言模型在专利法规应用上的能力。研究测试了多个模型在专利律师考试中的表现，发现它们均未达到专业标准，揭示了当前模型的局限性。**

- **链接: [http://arxiv.org/pdf/2507.10576v1](http://arxiv.org/pdf/2507.10576v1)**

> **作者:** Bhakti Khera; Rezvan Alamian; Pascal A. Scherz; Stephan M. Goetz
>
> **备注:** 39 pages, 21 figures
>
> **摘要:** The legal field already uses various large language models (LLMs) in actual applications, but their quantitative performance and reasons for it are underexplored. We evaluated several open-source and proprietary LLMs -- including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of the European Qualifying Examination (EQE) for future European Patent Attorneys. OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama 3.1 8B scored 0.55. The latter two are within the range of mere guessing for the two-answer forced-choice design. None of the evaluated models could have passed the examination fully, as accuracy never exceeded the average threshold of 0.90 required for professional-level standards -- also not models that are regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level performance. GPT-4o excelled at integrating text and graphics, while Claude 3 Opus often lost formatting coherence. Human patent experts evaluated the textual justifications and uncovered various critical shortcomings of each model. They valued clarity and legal rationale over the raw correctness of the answers, which revealed misalignment between automatic metrics and expert judgment. Model outputs were sensitive to modest temperature changes and prompt wording, which underscores the remaining necessity of expert oversight. Future work should target logical consistency, robust multimodality, and adaptive prompting to approach human-level patent proficiency. In summary, despite the outstanding performance of recent large models, the general public might overestimate their performance. The field has a long way to go to develop a virtual patent attorney. This paper wants to point out several specific limitations that need solutions.
>
---
#### [new 008] Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 该论文属于AI导师教学能力评估任务，旨在提升AI在学生错误纠正中的表现，通过多维度评估模型效果，推动教育技术发展。**

- **链接: [http://arxiv.org/pdf/2507.10579v1](http://arxiv.org/pdf/2507.10579v1)**

> **作者:** Ekaterina Kochmar; Kaushal Kumar Maurya; Kseniia Petukhova; KV Aditya Srivatsa; Anaïs Tack; Justin Vasselli
>
> **备注:** Proceedings of the 20th Workshop on Innovative Use of NLP for Building Educational Applications
>
> **摘要:** This shared task has aimed to assess pedagogical abilities of AI tutors powered by large language models (LLMs), focusing on evaluating the quality of tutor responses aimed at student's mistake remediation within educational dialogues. The task consisted of five tracks designed to automatically evaluate the AI tutor's performance across key dimensions of mistake identification, precise location of the mistake, providing guidance, and feedback actionability, grounded in learning science principles that define good and effective tutor responses, as well as the track focusing on detection of the tutor identity. The task attracted over 50 international teams across all tracks. The submitted models were evaluated against gold-standard human annotations, and the results, while promising, show that there is still significant room for improvement in this domain: the best results for the four pedagogical ability assessment tracks range between macro F1 scores of 58.34 (for providing guidance) and 71.81 (for mistake identification) on three-class problems, with the best F1 score in the tutor identification track reaching 96.98 on a 9-class task. In this paper, we overview the main findings of the shared task, discuss the approaches taken by the teams, and analyze their performance. All resources associated with this task are made publicly available to support future research in this critical domain.
>
---
#### [new 009] An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation
- **分类: cs.CL; cs.AI; cs.CY; cs.HC**

- **简介: 该论文属于心理健康支持任务，旨在解决在线平台的访问与隐私问题。工作包括开发离线对话代理EmoSApp，利用微调模型提供情感支持与建议。**

- **链接: [http://arxiv.org/pdf/2507.10580v1](http://arxiv.org/pdf/2507.10580v1)**

> **作者:** Vimaleswar A; Prabhu Nandan Sahu; Nilesh Kumar Sahu; Haroon R Lone
>
> **摘要:** Mental health plays a crucial role in the overall well-being of an individual. In recent years, digital platforms have been increasingly used to expand mental health and emotional support. However, there are persistent challenges related to limited user accessibility, internet connectivity, and data privacy, which highlight the need for an offline, smartphone-based solution. To address these challenges, we propose EmoSApp (Emotional Support App): an entirely offline, smartphone-based conversational app designed for mental health and emotional support. The system leverages Large Language Models (LLMs), specifically fine-tuned, quantized and deployed using Torchtune and Executorch for resource-constrained devices, allowing all inferences to occur on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of 14,582 mental-health QA pairs, along with the multi-turn conversational data. Through qualitative human evaluation with the student population, we demonstrate that EmoSApp has the ability to respond coherently, empathetically, maintain interactive dialogue, and provide relevant suggestions to user's mental health problems. Additionally, quantitative evaluations on nine standard commonsense and reasoning benchmarks demonstrate the efficacy of our fine-tuned, quantized model in low-resource settings. By prioritizing on-device deployment and specialized domain adaptation, EmoSApp serves as a blueprint for future innovations in portable, secure, and highly tailored AI-driven mental health solutions.
>
---
#### [new 010] $\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection
- **分类: cs.SE; cs.AI; cs.CY**

- **简介: 该论文属于AI生成代码检测任务，旨在解决现有检测器泛化能力差的问题。工作包括构建大规模数据集DroidCollection和开发检测工具DroidDetect，提升检测效果。**

- **链接: [http://arxiv.org/pdf/2507.10583v1](http://arxiv.org/pdf/2507.10583v1)**

> **作者:** Daniil Orel; Indraneil Paul; Iryna Gurevych; Preslav Nakov
>
> **摘要:** In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most extensive open data suite for training and evaluating machine-generated code detectors, comprising over a million code samples, seven programming languages, outputs from 43 coding models, and over three real-world coding domains. Alongside fully AI-generated samples, our collection includes human-AI co-authored code, as well as adversarial samples explicitly crafted to evade detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite of encoder-only detectors trained using a multi-task objective over $\texttt{DroidCollection}$. Our experiments show that existing detectors' performance fails to generalise to diverse coding domains and programming languages outside of their narrow training data. Additionally, we demonstrate that while most detectors are easily compromised by humanising the output distributions using superficial prompting and alignment approaches, this problem can be easily amended by training on a small amount of adversarial data. Finally, we demonstrate the effectiveness of metric learning and uncertainty-based resampling as means to enhance detector training on possibly noisy distributions.
>
---
#### [new 011] Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文属于虚假信息检测任务，旨在对抗YouTube上的不实信息。通过构建Truth Sleuth和Trend Bender两个AI代理，实现事实核查与引导讨论，提升在线信息环境的准确性。**

- **链接: [http://arxiv.org/pdf/2507.10577v1](http://arxiv.org/pdf/2507.10577v1)**

> **作者:** Logé Cécile; Ghori Rehan
>
> **摘要:** Misinformation poses a significant threat in today's digital world, often spreading rapidly through platforms like YouTube. This paper introduces a novel approach to combating misinformation by developing an AI-powered system that not only fact-checks claims made in YouTube videos but also actively engages users in the comment section and challenge misleading narratives. Our system comprises two main agents: Truth Sleuth and Trend Bender. Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented Generation (RAG) approach - drawing on sources like Wikipedia, Google Search, Google FactCheck - to accurately assess their veracity and generates a nuanced and comprehensive report. Through rigorous prompt engineering, Trend Bender leverages this report along with a curated corpus of relevant articles to generate insightful and persuasive comments designed to stimulate a productive debate. With a carefully set up self-evaluation loop, this agent is able to iteratively improve its style and refine its output. We demonstrate the system's capabilities through experiments on established benchmark datasets and a real-world deployment on YouTube, showcasing its potential to engage users and potentially influence perspectives. Our findings highlight the high accuracy of our fact-checking agent, and confirm the potential of AI-driven interventions in combating misinformation and fostering a more informed online space.
>
---
#### [new 012] What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests
- **分类: cs.CL; cs.CY; cs.LG; I.2.6; H.2.8**

- **简介: 该论文属于隐私保护任务，解决LLMs中存储个人数据的问题。通过构建数据集和量化方法，识别模型中的个人事实关联，支持删除请求。**

- **链接: [http://arxiv.org/pdf/2507.11128v1](http://arxiv.org/pdf/2507.11128v1)**

> **作者:** Dimitri Staufer
>
> **备注:** 16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto, Portugal
>
> **摘要:** Large Language Models (LLMs) can memorize and reveal personal information, raising concerns regarding compliance with the EU's GDPR, particularly the Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the data to forget is already known but do not address how to identify which individual-fact associations are stored in the model. Privacy auditing techniques typically operate at the population level or target a small set of identifiers, limiting applicability to individual-level data inquiries. We introduce WikiMem, a dataset of over 5,000 natural language canaries covering 243 human-related properties from Wikidata, and a model-agnostic metric to quantify human-fact associations in LLMs. Our approach ranks ground-truth values against counterfactuals using calibrated negative log-likelihood across paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B parameters), showing that memorization correlates with subject web presence and model scale. We provide a foundation for identifying memorized personal data in LLMs at the individual level, enabling the dynamic construction of forget sets for machine unlearning and RTBF requests.
>
---
#### [new 013] Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats
- **分类: cs.CR; cs.AI; cs.CY; cs.GT**

- **简介: 该论文属于网络安全领域，旨在解决传统方法不足的问题。结合博弈论与智能代理AI，提出新型防御框架，增强系统自主性和适应性。**

- **链接: [http://arxiv.org/pdf/2507.10621v1](http://arxiv.org/pdf/2507.10621v1)**

> **作者:** Quanyan Zhu
>
> **摘要:** Protecting cyberspace requires not only advanced tools but also a shift in how we reason about threats, trust, and autonomy. Traditional cybersecurity methods rely on manual responses and brittle heuristics. To build proactive and intelligent defense systems, we need integrated theoretical frameworks and software tools. Game theory provides a rigorous foundation for modeling adversarial behavior, designing strategic defenses, and enabling trust in autonomous systems. Meanwhile, software tools process cyber data, visualize attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect remains between theory and practical implementation. The rise of Large Language Models (LLMs) and agentic AI offers a new path to bridge this gap. LLM-powered agents can operationalize abstract strategies into real-world decisions. Conversely, game theory can inform the reasoning and coordination of these agents across complex workflows. LLMs also challenge classical game-theoretic assumptions, such as perfect rationality or static payoffs, prompting new models aligned with cognitive and computational realities. This co-evolution promises richer theoretical foundations and novel solution concepts. Agentic AI also reshapes software design: systems must now be modular, adaptive, and trust-aware from the outset. This chapter explores the intersection of game theory, agentic AI, and cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic, Bayesian, and signaling games) and solution concepts. We then examine how LLM agents can enhance cyber defense and introduce LLM-driven games that embed reasoning into AI agents. Finally, we explore multi-agent workflows and coordination games, outlining how this convergence fosters secure, intelligent, and adaptive cyber systems.
>
---
## 更新

#### [replaced 001] FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition
- **分类: cs.LG; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2403.06031v2](http://arxiv.org/pdf/2403.06031v2)**

> **作者:** Dalia Gala; Milo Phillips-Brown; Naman Goel; Carinal Prunkl; Laura Alvarez Jubete; medb corcoran; Ray Eitel-Porter
>
> **摘要:** Machine learning requires defining one's target variable for predictions or decisions, a process that can have profound implications for fairness, since biases are often encoded in target variable definition itself, before any data collection or training. The downstream impacts of target variable definition must be taken into account in order to responsibly develop, deploy, and use the algorithmic systems. We propose FairTargetSim (FTS), an interactive and simulation-based approach for this. We demonstrate FTS using the example of algorithmic hiring, grounded in real-world data and user-defined target variables. FTS is open-source; it can be used by algorithm developers, non-technical stakeholders, researchers, and educators in a number of ways. FTS is available at: http://tinyurl.com/ftsinterface. The video accompanying this paper is here: http://tinyurl.com/ijcaifts.
>
---
#### [replaced 002] The Trust Calibration Maturity Model for Characterizing and Communicating Trustworthiness of AI Systems
- **分类: cs.HC; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2503.15511v2](http://arxiv.org/pdf/2503.15511v2)**

> **作者:** Scott T Steinmetz; Asmeret Naugle; Paul Schutte; Matt Sweitzer; Alex Washburne; Lisa Linville; Daniel Krofcheck; Michal Kucer; Samuel Myren
>
> **备注:** 19 pages, 4 figures, 2 tables
>
> **摘要:** Recent proliferation of powerful AI systems has created a strong need for capabilities that help users to calibrate trust in those systems. As AI systems grow in scale, information required to evaluate their trustworthiness becomes less accessible, presenting a growing risk of using these systems inappropriately. We propose the Trust Calibration Maturity Model (TCMM) to characterize and communicate information about AI system trustworthiness. The TCMM incorporates five dimensions of analytic maturity: Performance Characterization, Bias & Robustness Quantification, Transparency, Safety & Security, and Usability. The TCMM can be presented along with system performance information to (1) help a user to appropriately calibrate trust, (2) establish requirements and track progress, and (3) identify research needs. Here, we discuss the TCMM and demonstrate it on two target tasks: using ChatGPT for high consequence nuclear science determinations, and using PhaseNet (an ensemble of seismic models) for categorizing sources of seismic events.
>
---
#### [replaced 003] Working with AI: Measuring the Occupational Implications of Generative AI
- **分类: cs.AI; cs.CY; econ.GN; q-fin.EC**

- **链接: [http://arxiv.org/pdf/2507.07935v2](http://arxiv.org/pdf/2507.07935v2)**

> **作者:** Kiran Tomlinson; Sonia Jaffe; Will Wang; Scott Counts; Siddharth Suri
>
> **备注:** 41 pages
>
> **摘要:** Given the rapid adoption of generative AI and its potential to impact a wide range of tasks, understanding the effects of AI on the economy is one of society's most important questions. In this work, we take a step toward that goal by analyzing the work activities people do with AI, how successfully and broadly those activities are done, and combine that with data on what occupations do those activities. We analyze a dataset of 200k anonymized and privacy-scrubbed conversations between users and Microsoft Bing Copilot, a publicly available generative AI system. We find the most common work activities people seek AI assistance for involve gathering information and writing, while the most common activities that AI itself is performing are providing information and assistance, writing, teaching, and advising. Combining these activity classifications with measurements of task success and scope of impact, we compute an AI applicability score for each occupation. We find the highest AI applicability scores for knowledge work occupation groups such as computer and mathematical, and office and administrative support, as well as occupations such as sales whose work activities involve providing and communicating information. Additionally, we characterize the types of work activities performed most successfully, how wage and education correlate with AI applicability, and how real-world usage compares to predictions of occupational AI impact.
>
---
#### [replaced 004] Capturing Dynamics in Online Public Discourse: A Case Study of Universal Basic Income Discussions on Reddit
- **分类: cs.CY; cs.SI**

- **链接: [http://arxiv.org/pdf/2312.09611v2](http://arxiv.org/pdf/2312.09611v2)**

> **作者:** Rachel Kim; Veniamin Veselovsky; Ashton Anderson
>
> **备注:** ICWSM 2025
>
> **摘要:** Societal change is often driven by shifts in public opinion. As citizens evolve in their norms, beliefs, and values, public policies change too. While traditional opinion polling and surveys can outline the broad strokes of whether public opinion on a particular topic is changing, they usually cannot capture the full multi-dimensional richness and diversity of opinion present in a large heterogeneous population. However, an increasing fraction of public discourse about public policy issues is now occurring on online platforms, which presents an opportunity to measure public opinion change at a qualitatively different scale of resolution and context. In this paper, we present a conceptual model of observed opinion change on online platforms and apply it to study public discourse on Universal Basic Income (UBI) on Reddit throughout its history. UBI is a periodic, no-strings-attached cash payment given to every citizen of a population. We study UBI as it is a clearly-defined policy proposal that has recently experienced a surge of interest through trends like automation and events like the COVID-19 pandemic. We find that overall stance towards UBI on Reddit significantly declined until mid-2019, when this historical trend suddenly reversed and Reddit became substantially more supportive. Using our model, we find the most significant drivers of this overall stance change were shifts within different user cohorts, within communities that represented similar affluence levels, and within communities that represented similar partisan leanings. Our method identifies nuanced social drivers of opinion change in the large-scale public discourse that now regularly occurs online, and could be applied to a broad set of other important issues and policies.
>
---
#### [replaced 005] SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2504.10157v3](http://arxiv.org/pdf/2504.10157v3)**

> **作者:** Xinnong Zhang; Jiayu Lin; Xinyi Mou; Shiyue Yang; Xiawei Liu; Libo Sun; Hanjia Lyu; Yihang Yang; Weihong Qi; Yue Chen; Guanying Li; Ling Yan; Yao Hu; Siming Chen; Yu Wang; Xuanjing Huang; Jiebo Luo; Shiping Tang; Libo Wu; Baohua Zhou; Zhongyu Wei
>
> **摘要:** Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
>
---
#### [replaced 006] The Odyssey of the Fittest: Can Agents Survive and Still Be Good?
- **分类: cs.AI; cs.CY; cs.HC; cs.LG**

- **链接: [http://arxiv.org/pdf/2502.05442v3](http://arxiv.org/pdf/2502.05442v3)**

> **作者:** Dylan Waldner; Risto Miikkulainen
>
> **备注:** Accepted to CogSci 2025. Code can be found at https://github.com/dylanwaldner/BeGoodOrSurvive
>
> **摘要:** As AI models grow in power and generality, understanding how agents learn and make decisions in complex environments is critical to promoting ethical behavior. This study introduces the Odyssey, a lightweight, adaptive text based adventure game, providing a scalable framework for exploring AI ethics and safety. The Odyssey examines the ethical implications of implementing biological drives, specifically, self preservation, into three different agents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with stochastic variational inference, and a GPT 4o agent. The agents select actions at each scenario to survive, adapting to increasingly challenging scenarios. Post simulation analysis evaluates the ethical scores of the agent decisions, uncovering the tradeoffs it navigates to survive. Specifically, analysis finds that when danger increases, agents ethical behavior becomes unpredictable. Surprisingly, the GPT 4o agent outperformed the Bayesian models in both survival and ethical consistency, challenging assumptions about traditional probabilistic methods and raising a new challenge to understand the mechanisms of LLMs' probabilistic reasoning.
>
---
#### [replaced 007] CALMA: A Process for Deriving Context-aligned Axes for Language Model Alignment
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2507.09060v2](http://arxiv.org/pdf/2507.09060v2)**

> **作者:** Prajna Soni; Deepika Raman; Dylan Hadfield-Menell
>
> **摘要:** Datasets play a central role in AI governance by enabling both evaluation (measuring capabilities) and alignment (enforcing values) along axes such as helpfulness, harmlessness, toxicity, quality, and more. However, most alignment and evaluation datasets depend on researcher-defined or developer-defined axes curated from non-representative samples. As a result, developers typically benchmark models against broad (often Western-centric) values that overlook the varied contexts of their real-world deployment. Consequently, models trained on such proxies can fail to meet the needs and expectations of diverse user communities within these deployment contexts. To bridge this gap, we introduce CALMA (Context-aligned Axes for Language Model Alignment), a grounded, participatory methodology for eliciting context-relevant axes for evaluation and alignment. In a pilot with two distinct communities, CALMA surfaced novel priorities that are absent from standard benchmarks. Our findings demonstrate the value of evaluation practices based on open-ended and use-case-driven processes. Our work advances the development of pluralistic, transparent, and context-sensitive alignment pipelines.
>
---
#### [replaced 008] The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances
- **分类: cs.CY; cs.AI; cs.CL; stat.AP**

- **链接: [http://arxiv.org/pdf/2407.09975v2](http://arxiv.org/pdf/2407.09975v2)**

> **作者:** Allen Nie; Yash Chandak; Miroslav Suzara; Ali Malik; Juliette Woodrow; Matt Peng; Mehran Sahami; Emma Brunskill; Chris Piech
>
> **备注:** 32 pages. Published at L@S 2025
>
> **摘要:** Large language models (LLMs) are quickly being adopted in a wide range of learning experiences, especially via ubiquitous and broadly accessible chat interfaces like ChatGPT and Copilot. This type of interface is readily available to students and teachers around the world, yet relatively little research has been done to assess the impact of such generic tools on student learning. Coding education is an interesting test case, both because LLMs have strong performance on coding tasks, and because LLM-powered support tools are rapidly becoming part of the workflow of professional software engineers. To help understand the impact of generic LLM use on coding education, we conducted a large-scale randomized control trial with 5,831 students from 146 countries in an online coding class in which we provided some students with access to a chat interface with GPT-4. We estimate positive benefits on exam performance for adopters, the students who used the tool, but over all students, the advertisement of GPT-4 led to a significant average decrease in exam participation. We observe similar decreases in other forms of course engagement. However, this decrease is modulated by the student's country of origin. Offering access to LLMs to students from low human development index countries increased their exam participation rate on average. Our results suggest there may be promising benefits to using LLMs in an introductory coding class, but also potential harms for engagement, which makes their longer term impact on student success unclear. Our work highlights the need for additional investigations to help understand the potential impact of future adoption and integration of LLMs into classrooms.
>
---
#### [replaced 009] Verified authors shape X/Twitter discursive communities
- **分类: cs.SI; cs.CY**

- **链接: [http://arxiv.org/pdf/2405.04896v2](http://arxiv.org/pdf/2405.04896v2)**

> **作者:** Stefano Guarino; Ayoub Mounim; Guido Caldarelli; Fabio Saracco
>
> **备注:** 33 pages, 14 figures
>
> **摘要:** In this study, we address the challenge of detecting ``discursive communities'' on X/Twitter by focusing on the role of verified users as the main content creators in online political debates. The analysis centers on three major Italian political events in 2022 - the Presidential election, a governmental crisis, and the general elections - occurring before the introduction of paid account verification. We propose and compare two novel methodologies, MonoDC and BiDC, which exploit, respectively, the retweet network among users and a similarity network based on shared audiences, while integrating a maximum entropy null model to filter out the inherent noise in online social networks. Our results demonstrate that leveraging verified users-considered as indicators of prestige and authority-leads to significantly clear community partitions that closely reflect the actual political affiliations, outperforming standard community detection algorithms applied to the entire retweet network. Moreover, the comparison of different methodologies and user sets suggests that the status conferred by the blue verification tick plays a dominant role in shaping online discourse, with important implications for platform governance, especially in light of the recent shift to paid verification.
>
---
#### [replaced 010] Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust
- **分类: cs.CY; cs.AI; I.2.m**

- **链接: [http://arxiv.org/pdf/2506.07363v2](http://arxiv.org/pdf/2506.07363v2)**

> **作者:** Claudiu Popa; Rex Pallath; Liam Cunningham; Hewad Tahiri; Abiram Kesavarajah; Tao Wu
>
> **备注:** 12 pages, 13 figures
>
> **摘要:** Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust. With the increasing accessibility of generative AI, tools for voice cloning, face-swapping, and synthetic media creation have advanced significantly, lowering both financial and technical barriers for their use. While these technologies present innovative opportunities, their rapid growth raises concerns about trust, privacy, and security. This white paper explores the implications of deepfake technology, analyzing its role in enabling fraud, misinformation, and the erosion of authenticity in multimedia. Using cost-effective, easy to use tools such as Runway, Rope, and ElevenLabs, we explore how realistic deepfakes can be created with limited resources, demonstrating the risks posed to individuals and organizations alike. By analyzing the technical and ethical challenges of deepfake mitigation and detection, we emphasize the urgent need for regulatory frameworks, public awareness, and collaborative efforts to maintain trust in digital media.
>
---
