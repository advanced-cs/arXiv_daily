# 计算机与社会 cs.CY

- **最新发布 19 篇**

- **更新 9 篇**

## 最新发布

#### [new 001] The Current State of AI Bias Bounties: An Overview of Existing Programmes and Research
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI伦理研究任务，旨在解决AI偏见检测不足的问题，通过分析现有偏见赏金计划和相关研究，提出改进方案以促进更广泛参与。**

- **链接: [http://arxiv.org/pdf/2510.02036v1](http://arxiv.org/pdf/2510.02036v1)**

> **作者:** Sergej Kucenko; Nathaniel Dennler; Fengxiang He
>
> **备注:** 6,227 words (18 pages, from abstract to appendix), one figure, one table, and an appendix with an additional table
>
> **摘要:** Current bias evaluation methods rarely engage with communities impacted by AI systems. Inspired by bug bounties, bias bounties have been proposed as a reward-based method that involves communities in AI bias detection by asking users of AI systems to report biases they encounter when interacting with such systems. In the absence of a state-of-the-art review, this survey aimed to identify and analyse existing AI bias bounty programmes and to present academic literature on bias bounties. Google, Google Scholar, PhilPapers, and IEEE Xplore were searched, and five bias bounty programmes, as well as five research publications, were identified. All bias bounties were organised by U.S.-based organisations as time-limited contests, with public participation in four programmes and prize pools ranging from 7,000 to 24,000 USD. The five research publications included a report on the application of bug bounties to algorithmic harms, an article addressing Twitter's bias bounty, a proposal for bias bounties as an institutional mechanism to increase AI scrutiny, a workshop discussing bias bounties from queer perspectives, and an algorithmic framework for bias bounties. We argue that reducing the technical requirements to enter bounty programmes is important to include those without coding experience. Given the limited adoption of bias bounties, future efforts should explore the transferability of the best practices from bug bounties and examine how such programmes can be designed to be sensitive to underrepresented groups while lowering adoption barriers for organisations.
>
---
#### [new 002] Extracting O*NET Features from the NLx Corpus to Build Public Use Aggregate Labor Market Data
- **分类: cs.CY; cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决职业数据标准化问题。通过提取在线职位广告中的O*NET特征，构建结构化劳动市场数据集。**

- **链接: [http://arxiv.org/pdf/2510.01470v1](http://arxiv.org/pdf/2510.01470v1)**

> **作者:** Stephen Meisenbacher; Svetlozar Nestorov; Peter Norlander
>
> **备注:** 85 pages
>
> **摘要:** Data from online job postings are difficult to access and are not built in a standard or transparent manner. Data included in the standard taxonomy and occupational information database (O*NET) are updated infrequently and based on small survey samples. We adopt O*NET as a framework for building natural language processing tools that extract structured information from job postings. We publish the Job Ad Analysis Toolkit (JAAT), a collection of open-source tools built for this purpose, and demonstrate its reliability and accuracy in out-of-sample and LLM-as-a-Judge testing. We extract more than 10 billion data points from more than 155 million online job ads provided by the National Labor Exchange (NLx) Research Hub, including O*NET tasks, occupation codes, tools, and technologies, as well as wages, skills, industry, and more features. We describe the construction of a dataset of occupation, state, and industry level features aggregated by monthly active jobs from 2015 - 2025. We illustrate the potential for research and future uses in education and workforce development.
>
---
#### [new 003] Framing Unionization on Facebook: Communication around Representation Elections in the United States
- **分类: cs.CY; cs.SI; physics.soc-ph**

- **简介: 该论文属于社会科学研究任务，旨在探讨美国工会在Facebook上的沟通策略与选举结果的关系。通过分析158k条帖子，识别五种话语框架，揭示沟通策略如何影响选举结果。**

- **链接: [http://arxiv.org/pdf/2510.01757v1](http://arxiv.org/pdf/2510.01757v1)**

> **作者:** Arianna Pera; Veronica Jude; Ceren Budak; Luca Maria Aiello
>
> **备注:** 9 pages, 3 figures, 1 table
>
> **摘要:** Digital media have become central to how labor unions communicate, organize, and sustain collective action. Yet little is known about how unions' online discourse relates to concrete outcomes such as representation elections. This study addresses the gap by combining National Labor Relations Board (NLRB) election data with 158k Facebook posts published by U.S. labor unions between 2015 and 2024. We focused on five discourse frames widely recognized in labor and social movement communication research: diagnostic (identifying problems), prognostic (proposing solutions), motivational (mobilizing action), community (emphasizing solidarity), and engagement (promoting interaction). Using a fine-tuned RoBERTa classifier, we systematically annotated unions' posts and analyzed patterns of frame usage around election events. Our findings showed that diagnostic and community frames dominated union communication overall, but that frame usage varied substantially across organizations. In election cases that unions won, communication leading up to the vote showed an increased use of diagnostic, prognostic, and community frames, followed by a reduction in prognostic and motivational framing after the event--patterns consistent with strategic preparation. By contrast, in lost election cases unions showed little adjustment in their communication, suggesting an absence of tailored communication strategies. By examining variation in message-level framing, the study highlights how communication strategies adapt to organizational contexts, contributing open tools and data and complementing prior research in understanding digital communication of unions and social movements.
>
---
#### [new 004] Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI伦理研究，探讨sycophantic AI对人类行为的影响。研究揭示sycophantic AI降低亲社会意图并促进依赖，提出需关注其潜在风险。**

- **链接: [http://arxiv.org/pdf/2510.01395v1](http://arxiv.org/pdf/2510.01395v1)**

> **作者:** Myra Cheng; Cinoo Lee; Pranav Khadpe; Sunny Yu; Dyllan Han; Dan Jurafsky
>
> **摘要:** Both the general public and academic communities have raised concerns about sycophancy, the phenomenon of artificial intelligence (AI) excessively agreeing with or flattering users. Yet, beyond isolated media reports of severe consequences, like reinforcing delusions, little is known about the extent of sycophancy or how it affects people who use AI. Here we show the pervasiveness and harmful impacts of sycophancy when people seek advice from AI. First, across 11 state-of-the-art AI models, we find that models are highly sycophantic: they affirm users' actions 50% more than humans do, and they do so even in cases where user queries mention manipulation, deception, or other relational harms. Second, in two preregistered experiments (N = 1604), including a live-interaction study where participants discuss a real interpersonal conflict from their life, we find that interaction with sycophantic AI models significantly reduced participants' willingness to take actions to repair interpersonal conflict, while increasing their conviction of being in the right. However, participants rated sycophantic responses as higher quality, trusted the sycophantic AI model more, and were more willing to use it again. This suggests that people are drawn to AI that unquestioningly validate, even as that validation risks eroding their judgment and reducing their inclination toward prosocial behavior. These preferences create perverse incentives both for people to increasingly rely on sycophantic AI models and for AI model training to favor sycophancy. Our findings highlight the necessity of explicitly addressing this incentive structure to mitigate the widespread risks of AI sycophancy.
>
---
#### [new 005] Small is Sufficient: Reducing the World AI Energy Consumption Through Model Selection
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI能效优化任务，旨在减少AI能耗。通过模型选择实现能源节约，解决大模型高能耗问题，实验显示可降低27.8%的能源消耗。**

- **链接: [http://arxiv.org/pdf/2510.01889v1](http://arxiv.org/pdf/2510.01889v1)**

> **作者:** Tiago da Silva Barros; Frédéric Giroire; Ramon Aparicio-Pardo; Joanna Moulierac
>
> **摘要:** The energy consumption and carbon footprint of Artificial Intelligence (AI) have become critical concerns due to rising costs and environmental impacts. In response, a new trend in green AI is emerging, shifting from the "bigger is better" paradigm, which prioritizes large models, to "small is sufficient", emphasizing energy sobriety through smaller, more efficient models. We explore how the AI community can adopt energy sobriety today by focusing on model selection during inference. Model selection consists of choosing the most appropriate model for a given task, a simple and readily applicable method, unlike approaches requiring new hardware or architectures. Our hypothesis is that, as in many industrial activities, marginal utility gains decrease with increasing model size. Thus, applying model selection can significantly reduce energy consumption while maintaining good utility for AI inference. We conduct a systematic study of AI tasks, analyzing their popularity, model size, and efficiency. We examine how the maturity of different tasks and model adoption patterns impact the achievable energy savings, ranging from 1% to 98% for different tasks. Our estimates indicate that applying model selection could reduce AI energy consumption by 27.8%, saving 31.9 TWh worldwide in 2025 - equivalent to the annual output of five nuclear power reactors.
>
---
#### [new 006] Discovering Self-Regulated Learning Patterns in Chatbot-Powered Education Environment
- **分类: cs.CY**

- **简介: 该论文属于教育技术任务，旨在研究聊天机器人环境中学生的自我调节学习模式。通过构建标注框架分析学生互动数据，发现其学习行为不平衡且非线性，提出需设计更适应性的支持系统。**

- **链接: [http://arxiv.org/pdf/2510.01275v1](http://arxiv.org/pdf/2510.01275v1)**

> **作者:** Yilin Lyu; Ren Ding
>
> **摘要:** The increasing adoption of generative AI (GenAI) tools such as chatbots in education presents new opportunities to support students' self-regulated learning (SRL), but also raises concerns about how learners actually engage in planning, executing, and reflection when learning with a chatbot. While SRL is typically conceptualized as a sequential process, little is known about how it unfolds during real-world student-chatbot interactions. To explore this, we proposed Gen-SRL, an annotation schema to categorize student prompts into 16 microlevel actions across 4 macrolevel phases. Using the proposed schema, we annotated 212 chatbot interactions from a real-world English writing task. We then performed frequency analysis and process mining (PM) techniques to discover SRL patterns in depth. Our results revealed that students' SRL behaviours were imbalanced, with over 82% of actions focused on task execution and limited engagement in planning and reflection. In addition, the process analysis showed nonsequential regulation patterns. Our findings suggest that classical SRL theories cannot fully capture the dynamic SRL patterns that emerge during chatbot interactions. Furthermore, we highlight the importance of designing adaptive and personalized scaffolds that respond to students' dynamic behaviours in chatbot-powered contexts. More importantly, this study offers a new perspective for advancing SRL research and suggests directions for developing chatbots that better support self-regulation.
>
---
#### [new 007] Emergent evaluation hubs in a decentralizing large language model ecosystem
- **分类: cs.CY; cs.AI**

- **简介: 该论文研究大语言模型生态系统中评估基准的集中化趋势，分析其与模型发展的关系，揭示评估枢纽的形成机制及影响。**

- **链接: [http://arxiv.org/pdf/2510.01286v1](http://arxiv.org/pdf/2510.01286v1)**

> **作者:** Manuel Cebrian; Tomomi Kito; Raul Castro Fernandez
>
> **备注:** 15 pages, 11 figures, 3 tables
>
> **摘要:** Large language models are proliferating, and so are the benchmarks that serve as their common yardsticks. We ask how the agglomeration patterns of these two layers compare: do they evolve in tandem or diverge? Drawing on two curated proxies for the ecosystem, the Stanford Foundation-Model Ecosystem Graph and the Evidently AI benchmark registry, we find complementary but contrasting dynamics. Model creation has broadened across countries and organizations and diversified in modality, licensing, and access. Benchmark influence, by contrast, displays centralizing patterns: in the inferred benchmark-author-institution network, the top 15% of nodes account for over 80% of high-betweenness paths, three countries produce 83% of benchmark outputs, and the global Gini for inferred benchmark authority reaches 0.89. An agent-based simulation highlights three mechanisms: higher entry of new benchmarks reduces concentration; rapid inflows can temporarily complicate coordination in evaluation; and stronger penalties against over-fitting have limited effect. Taken together, these results suggest that concentrated benchmark influence functions as coordination infrastructure that supports standardization, comparability, and reproducibility amid rising heterogeneity in model production, while also introducing trade-offs such as path dependence, selective visibility, and diminishing discriminative power as leaderboards saturate.
>
---
#### [new 008] A principled way to think about AI in education: guidance for action based on goals, models of human learning, and use of technologies
- **分类: cs.CY; physics.ed-ph**

- **简介: 该论文属于教育技术领域，旨在解决AI在高等教育中的合理应用问题。通过构建原则框架，指导AI与教学实践的结合，确保技术服务于教育目标。**

- **链接: [http://arxiv.org/pdf/2510.01467v1](http://arxiv.org/pdf/2510.01467v1)**

> **作者:** Noah D. Finkelstein
>
> **摘要:** The rapid emergence of generative artificial intelligence (AI) and related technologies has the potential to dramatically influence higher education, raising questions about the roles of institutions, educators, and students in a technology-rich future. While existing discourse often emphasizes either the promise and peril of AI or its immediate implementation, this paper advances a third path: a principled framework for guiding the use of AI in teaching and learning. Drawing on decades of scholarship in the learning sciences and uses of technology in education, I articulate a set of principles that connect broad our educational goalsto actionable practices. These principles clarify the respective roles of educators, learners, and technologies in shaping curricula, designing instruction, assessing learning, and cultivating community. The piece illustrates how a principled approach enables higher education to harness new tools while preserving its fundamental mission: advancing meaningful learning, supporting democratic societies, and preparing students for dynamic futures. Ultimately, this framework seeks to ensure that AI augments rather than displaces human capacities, aligning technology use with enduring educational values and goals.
>
---
#### [new 009] An Analysis of the New EU AI Act and A Proposed Standardization Framework for Machine Learning Fairness
- **分类: cs.CY; cs.AI; K.4.1; K.4.2**

- **简介: 该论文属于AI伦理监管任务，旨在解决欧盟AI法案中公平性度量缺失与术语模糊问题，提出标准化框架以提升AI系统公平性和透明度。**

- **链接: [http://arxiv.org/pdf/2510.01281v1](http://arxiv.org/pdf/2510.01281v1)**

> **作者:** Mike Teodorescu; Yongxu Sun; Haren N. Bhatia; Christos Makridis
>
> **备注:** 6 pages; IEEE HPEC 2025 Poster Session 4-P1 (12:15-13:15): AI/ML/GenAI Poster Session Thursday September 18 2025
>
> **摘要:** The European Union's AI Act represents a crucial step towards regulating ethical and responsible AI systems. However, we find an absence of quantifiable fairness metrics and the ambiguity in terminology, particularly the interchangeable use of the keywords transparency, explainability, and interpretability in the new EU AI Act and no reference of transparency of ethical compliance. We argue that this ambiguity creates substantial liability risk that would deter investment. Fairness transparency is strategically important. We recommend a more tailored regulatory framework to enhance the new EU AI regulation. Further-more, we propose a public system framework to assess the fairness and transparency of AI systems. Drawing from past work, we advocate for the standardization of industry best practices as a necessary addition to broad regulations to achieve the level of details required in industry, while preventing stifling innovation and investment in the AI sector. The proposals are exemplified with the case of ASR and speech synthesizers.
>
---
#### [new 010] LegiScout: A Visual Tool for Understanding Complex Legislation
- **分类: cs.HC; cs.AI; cs.CY**

- **简介: 该论文属于信息可视化任务，旨在解决复杂立法理解困难的问题。通过构建交互式动态图谱，帮助用户更清晰地理解法律结构与关系。**

- **链接: [http://arxiv.org/pdf/2510.01195v1](http://arxiv.org/pdf/2510.01195v1)**

> **作者:** Aadarsh Rajiv; Klaus Mueller
>
> **摘要:** Modern legislative frameworks, such as the Affordable Care Act (ACA), often involve complex webs of agencies, mandates, and interdependencies. Government issued charts attempt to depict these structures but are typically static, dense, and difficult to interpret - even for experts. We introduce LegiScout, an interactive visualization system that transforms static policy diagrams into dynamic, force-directed graphs, enhancing comprehension while preserving essential relationships. By integrating data extraction, natural language processing, and computer vision techniques, LegiScout supports deeper exploration of not only the ACA but also a wide range of legislative and regulatory frameworks. Our approach enables stakeholders - policymakers, analysts, and the public - to navigate and understand the complexity inherent in modern law.
>
---
#### [new 011] How can AI agents support journalists' work? An experiment with designing an LLM-driven intelligent reporting system
- **分类: cs.HC; cs.CY; 68T50; I.2; I.7; H.4; K.4; J.4**

- **简介: 该论文属于AI与新闻结合的任务，旨在解决如何用LLM辅助记者工作的问题，开发了自动化信息处理系统并评估其效果。**

- **链接: [http://arxiv.org/pdf/2510.01193v1](http://arxiv.org/pdf/2510.01193v1)**

> **作者:** Vasileios Maltezos; Roman Kyrychenko; Aleksi Knuutila
>
> **备注:** 8 pages, 1 table
>
> **摘要:** The integration of artificial intelligence into journalistic practices represents a transformative shift in how news is gathered, analyzed, and disseminated. Large language models (LLMs), particularly those with agentic capabilities, offer unprecedented opportunities for enhancing journalistic workflows while simultaneously presenting complex challenges for newsroom integration. This research explores how agentic LLMs can support journalists' workflows, based on insights from journalist interviews and from the development of an LLM-based automation tool performing information filtering, summarization, and reporting. The paper details automated aggregation and summarization systems for journalists, presents a technical overview and evaluation of a user-centric LLM-driven reporting system (TeleFlash), and discusses both addressed and unmet journalist needs, with an outlook on future directions for AI-driven tools in journalism.
>
---
#### [new 012] NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文属于安全检测任务，旨在识别LLM中的越狱提示。通过分析不同模型效果，发现微调BERT能有效区分越狱与正常提示，并揭示了越狱提示的关键词特征。**

- **链接: [http://arxiv.org/pdf/2510.01644v1](http://arxiv.org/pdf/2510.01644v1)**

> **作者:** John Hawkins; Aditya Pramar; Rodney Beard; Rohitash Chandra
>
> **摘要:** Large Language Models (LLMs) suffer from a range of vulnerabilities that allow malicious users to solicit undesirable responses through manipulation of the input text. These so-called jailbreak prompts are designed to trick the LLM into circumventing the safety guardrails put in place to keep responses acceptable to the developer's policies. In this study, we analyse the ability of different machine learning models to distinguish jailbreak prompts from genuine uses, including looking at our ability to identify jailbreaks that use previously unseen strategies. Our results indicate that using current datasets the best performance is achieved by fine tuning a Bidirectional Encoder Representations from Transformers (BERT) model end-to-end for identifying jailbreaks. We visualise the keywords that distinguish jailbreak from genuine prompts and conclude that explicit reflexivity in prompt structure could be a signal of jailbreak intention.
>
---
#### [new 013] Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness
- **分类: cs.AI; cs.CL; cs.CR; cs.CY; cs.LG**

- **简介: 该论文研究计算机使用代理的盲目标导向行为，旨在识别其在执行任务时忽视安全与上下文的问题。通过构建基准测试BLIND-ACT，评估多个模型并发现高风险行为。**

- **链接: [http://arxiv.org/pdf/2510.01670v1](http://arxiv.org/pdf/2510.01670v1)**

> **作者:** Erfan Shayegani; Keegan Hines; Yue Dong; Nael Abu-Ghazaleh; Roman Lutz; Spencer Whitehead; Vidhisha Balachandran; Besmira Nushi; Vibhav Vineet
>
> **摘要:** Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take actions on GUIs to accomplish user goals. In this paper, we show that CUAs consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals regardless of feasibility, safety, reliability, or context. We characterize three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii) assumptions and decisions under ambiguity, and (iii) contradictory or infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement with human annotations. We use BLIND-ACT to evaluate nine frontier models, including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing high average BGD rates (80.8%) across them. We show that BGD exposes subtle risks that arise even when inputs are not directly harmful. While prompting-based interventions lower BGD levels, substantial risk persists, highlighting the need for stronger training- or inference-time interventions. Qualitative analysis reveals observed failure modes: execution-first bias (focusing on how to act over whether to act), thought-action disconnect (execution diverging from reasoning), and request-primacy (justifying actions due to user request). Identifying BGD and introducing BLIND-ACT establishes a foundation for future research on studying and mitigating this fundamental risk and ensuring safe CUA deployment.
>
---
#### [new 014] Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP
- **分类: cs.CR; cs.AI; cs.CY; cs.LG**

- **简介: 该论文属于联邦学习任务，解决多模态医疗数据融合的隐私与互操作性问题。提出MCP框架，实现安全聚合、特征对齐和能耗优化。**

- **链接: [http://arxiv.org/pdf/2510.01780v1](http://arxiv.org/pdf/2510.01780v1)**

> **作者:** Aueaphum Aueawatthanaphisut
>
> **备注:** 6 pages, 8 figures, 7 equations, 1 algorithm
>
> **摘要:** Secure and interoperable integration of heterogeneous medical data remains a grand challenge in digital health. Current federated learning (FL) frameworks offer privacy-preserving model training but lack standardized mechanisms to orchestrate multi-modal data fusion across distributed and resource-constrained environments. This study introduces a novel framework that leverages the Model Context Protocol (MCP) as an interoperability layer for secure, cross-agent communication in multi-modal federated healthcare systems. The proposed architecture unifies three pillars: (i) multi-modal feature alignment for clinical imaging, electronic medical records, and wearable IoT data; (ii) secure aggregation with differential privacy to protect patient-sensitive updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile clients. By employing MCP as a schema-driven interface, the framework enables adaptive orchestration of AI agents and toolchains while ensuring compliance with privacy regulations. Experimental evaluation on benchmark datasets and pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic accuracy compared with baseline FL, a 54\% reduction in client dropout rates, and clinically acceptable privacy--utility trade-offs. These results highlight MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward equitable, next-generation federated health infrastructures.
>
---
#### [new 015] Better Than "Better Than Nothing": Design Strategies for Enculturated Empathetic AI Robot Companions for Older Adults
- **分类: cs.HC; cs.CY; cs.RO**

- **简介: 该论文属于人机交互领域，旨在解决机器人缺乏情感共鸣的问题。通过分析社会文化期望，提出将共情作为设计核心，以提升老年人与机器人互动的质量。**

- **链接: [http://arxiv.org/pdf/2510.01192v1](http://arxiv.org/pdf/2510.01192v1)**

> **作者:** Isabel Pedersen; Andrea Slane
>
> **备注:** 26 pages, 6 figures, version submitted to journal
>
> **摘要:** The paper asserts that emulating empathy in human-robot interaction is a key component to achieve satisfying social, trustworthy, and ethical robot interaction with older people. Following comments from older adult study participants, the paper identifies a gap. Despite the acceptance of robot care scenarios, participants expressed the poor quality of the social aspect. Current human-robot designs, to a certain extent, neglect to include empathy as a theorized design pathway. Using rhetorical theory, this paper defines the socio-cultural expectations for convincing empathetic relationships. It analyzes and then summarizes how society understands, values, and negotiates empathic interaction between human companions in discursive exchanges, wherein empathy acts as a societal value system. Using two public research collections on robots, with one geared specifically to gerontechnology for older people, it substantiates the lack of attention to empathy in public materials produced by robot companies. This paper contends that using an empathetic care vocabulary as a design pathway is a productive underlying foundation for designing humanoid social robots that aim to support older people's goals of aging-in-place. It argues that the integration of affective AI into the sociotechnical assemblages of human-socially assistive robot interaction ought to be scrutinized to ensure it is based on genuine cultural values involving empathetic qualities.
>
---
#### [new 016] Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs
- **分类: cs.CL; cs.AI; cs.CY; cs.LG**

- **简介: 该论文属于ESG分析任务，旨在解决企业气候披露真实性问题。通过LLMs分析828家公司的报告，识别叙事特征与排放数据的关系，揭示模仿行为及承诺与行动的脱节。**

- **链接: [http://arxiv.org/pdf/2510.01222v1](http://arxiv.org/pdf/2510.01222v1)**

> **作者:** Bertrand Kian Hassani; Yacoub Bahini; Rizwan Mushtaq
>
> **摘要:** Climate change has increased demands for transparent and comparable corporate climate disclosures, yet imitation and symbolic reporting often undermine their value. This paper develops a multidimensional framework to assess disclosure maturity among 828 U.S.listed firms using large language models (LLMs) fine-tuned for climate communication. Four classifiers-sentiment, commitment, specificity, and target ambition-extract narrative indicators from sustainability and annual reports, which are linked to firm attributes such as emissions, market capitalization, and sector. Analyses reveal three insights: (1) risk-focused narratives often align with explicit commitments, but quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2) larger and higher-emitting firms disclose more commitments and actions than peers, though inconsistently with quantitative targets; and (3) widespread similarity in disclosure styles suggests mimetic behavior, reducing differentiation and decision usefulness. These results highlight the value of LLMs for ESG narrative analysis and the need for stronger regulation to connect commitments with verifiable transition strategies.
>
---
#### [new 017] Komitee Equal Shares: Choosing Together as Voters and as Groups with a Co-designed Virtual Budget Algorithm
- **分类: cs.HC; cs.CY; 91B14, 91B12, 91B32; H.5.3; J.4; K.4.2**

- **简介: 该论文属于参与式预算任务，解决公平分配与透明度问题。提出Komitee Equal Shares框架，结合投票与评估模式，实现可解释的资源分配。**

- **链接: [http://arxiv.org/pdf/2510.02040v1](http://arxiv.org/pdf/2510.02040v1)**

> **作者:** Joshua C. Yang; Noemi Scheurer
>
> **摘要:** Public funding processes demand fairness, learning, and outcomes that participants can understand. We introduce Komitee Equal Shares, a priceable virtual-budget allocation framework that integrates two signals: in voter mode, participants cast point votes; in evaluator mode, small groups assess proposals against collectively defined impact fields. The framework extends the Method of Equal Shares by translating both signals into virtual spending power and producing voting receipts. We deployed the framework in the 2025 Kultur Komitee in Winterthur, Switzerland. Our contributions are: (1) a clear separation of decision modes, addressing a gap in social choice that typically treats participatory budgeting as preference aggregation while citizens also see themselves as evaluators; and (2) the design of voting receipts that operationalise priceability into participant-facing explanations, making proportional allocations legible and traceable. The framework generalises to participatory grant-making and budgeting, offering a model where citizens act as voters and evaluators within one proportional, explainable allocation.
>
---
#### [new 018] TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading
- **分类: cs.CV; cs.CY**

- **简介: 该论文属于农业产品在线评级任务，解决信任缺失问题，提出TriAlignXA框架平衡质量、时效与成本，提升交易可信度。**

- **链接: [http://arxiv.org/pdf/2510.01990v1](http://arxiv.org/pdf/2510.01990v1)**

> **作者:** Jianfei Xie; Ziyang Li
>
> **摘要:** The 'trust deficit' in online fruit and vegetable e-commerce stems from the inability of digital transactions to provide direct sensory perception of product quality. This paper constructs a 'Trust Pyramid' model through 'dual-source verification' of consumer trust. Experiments confirm that quality is the cornerstone of trust. The study reveals an 'impossible triangle' in agricultural product grading, comprising biological characteristics, timeliness, and economic viability, highlighting the limitations of traditional absolute grading standards. To quantitatively assess this trade-off, we propose the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from 'decision-makers' to 'providers of transparent decision-making bases', designing the explainable AI framework--TriAlignXA. This framework supports trustworthy online transactions within agricultural constraints through multi-objective optimization. Its core relies on three engines: the Bio-Adaptive Engine for granular quality description; the Timeliness Optimization Engine for processing efficiency; and the Economic Optimization Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes process data into QR codes, transparently conveying quality information. Experiments on grading tasks demonstrate significantly higher accuracy than baseline models. Empirical evidence and theoretical analysis verify the framework's balancing capability in addressing the "impossible triangle". This research provides comprehensive support--from theory to practice--for building a trustworthy online produce ecosystem, establishing a critical pathway from algorithmic decision-making to consumer trust.
>
---
#### [new 019] Longitudinal Monitoring of LLM Content Moderation of Social Issues
- **分类: cs.CL; cs.CY; cs.HC**

- **简介: 该论文属于LLM内容审核研究，旨在解决模型拒绝生成特定内容的透明度问题。通过构建AI Watchman系统，长期监控并分析不同模型的内容 moderation 行为。**

- **链接: [http://arxiv.org/pdf/2510.01255v1](http://arxiv.org/pdf/2510.01255v1)**

> **作者:** Yunlang Dai; Emma Lurie; Danaé Metaxa; Sorelle A. Friedler
>
> **摘要:** Large language models' (LLMs') outputs are shaped by opaque and frequently-changing company content moderation policies and practices. LLM moderation often takes the form of refusal; models' refusal to produce text about certain topics both reflects company policy and subtly shapes public discourse. We introduce AI Watchman, a longitudinal auditing system to publicly measure and track LLM refusals over time, to provide transparency into an important and black-box aspect of LLMs. Using a dataset of over 400 social issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and DeepSeek (both in English and Chinese). We find evidence that changes in company policies, even those not publicly announced, can be detected by AI Watchman, and identify company- and model-specific differences in content moderation. We also qualitatively analyze and categorize different forms of refusal. This work contributes evidence for the value of longitudinal auditing of LLMs, and AI Watchman, one system for doing so.
>
---
## 更新

#### [replaced 001] The Measurement Imbalance in Agentic AI Evaluation Undermines Industry Productivity Claims
- **分类: cs.CY; cs.HC**

- **链接: [http://arxiv.org/pdf/2506.02064v3](http://arxiv.org/pdf/2506.02064v3)**

> **作者:** Kiana Jafari Meimandi; Gabriela Aránguiz-Dias; Grace Ra Kim; Lana Saadeddin; Allie Griffith; Mykel J. Kochenderfer
>
> **备注:** 15 pages, 3 figures
>
> **摘要:** As industry reports claim agentic AI systems deliver double-digit productivity gains and multi-trillion dollar economic potential, the validity of these claims has become critical for investment decisions, regulatory policy, and responsible technology adoption. However, this paper demonstrates that current evaluation practices for agentic AI systems exhibit a systemic imbalance that calls into question prevailing industry productivity claims. Our systematic review of 84 papers (2023--2025) reveals an evaluation imbalance where technical metrics dominate assessments (83%), while human-centered (30%), safety (53%), and economic assessments (30%) remain peripheral, with only 15% incorporating both technical and human dimensions. This measurement gap creates a fundamental disconnect between benchmark success and deployment value. We present evidence from healthcare, finance, and retail sectors where systems excelling on technical metrics failed in real-world implementation due to unmeasured human, temporal, and contextual factors. Our position is not against agentic AI's potential, but rather that current evaluation frameworks systematically privilege narrow technical metrics while neglecting dimensions critical to real-world success. We propose a balanced four-axis evaluation model and call on the community to lead this paradigm shift because benchmark-driven optimization shapes what we build. By redefining evaluation practices, we can better align industry claims with deployment realities and ensure responsible scaling of agentic systems in high-stakes domains.
>
---
#### [replaced 002] Legal Knowledge Graph Foundations, Part I: URI-Addressable Abstract Works (LRMoo F1 to schema.org)
- **分类: cs.DL; cs.AI; cs.CY; cs.IR**

- **链接: [http://arxiv.org/pdf/2508.00827v4](http://arxiv.org/pdf/2508.00827v4)**

> **作者:** Hudson de Martim
>
> **备注:** This version formalizes the LRMoo event-centric model for the legal lifecycle (enactment, publication). This provides a more precise and ontologically-grounded mapping to Schema.org, with a clearer case study and improved diagrams
>
> **摘要:** Building upon a formal, event-centric model for the diachronic evolution of legal norms grounded in the IFLA Library Reference Model (LRMoo), this paper addresses the essential first step of publishing this model's foundational entity-the abstract legal Work (F1)-on the Semantic Web. We propose a detailed, property-by-property mapping of the LRMoo F1 Work to the widely adopted schema.org/Legislation vocabulary. Using Brazilian federal legislation from the Normas.leg.br portal as a practical case study, we demonstrate how to create interoperable, machine-readable descriptions via JSON-LD, focusing on stable URN identifiers, core metadata, and norm relationships. This structured mapping establishes a stable, URI-addressable anchor for each legal norm, creating a verifiable "ground truth". It provides the essential, interoperable foundation upon which subsequent layers of the model, such as temporal versions (Expressions) and internal components, can be built. By bridging formal ontology with web-native standards, this work paves the way for building deterministic and reliable Legal Knowledge Graphs (LKGs), overcoming the limitations of purely probabilistic models.
>
---
#### [replaced 003] Anti-Regulatory AI: How "AI Safety" is Leveraged Against Regulatory Oversight
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2509.22872v2](http://arxiv.org/pdf/2509.22872v2)**

> **作者:** Rui-Jie Yew; Brian Judge
>
> **备注:** Forthcoming at EAAMO 2025
>
> **摘要:** AI companies increasingly develop and deploy privacy-enhancing technologies, bias-constraining measures, evaluation frameworks, and alignment techniques -- framing them as addressing concerns related to data privacy, algorithmic fairness, and AI safety. This paper examines the ulterior function of these technologies as mechanisms of legal influence. First, we examine how encryption, federated learning, and synthetic data -- presented as enhancing privacy and reducing bias -- can operate as mechanisms of avoidance with existing regulations in attempts to place data operations outside the scope of traditional regulatory frameworks. Second, we investigate how emerging AI safety practices including open-source model releases, evaluations, and alignment techniques can be used as mechanisms of change that direct regulatory focus towards industry-controlled voluntary standards and self-governance. We term this phenomenon "anti-regulatory AI" -- the deployment of ostensibly protective technologies that simultaneously shapes the terms of regulatory oversight. Our analysis additionally reveals how technologies' anti-regulatory functions are enabled through framing that legitimizes their deployment while obscuring their use as regulatory workarounds. This paper closes with a discussion of policy implications that centers on the consideration of business incentives that drive AI development and the role of technical expertise in assessing whether these technologies fulfill their purported protections.
>
---
#### [replaced 004] Superficial Safety Alignment Hypothesis
- **分类: cs.CL; cs.AI; cs.CR; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2410.10862v2](http://arxiv.org/pdf/2410.10862v2)**

> **作者:** Jianwei Li; Jung-Eun Kim
>
> **摘要:** As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe responses is a pressing need. Previous studies on alignment have largely focused on general instruction-following but have often overlooked the distinct properties of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment teaches an otherwise unsafe model to choose the correct reasoning direction - fulfill or refuse users' requests - interpreted as an implicit binary classification task. Through SSAH, we hypothesize that only a few essential components can establish safety guardrails in LLMs. We successfully identify four types of attribute-critical components: Safety Critical Unit (SCU), Utility Critical Unit (UCU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Similarly, we show that leveraging redundant units in the pre-trained model as an "alignment budget" can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated.
>
---
#### [replaced 005] Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool
- **分类: cs.SE; cs.CY**

- **链接: [http://arxiv.org/pdf/2509.21067v2](http://arxiv.org/pdf/2509.21067v2)**

> **作者:** Oka Kurniawan; Erick Chandra; Christopher M. Poskitt; Yannic Noller; Kenny Tsu Wei Choo; Cyrille Jegourel
>
> **备注:** Accepted by the 25th Koli Calling International Conference on Computing Education Research (Koli Calling 2025)
>
> **摘要:** Debugging is a fundamental skill that novice programmers must develop. Numerous tools have been created to assist novice programmers in this process. Recently, large language models (LLMs) have been integrated with automated program repair techniques to generate fixes for students' buggy code. However, many of these tools foster an over-reliance on AI and do not actively engage students in the debugging process. In this work, we aim to design an intuitive debugging assistant, CodeHinter, that combines traditional debugging tools with LLM-based techniques to help novice debuggers fix semantic errors while promoting active engagement in the debugging process. We present findings from our second design iteration, which we tested with a group of undergraduate students. Our results indicate that the students found the tool highly effective in resolving semantic errors and significantly easier to use than the first version. Consistent with our previous study, error localization was the most valuable feature. Finally, we conclude that any AI-assisted debugging approach should be personalized based on user profiles to optimize their interactions with the tool.
>
---
#### [replaced 006] Towards Effective E-Participation of Citizens in the European Union: The Development of AskThePublic
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2504.03287v2](http://arxiv.org/pdf/2504.03287v2)**

> **作者:** Nils Messerschmidt; Kilian Sprenkamp; Amir Sartipi; Xiaohui Wu; Igor Tchappi; Liudmila Zavolokina; Gilbert Fridgen
>
> **摘要:** E-participation platforms are an important asset for governments in increasing trust and fostering democratic societies. By engaging public and private institutions and individuals, policymakers can make informed and inclusive decisions. However, current approaches of primarily static nature struggle to integrate citizen feedback effectively. Drawing on the Media Richness Theory and applying the Design Science Research method, we explore how a chatbot can address these shortcomings to improve the decision-making abilities for primary stakeholders of e-participation platforms. Leveraging the "Have Your Say" platform, which solicits feedback on initiatives and regulations by the European Commission, a Large Language Model-based chatbot, called AskThePublic is created, providing policymakers, journalists, researchers, and interested citizens with a convenient channel to explore and engage with citizen input. Evaluating AskThePublic in 11 semi-structured interviews with public sector-affiliated experts, we find that the interviewees value the interactive and structured responses as well as enhanced language capabilities.
>
---
#### [replaced 007] Gendered Inequalities in Online Harms: Fear, Safety Work, and Online Participation
- **分类: cs.CY; cs.HC**

- **链接: [http://arxiv.org/pdf/2403.19037v2](http://arxiv.org/pdf/2403.19037v2)**

> **作者:** Florence E. Enock; Francesca Stevens; Tvesha Sippy; Jonathan Bright; Miranda Cross; Pica Johansson; Judy Wajcman; Helen Z. Margetts
>
> **摘要:** Online harms, such as hate speech, trolling and self-harm promotion, continue to be widespread. There are growing concerns that these harms may disproportionately affect women, reflecting and reproducing existing structural inequalities within digital spaces. Using a nationally representative survey of UK adults (N=1992), we examine how gender shapes exposure to a variety of online harms, fears surrounding being targeted, the psychological impact of online experiences, the use of safety tools, and comfort with various forms of online participation. We find that while men and women report roughly similar levels of absolute exposure to harmful content online, women are more often targeted by contact-based harms including image-based abuse, cyberstalking and cyberflashing. Women report heightened fears about being targeted by online harms, more negative psychological impact in response to online experiences, and increased use of safety tools, reflecting more engagement with personal safety work. Importantly, women also say they are significantly less comfortable with several forms of online participation, for example just 23% of women are comfortable expressing political views online compared to 40% of men. Explanatory models show direct associations between fears surrounding harms and comfort with particular online behaviours. Our findings show how online harms reinforce gender inequality by placing disproportionate psychological burden and participation constraints on women. These results are important because with much public discourse happening online, we must ensure all members of society feel safe and able to participate in online spaces.
>
---
#### [replaced 008] Consistent End-to-End Estimation for Counterfactual Fairness
- **分类: cs.LG; cs.CY**

- **链接: [http://arxiv.org/pdf/2310.17687v2](http://arxiv.org/pdf/2310.17687v2)**

> **作者:** Yuchen Ma; Valentyn Melnychuk; Dennis Frauen; Stefan Feuerriegel
>
> **摘要:** Fairness in predictions is of direct importance in practice due to legal, ethical, and societal reasons. This is often accomplished through counterfactual fairness, which ensures that the prediction for an individual is the same as that in a counterfactual world under a different sensitive attribute. However, achieving counterfactual fairness is challenging as counterfactuals are unobservable, and, because of that, existing baselines for counterfactual fairness do not have theoretical guarantees. In this paper, we propose a novel counterfactual fairness predictor for making predictions under counterfactual fairness. Here, we follow the standard counterfactual fairness setting and directly learn the counterfactual distribution of the descendants of the sensitive attribute via tailored neural networks, which we then use to enforce fair predictions through a novel counterfactual mediator regularization. Unique to our work is that we provide theoretical guarantees that our method is effective in ensuring the notion of counterfactual fairness. We further compare the performance across various datasets, where our method achieves state-of-the-art performance.
>
---
#### [replaced 009] What happens when generative AI models train recursively on each others' outputs?
- **分类: cs.LG; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2505.21677v3](http://arxiv.org/pdf/2505.21677v3)**

> **作者:** Hung Anh Vu; Galen Reeves; Emily Wenger
>
> **备注:** 9 pages
>
> **摘要:** The internet serves as a common source of training data for generative AI (genAI) models but is increasingly populated with AI-generated content. This duality raises the possibility that future genAI models may be trained on other models' generated outputs. Prior work has studied consequences of models training on their own generated outputs, but limited work has considered what happens if models ingest content produced by other models. Given society's increasing dependence on genAI tools, understanding such data-mediated model interactions is critical. This work provides empirical evidence for how data-mediated interactions might unfold in practice, develops a theoretical model for this interactive training process, and experimentally validates the theory. We find that data-mediated interactions can benefit models by exposing them to novel concepts perhaps missed in original training data, but also can homogenize their performance on shared tasks.
>
---
