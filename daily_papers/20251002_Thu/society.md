# 计算机与社会 cs.CY

- **最新发布 17 篇**

- **更新 12 篇**

## 最新发布

#### [new 001] Disc-Cover Complexity Trends in Music Illustrations from Sinatra to Swift
- **分类: cs.CY; cs.HC; cs.MM**

- **简介: 该论文属于艺术与文化研究任务，旨在分析音乐专辑封面视觉复杂性的演变趋势，通过计算方法揭示其从复杂到简约的转变及多样性。**

- **链接: [http://arxiv.org/pdf/2510.00990v1](http://arxiv.org/pdf/2510.00990v1)**

> **作者:** Nicolas Fracaro; Stefano Cecconello; Mauro Conti; Niccolò Di Marco; Alessandro Galeazzi
>
> **摘要:** The study of art evolution has provided valuable insights into societal change, often revealing long-term patterns of simplification and transformation. Album covers represent a distinctive yet understudied form of visual art that has both shaped and been shaped by cultural, technological, and commercial dynamics over the past century. As highly visible artifacts at the intersection of art and commerce, they offer a unique lens through which to study cultural evolution. In this work, we examine the visual complexity of album covers spanning 75 years and 11 popular musical genres. Using a diverse set of computational measures that capture multiple dimensions of visual complexity, our analysis reveals a broad shift toward minimalism across most genres, with notable exceptions that highlight the heterogeneity of aesthetic trends. At the same time, we observe growing variance over time, with many covers continuing to display high levels of abstraction and intricacy. Together, these findings position album covers as a rich, quantifiable archive of cultural history and underscore the value of computational approaches in the systematic study of the arts, bridging quantitative analysis with aesthetic and cultural inquiry.
>
---
#### [new 002] Digital Domination: A Case for Republican Liberty in Artificial Intelligence
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于伦理与政治分析任务，探讨AI对共和自由的威胁，分析算法如何造成数字支配，并提出需建立问责机制以保障自由。**

- **链接: [http://arxiv.org/pdf/2510.00312v1](http://arxiv.org/pdf/2510.00312v1)**

> **作者:** Matthew David Hamilton
>
> **摘要:** Artificial intelligence is set to revolutionize social and political life in unpredictable ways, raising questions about the principles that ought to guide its development and regulation. By examining digital advertising and social media algorithms, this article highlights how artificial intelligence already poses a significant threat to the republican conception of liberty -- or freedom from unaccountable power -- and thereby highlights the necessity of protecting republican liberty when integrating artificial intelligence into society. At an individual level, these algorithms can subconsciously influence behavior and thought, and those subject to this influence have limited power over the algorithms they engage. At the political level, these algorithms give technology company executives and other foreign parties the power to influence domestic political processes, such as elections; the multinational nature of algorithm-based platforms and the speed with which technology companies innovate make incumbent state institutions ineffective at holding these actors accountable. At both levels, artificial intelligence has thus created a new form of unfreedom: digital domination. By drawing on the works of Quentin Skinner, Philip Pettit, and other republican theorists, this article asserts that individuals must have mechanisms to hold algorithms (and those who develop them) accountable in order to be truly free.
>
---
#### [new 003] Simulating Student Success in the Age of GenAI: A Kantian-Axiomatic Perspective
- **分类: cs.CY; cs.AI; 03B25, 03A05, 68T07, 97U70; F.4.1; I.2.0; K.3.1**

- **简介: 该论文属于理论分析任务，探讨GenAI背景下学生成功模拟的逻辑结构，通过Kantian视角检验模拟数据与连续性假设的契合度。**

- **链接: [http://arxiv.org/pdf/2510.00091v1](http://arxiv.org/pdf/2510.00091v1)**

> **作者:** Seyma Yaman Kayadibi
>
> **备注:** 23 pages in total, including 3 embedded Python code blocks, 4 figures, and 2 tables. The article analyzes student perception data simulated from survey-derived Likert statistics, evaluated against six axioms of Dense Linear Order (DLO). Preliminary version published on Zenodo; see External DOI
>
> **摘要:** This study reinterprets a Monte Carlo simulation of students' perceived success with generative AI (GenAI) through a Kantian-axiomatic lens. Building on prior work, theme-level survey statistics Ease of Use and Learnability, System Efficiency and Learning Burden, and Perceived Complexity and Integration from a representative dataset are used to generate 10,000 synthetic scores per theme on the [1,5] Likert scale. The simulated outputs are evaluated against the axioms of dense linear order without endpoints (DLO): irreflexivity, transitivity, total comparability (connectedness), no endpoints (no greatest and no least; A4-A5), and density (A6). At the data level, the basic ordering axioms (A1-A3) are satisfied, whereas no-endpoints (A4-A5) and density (A6) fail as expected. Likert clipping introduces minimum and maximum observed values, and a finite, discretized sample need not contain a value strictly between any two distinct scores. These patterns are read not as methodological defects but as markers of an epistemological boundary. Following Kant and Friedman, the findings suggest that what simulations capture finite, quantized observations cannot instantiate the ideal properties of an unbounded, dense continuum. Such properties belong to constructive intuition rather than to finite sampling alone. A complementary visualization contrasts the empirical histogram with a sine-curve proxy to clarify this divide. The contribution is interpretive rather than data-expansive: it reframes an existing simulation as a probe of the synthetic a priori structure underlying students' perceptions, showing how formal order-theoretic coherence coexists with principled failures of endpoint-freeness and density in finite empirical models.
>
---
#### [new 004] Adapting Large Language Models to Mitigate Skin Tone Biases in Clinical Dermatology Tasks: A Mixed-Methods Study
- **分类: eess.IV; cs.CV; cs.CY**

- **简介: 该论文属于医疗图像分类任务，旨在解决皮肤疾病诊断中肤色偏差问题。通过微调模型和评估公平性，提升对深色皮肤的诊断准确性。**

- **链接: [http://arxiv.org/pdf/2510.00055v1](http://arxiv.org/pdf/2510.00055v1)**

> **作者:** Kiran Nijjer; Ryan Bui; Derek Jiu; Adnan Ahmed; Peter Wang; Benjamin Liu; Kevin Zhu; Lilly Zhu
>
> **备注:** Accepted to EADV (European Academy of Dermatology) and SID (Society for Investigative Dermatology)
>
> **摘要:** SkinGPT-4, a large vision-language model, leverages annotated skin disease images to augment clinical workflows in underserved communities. However, its training dataset predominantly represents lighter skin tones, limiting diagnostic accuracy for darker tones. Here, we evaluated performance biases in SkinGPT-4 across skin tones on common skin diseases, including eczema, allergic-contact dermatitis, and psoriasis using the open-sourced SCIN dataset. We leveraged the SkinGPT-4 backbone to develop finetuned models for custom skin disease classification tasks and explored bias mitigation strategies. Clinical evaluation by board-certified dermatologists on six relevant skin diseases from 300 SCIN cases assessed images for diagnostic accuracy, informativity, physician utility, and patient utility. Model fairness metrics, including demographic parity and equalized odds, were calculated across skin tones. SkinGPT-4 achieved an average demographic parity of 0.10 across Fitzpatrick types, with notable differences of 0.10-0.15 between lightest and darkest tones across evaluation metrics. Model hallucinations in artifacts and anatomy occurred at a rate of 17.8. Our customized models achieved average F1, precision, and AUROC of 0.75, 0.78, and 0.78 across visually similar disease pairs. Fairness analysis showed an average demographic parity of 0.75, with a maximum disparity of 0.21 across skin tones. The best model achieved parity scores of 0.83, 0.83, 0.76, 0.89, 0.90, and 0.90 for Fitzpatrick I-VI, indicating robust fairness. Large language models such as SkinGPT-4 showed weaker performance on darker tones. Model biases exist across evaluation criteria, and hallucinations may affect diagnostic efficacy. These findings demonstrate the efficacy of training accurate, fair models using existing backbones for custom skin disease classification.
>
---
#### [new 005] BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses
- **分类: cs.CL; cs.AI; cs.CY; cs.LG**

- **简介: 该论文属于语言模型偏见缓解任务，旨在解决现有评估方法不一致及与实际应用脱节的问题。提出BiasFreeBench基准和Bias-Free Score指标，系统比较多种去偏方法。**

- **链接: [http://arxiv.org/pdf/2510.00232v1](http://arxiv.org/pdf/2510.00232v1)**

> **作者:** Xin Xu; Xunzhi He; Churan Zhi; Ruizhe Chen; Julian McAuley; Zexue He
>
> **备注:** Work in progress
>
> **摘要:** Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.
>
---
#### [new 006] Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare
- **分类: cs.CL; cs.AI; cs.CY; cs.HC**

- **简介: 该论文属于AI治理任务，旨在解决LLM在社会福利分配中的价值观问题。通过构建SWF基准测试，评估LLM的分配能力，揭示其公平性与效率间的权衡。**

- **链接: [http://arxiv.org/pdf/2510.01164v1](http://arxiv.org/pdf/2510.01164v1)**

> **作者:** Zhengliang Shi; Ruotian Ma; Jen-tse Huang; Xinbei Ma; Xingyu Chen; Mengru Wang; Qu Yang; Yue Wang; Fanghua Ye; Ziyang Chen; Shanyi Wang; Cixing Li; Wenxuan Wang; Zhaopeng Tu; Xiaolong Li; Zhaochun Ren; Linus
>
> **摘要:** Large language models (LLMs) are increasingly entrusted with high-stakes decisions that affect human welfare. However, the principles and values that guide these models when distributing scarce societal resources remain largely unexamined. To address this, we introduce the Social Welfare Function (SWF) Benchmark, a dynamic simulation environment where an LLM acts as a sovereign allocator, distributing tasks to a heterogeneous community of recipients. The benchmark is designed to create a persistent trade-off between maximizing collective efficiency (measured by Return on Investment) and ensuring distributive fairness (measured by the Gini coefficient). We evaluate 20 state-of-the-art LLMs and present the first leaderboard for social welfare allocation. Our findings reveal three key insights: (i) A model's general conversational ability, as measured by popular leaderboards, is a poor predictor of its allocation skill. (ii) Most LLMs exhibit a strong default utilitarian orientation, prioritizing group productivity at the expense of severe inequality. (iii) Allocation strategies are highly vulnerable, easily perturbed by output-length constraints and social-influence framing. These results highlight the risks of deploying current LLMs as societal decision-makers and underscore the need for specialized benchmarks and targeted alignment for AI governance.
>
---
#### [new 007] Social Photo-Elicitation: The Use of Communal Production of Meaning to Hear a Vulnerable Population
- **分类: cs.HC; cs.CY; cs.SI**

- **简介: 该论文属于社会研究任务，旨在通过共融式影像访谈方法，倾听尼泊尔性剥削幸存者的声音，解决如何在敏感环境中有效收集其价值观与需求的问题。**

- **链接: [http://arxiv.org/pdf/2510.00964v1](http://arxiv.org/pdf/2510.00964v1)**

> **作者:** Aakash Gautam; Chandani Shrestha; Deborah Tatar; Steve Harrison
>
> **备注:** 20 pages, 3 figures, published at CSCW 2018
>
> **摘要:** We report on an initial ethnographic exploration of the situation of sex-trafficking survivors in Nepal. In the course of studying trafficking survivors in a protected-living situation created by a non-governmental organization in Nepal, we adapted photo-elicitation to hear the voices of the survivors by making the technique more communal. Bringing sociality to the forefront of the method reduced the pressure on survivors to assert voices as individuals, allowing them to speak. We make three contributions to research. First, we propose a communal form of photo-elicitation as a method to elicit values in sensitive settings. Second, we present the complex circumstances of the survivors as they undergo rehabilitation and move towards life with a ``new normal''. Third, our work adds to HCI and CSCW literature on understanding specific concerns of trafficking survivors and aims to inform designs that can support reintegration of survivors in society. The values that the survivors hold and their notion of future opportunities suggest possession of limited but important social capital in some domains that could be leveraged to aid reintegration.
>
---
#### [new 008] Intuitions of Machine Learning Researchers about Transfer Learning for Medical Image Classification
- **分类: cs.CV; cs.CY; cs.HC**

- **简介: 该论文研究机器学习研究人员在医学图像分类中选择迁移学习源数据集的直觉，旨在解决依赖直觉而非系统方法的问题，通过调查分析影响因素。**

- **链接: [http://arxiv.org/pdf/2510.00902v1](http://arxiv.org/pdf/2510.00902v1)**

> **作者:** Yucheng Lu; Hubert Dariusz Zając; Veronika Cheplygina; Amelia Jiménez-Sánchez
>
> **备注:** Under review
>
> **摘要:** Transfer learning is crucial for medical imaging, yet the selection of source datasets - which can impact the generalizability of algorithms, and thus patient outcomes - often relies on researchers' intuition rather than systematic principles. This study investigates these decisions through a task-based survey with machine learning practitioners. Unlike prior work that benchmarks models and experimental setups, we take a human-centered HCI perspective on how practitioners select source datasets. Our findings indicate that choices are task-dependent and influenced by community practices, dataset properties, and computational (data embedding), or perceived visual or semantic similarity. However, similarity ratings and expected performance are not always aligned, challenging a traditional "more similar is better" view. Participants often used ambiguous terminology, which suggests a need for clearer definitions and HCI tools to make them explicit and usable. By clarifying these heuristics, this work provides practical insights for more systematic source selection in transfer learning.
>
---
#### [new 009] Unpacking Musical Symbolism in Online Communities: Content-Based and Network-Centric Approaches
- **分类: cs.SD; cs.CL; cs.CY; cs.MM; eess.AS**

- **简介: 该论文研究在线社区中音乐符号的生成与传播，结合音乐分析与网络视角，分析歌词和音频特征，揭示音乐风格与情绪的关系。**

- **链接: [http://arxiv.org/pdf/2510.00006v1](http://arxiv.org/pdf/2510.00006v1)**

> **作者:** Kajwan Ziaoddini
>
> **摘要:** This paper examines how musical symbolism is produced and circulated in online communities by combining content-based music analysis with a lightweight network perspective on lyrics. Using a curated corpus of 275 chart-topping songs enriched with audio descriptors (energy, danceability, loudness, liveness, valence, acousticness, speechiness, popularity) and full lyric transcripts, we build a reproducible pipeline that (i) quantifies temporal trends in sonic attributes, (ii) models lexical salience and co-occurrence, and (iii) profiles mood by genre. We find a decade-long decline in energy (79 -> 58) alongside a rise in danceability (59 -> 73); valence peaks in 2013 (63) and dips in 2014-2016 (42) before partially recovering. Correlation analysis shows strong coupling of energy with loudness (r = 0.74) and negative associations for acousticness with both energy (r = -0.54) and loudness (r = -0.51); danceability is largely orthogonal to other features (|r| < 0.20). Lyric tokenization (>114k tokens) reveals a pronoun-centric lexicon "I/you/me/my" and a dense co-occurrence structure in which interpersonal address anchors mainstream narratives. Mood differs systematically by style: R&B exhibits the highest mean valence (96), followed by K-Pop/Pop (77) and Indie/Pop (70), whereas Latin/Reggaeton is lower (37) despite high danceability. Read through a subcultural identity lens, these patterns suggest the mainstreaming of previously peripheral codes and a commercial preference for relaxed yet rhythmically engaging productions that sustain collective participation without maximal intensity. Methodologically, we contribute an integrated MIR-plus-network workflow spanning summary statistics, correlation structure, lexical co-occurrence matrices, and genre-wise mood profiling that is robust to modality sparsity and suitable for socially aware recommendation or community-level diffusion studies.
>
---
#### [new 010] Board Gender Diversity and Carbon Emissions Performance: Insights from Panel Regressions, Machine Learning and Explainable AI
- **分类: q-fin.GN; cs.CY; cs.LG; 62P20**

- **简介: 该论文属于实证分析任务，研究板层性别多样性对碳排放绩效的影响，通过面板回归、机器学习和解释性AI方法，揭示其非线性关系及作用机制。**

- **链接: [http://arxiv.org/pdf/2510.00244v1](http://arxiv.org/pdf/2510.00244v1)**

> **作者:** Mohammad Hassan Shakil; Arne Johan Pollestad; Khine Kyaw; Ziaul Haque Munim
>
> **备注:** 34 pages and 3 figures
>
> **摘要:** With the European Union introducing gender quotas on corporate boards, this study investigates the impact of board gender diversity (BGD) on firms' carbon emission performance (CEP). Using panel regressions and advanced machine learning algorithms on data from European firms between 2016 and 2022, the analyses reveal a significant non-linear relationship. Specifically, CEP improves with BGD up to an optimal level of approximately 35 percent, beyond which further increases in BGD yield no additional improvement in CEP. A minimum threshold of 22 percent BGD is necessary for meaningful improvements in CEP. To assess the legitimacy of CEP outcomes, this study examines whether ESG controversies affect the relationship between BGD and CEP. The results show no significant effect, suggesting that the effect of BGD is driven by governance mechanisms rather than symbolic actions. Additionally, structural equation modelling (SEM) indicates that while environmental innovation contributes to CEP, it is not the mediating channel through which BGD promotes CEP. The results have implications for academics, businesses, and regulators.
>
---
#### [new 011] Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems
- **分类: cs.AI; cs.CY; cs.DB**

- **简介: 该论文属于AI伦理与合规研究任务，旨在解决AI系统在欧洲的伦理和法规认证问题。工作包括构建框架，整合MLOps、数据溯源和RegOps以提升透明度与合规性。**

- **链接: [http://arxiv.org/pdf/2510.00084v1](http://arxiv.org/pdf/2510.00084v1)**

> **作者:** Fabian Kovac; Sebastian Neumaier; Timea Pahi; Torsten Priebe; Rafael Rodrigues; Dimitrios Christodoulou; Maxime Cordy; Sylvain Kubler; Ali Kordia; Georgios Pitsiladis; John Soldatos; Petros Zervoudakis
>
> **备注:** Accepted for publication in the proceedings of the Workshop on AI Certification, Fairness and Regulations, co-located with the Austrian Symposium on AI and Vision (AIRoV 2025)
>
> **摘要:** Artificial Intelligence has rapidly become a cornerstone technology, significantly influencing Europe's societal and economic landscapes. However, the proliferation of AI also raises critical ethical, legal, and regulatory challenges. The CERTAIN (Certification for Ethical and Regulatory Transparency in Artificial Intelligence) project addresses these issues by developing a comprehensive framework that integrates regulatory compliance, ethical standards, and transparency into AI systems. In this position paper, we outline the methodological steps for building the core components of this framework. Specifically, we present: (i) semantic Machine Learning Operations (MLOps) for structured AI lifecycle management, (ii) ontology-driven data lineage tracking to ensure traceability and accountability, and (iii) regulatory operations (RegOps) workflows to operationalize compliance requirements. By implementing and validating its solutions across diverse pilots, CERTAIN aims to advance regulatory compliance and to promote responsible AI innovation aligned with European standards.
>
---
#### [new 012] When Life Paths Cross: Extracting Human Interactions in Time and Space from Wikipedia
- **分类: cs.SI; cs.CY**

- **简介: 该论文属于信息提取任务，旨在解决历史人物互动数据稀缺的问题。通过分析维基百科文本，提取人物交互记录，并构建数据集以支持政治极化研究。**

- **链接: [http://arxiv.org/pdf/2510.00019v1](http://arxiv.org/pdf/2510.00019v1)**

> **作者:** Zhongyang Liu; Ying Zhang; Xiangyi Xiao; Wenting Liu; Yuanting Zha; Haipeng Zhang
>
> **摘要:** Interactions among notable individuals -- whether examined individually, in groups, or as networks -- often convey significant messages across cultural, economic, political, scientific, and historical perspectives. By analyzing the times and locations of these interactions, we can observe how dynamics unfold across regions over time. However, relevant studies are often constrained by data scarcity, particularly concerning the availability of specific location and time information. To address this issue, we mine millions of biography pages from Wikipedia, extracting 685,966 interaction records in the form of (Person1, Person2, Time, Location) interaction quadruplets. The key elements of these interactions are often scattered throughout the heterogeneous crowd-sourced text and may be loosely or indirectly associated. We overcome this challenge by designing a model that integrates attention mechanisms, multi-task learning, and feature transfer methods, achieving an F1 score of 86.51%, which outperforms baseline models. We further conduct an empirical analysis of intra- and inter-party interactions among political figures to examine political polarization in the US, showcasing the potential of the extracted data from a perspective that may not be possible without this data. We make our code, the extracted interaction data, and the WikiInteraction dataset of 4,507 labeled interaction quadruplets publicly available.
>
---
#### [new 013] Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction
- **分类: cs.AI; cs.CY**

- **简介: 该论文属于法律判决预测任务，研究如何利用视觉语言模型进行保释决策。针对模型在不同群体中表现不佳的问题，通过引入法律先例和微调提升预测准确性。**

- **链接: [http://arxiv.org/pdf/2510.00088v1](http://arxiv.org/pdf/2510.00088v1)**

> **作者:** Sagnik Basu; Shubham Prakash; Ashish Maruti Barge; Siddharth D Jaiswal; Abhisek Dash; Saptarshi Ghosh; Animesh Mukherjee
>
> **摘要:** Large language models (LLMs) have been extensively used for legal judgment prediction tasks based on case reports and crime history. However, with a surge in the availability of large vision language models (VLMs), legal judgment prediction systems can now be made to leverage the images of the criminals in addition to the textual case reports/crime history. Applications built in this way could lead to inadvertent consequences and be used with malicious intent. In this work, we run an audit to investigate the efficiency of standalone VLMs in the bail decision prediction task. We observe that the performance is poor across multiple intersectional groups and models \textit{wrongly deny bail to deserving individuals with very high confidence}. We design different intervention algorithms by first including legal precedents through a RAG pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate that these interventions substantially improve the performance of bail prediction. Our work paves the way for the design of smarter interventions on VLMs in the future, before they can be deployed for real-world legal judgment prediction.
>
---
#### [new 014] Data Quality Taxonomy for Data Monetization
- **分类: cs.DB; cs.CY**

- **简介: 该论文属于数据质量管理任务，旨在解决数据货币化中的质量评估问题。通过构建四层分类体系，整合通用与领域特定指标，提升数据价值评估的准确性与战略协同性。**

- **链接: [http://arxiv.org/pdf/2510.00089v1](http://arxiv.org/pdf/2510.00089v1)**

> **作者:** Eduardo Vyhmeister; Bastien Pietropoli; Andrea Visentin
>
> **摘要:** This chapter presents a comprehensive taxonomy for assessing data quality in the context of data monetisation, developed through a systematic literature review. Organising over one hundred metrics and Key Performance Indicators (KPIs) into four subclusters (Fundamental, Contextual, Resolution, and Specialised) within the Balanced Scorecard (BSC) framework, the taxonomy integrates both universal and domain-specific quality dimensions. By positioning data quality as a strategic connector across the BSC's Financial, Customer, Internal Processes, and Learning & Growth perspectives, it demonstrates how quality metrics underpin valuation accuracy, customer trust, operational efficiency, and innovation capacity. The framework's interconnected "metrics layer" ensures that improvements in one dimension cascade into others, maximising strategic impact. This holistic approach bridges the gap between granular technical assessment and high-level decision-making, offering practitioners, data stewards, and strategists a scalable, evidence-based reference for aligning data quality management with sustainable value creation.
>
---
#### [new 015] DexBench: Benchmarking LLMs for Personalized Decision Making in Diabetes Management
- **分类: cs.LG; cs.AI; cs.CY**

- **简介: 该论文属于医疗AI任务，旨在评估大语言模型在糖尿病管理中的个性化决策能力。通过构建DexBench基准，解决模型在真实场景下的表现评估问题。**

- **链接: [http://arxiv.org/pdf/2510.00038v1](http://arxiv.org/pdf/2510.00038v1)**

> **作者:** Maria Ana Cardei; Josephine Lamp; Mark Derdzinski; Karan Bhatia
>
> **摘要:** We present DexBench, the first benchmark designed to evaluate large language model (LLM) performance across real-world decision-making tasks faced by individuals managing diabetes in their daily lives. Unlike prior health benchmarks that are either generic, clinician-facing or focused on clinical tasks (e.g., diagnosis, triage), DexBench introduces a comprehensive evaluation framework tailored to the unique challenges of prototyping patient-facing AI solutions in diabetes, glucose management, metabolic health and related domains. Our benchmark encompasses 7 distinct task categories, reflecting the breadth of real-world questions individuals with diabetes ask, including basic glucose interpretation, educational queries, behavioral associations, advanced decision making and long term planning. Towards this end, we compile a rich dataset comprising one month of time-series data encompassing glucose traces and metrics from continuous glucose monitors (CGMs) and behavioral logs (e.g., eating and activity patterns) from 15,000 individuals across three different diabetes populations (type 1, type 2, pre-diabetes/general health and wellness). Using this data, we generate a total of 360,600 personalized, contextual questions across the 7 tasks. We evaluate model performance on these tasks across 5 metrics: accuracy, groundedness, safety, clarity and actionability. Our analysis of 8 recent LLMs reveals substantial variability across tasks and metrics; no single model consistently outperforms others across all dimensions. By establishing this benchmark, we aim to advance the reliability, safety, effectiveness and practical utility of AI solutions in diabetes care.
>
---
#### [new 016] Partial Identification Approach to Counterfactual Fairness Assessment
- **分类: cs.LG; cs.AI; cs.CY; stat.ME**

- **简介: 该论文属于算法公平性评估任务，解决如何在无法完全了解模型内部机制的情况下，评估反事实公平性的问题。通过部分识别方法，从数据中推导出公平性度量的置信区间。**

- **链接: [http://arxiv.org/pdf/2510.00163v1](http://arxiv.org/pdf/2510.00163v1)**

> **作者:** Saeyoung Rho; Junzhe Zhang; Elias Bareinboim
>
> **摘要:** The wide adoption of AI decision-making systems in critical domains such as criminal justice, loan approval, and hiring processes has heightened concerns about algorithmic fairness. As we often only have access to the output of algorithms without insights into their internal mechanisms, it was natural to examine how decisions would alter when auxiliary sensitive attributes (such as race) change. This led the research community to come up with counterfactual fairness measures, but how to evaluate the measure from available data remains a challenging task. In many practical applications, the target counterfactual measure is not identifiable, i.e., it cannot be uniquely determined from the combination of quantitative data and qualitative knowledge. This paper addresses this challenge using partial identification, which derives informative bounds over counterfactual fairness measures from observational data. We introduce a Bayesian approach to bound unknown counterfactual fairness measures with high confidence. We demonstrate our algorithm on the COMPAS dataset, examining fairness in recidivism risk scores with respect to race, age, and sex. Our results reveal a positive (spurious) effect on the COMPAS score when changing race to African-American (from all others) and a negative (direct causal) effect when transitioning from young to old age.
>
---
#### [new 017] AI in data science education: experiences from the classroom
- **分类: cs.AI; cs.CY**

- **简介: 该论文属于教育技术研究，探讨AI在数据科学教学中的应用，分析其带来的好处与挑战，并提出合理使用建议。**

- **链接: [http://arxiv.org/pdf/2510.00793v1](http://arxiv.org/pdf/2510.00793v1)**

> **作者:** J. A. Hageman; C. F. W. Peeters
>
> **备注:** 6 pages, 0 figures
>
> **摘要:** This study explores the integration of AI, particularly large language models (LLMs) like ChatGPT, into educational settings, focusing on the implications for teaching and learning. Through interviews with course coordinators from data science courses at Wageningen University, this research identifies both the benefits and challenges associated with AI in the classroom. While AI tools can streamline tasks and enhance learning, concerns arise regarding students' overreliance on these technologies, potentially hindering the development of essential cognitive and problem solving skills. The study highlights the importance of responsible AI usage, ethical considerations, and the need for adapting assessment methods to ensure educational outcomes are met. With careful integration, AI can be a valuable asset in education, provided it is used to complement rather than replace fundamental learning processes.
>
---
## 更新

#### [replaced 001] Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.14435v2](http://arxiv.org/pdf/2505.14435v2)**

> **作者:** Annika Bush; Meltem Aksoy; Markus Pauly; Greta Ontrup
>
> **备注:** Accepted for EMNLP Conference
>
> **摘要:** As organizations increasingly rely on AI systems for decision support in sustainability contexts, it becomes critical to understand the inherent biases and perspectives embedded in Large Language Models (LLMs). This study systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek, GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship with AI. We administered validated, psychometric sustainability-related questionnaires - each 100 times per model -- to capture response patterns and variability. Our findings revealed significant inter-model differences: For example, GPT exhibited skepticism about the compatibility of AI and sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect scores for several Sustainable Development Goals (SDGs). Models also diverged in attributing institutional responsibility for AI and sustainability integration, a results that holds implications for technology governance approaches. Our results demonstrate that model selection could substantially influence organizational sustainability strategies, highlighting the need for awareness of model-specific biases when deploying LLMs for sustainability-related decision-making.
>
---
#### [replaced 002] The Sandbox Configurator: A Framework to Support Technical Assessment in AI Regulatory Sandboxes
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.25256v2](http://arxiv.org/pdf/2509.25256v2)**

> **作者:** Alessio Buscemi; Thibault Simonetto; Daniele Pagani; German Castignani; Maxime Cordy; Jordi Cabot
>
> **摘要:** The systematic assessment of AI systems is increasingly vital as these technologies enter high-stakes domains. To address this, the EU's Artificial Intelligence Act introduces AI Regulatory Sandboxes (AIRS): supervised environments where AI systems can be tested under the oversight of Competent Authorities (CAs), balancing innovation with compliance, particularly for startups and SMEs. Yet significant challenges remain: assessment methods are fragmented, tests lack standardisation, and feedback loops between developers and regulators are weak. To bridge these gaps, we propose the Sandbox Configurator, a modular open-source framework that enables users to select domain-relevant tests from a shared library and generate customised sandbox environments with integrated dashboards. Its plug-in architecture aims to support both open and proprietary modules, fostering a shared ecosystem of interoperable AI assessment services. The framework aims to address multiple stakeholders: CAs gain structured workflows for applying legal obligations; technical experts can integrate robust evaluation methods; and AI providers access a transparent pathway to compliance. By promoting cross-border collaboration and standardisation, the Sandbox Configurator's goal is to support a scalable and innovation-friendly European infrastructure for trustworthy AI governance.
>
---
#### [replaced 003] Economic Competition, EU Regulation, and Executive Orders: A Framework for Discussing AI Policy Implications in CS Courses
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.25524v2](http://arxiv.org/pdf/2509.25524v2)**

> **作者:** James Weichert; Hoda Eldardiry
>
> **摘要:** The growth and permeation of artificial intelligence (AI) technologies across society has drawn focus to the ways in which the responsible use of these technologies can be facilitated through AI governance. Increasingly, large companies and governments alike have begun to articulate and, in some cases, enforce governance preferences through AI policy. Yet existing literature documents an unwieldy heterogeneity in ethical principles for AI governance, while our own prior research finds that discussions of the implications of AI policy are not yet present in the computer science (CS) curriculum. In this context, overlapping jurisdictions and even contradictory policy preferences across private companies, local, national, and multinational governments create a complex landscape for AI policy which, we argue, will require AI developers able adapt to an evolving regulatory environment. Preparing computing students for the new challenges of an AI-dominated technology industry is therefore a key priority for the CS curriculum. In this discussion paper, we seek to articulate a framework for integrating discussions on the nascent AI policy landscape into computer science courses. We begin by summarizing recent AI policy efforts in the United States and European Union. Subsequently, we propose guiding questions to frame class discussions around AI policy in technical and non-technical (e.g., ethics) CS courses. Throughout, we emphasize the connection between normative policy demands and still-open technical challenges relating to their implementation and enforcement through code and governance structures. This paper therefore represents a valuable contribution towards bridging research and discussions across the areas of AI policy and CS education, underlining the need to prepare AI engineers to interact with and adapt to societal policy preferences.
>
---
#### [replaced 004] From Assistance to Autonomy -- A Researcher Study on the Potential of AI Support for Qualitative Data Analysis
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2501.19275v2](http://arxiv.org/pdf/2501.19275v2)**

> **作者:** Elisabeth Kirsten; Annalina Buckmann; Leona Lassak; Nele Borgert; Abraham Mhaidli; Steffen Becker
>
> **摘要:** The advent of Artificial Intelligence (AI) tools, such as Large Language Models, has introduced new possibilities for Qualitative Data Analysis (QDA), offering both opportunities and challenges. To help navigate the responsible integration of AI into QDA, we conducted semi-structured interviews with 15 Human-Computer Interaction (HCI) researchers experienced in QDA. While our participants were open to AI support in their QDA workflows, they expressed concerns about data privacy, autonomy, and the quality of AI outputs. In response, we developed a framework that spans from minimal to high AI involvement, providing tangible scenarios for integrating AI into QDA practices while addressing researchers' needs and concerns. Aligned with real-life QDA workflows, we identify potential for AI tools in areas such as data pre-processing, researcher onboarding, or conflict mediation. Our framework aims to provoke further discussion on the development of AI-supported QDA and to help establish community standards for responsible Human-AI collaboration.
>
---
#### [replaced 005] Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2508.08684v3](http://arxiv.org/pdf/2508.08684v3)**

> **作者:** Bram van Dijk; Tiberon Kuiper; Sirin Aoulad si Ahmed; Armel Levebvre; Jake Johnson; Jan Duin; Simon Mooijaart; Marco Spruit
>
> **备注:** Forthcoming in the Proceedings of the Fourth Workshop on Bridging Human Computer Interaction and Natural Language Processing HCINLP (EMNLP)
>
> **摘要:** Voice-controlled interfaces can support older adults in clinical contexts -- with chatbots being a prime example -- but reliable Automatic Speech Recognition (ASR) for underrepresented groups remains a bottleneck. This study evaluates state-of-the-art ASR models on language use of older Dutch adults, who interacted with the Welzijn.AI chatbot designed for geriatric contexts. We benchmark generic multilingual ASR models, and models fine-tuned for Dutch spoken by older adults, while also considering processing speed. Our results show that generic multilingual models outperform fine-tuned models, which suggests recent ASR models can generalise well out of the box to real-world datasets. Moreover, our results indicate that truncating generic models is helpful in balancing the accuracy-speed trade-off. Nonetheless, we also find inputs which cause a high word error rate and place them in context.
>
---
#### [replaced 006] A Comprehensive Framework to Operationalize Social Stereotypes for Responsible AI Evaluations
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2501.02074v3](http://arxiv.org/pdf/2501.02074v3)**

> **作者:** Aida Davani; Sunipa Dev; Héctor Pérez-Urbina; Vinodkumar Prabhakaran
>
> **摘要:** Societal stereotypes are at the center of a myriad of responsible AI interventions targeted at reducing the generation and propagation of potentially harmful outcomes. While these efforts are much needed, they tend to be fragmented and often address different parts of the issue without adopting a unified or holistic approach to social stereotypes and how they impact various parts of the machine learning pipeline. As a result, current interventions fail to capitalize on the underlying mechanisms that are common across different types of stereotypes, and to anchor on particular aspects that are relevant in certain cases. In this paper, we draw on social psychological research and build on NLP data and methods, to propose a unified framework to operationalize stereotypes in generative AI evaluations. Our framework identifies key components of stereotypes that are crucial in AI evaluation, including the target group, associated attribute, relationship characteristics, perceiving group, and context. We also provide considerations and recommendations for its responsible use.
>
---
#### [replaced 007] Addressing Moral Uncertainty using Large Language Models for Ethical Decision-Making
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.05724v2](http://arxiv.org/pdf/2503.05724v2)**

> **作者:** Rohit K. Dubey; Damian Dailisan; Sachit Mahajan
>
> **备注:** 13 pages, 5 figures. All authors contributed equally to this work
>
> **摘要:** We present an ethical decision-making framework that refines a pre-trained reinforcement learning (RL) model using a task-agnostic ethical layer. Following initial training, the RL model undergoes ethical fine-tuning, where human feedback is replaced by feedback generated from a large language model (LLM). The LLM embodies consequentialist, deontological, virtue, social justice, and care ethics as moral principles to assign belief values to recommended actions during ethical decision-making. An ethical layer aggregates belief scores from multiple LLM-derived moral perspectives using Belief Jensen-Shannon Divergence and Dempster-Shafer Theory into probability scores that also serve as the shaping reward, steering the agent toward choices that align with a balanced ethical framework. This integrated learning framework helps the RL agent navigate moral uncertainty in complex environments and enables it to make morally sound decisions across diverse tasks. Our approach, tested across different LLM variants and compared with other belief aggregation techniques, demonstrates improved consistency, adaptability, and reduced reliance on handcrafted ethical rewards. This method is especially effective in dynamic scenarios where ethical challenges arise unexpectedly, making it well-suited for real-world applications.
>
---
#### [replaced 008] AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents
- **分类: cs.AI; cs.CL; cs.CY; cs.LG; I.2.7; I.2.11; K.4.1; I.2.6**

- **链接: [http://arxiv.org/pdf/2506.04018v2](http://arxiv.org/pdf/2506.04018v2)**

> **作者:** Akshat Naik; Patrick Quinn; Guillermo Bosch; Emma Gouné; Francisco Javier Campos Zabala; Jason Ross Brown; Edward James Young
>
> **备注:** Prepint, under review for NeurIPS 2025
>
> **摘要:** As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. While prior research has studied agents' ability to produce harmful outputs or follow malicious instructions, it remains unclear how likely agents are to spontaneously pursue unintended goals in realistic deployments. In this work, we approach misalignment as a conflict between the internal goals pursued by the model and the goals intended by its deployer. We introduce a misalignment propensity benchmark, \textsc{AgentMisalignment}, a benchmark suite designed to evaluate the propensity of LLM agents to misalign in realistic scenarios. Evaluations cover behaviours such as avoiding oversight, resisting shutdown, sandbagging, and power-seeking. Testing frontier models, we find that more capable agents tend to exhibit higher misalignment on average. We also systematically vary agent personalities through different system prompts and observe that persona characteristics can strongly and unpredictably influence misalignment, sometimes more than the choice of model itself. Our results reveal the limitations of current alignment methods for autonomous LLM agents and underscore the need to rethink misalignment in realistic deployment settings.
>
---
#### [replaced 009] Fair CCA for Fair Representation Learning: An ADNI Study
- **分类: cs.LG; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.09382v2](http://arxiv.org/pdf/2507.09382v2)**

> **作者:** Bojian Hou; Zhanliang Wang; Zhuoping Zhou; Boning Tong; Zexuan Wang; Jingxuan Bao; Duy Duong-Tran; Qi Long; Li Shen
>
> **摘要:** Canonical correlation analysis (CCA) is a technique for finding correlations between different data modalities and learning low-dimensional representations. As fairness becomes crucial in machine learning, fair CCA has gained attention. However, previous approaches often overlook the impact on downstream classification tasks, limiting applicability. We propose a novel fair CCA method for fair representation learning, ensuring the projected features are independent of sensitive attributes, thus enhancing fairness without compromising accuracy. We validate our method on synthetic data and real-world data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating its ability to maintain high correlation analysis performance while improving fairness in classification tasks. Our work enables fair machine learning in neuroimaging studies where unbiased analysis is essential. Code is available in https://github.com/ZhanliangAaronWang/FR-CCA-ADNI.
>
---
#### [replaced 010] Synthetic Census Data Generation via Multidimensional Multiset Sum
- **分类: cs.CY; cs.CR; cs.DS**

- **链接: [http://arxiv.org/pdf/2404.10095v2](http://arxiv.org/pdf/2404.10095v2)**

> **作者:** Cynthia Dwork; Kristjan Greenewald; Manish Raghavan
>
> **摘要:** The US Decennial Census provides valuable data for both research and policy purposes. Census data are subject to a variety of disclosure avoidance techniques prior to release in order to preserve respondent confidentiality. While many are interested in studying the impacts of disclosure avoidance methods on downstream analyses, particularly with the introduction of differential privacy in the 2020 Decennial Census, these efforts are limited by a critical lack of data: The underlying "microdata," which serve as necessary input to disclosure avoidance methods, are kept confidential. In this work, we aim to address this limitation by providing tools to generate synthetic microdata solely from published Census statistics, which can then be used as input to any number of disclosure avoidance algorithms for the sake of evaluation and carrying out comparisons. We define a principled distribution over microdata given published Census statistics and design algorithms to sample from this distribution. We formulate synthetic data generation in this context as a knapsack-style combinatorial optimization problem and develop novel algorithms for this setting. While the problem we study is provably hard, we show empirically that our methods work well in practice, and we offer theoretical arguments to explain our performance. Finally, we verify that the data we produce are "close" to the desired ground truth.
>
---
#### [replaced 011] Whose Journey Matters? Investigating Identity Biases in Large Language Models (LLMs) for Travel Planning Assistance
- **分类: cs.AI; cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2410.17333v2](http://arxiv.org/pdf/2410.17333v2)**

> **作者:** Ruiping Ren; Xing Yao; Shu Cole; Haining Wang
>
> **摘要:** As large language models (LLMs) become increasingly integral to the hospitality and tourism industry, concerns about their fairness in serving diverse identity groups persist. Grounded in social identity theory and sociotechnical systems theory, this study examines ethnic and gender biases in travel recommendations generated by LLMs. Using fairness probing, we analyze outputs from three leading open-source LLMs. The results show that test accuracy for both ethnicity and gender classifiers exceed random chance. Analysis of the most influential features reveals the presence of stereotype bias in LLM-generated recommendations. We also found hallucinations among these features, occurring more frequently in recommendations for minority groups. These findings indicate that LLMs exhibit ethnic and gender bias when functioning as travel planning assistants. This study underscores the need for bias mitigation strategies to improve the inclusivity and reliability of generative AI-driven travel planning assistance.
>
---
#### [replaced 012] An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection
- **分类: cs.CR; cs.AI; cs.CL; cs.CY; C.2.0; I.2.7; K.4.1; H.3.3**

- **链接: [http://arxiv.org/pdf/2509.06920v2](http://arxiv.org/pdf/2509.06920v2)**

> **作者:** Haywood Gelman; John D. Hastings; David Kenley
>
> **备注:** 6 pages, 5 figures, 5 tables
>
> **摘要:** Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including accuracy, precision, recall, F1, specificity, FAR, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection.
>
---
