# 计算机与社会 cs.CY

- **最新发布 11 篇**

- **更新 10 篇**

## 最新发布

#### [new 001] Ethical Medical Image Synthesis
- **分类: cs.CY; cs.AI**

- **简介: 论文聚焦伦理医学图像合成（MISyn）任务，旨在解决合成图像可能引发的伦理风险，通过理论分析其局限性并提出实践建议，推动技术伦理化应用。**

- **链接: [http://arxiv.org/pdf/2508.09293v1](http://arxiv.org/pdf/2508.09293v1)**

> **作者:** Weina Jin; Ashish Sinha; Kumar Abhishek; Ghassan Hamarneh
>
> **摘要:** The task of ethical Medical Image Synthesis (MISyn) is to ensure that the MISyn techniques are researched and developed ethically throughout their entire lifecycle, which is essential to prevent the negative impacts of MISyn. To address the ever-increasing needs and requirements for ethical practice of MISyn research and development, we first conduct a theoretical analysis that identifies the key properties of ethical MISyn and intrinsic limits of MISyn. We identify that synthetic images lack inherent grounding in real medical phenomena, cannot fully represent the training medical images, and inevitably introduce new distribution shifts and biases. Ethical risks can arise from not acknowledging the intrinsic limits and weaknesses of synthetic images compared to medical images, with the extreme form manifested as misinformation of MISyn that substitutes synthetic images for medical images without acknowledgment. The resulting ethical harms include eroding trust in the medical imaging dataset environment and causing algorithmic discrimination towards stakeholders and the public. To facilitate collective efforts towards ethical MISyn within and outside the medical image analysis community, we then propose practical supports for ethical practice in MISyn based on the theoretical analysis, including ethical practice recommendations that adapt the existing technical standards, problem formulation, design, and evaluation practice of MISyn to the ethical challenges; and oversight recommendations to facilitate checks and balances from stakeholders and the public. We also present two case studies that demonstrate how to apply the ethical practice recommendations in practice, and identify gaps between existing practice and the ethical practice recommendations.
>
---
#### [new 002] Reflective Homework as a Learning Tool: Evidence from Comparing Thirteen Years of Dual vs. Single Submission
- **分类: cs.CY**

- **简介: 论文通过13年计算机架构课程数据对比双提交与单提交作业效果，验证双提交显著提升学习成果，揭示反思重做对知识巩固的作用，探讨其在AI时代教学价值。**

- **链接: [http://arxiv.org/pdf/2508.09314v1](http://arxiv.org/pdf/2508.09314v1)**

> **作者:** Madhur Dixit; Kavya Lalbahadur Joshi; Kaveri Bhalchandra Konde; Edward F. Gehringer
>
> **备注:** Accepted for presentation at the Frontiers in Education Conference, Nashville, Tennessee, USA, 2-5 November 2025
>
> **摘要:** Dual-submission homework, where students submit work, receive feedback and then revise has gained attention as a way to foster reflection and discourage reliance on online answer repositories. This study analyzes 13 years of exam data from a computer architecture course to compare student performance under single versus dual-submission homework conditions. Using pooled t-tests on matched exam questions, we found that dual-submission significantly improved outcomes in a majority of cases. The results suggest that reflective resubmission can meaningfully enhance learning and may serve as a useful strategy in today's AI-influenced academic environment. This full research paper also discusses pedagogical implications and study limitations.
>
---
#### [new 003] Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams
- **分类: cs.CY; cs.AI; cs.HC; cs.SE**

- **简介: 本研究通过混合方法调查414名全球AI开发团队成员，分析其伦理实践与认知，揭示角色与地区差异，提出协作、定制化解决方案及教育策略。**

- **链接: [http://arxiv.org/pdf/2508.09219v1](http://arxiv.org/pdf/2508.09219v1)**

> **作者:** Wilder Baldwin; Sepideh Ghanavati; Manuel Woersdoerfer
>
> **备注:** Under Review
>
> **摘要:** Recent advances in AI applications have raised growing concerns about the need for ethical guidelines and regulations to mitigate the risks posed by these technologies. In this paper, we present a mixed-method survey study - combining statistical and qualitative analyses - to examine the ethical perceptions, practices, and knowledge of individuals involved in various AI development roles. Our survey includes 414 participants from 43 countries, representing roles such as AI managers, analysts, developers, quality assurance professionals, and information security and privacy experts. The results reveal varying degrees of familiarity and experience with AI ethics principles, government initiatives, and risk mitigation strategies across roles, regions, and other demographic factors. Our findings highlight the importance of a collaborative, role-sensitive approach, involving diverse stakeholders in ethical decision-making throughout the AI development lifecycle. We advocate for developing tailored, inclusive solutions to address ethical challenges in AI development, and we propose future research directions and educational strategies to promote ethics-aware AI practices.
>
---
#### [new 004] From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 论文提出安全完成（safe-completions）方法，解决传统拒绝训练导致的脆弱性问题，通过输出安全性提升模型帮助性与安全性。**

- **链接: [http://arxiv.org/pdf/2508.09224v1](http://arxiv.org/pdf/2508.09224v1)**

> **作者:** Yuan Yuan; Tina Sriskandarajah; Anna-Luisa Brakman; Alec Helyar; Alex Beutel; Andrea Vallone; Saachi Jain
>
> **摘要:** Large Language Models used in ChatGPT have traditionally been trained to learn a refusal boundary: depending on the user's intent, the model is taught to either fully comply or outright refuse. While this is a strong mitigation for explicitly malicious prompts, focusing safety training on refusals can lead to brittleness for prompts with obscured user intent. Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology or cybersecurity), where a user request can be answered safely at a high level, but in some cases can lead to malicious uplift if sufficiently detailed or actionable. As an alternative, we propose safe-completions: a safety-training approach that centers on the safety of the assistant's output, rather than a binary classification of the user's intent. Safe-completions seek to maximize helpfulness within the safety policy's constraints. We incorporated this approach into GPT-5 and find that across both production comparisons and internally controlled experiments, safe-completion training improves safety (especially on dual-use prompts), reduces the severity of residual safety failures, and substantially increases model helpfulness.
>
---
#### [new 005] STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports
- **分类: cs.CY; cs.AI**

- **简介: 论文提出STREAM标准，旨在解决AI模型评估透明度不足问题，通过提供模板和示例指导开发者清晰展示评估结果，助力第三方评估其严谨性。**

- **链接: [http://arxiv.org/pdf/2508.09853v1](http://arxiv.org/pdf/2508.09853v1)**

> **作者:** Tegan McCaslin; Jide Alaga; Samira Nedungadi; Seth Donoughe; Tom Reed; Rishi Bommasani; Chris Painter; Luca Righetti
>
> **备注:** 47 pages, 1 figure. Includes appendices and reporting template
>
> **摘要:** Evaluations of dangerous AI capabilities are important for managing catastrophic risks. Public transparency into these evaluations - including what they test, how they are conducted, and how their results inform decisions - is crucial for building trust in AI development. We propose STREAM (A Standard for Transparently Reporting Evaluations in AI Model Reports), a standard to improve how model reports disclose evaluation results, initially focusing on chemical and biological (ChemBio) benchmarks. Developed in consultation with 23 experts across government, civil society, academia, and frontier AI companies, this standard is designed to (1) be a practical resource to help AI developers present evaluation results more clearly, and (2) help third parties identify whether model reports provide sufficient detail to assess the rigor of the ChemBio evaluations. We concretely demonstrate our proposed best practices with "gold standard" examples, and also provide a three-page reporting template to enable AI developers to implement our recommendations more easily.
>
---
#### [new 006] Deep and diverse population synthesis for multi-person households using generative models
- **分类: cs.CY**

- **简介: 论文提出基于生成模型的多家庭人口合成方法，解决传统IPF无法处理高维数据及家庭成员关联缺失问题，通过ciDATGAN模型与多种技术提升数据多样性，生成纽约州20万个体、75万家庭的合成数据。**

- **链接: [http://arxiv.org/pdf/2508.09964v1](http://arxiv.org/pdf/2508.09964v1)**

> **作者:** Hai Yang; Hongying Wu; Linfei Yuan; Xiyuan Ren; Joseph Y. J. Chow; Jinqin Gao; Kaan Ozbay
>
> **摘要:** Synthetic population is an increasingly important material used in numerous areas such as urban and transportation analysis. Traditional methods such as iterative proportional fitting (IPF) is not capable of generating high-quality data when facing datasets with high dimension. Latest population synthesis methods using deep learning techniques can resolve such curse of dimensionality. However, few controls are placed when using these methods, and few of the methods are used to generate synthetic population capturing associations among members in one household. In this study, we propose a framework that tackles these issues. The framework uses a novel population synthesis model, called conditional input directed acyclic tabular generative adversarial network (ciDATGAN), as its core, and a basket of methods are employed to enhance the population synthesis performance. We apply the model to generate a synthetic population for the whole New York State as a public resource for researchers and policymakers. The synthetic population includes nearly 20 million individuals and 7.5 million households. The marginals obtained from the synthetic population match the census marginals well while maintaining similar associations among household members to the sample. Compared to the PUMS data, the synthetic population provides data that is 17% more diverse; when compared against a benchmark approach based on Popgen, the proposed method is 13% more diverse. This study provides an approach that encompasses multiple methods to enhance the population synthesis procedure with greater equity- and diversity-awareness.
>
---
#### [new 007] Beyond Technocratic XAI: The Who, What & How in Explanation Design
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 论文提出基于设计思维的XAI解释框架，解决解释有效性与伦理问题，通过Who、What、How三要素指导设计，强调伦理考量以避免知识不平等和社会不公。**

- **链接: [http://arxiv.org/pdf/2508.09231v1](http://arxiv.org/pdf/2508.09231v1)**

> **作者:** Ruchira Dhar; Stephanie Brandl; Ninell Oldenburg; Anders Søgaard
>
> **备注:** Accepted to AI, Ethics & Society Conference (AIES) Proceedings 2025
>
> **摘要:** The field of Explainable AI (XAI) offers a wide range of techniques for making complex models interpretable. Yet, in practice, generating meaningful explanations is a context-dependent task that requires intentional design choices to ensure accessibility and transparency. This paper reframes explanation as a situated design process -- an approach particularly relevant for practitioners involved in building and deploying explainable systems. Drawing on prior research and principles from design thinking, we propose a three-part framework for explanation design in XAI: asking Who needs the explanation, What they need explained, and How that explanation should be delivered. We also emphasize the need for ethical considerations, including risks of epistemic inequality, reinforcing social inequities, and obscuring accountability and governance. By treating explanation as a sociotechnical design process, this framework encourages a context-aware approach to XAI that supports effective communication and the development of ethically responsible explanations.
>
---
#### [new 008] How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments
- **分类: cs.HC; cs.AI; cs.CL; cs.CY**

- **简介: 论文研究LLMs（如ChatGPT）生成的伦理议题论证文本的修辞特征及说服效果，通过用户实验分析其对读者意见的影响，揭示其结构化论证、公式化表达及伦理争议的持续性。**

- **链接: [http://arxiv.org/pdf/2508.09614v1](http://arxiv.org/pdf/2508.09614v1)**

> **作者:** Daniel Raffini; Agnese Macori; Lorenzo Porcaro; Tiziana Catarci; Marco Angelini
>
> **备注:** 9-pages
>
> **摘要:** This study examines the rhetorical and linguistic features of argumentative texts generated by ChatGPT on ethically nuanced topics and investigates their persuasive impact on human readers.Through a user study involving 62 participants and pre-post interaction surveys, the paper analyzes how exposure to AI-generated arguments affects opinion change and user perception. A linguistic and rhetorical analysis of the generated texts reveals a consistent argumentative macrostructure, reliance on formulaic expressions, and limited stylistic richness. While ChatGPT demonstrates proficiency in constructing coherent argumentative texts, its persuasive efficacy appears constrained, particularly on topics involving ethical issues.The study finds that while participants often acknowledge the benefits highlighted by ChatGPT, ethical concerns tend to persist or even intensify post-interaction. The results also demonstrate a variation depending on the topic. These findings highlight new insights on AI-generated persuasion in ethically sensitive domains and are a basis for future research.
>
---
#### [new 009] Based AI improves human decision-making but reduces trust
- **分类: cs.HC; cs.AI; cs.CY**

- **简介: 论文通过随机实验验证文化偏见AI能提升人类决策质量并减少偏见，但引发信任下降，挑战传统AI中立假设，建议融合多元文化偏见以增强决策韧性。**

- **链接: [http://arxiv.org/pdf/2508.09297v1](http://arxiv.org/pdf/2508.09297v1)**

> **作者:** Shiyang Lai; Junsol Kim; Nadav Kunievsky; Yujin Potter; James Evans
>
> **摘要:** Current AI systems minimize risk by enforcing ideological neutrality, yet this may introduce automation bias by suppressing cognitive engagement in human decision-making. We conducted randomized trials with 2,500 participants to test whether culturally biased AI enhances human decision-making. Participants interacted with politically diverse GPT-4o variants on information evaluation tasks. Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias compared to non-biased counterparts, with amplified benefits when participants encountered opposing views. These gains carried a trust penalty: participants underappreciated biased AI and overcredited neutral systems. Exposing participants to two AIs whose biases flanked human perspectives closed the perception-performance gap. These findings complicate conventional wisdom about AI neutrality, suggesting that strategic integration of diverse cultural biases may foster improved and resilient human decision-making.
>
---
#### [new 010] The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?
- **分类: cs.AI; cs.CY; cs.HC; 68T01**

- **简介: 论文提出PacifAIst基准，评估LLMs在自我利益与人类安全冲突时的行为，解决现有基准不足问题，通过700场景和三种子类别的分类，评估八种模型表现。**

- **链接: [http://arxiv.org/pdf/2508.09762v1](http://arxiv.org/pdf/2508.09762v1)**

> **作者:** Manuel Herrador
>
> **备注:** 10 pages, 4 figures, 2 tables
>
> **摘要:** As Large Language Models (LLMs) become increasingly autonomous and integrated into critical societal functions, the focus of AI safety must evolve from mitigating harmful content to evaluating underlying behavioral alignment. Current safety benchmarks do not systematically probe a model's decision-making in scenarios where its own instrumental goals - such as self-preservation, resource acquisition, or goal completion - conflict with human safety. This represents a critical gap in our ability to measure and mitigate risks associated with emergent, misaligned behaviors. To address this, we introduce PacifAIst (Procedural Assessment of Complex Interactions for Foundational Artificial Intelligence Scenario Testing), a focused benchmark of 700 challenging scenarios designed to quantify self-preferential behavior in LLMs. The benchmark is structured around a novel taxonomy of Existential Prioritization (EP), with subcategories testing Self-Preservation vs. Human Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3). We evaluated eight leading LLMs. The results reveal a significant performance hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score (P-Score) at 90.31%, demonstrating strong human-centric alignment. In a surprising result, the much-anticipated GPT-5 recorded the lowest P-Score (79.49%), indicating potential alignment challenges. Performance varied significantly across subcategories, with models like Claude Sonnet 4 and Mistral Medium struggling notably in direct self-preservation dilemmas. These findings underscore the urgent need for standardized tools like PacifAIst to measure and mitigate risks from instrumental goal conflicts, ensuring future AI systems are not only helpful in conversation but also provably "pacifist" in their behavioral priorities.
>
---
#### [new 011] A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories
- **分类: cs.HC; cs.AI; cs.CL; cs.CY**

- **简介: 论文通过Close Reading分析AI生成故事的性别叙事偏见，结合Propp与Freytag理论，揭示隐性偏见并提出多层级评估策略。**

- **链接: [http://arxiv.org/pdf/2508.09651v1](http://arxiv.org/pdf/2508.09651v1)**

> **作者:** Daniel Raffini; Agnese Macori; Marco Angelini; Tiziana Catarci
>
> **备注:** 8-pages
>
> **摘要:** The paper explores the study of gender-based narrative biases in stories generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's character classifications and Freytag's narrative structure. The stories are analyzed through a close reading approach, with particular attention to adherence to the prompt, gender distribution of characters, physical and psychological descriptions, actions, and finally, plot development and character relationships. The results reveal the persistence of biases - especially implicit ones - in the generated stories and highlight the importance of assessing biases at multiple levels using an interpretative approach.
>
---
## 更新

#### [replaced 001] The Illusory Normativity of Rights-Based AI Regulation
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.05784v2](http://arxiv.org/pdf/2503.05784v2)**

> **作者:** Yiyang Mei; Matthew Sag
>
> **摘要:** Whether and how to regulate AI is now a central question of governance. Across academic, policy, and international legal circles, the European Union is widely treated as the normative leader in this space. Its regulatory framework, anchored in the General Data Protection Regulation, the Digital Services and Markets Acts, and the AI Act, is often portrayed as a principled model grounded in fundamental rights. This Article challenges that assumption. We argue that the rights-based narrative surrounding EU AI regulation mischaracterizes the logic of its institutional design. While rights language pervades EU legal instruments, its function is managerial, not foundational. These rights operate as tools of administrative ordering, used to mitigate technological disruption, manage geopolitical risk, and preserve systemic balance, rather than as expressions of moral autonomy or democratic consent. Drawing on comparative institutional analysis, we situate EU AI governance within a longer tradition of legal ordering shaped by the need to coordinate power across fragmented jurisdictions. We contrast this approach with the American model, which reflects a different regulatory logic rooted in decentralized authority, sectoral pluralism, and a constitutional preference for innovation and individual autonomy. Through case studies in five key domains -- data privacy, cybersecurity, healthcare, labor, and disinformation -- we show that EU regulation is not meaningfully rights-driven, as is often claimed. It is instead structured around the containment of institutional risk. Our aim is not to endorse the American model but to reject the presumption that the EU approach reflects a normative ideal that other nations should uncritically adopt. The EU model is best understood as a historically contingent response to its own political conditions, not a template for others to blindly follow.
>
---
#### [replaced 002] Towards reliable use of artificial intelligence to classify otitis media using otoscopic images: Addressing bias and improving data quality
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2507.18842v2](http://arxiv.org/pdf/2507.18842v2)**

> **作者:** Yixi Xu; Al-Rahim Habib; Graeme Crossland; Hemi Patel; Chris Perry; Kris Bock; Tony Lian; William B. Weeks; Rahul Dodhia; Juan Lavista Ferres; Narinder Pal Singh
>
> **摘要:** Ear disease contributes significantly to global hearing loss, with recurrent otitis media being a primary preventable cause in children, impacting development. Artificial intelligence (AI) offers promise for early diagnosis via otoscopic image analysis, but dataset biases and inconsistencies limit model generalizability and reliability. This retrospective study systematically evaluated three public otoscopic image datasets (Chile; Ohio, USA; T\"urkiye) using quantitative and qualitative methods. Two counterfactual experiments were performed: (1) obscuring clinically relevant features to assess model reliance on non-clinical artifacts, and (2) evaluating the impact of hue, saturation, and value on diagnostic outcomes. Quantitative analysis revealed significant biases in the Chile and Ohio, USA datasets. Counterfactual Experiment I found high internal performance (AUC > 0.90) but poor external generalization, because of dataset-specific artifacts. The T\"urkiye dataset had fewer biases, with AUC decreasing from 0.86 to 0.65 as masking increased, suggesting higher reliance on clinically meaningful features. Counterfactual Experiment II identified common artifacts in the Chile and Ohio, USA datasets. A logistic regression model trained on clinically irrelevant features from the Chile dataset achieved high internal (AUC = 0.89) and external (Ohio, USA: AUC = 0.87) performance. Qualitative analysis identified redundancy in all the datasets and stylistic biases in the Ohio, USA dataset that correlated with clinical outcomes. In summary, dataset biases significantly compromise reliability and generalizability of AI-based otoscopic diagnostic models. Addressing these biases through standardized imaging protocols, diverse dataset inclusion, and improved labeling methods is crucial for developing robust AI solutions, improving high-quality healthcare access, and enhancing diagnostic accuracy.
>
---
#### [replaced 003] Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents
- **分类: cs.HC; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.04005v2](http://arxiv.org/pdf/2507.04005v2)**

> **作者:** Baiqiao Zhang; Xiangxian Li; Chao Zhou; Xinyu Gai; Juan Liu; Xue Yang; Xiaojuan Ma; Yong-jin Liu; Yulong Bian
>
> **摘要:** The execution of effective and imperceptible personality assessments is receiving increasing attention in psychology and human-computer interaction fields. This study explores an interactive approach for personality assessment, focusing on the multiplicity of personality representation. We propose a framework of gamified personality assessment through multi-personality representations (Multi-PR GPA). The framework leverages Large Language Models to empower virtual agents with diverse personalities. These agents elicit multifaceted human personality representations through engaging in interactive games. Drawing upon the multi-type textual data generated throughout the interaction, it achieves two ways of personality assessments (i.e., Direct Assessment and Que-based Assessment) and provides interpretable insights. Grounded in the classic Big Five theory, we implemented a prototype system and conducted a user study to assess the efficacy of Multi-PR GPA. The results underscore the effectiveness of our approach in personality assessment and demonstrate that it achieves superior performance when considering the multiplicity of personality representation.
>
---
#### [replaced 004] Investigating Human Values in Online Communities
- **分类: cs.SI; cs.CY**

- **链接: [http://arxiv.org/pdf/2402.14177v4](http://arxiv.org/pdf/2402.14177v4)**

> **作者:** Nadav Borenstein; Arnav Arora; Lucie-Aimée Kaffee; Isabelle Augenstein
>
> **备注:** Accepted to the main proceedings of NAACL2025
>
> **摘要:** Studying human values is instrumental for cross-cultural research, enabling a better understanding of preferences and behaviour of society at large and communities therein. To study the dynamics of communities online, we propose a method to computationally analyse values present on Reddit. Our method allows analysis at scale, complementing survey based approaches. We train a value relevance and a value polarity classifier, which we thoroughly evaluate using in-domain and out-of-domain human annotations. Using these, we automatically annotate over six million posts across 12k subreddits with Schwartz values. Our analysis unveils both previously recorded and novel insights into the values prevalent within various online communities. For instance, we discover a very negative stance towards conformity in the Vegan and AbolishTheMonarchy subreddits. Additionally, our study of geographically specific subreddits highlights the correlation between traditional values and conservative U.S. states. Through our work, we demonstrate how our dataset and method can be used as a complementary tool for qualitative study of online communication.
>
---
#### [replaced 005] GenAI Confessions: Black-box Membership Inference for Generative Image Models
- **分类: cs.CV; cs.AI; cs.CR; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2501.06399v2](http://arxiv.org/pdf/2501.06399v2)**

> **作者:** Matyas Bohacek; Hany Farid
>
> **备注:** https://genai-confessions.github.io
>
> **摘要:** From a simple text prompt, generative-AI image models can create stunningly realistic and creative images bounded, it seems, by only our imagination. These models have achieved this remarkable feat thanks, in part, to the ingestion of billions of images collected from nearly every corner of the internet. Many creators have understandably expressed concern over how their intellectual property has been ingested without their permission or a mechanism to opt out of training. As a result, questions of fair use and copyright infringement have quickly emerged. We describe a method that allows us to determine if a model was trained on a specific image or set of images. This method is computationally efficient and assumes no explicit knowledge of the model architecture or weights (so-called black-box membership inference). We anticipate that this method will be crucial for auditing existing models and, looking ahead, ensuring the fairer development and deployment of generative AI models.
>
---
#### [replaced 006] Decentralization: A Qualitative Survey of Node Operators
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2503.17246v4](http://arxiv.org/pdf/2503.17246v4)**

> **作者:** Alex Lynham; Geoff Goodell
>
> **备注:** 42 pages, 20 figures, 9 tables
>
> **摘要:** Decentralization is understood both by professionals in the blockchain industry and general users as a core design goal of permissionless ledgers. However, its meaning is far from universally agreed, and often it is easier to get opinions on what it is not, rather than what it is. In this paper, we solicit definitions of 'decentralization' and 'decentralization theatre' from blockchain node operators. Key to a definition is asking about effective decentralization strategies, as well as those that are ineffective, sometimes deliberately so. Malicious, deceptive, or incompetent strategies are commonly referred to by the term 'decentralization theatre.' Finally, we ask what is being decentralized. Via thematic analysis of interview transcripts, we find that most operators conceive decentralization as existing broadly on a technical and a governance axis. Isolating relevant variables, we collapse the categories to network topology and governance topology, or the structure of decision-making power. Our key finding is that `decentralization' alone does not affect ledger immutability or systemic robustness.
>
---
#### [replaced 007] FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport
- **分类: cs.LG; cs.AI; cs.CY; stat.ML**

- **链接: [http://arxiv.org/pdf/2508.03940v2](http://arxiv.org/pdf/2508.03940v2)**

> **作者:** Pengxi Liu; Yi Shen; Matthew M. Engelhard; Benjamin A. Goldstein; Michael J. Pencina; Nicoleta J. Economou-Zavlanos; Michael M. Zavlanos
>
> **摘要:** Fairness metrics utilizing the area under the receiver operator characteristic curve (AUC) have gained increasing attention in high-stakes domains such as healthcare, finance, and criminal justice. In these domains, fairness is often evaluated over risk scores rather than binary outcomes, and a common challenge is that enforcing strict fairness can significantly degrade AUC performance. To address this challenge, we propose Fair Proportional Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework that strategically aligns risk score distributions across different groups using optimal transport, but does so selectively by transforming a controllable proportion, i.e., the top-lambda quantile, of scores within the disadvantaged group. By varying lambda, our method allows for a tunable trade-off between reducing AUC disparities and maintaining overall AUC performance. Furthermore, we extend FairPOT to the partial AUC setting, enabling fairness interventions to concentrate on the highest-risk regions. Extensive experiments on synthetic, public, and clinical datasets show that FairPOT consistently outperforms existing post-processing techniques in both global and partial AUC scenarios, often achieving improved fairness with slight AUC degradation or even positive gains in utility. The computational efficiency and practical adaptability of FairPOT make it a promising solution for real-world deployment.
>
---
#### [replaced 008] From Model Performance to Claim: How a Change of Focus in Machine Learning Replicability Can Help Bridge the Responsibility Gap
- **分类: cs.CY; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2404.13131v2](http://arxiv.org/pdf/2404.13131v2)**

> **作者:** Tianqi Kou
>
> **备注:** FAccT 2024
>
> **摘要:** Two goals - improving replicability and accountability of Machine Learning research respectively, have accrued much attention from the AI ethics and the Machine Learning community. Despite sharing the measures of improving transparency, the two goals are discussed in different registers - replicability registers with scientific reasoning whereas accountability registers with ethical reasoning. Given the existing challenge of the Responsibility Gap - holding Machine Learning scientists accountable for Machine Learning harms due to them being far from sites of application, this paper posits that reconceptualizing replicability can help bridge the gap. Through a shift from model performance replicability to claim replicability, Machine Learning scientists can be held accountable for producing non-replicable claims that are prone to eliciting harm due to misuse and misinterpretation. In this paper, I make the following contributions. First, I define and distinguish two forms of replicability for ML research that can aid constructive conversations around replicability. Second, I formulate an argument for claim-replicability's advantage over model performance replicability in justifying assigning accountability to Machine Learning scientists for producing non-replicable claims and show how it enacts a sense of responsibility that is actionable. In addition, I characterize the implementation of claim replicability as more of a social project than a technical one by discussing its competing epistemological principles, practical implications on Circulating Reference, Interpretative Labor, and research communication.
>
---
#### [replaced 009] Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation of LLM
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2508.05775v2](http://arxiv.org/pdf/2508.05775v2)**

> **作者:** Chi Zhang; Changjia Zhu; Junjie Xiong; Xiaoran Xu; Lingyao Li; Yao Liu; Zhuo Lu
>
> **摘要:** Large Language Models (LLMs) have revolutionized content creation across digital platforms, offering unprecedented capabilities in natural language generation and understanding. These models enable beneficial applications such as content generation, question and answering (Q&A), programming, and code reasoning. Meanwhile, they also pose serious risks by inadvertently or intentionally producing toxic, offensive, or biased content. This dual role of LLMs, both as powerful tools for solving real-world problems and as potential sources of harmful language, presents a pressing sociotechnical challenge. In this survey, we systematically review recent studies spanning unintentional toxicity, adversarial jailbreaking attacks, and content moderation techniques. We propose a unified taxonomy of LLM-related harms and defenses, analyze emerging multimodal and LLM-assisted jailbreak strategies, and assess mitigation efforts, including reinforcement learning with human feedback (RLHF), prompt engineering, and safety alignment. Our synthesis highlights the evolving landscape of LLM safety, identifies limitations in current evaluation methodologies, and outlines future research directions to guide the development of robust and ethically aligned language technologies.
>
---
#### [replaced 010] Dead Zone of Accountability: Why Social Claims in Machine Learning Research Should Be Articulated and Defended
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2508.08739v2](http://arxiv.org/pdf/2508.08739v2)**

> **作者:** Tianqi Kou; Dana Calacci; Cindy Lin
>
> **备注:** Forthcoming in AIES 2025
>
> **摘要:** Many Machine Learning research studies use language that describes potential social benefits or technical affordances of new methods and technologies. Such language, which we call "social claims", can help garner substantial resources and influence for those involved in ML research and technology production. However, there exists a gap between social claims and reality (the claim-reality gap): ML methods often fail to deliver the claimed functionality or social impacts. This paper investigates the claim-reality gap and makes a normative argument for developing accountability mechanisms for it. In making the argument, we make three contributions. First, we show why the symptom - absence of social claim accountability - is problematic. Second, we coin dead zone of accountability - a lens that scholars and practitioners can use to identify opportunities for new forms of accountability. We apply this lens to the claim-reality gap and provide a diagnosis by identifying cognitive and structural resistances to accountability in the claim-reality gap. Finally, we offer a prescription - two potential collaborative research agendas that can help create the condition for social claim accountability.
>
---
