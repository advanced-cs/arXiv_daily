# 音频 cs.SD;  eess.SP

- **最新发布 31 篇**

- **更新 10 篇**

## 最新发布

#### [new 001] SC-TSE: Speaker Consistency-Aware Target Speaker Extraction
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于目标说话人提取（TSE）任务，旨在解决音频中目标语音提取时的说话人身份混淆问题。作者提出了一种基于中心点的说话人一致性损失，并引入条件损失抑制机制，提升了TSE系统的性能。**

- **链接: [http://arxiv.org/pdf/2507.09510v1](http://arxiv.org/pdf/2507.09510v1)**

> **作者:** Shu Wu; Anbin Qi; Yanzhang Xie; Xiang Xie
>
> **备注:** Accept to Interspeech2025
>
> **摘要:** Target Speaker Extraction (TSE) uses a reference cue to extract the target speech from a mixture. In TSE systems relying on audio cues, the speaker embedding from the enrolled speech is crucial to performance. However, these embeddings may suffer from speaker identity confusion. Unlike previous studies that focus on improving speaker embedding extraction, we improve TSE performance from the perspective of speaker consistency. In this paper, we propose a speaker consistency-aware target speaker extraction method that incorporates a centroid-based speaker consistency loss. This approach enhances TSE performance by ensuring speaker consistency between the enrolled and extracted speech. In addition, we integrate conditional loss suppression into the training process. The experimental results validate the effectiveness of our proposed methods in advancing the TSE performance. A speech demo is available online.\footnote{https://sc-tse.netlify.app/
>
---
#### [new 002] Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM Generative Error Correction for Accented Speech Recognition
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于语音识别任务，旨在解决口音语音识别中准确率下降的问题。通过引入多模态和多粒度的生成错误纠正方法，并结合LoRA专家混合策略，有效融合发音与语义信息，显著提升了识别准确率。**

- **链接: [http://arxiv.org/pdf/2507.09116v1](http://arxiv.org/pdf/2507.09116v1)**

> **作者:** Bingshen Mu; Kun Wei; Pengcheng Guo; Lei Xie
>
> **备注:** IEEE Transactions on Audio, Speech and Language Processing
>
> **摘要:** Despite substantial improvements in ASR, performance tends to degrade when faced with adverse conditions such as speaker accents. Generative error correction (GER) leverages the rich linguistic knowledge and exceptional reasoning ability of LLMs, significantly outperforming typical LM methods. However, it lacks specificity in accented speech scenarios. In this study, we leverage GER to improve the accuracy of transcription predictions by addressing the two primary features of accented speech recognition. To fully leverage pronunciation information, we propose the multi-modal GER, which integrates pronunciation information from the speech modality, and the multi-granularity GER, which incorporates fine-grained phoneme-level information related to pronunciation. These two methods enable the LLM to utilize the pronunciation information of accented speech and the semantic information from word-level hypotheses for accurate transcription predictions through LoRA fine-tuning. On the one hand, we employ a three-stage training strategy to train separate multi-modal GER models for each accent to obtain mono-accent LoRA experts. By adopting our proposed HDMoLE method, which incorporates hierarchical routing and dynamic thresholds within the mixture of LoRA experts, we effectively merge multiple mono-accent LoRA experts within a single multi-modal GER to overcome the challenges posed by accent diversity. On the other hand, multi-granularity GER leverages the N-best word-level and phoneme-level hypotheses generated by the HDMoLE model to predict the final accented speech transcriptions. Experimental results on the multi-accent English dataset demonstrate the efficacy of our proposed methods. Our methods achieve a remarkable relative WER reduction of 67.35% compared to the Whisper-large-v3 baseline.
>
---
#### [new 003] AudioMAE++: learning better masked audio representations with SwiGLU FFNs
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文属于音频表示学习任务，旨在提升自监督音频表征性能。通过引入改进的Transformer模块（SwiGLU FFNs），提出AudioMAE++模型，在多个下游任务中表现出色，并具备良好扩展性。**

- **链接: [http://arxiv.org/pdf/2507.10464v1](http://arxiv.org/pdf/2507.10464v1)**

> **作者:** Sarthak Yadav; Sergios Theodoridis; Zheng-Hua Tan
>
> **备注:** TO APPEAR AT IEEE MLSP 2025
>
> **摘要:** Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged as a prominent approach for learning self-supervised audio representations. While several recent papers have evaluated key aspects of training MAEs on audio data, the majority of these approaches still leverage vanilla transformer building blocks, whereas the transformer community has seen steady integration of newer architectural advancements. In this work, we propose AudioMAE++, a revamped audio masked autoencoder with two such enhancements, namely macaron-style transformer blocks with gated linear units. When pretrained on the AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE based approaches on 10 diverse downstream tasks, demonstrating excellent performance on audio classification and speech-based benchmarks. The proposed AudioMAE++ models also demonstrate excellent scaling characteristics, outperforming directly comparable standard MAE baselines with up to 4x more parameters.
>
---
#### [new 004] Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers
- **分类: cs.SD; cs.CL; eess.AS; I.2.7; I.5.5**

- **简介: 该论文属于语音情感分析任务，旨在解决在保护隐私的前提下，通过匿名化语音数据实现空中交通管制员（ATCO）压力检测的问题。作者评估了多种深度学习架构，在两个匿名化语音数据集上进行了实验，取得了较高准确率，表明隐私保护与模型性能可兼顾。**

- **链接: [http://arxiv.org/pdf/2507.08882v1](http://arxiv.org/pdf/2507.08882v1)**

> **作者:** Janaki Viswanathan; Alexander Blatt; Konrad Hagemann; Dietrich Klakow
>
> **备注:** 8 pages, 2 figures, 4 tables, publication identification number (URN)- urn:nbn:de:101:1-2022122008393409239462, see archived online publication- https://d-nb.info/127614606X/34 & Katalogeintrag: https://d-nb.info/127614606X/
>
> **摘要:** Air traffic control (ATC) demands multi-tasking under time pressure with high consequences of an error. This can induce stress. Detecting stress is a key point in maintaining the high safety standards of ATC. However, processing ATC voice data entails privacy restrictions, e.g. the General Data Protection Regulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with these restrictions. In this paper, different architectures for stress detection for anonymized ATCO speech are evaluated. Our best networks reach a stress detection accuracy of 93.6% on an anonymized version of the Speech Under Simulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our anonymized ATC simulation dataset. This shows that privacy does not have to be an impediment in building well-performing deep-learning-based models.
>
---
#### [new 005] THAI Speech Emotion Recognition (THAI-SER) corpus
- **分类: cs.SD; eess.AS**

- **简介: 该论文构建了首个大规模泰语语音情感识别语料库THAI-SER，包含27,854条语句，涵盖五种情绪。通过众包标注并设计质量控制方案，确保标注质量。评估显示标注一致性与识别准确率较高，训练模型效果良好。语料库公开，推动语音情感识别研究。**

- **链接: [http://arxiv.org/pdf/2507.09618v1](http://arxiv.org/pdf/2507.09618v1)**

> **作者:** Jilamika Wongpithayadisai; Chompakorn Chaksangchaichot; Soravitt Sangnark; Patawee Prakrankamanant; Krit Gangwanpongpun; Siwa Boonpunmongkol; Premmarin Milindasuta; Dangkamon Na-Pombejra; Sarana Nutanong; Ekapol Chuangsuwanich
>
> **摘要:** We present the first sizeable corpus of Thai speech emotion recognition, THAI-SER, containing 41 hours and 36 minutes (27,854 utterances) from 100 recordings made in different recording environments: Zoom and two studio setups. The recordings contain both scripted and improvised sessions, acted by 200 professional actors (112 females and 88 males, aged 18 to 55) and were directed by professional directors. There are five primary emotions: neutral, angry, happy, sad, and frustrated, assigned to the actors when recording utterances. The utterances are annotated with an emotional category using crowdsourcing. To control the annotation process's quality, we also design an extensive filtering and quality control scheme to ensure that the majority agreement score remains above 0.71. We evaluate our annotated corpus using two metrics: inter-annotator reliability and human recognition accuracy. Inter-annotator reliability score was calculated using Krippendorff's alpha, where our corpus, after filtering, achieved an alpha score of 0.692, higher than a recommendation of 0.667. For human recognition accuracy, our corpus scored up to 0.772 post-filtering. We also provide the results of the model trained on the corpus evaluated on both in-corpus and cross-corpus setups. The corpus is publicly available under a Creative Commons BY-SA 4.0, as well as our codes for the experiments.
>
---
#### [new 006] Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering
- **分类: cs.SD; cs.HC; cs.MM; eess.AS; H.5.5**

- **简介: 该论文属于声学建模与虚拟现实音频渲染任务，旨在解决当前工业方法在低频声波传播模拟中的不足。作者实现了基于2D FDTD的波模型，在Unreal Engine中仿真声音传播现象（如衍射、反射等），并生成具有空间方向性的多通道脉冲响应，以提升动态音效的真实感与沉浸体验。**

- **链接: [http://arxiv.org/pdf/2507.09376v1](http://arxiv.org/pdf/2507.09376v1)**

> **作者:** Bilkent Samsurya
>
> **备注:** Accepted to the 50th International Computer Music Conference (ICMC), 2025
>
> **摘要:** Accurate sound propagation simulation is essential for delivering immersive experiences in virtual applications, yet industry methods for acoustic modeling often do not account for the full breadth of acoustic wave phenomena. This paper proposes a novel two-dimensional (2D) finite-difference time-domain (FDTD) framework that simulates sound propagation as a wave-based model in Unreal Engine, with an emphasis on capturing lower frequency wave phenomena, embedding occlusion, diffraction, reflection and interference in generated impulse responses. The process begins by discretizing the scene geometry into a 2D grid via a top-down projection from which obstacle masks and boundary conditions are derived. A Python-based FDTD solver injects a sine sweep at a source position, and virtual quadraphonic microphone arrays record pressure field responses at pre-defined listener positions. De-convolution of the pressure responses yields multi-channel impulse responses that retain spatial directionality which are then integrated into Unreal Engine's audio pipeline for dynamic playback. Benchmark tests confirm agreement with analytical expectations, and the paper outlines hybrid extensions aimed at commercial viability.
>
---
#### [new 007] BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct Speech-to-Speech Translation Corpus
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于语音到语音翻译任务，旨在解决英语到约鲁巴语低资源语言对数据短缺问题。作者构建了BENYO-S2ST-Corpus-1，结合已有语料与生成音频，提出音频增强算法AcoustAug，并用于训练预训练模型YoruTTS-0.5，推动多语言非洲语言翻译研究。**

- **链接: [http://arxiv.org/pdf/2507.09342v1](http://arxiv.org/pdf/2507.09342v1)**

> **作者:** Emmanuel Adetiba; Abdultaofeek Abayomi; Raymond J. Kala; Ayodele H. Ifijeh; Oluwatobi E. Dare; Olabode Idowu-Bismark; Gabriel O. Sobola; Joy N. Adetiba; Monsurat Adepeju Lateef; Heather Cole-Lewis
>
> **摘要:** There is a major shortage of Speech-to-Speech Translation (S2ST) datasets for high resource-to-low resource language pairs such as English-to-Yoruba. Thus, in this study, we curated the Bilingual English-to-Yoruba Speech-to-Speech Translation Corpus Version 1 (BENYO-S2ST-Corpus-1). The corpus is based on a hybrid architecture we developed for large-scale direct S2ST corpus creation at reduced cost. To achieve this, we leveraged non speech-to-speech Standard Yoruba (SY) real-time audios and transcripts in the YORULECT Corpus as well as the corresponding Standard English (SE) transcripts. YORULECT Corpus is small scale(1,504) samples, and it does not have paired English audios. Therefore, we generated the SE audios using pre-trained AI models (i.e. Facebook MMS). We also developed an audio augmentation algorithm named AcoustAug based on three latent acoustic features to generate augmented audios from the raw audios of the two languages. BENYO-S2ST-Corpus-1 has 12,032 audio samples per language, which gives a total of 24,064 sample size. The total audio duration for the two languages is 41.20 hours. This size is quite significant. Beyond building S2ST models, BENYO-S2ST-Corpus-1 can be used to build pretrained models or improve existing ones. The created corpus and Coqui framework were used to build a pretrained Yoruba TTS model (named YoruTTS-0.5) as a proof of concept. The YoruTTS-0.5 gave a F0 RMSE value of 63.54 after 1,000 epochs, which indicates moderate fundamental pitch similarity with the reference real-time audio. Ultimately, the corpus architecture in this study can be leveraged by researchers and developers to curate datasets for multilingual high-resource-to-low-resource African languages. This will bridge the huge digital divides in translations among high and low-resource language pairs. BENYO-S2ST-Corpus-1 and YoruTTS-0.5 are publicly available at (https://bit.ly/40bGMwi).
>
---
#### [new 008] Towards Spatial Audio Understanding via Question Answering
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于空间音频理解任务，旨在通过问答范式解决声音事件定位与检测（SELD）向空间场景理解和推理扩展的问题。作者构建了基于STARSS23数据集的细粒度问答数据，并开发了一个以FOA信号和自然语言问题作为输入的基线模型，实现了对声音事件的分类回答。**

- **链接: [http://arxiv.org/pdf/2507.09195v1](http://arxiv.org/pdf/2507.09195v1)**

> **作者:** Parthasaarathy Sudarsanam; Archontis Politis
>
> **摘要:** In this paper, we introduce a novel framework for spatial audio understanding of first-order ambisonic (FOA) signals through a question answering (QA) paradigm, aiming to extend the scope of sound event localization and detection (SELD) towards spatial scene understanding and reasoning. First, we curate and release fine-grained spatio-temporal textual descriptions for the STARSS23 dataset using a rule-based approach, and further enhance linguistic diversity using large language model (LLM)-based rephrasing. We also introduce a QA dataset aligned with the STARSS23 scenes, covering various aspects such as event presence, localization, spatial, and temporal relationships. To increase language variety, we again leverage LLMs to generate multiple rephrasings per question. Finally, we develop a baseline spatial audio QA model that takes FOA signals and natural language questions as input and provides answers regarding various occurrences, temporal, and spatial relationships of sound events in the scene formulated as a classification task. Despite being trained solely with scene-level question answering supervision, our model achieves performance that is comparable to a fully supervised sound event localization and detection model trained with frame-level spatiotemporal annotations. The results highlight the potential of language-guided approaches for spatial audio understanding and open new directions for integrating linguistic supervision into spatial scene analysis.
>
---
#### [new 009] MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients
- **分类: cs.SD; cs.LG; eess.AS**

- **简介: 该论文属于语音增强任务，旨在提升合成房间脉冲响应（RIR）数据集的生态有效性。作者引入多频段吸收系数等特征，并结合Mesh-based RIRs，训练模型后在真实RIR数据上评估性能。结果表明，使用MB-RIRs可显著提升语音增强效果。**

- **链接: [http://arxiv.org/pdf/2507.09750v1](http://arxiv.org/pdf/2507.09750v1)**

> **作者:** Enric Gusó; Joanna Luberadzka; Umut Sayin; Xavier Serra
>
> **备注:** Accepted to WASPAA25
>
> **摘要:** We investigate the effects of four strategies for improving the ecological validity of synthetic room impulse response (RIR) datasets for monoaural Speech Enhancement (SE). We implement three features on top of the traditional image source method-based (ISM) shoebox RIRs: multiband absorption coefficients, source directivity and receiver directivity. We additionally consider mesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3 model for each RIR dataset and evaluate the performance on a test set of real RIRs both objectively and subjectively. We find that RIRs which use frequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain +0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs dataset is publicly available for free download.
>
---
#### [new 010] Evaluating Fake Music Detection Performance Under Audio Augmentations
- **分类: cs.SD; cs.AI; cs.LG; eess.AS**

- **简介: 该论文属于音频生成检测任务，旨在解决如何有效识别合成音乐的问题。随着生成模型的进步，区分人工创作与合成音乐变得困难。作者构建了包含真实与合成音乐的数据集，并测试现有检测模型在多种音频增强下的表现，发现即使轻微增强也会显著降低模型性能。**

- **链接: [http://arxiv.org/pdf/2507.10447v1](http://arxiv.org/pdf/2507.10447v1)**

> **作者:** Tomasz Sroka; Tomasz Wężowicz; Dominik Sidorczuk; Mateusz Modrzejewski
>
> **备注:** ISMIR 2025 LBD, 2 pages + bibliography, 1 figure
>
> **摘要:** With the rapid advancement of generative audio models, distinguishing between human-composed and generated music is becoming increasingly challenging. As a response, models for detecting fake music have been proposed. In this work, we explore the robustness of such systems under audio augmentations. To evaluate model generalization, we constructed a dataset consisting of both real and synthetic music generated using several systems. We then apply a range of audio transformations and analyze how they affect classification accuracy. We test the performance of a recent state-of-the-art musical deepfake detection model in the presence of audio augmentations. The performance of the model decreases significantly even with the introduction of light augmentations.
>
---
#### [new 011] Radif corpus: a symbolic dataset for non-metric iranian classical music
- **分类: cs.SD; eess.AS**

- **简介: 论文介绍了Radif语料库，首个完整非节拍伊朗古典音乐曲目的数字集合，包含228首乐曲的MIDI文件及结构化数据。该任务旨在为伊朗古典音乐提供基础资源，解决其数字化缺失问题，支持旋律分析、即兴风格研究及音乐信息检索等任务。**

- **链接: [http://arxiv.org/pdf/2507.10456v1](http://arxiv.org/pdf/2507.10456v1)**

> **作者:** Maziar Kanani; Sean O Leary; James McDermott
>
> **摘要:** Non-metric music forms the core of the repertoire in Iranian classical music. Dastgahi music serves as the underlying theoretical system for both Iranian art music and certain folk traditions. At the heart of Iranian classical music lies the radif, a foundational repertoire that organizes melodic material central to performance and pedagogy. In this study, we introduce the first digital corpus representing the complete non-metrical radif repertoire, covering all 13 existing components of this repertoire. We provide MIDI files (about 281 minutes in total) and data spreadsheets describing notes, note durations, intervals, and hierarchical structures for 228 pieces of music. We faithfully represent the tonality including quarter-tones, and the non-metric aspect. Furthermore, we provide supporting basic statistics, and measures of complexity and similarity over the corpus. Our corpus provides a platform for computational studies of Iranian classical music. Researchers might employ it in studying melodic patterns, investigating improvisational styles, or for other tasks in music information retrieval, music theory, and computational (ethno)musicology.
>
---
#### [new 012] WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling
- **分类: cs.SD; cs.AI**

- **简介: 该论文属于音频信号处理任务，旨在解决AI建模专业音频效果图难题。作者提出WildFX管道，利用专业DAW与Docker容器技术，生成含复杂效果图的多轨音频数据集，支持跨平台插件集成，实现高效并行处理，并通过实验验证其在混合图盲估计等方面的有效性。**

- **链接: [http://arxiv.org/pdf/2507.10534v1](http://arxiv.org/pdf/2507.10534v1)**

> **作者:** Qihui Yang; Taylor Berg-Kirkpatrick; Julian McAuley; Zachary Novack
>
> **摘要:** Despite rapid progress in end-to-end AI music generation, AI-driven modeling of professional Digital Signal Processing (DSP) workflows remains challenging. In particular, while there is growing interest in neural black-box modeling of audio effect graphs (e.g. reverb, compression, equalization), AI-based approaches struggle to replicate the nuanced signal flow and parameter interactions used in professional workflows. Existing differentiable plugin approaches often diverge from real-world tools, exhibiting inferior performance relative to simplified neural controllers under equivalent computational constraints. We introduce WildFX, a pipeline containerized with Docker for generating multi-track audio mixing datasets with rich effect graphs, powered by a professional Digital Audio Workstation (DAW) backend. WildFX supports seamless integration of cross-platform commercial plugins or any plugins in the wild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g., sidechains, crossovers) and achieving efficient parallelized processing. A minimalist metadata interface simplifies project/plugin configuration. Experiments demonstrate the pipeline's validity through blind estimation of mixing graphs, plugin/gain parameters, and its ability to bridge AI research with practical DSP demands. The code is available on: https://github.com/IsaacYQH/WildFX.
>
---
#### [new 013] ASTAR-NTU solution to AudioMOS Challenge 2025 Track1
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于音频与文本评估任务，旨在解决自动生成音乐的质量评价问题。作者提出了一种双分支模型，结合预训练音频和文本编码器，并引入交叉注意力机制融合多模态信息，通过分类方式预测音乐印象和文本对齐得分，取得了显著提升的系统级相关系数。**

- **链接: [http://arxiv.org/pdf/2507.09904v1](http://arxiv.org/pdf/2507.09904v1)**

> **作者:** Fabian Ritter-Gutierrez; Yi-Cheng Lin; Jui-Chiang Wei; Jeremy H. M. Wong; Nancy F. Chen; Hung-yi Lee
>
> **备注:** Under Review - Submitted to AudioMOS Challenge 2025 - ASRU 2025
>
> **摘要:** Evaluation of text-to-music systems is constrained by the cost and availability of collecting experts for assessment. AudioMOS 2025 Challenge track 1 is created to automatically predict music impression (MI) as well as text alignment (TA) between the prompt and the generated musical piece. This paper reports our winning system, which uses a dual-branch architecture with pre-trained MuQ and RoBERTa models as audio and text encoders. A cross-attention mechanism fuses the audio and text representations. For training, we reframe the MI and TA prediction as a classification task. To incorporate the ordinal nature of MOS scores, one-hot labels are converted to a soft distribution using a Gaussian kernel. On the official test set, a single model trained with this method achieves a system-level Spearman's Rank Correlation Coefficient (SRCC) of 0.991 for MI and 0.952 for TA, corresponding to a relative improvement of 21.21\% in MI SRCC and 31.47\% in TA SRCC over the challenge baseline.
>
---
#### [new 014] Ensemble Confidence Calibration for Sound Event Detection in Open-environment
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于声音事件检测任务，旨在解决开放环境中模型对未知场景预测过于自信、缺乏不确定性度量的问题。作者提出了一种基于集成方法的置信度校准技术EOW-Softmax，并应用于声音发生与重叠检测，提升了模型在开放环境下的鲁棒性和适应能力。**

- **链接: [http://arxiv.org/pdf/2507.09606v1](http://arxiv.org/pdf/2507.09606v1)**

> **作者:** Yuanjian Chen; Han Yin
>
> **摘要:** Sound event detection (SED) has made strong progress in controlled environments with clear event categories. However, real-world applications often take place in open environments. In such cases, current methods often produce predictions with too much confidence and lack proper ways to measure uncertainty. This limits their ability to adapt and perform well in new situations. To solve this problem, we are the first to use ensemble methods in SED to improve robustness against out-of-domain (OOD) inputs. We propose a confidence calibration method called Energy-based Open-World Softmax (EOW-Softmax), which helps the system better handle uncertainty in unknown scenes. We further apply EOW-Softmax to sound occurrence and overlap detection (SOD) by adjusting the prediction. In this way, the model becomes more adaptable while keeping its ability to detect overlapping events. Experiments show that our method improves performance in open environments. It reduces overconfidence and increases the ability to handle OOD situations.
>
---
#### [new 015] DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation
- **分类: cs.SD; eess.AS**

- **简介: 论文提出DQLoRA，一种轻量级的语音识别框架，用于低资源和噪声环境下的鲁棒语音识别。使用冻结的Whisper模型作为教师提供语义监督，学生模型为带有QLoRA适配器的轻量级Wav2Vec2，并通过CTC损失和KL散度蒸馏损失联合优化。**

- **链接: [http://arxiv.org/pdf/2507.10313v1](http://arxiv.org/pdf/2507.10313v1)**

> **作者:** Yiru Yang
>
> **摘要:** We present a demo of DQLoRA, an Adapter-Guided Distillation framework for robust speech recognition under low-resource and noisy conditions. Our method employs a frozen Whisper model as the teacher to provide semantic supervision, and a lightweight Wav2Vec2 student equipped with QLoRA-based Adapters. Training is conducted on the FLEURS dataset augmented with DNS-style noise. The student is optimized by jointly minimizing CTC loss and KL-based distillation loss, enabling efficient adaptation while preserving recognition accuracy.
>
---
#### [new 016] Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning
- **分类: cs.SD; cs.CL; eess.AS**

- **简介: 该论文属于语音转换任务，旨在解决缺乏目标说话人朗伯德语料的问题。通过比较隐式与显式声学特征条件模型，实现说话人身份转换同时保留朗伯德风格特征，提升语音可懂度并保持说话人相似性。**

- **链接: [http://arxiv.org/pdf/2507.09310v1](http://arxiv.org/pdf/2507.09310v1)**

> **作者:** Dominika Woszczyk; Manuel Sam Ribeiro; Thomas Merritt; Daniel Korzekwa
>
> **备注:** Presented at Clarity Challenge 2023
>
> **摘要:** Text-to-Speech (TTS) systems in Lombard speaking style can improve the overall intelligibility of speech, useful for hearing loss and noisy conditions. However, training those models requires a large amount of data and the Lombard effect is challenging to record due to speaker and noise variability and tiring recording conditions. Voice conversion (VC) has been shown to be a useful augmentation technique to train TTS systems in the absence of recorded data from the target speaker in the target speaking style. In this paper, we are concerned with Lombard speaking style transfer. Our goal is to convert speaker identity while preserving the acoustic attributes that define the Lombard speaking style. We compare voice conversion models with implicit and explicit acoustic feature conditioning. We observe that our proposed implicit conditioning strategy achieves an intelligibility gain comparable to the model conditioned on explicit acoustic features, while also preserving speaker similarity.
>
---
#### [new 017] SemAlignVC: Enhancing zero-shot timbre conversion using semantic alignment
- **分类: eess.AS; cs.SD; 68T07; I.2.7; I.2.6; G.3; H.5.5**

- **简介: 该论文属于零样本语音转换任务，旨在解决音色泄漏问题。作者提出了SemAlignVC，通过语义对齐方法分离音色与内容表示，并使用自回归Transformer实现高质量语音转换，有效提升了音色相似性、可懂度和自然度。**

- **链接: [http://arxiv.org/pdf/2507.09070v1](http://arxiv.org/pdf/2507.09070v1)**

> **作者:** Shivam Mehta; Yingru Liu; Zhenyu Tang; Kainan Peng; Vimal Manohar; Shun Zhang; Mike Seltzer; Qing He; Mingbo Ma
>
> **备注:** 6 pages, 2 figures, Accepted at the ISCA Speech Synthesis Workshop (SSW) 2025
>
> **摘要:** Zero-shot voice conversion (VC) synthesizes speech in a target speaker's voice while preserving linguistic and paralinguistic content. However, timbre leakage-where source speaker traits persist-remains a challenge, especially in neural codec and LLM-based VC, where quantized representations entangle speaker identity with content. We introduce SemAlignVC, an architecture designed to prevent timbre leakage using SemAlign, a novel method that aligns text and audio representations to ensure speaker-independent semantic encoding. This disentangled representation conditions an autoregressive transformer for high-fidelity conversion without explicit speaker embeddings. Experiments show SemAlignVC significantly reduces timbre leakage, outperforming baselines in speaker timbre similarity, intelligibility, and naturalness, making it a robust, privacy-preserving, and generalizable VC solution. Audio samples can be accessed at https://shivammehta25.github.io/SemAlignVC/
>
---
#### [new 018] ClaritySpeech: Dementia Obfuscation in Speech
- **分类: cs.CL; cs.CR; cs.LG; cs.SD; eess.AS**

- **简介: 该论文属于语音处理任务，旨在解决失智症导致的语音障碍问题。通过结合自动语音识别、文本模糊化和零样本语音合成技术，提出ClaritySpeech框架，在低数据环境下修正受失智症影响的语音，同时保持说话人身份特征，以提升隐私保护与沟通效率。**

- **链接: [http://arxiv.org/pdf/2507.09282v1](http://arxiv.org/pdf/2507.09282v1)**

> **作者:** Dominika Woszczyk; Ranya Aloufi; Soteris Demetriou
>
> **备注:** Accepted at Interspeech 2025
>
> **摘要:** Dementia, a neurodegenerative disease, alters speech patterns, creating communication barriers and raising privacy concerns. Current speech technologies, such as automatic speech transcription (ASR), struggle with dementia and atypical speech, further challenging accessibility. This paper presents a novel dementia obfuscation in speech framework, ClaritySpeech, integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to correct dementia-affected speech while preserving speaker identity in low-data environments without fine-tuning. Results show a 16% and 10% drop in mean F1 score across various adversarial settings and modalities (audio, text, fusion) for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15 for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and accessibility.
>
---
#### [new 019] Cyclic Multichannel Wiener Filter for Acoustic Beamforming
- **分类: eess.AS; eess.SP**

- **简介: 该论文属于语音增强任务，旨在解决传统波束成形模型对非平稳语音信号建模不足的问题。作者提出了一种基于循环平稳模型的多通道维纳滤波器（cMWF），利用语音谐波间的谱相关性降低均方误差，从而提升语音质量。实验表明其在合成数据上效果显著，但对基频估计精度敏感。**

- **链接: [http://arxiv.org/pdf/2507.10159v1](http://arxiv.org/pdf/2507.10159v1)**

> **作者:** Giovanni Bologni; Richard Heusdens; Richard C. Hendriks
>
> **备注:** Comments: Accepted for publication at the 2025 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA 2025). IEEE retains copyright
>
> **摘要:** Acoustic beamforming models typically assume wide-sense stationarity of speech signals within short time frames. However, voiced speech is better modeled as a cyclostationary (CS) process, a random process whose mean and autocorrelation are $T_1$-periodic, where $\alpha_1=1/T_1$ corresponds to the fundamental frequency of vowels. Higher harmonic frequencies are found at integer multiples of the fundamental. This work introduces a cyclic multichannel Wiener filter (cMWF) for speech enhancement derived from a cyclostationary model. This beamformer exploits spectral correlation across the harmonic frequencies of the signal to further reduce the mean-squared error (MSE) between the target and the processed input. The proposed cMWF is optimal in the MSE sense and reduces to the MWF when the target is wide-sense stationary. Experiments on simulated data demonstrate considerable improvements in scale-invariant signal-to-distortion ratio (SI-SDR) on synthetic data but also indicate high sensitivity to the accuracy of the estimated fundamental frequency $\alpha_1$, which limits effectiveness on real data.
>
---
#### [new 020] Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications
- **分类: cs.RO; eess.SP**

- **简介: 论文研究任务为导航中的状态估计问题，旨在提升非线性滤波精度。针对传统方法在预测均值和协方差时的不足，提出一种基于非线性传播模型的无迹卡尔曼滤波方法，并通过自主水下机器人实测数据验证其有效性。**

- **链接: [http://arxiv.org/pdf/2507.10082v1](http://arxiv.org/pdf/2507.10082v1)**

> **作者:** Amit Levy; Itzik Klein
>
> **备注:** 5 pages, 2 figures
>
> **摘要:** The unscented Kalman filter is a nonlinear estimation algorithm commonly used in navigation applications. The prediction of the mean and covariance matrix is crucial to the stable behavior of the filter. This prediction is done by propagating the sigma points according to the dynamic model at hand. In this paper, we introduce an innovative method to propagate the sigma points according to the nonlinear dynamic model of the navigation error state vector. This improves the filter accuracy and navigation performance. We demonstrate the benefits of our proposed approach using real sensor data recorded by an autonomous underwater vehicle during several scenarios.
>
---
#### [new 021] Harmonics to the Rescue: Why Voiced Speech is Not a Wss Process
- **分类: eess.AS; eess.SP**

- **简介: 该论文属于语音信号处理任务，旨在解决传统将语音建模为广义平稳（WSS）过程所导致的频谱不相关问题。作者提出使用循环平稳（CS）模型，利用谐波频率间的相关性，更准确估计功率谱密度、实现源分离和波束成形，并通过仿真和真实数据验证方法有效性。**

- **链接: [http://arxiv.org/pdf/2507.10176v1](http://arxiv.org/pdf/2507.10176v1)**

> **作者:** Giovanni Bologni; Richard Heusdens; Richard C. Hendriks
>
> **备注:** Comments: Accepted at the 2024 International Workshop on Acoustic Signal Enhancement (IWAENC 2024)
>
> **摘要:** Speech processing algorithms often rely on statistical knowledge of the underlying process. Despite many years of research, however, the debate on the most appropriate statistical model for speech still continues. Speech is commonly modeled as a wide-sense stationary (WSS) process. However, the use of the WSS model for spectrally correlated processes is fundamentally wrong, as WSS implies spectral uncorrelation. In this paper, we demonstrate that voiced speech can be more accurately represented as a cyclostationary (CS) process. By employing the CS rather than the WSS model for processes that are inherently correlated across frequency, it is possible to improve the estimation of cross-power spectral densities (PSDs), source separation, and beamforming. We illustrate how the correlation between harmonic frequencies of CS processes can enhance system identification, and validate our findings using both simulated and real speech data.
>
---
#### [new 022] DualDub: Video-to-Soundtrack Generation via Joint Speech and Background Audio Synthesis
- **分类: cs.MM; cs.SD; eess.AS**

- **简介: 该论文提出视频到原声带生成（V2ST）任务，旨在联合生成同步的背景音频与语音。为解决现有方法忽略语音的问题，作者设计了DualDub框架，结合多模态编码、跨模态对齐及双解码头，并采用因果注意力机制和课程学习策略提升效果。**

- **链接: [http://arxiv.org/pdf/2507.10109v1](http://arxiv.org/pdf/2507.10109v1)**

> **作者:** Wenjie Tian; Xinfa Zhu; Haohe Liu; Zhixian Zhao; Zihao Chen; Chaofan Ding; Xinhan Di; Junjie Zheng; Lei Xie
>
> **摘要:** While recent video-to-audio (V2A) models can generate realistic background audio from visual input, they largely overlook speech, an essential part of many video soundtracks. This paper proposes a new task, video-to-soundtrack (V2ST) generation, which aims to jointly produce synchronized background audio and speech within a unified framework. To tackle V2ST, we introduce DualDub, a unified framework built on a multimodal language model that integrates a multimodal encoder, a cross-modal aligner, and dual decoding heads for simultaneous background audio and speech generation. Specifically, our proposed cross-modal aligner employs causal and non-causal attention mechanisms to improve synchronization and acoustic harmony. Besides, to handle data scarcity, we design a curriculum learning strategy that progressively builds the multimodal capability. Finally, we introduce DualBench, the first benchmark for V2ST evaluation with a carefully curated test set and comprehensive metrics. Experimental results demonstrate that DualDub achieves state-of-the-art performance, generating high-quality and well-synchronized soundtracks with both speech and background audio.
>
---
#### [new 023] Knowing When to Quit: Probabilistic Early Exits for Speech Separation
- **分类: cs.LG; cs.SD; eess.AS**

- **简介: 该论文属于语音分离任务，旨在解决固定计算资源限制下模型难以适应不同设备的问题。作者设计了一种支持提前退出的神经网络架构，并提出基于不确定性的概率框架，动态调整计算量，实现高效且可扩展的语音分离与增强。**

- **链接: [http://arxiv.org/pdf/2507.09768v1](http://arxiv.org/pdf/2507.09768v1)**

> **作者:** Kenny Falkær Olsen. Mads Østergaard; Karl Ulbæk; Søren Føns Nielsen; Rasmus Malik Høegh Lindrup; Bjørn Sand Jensen; Morten Mørup
>
> **摘要:** In recent years, deep learning-based single-channel speech separation has improved considerably, in large part driven by increasingly compute- and parameter-efficient neural network architectures. Most such architectures are, however, designed with a fixed compute and parameter budget, and consequently cannot scale to varying compute demands or resources, which limits their use in embedded and heterogeneous devices such as mobile phones and hearables. To enable such use-cases we design a neural network architecture for speech separation capable of early-exit, and we propose an uncertainty-aware probabilistic framework to jointly model the clean speech signal and error variance which we use to derive probabilistic early-exit conditions in terms of desired signal-to-noise ratios. We evaluate our methods on both speech separation and enhancement tasks, and we show that a single early-exit model can be competitive with state-of-the-art models trained at many compute and parameter budgets. Our framework enables fine-grained dynamic compute-scaling of speech separation networks while achieving state-of-the-art performance and interpretable exit conditions.
>
---
#### [new 024] The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents
- **分类: cs.CR; cs.SD; eess.AS**

- **简介: 该论文研究多模态大语言模型通过音频数据推断敏感个人信息的隐私风险，构建了带敏感属性标注的音频数据集AP²，并提出了结合音频-语言模型与大语言模型的框架Gifts以提升推断能力，探索防御策略。属于音频隐私分析任务，旨在解决音频数据被用于敏感属性推测的问题。**

- **链接: [http://arxiv.org/pdf/2507.10016v1](http://arxiv.org/pdf/2507.10016v1)**

> **作者:** Lixu Wang; Kaixiang Yao; Xinfeng Li; Dong Yang; Haoyang Li; Xiaofeng Wang; Wei Dong
>
> **备注:** 22 pages, 4 figures
>
> **摘要:** Our research uncovers a novel privacy risk associated with multimodal large language models (MLLMs): the ability to infer sensitive personal attributes from audio data -- a technique we term audio private attribute profiling. This capability poses a significant threat, as audio can be covertly captured without direct interaction or visibility. Moreover, compared to images and text, audio carries unique characteristics, such as tone and pitch, which can be exploited for more detailed profiling. However, two key challenges exist in understanding MLLM-employed private attribute profiling from audio: (1) the lack of audio benchmark datasets with sensitive attribute annotations and (2) the limited ability of current MLLMs to infer such attributes directly from audio. To address these challenges, we introduce AP^2, an audio benchmark dataset that consists of two subsets collected and composed from real-world data, and both are annotated with sensitive attribute labels. Additionally, we propose Gifts, a hybrid multi-agent framework that leverages the complementary strengths of audio-language models (ALMs) and large language models (LLMs) to enhance inference capabilities. Gifts employs an LLM to guide the ALM in inferring sensitive attributes, then forensically analyzes and consolidates the ALM's inferences, overcoming severe hallucinations of existing ALMs in generating long-context responses. Our evaluations demonstrate that Gifts significantly outperforms baseline approaches in inferring sensitive attributes. Finally, we investigate model-level and data-level defense strategies to mitigate the risks of audio private attribute profiling. Our work validates the feasibility of audio-based privacy attacks using MLLMs, highlighting the need for robust defenses, and provides a dataset and framework to facilitate future research.
>
---
#### [new 025] Enhancing Stereo Sound Event Detection with BiMamba and Pretrained PSELDnet
- **分类: eess.AS; cs.SD**

- **简介: 该论文属于声音事件定位与检测（SELD）任务，旨在解决现有模型计算成本高的问题。作者提出了一种结合预训练PSELDnet与双向Mamba序列模型的新方法，并引入非对称卷积以更有效地捕捉音频信号的时频关系。**

- **链接: [http://arxiv.org/pdf/2507.09570v1](http://arxiv.org/pdf/2507.09570v1)**

> **作者:** Wenmiao Gao; Han Yin
>
> **摘要:** Pre-training methods have greatly improved the performance of sound event localization and detection (SELD). However, existing Transformer-based models still face high computational cost. To solve this problem, we present a stereo SELD system using a pre-trained PSELDnet and a bidirectional Mamba sequence model. Specifically, we replace the Conformer module with a BiMamba module. We also use asymmetric convolutions to better capture the time and frequency relationships in the audio signal. Test results on the DCASE2025 Task 3 development dataset show that our method performs better than both the baseline and the original PSELDnet with a Conformer decoder. In addition, the proposed model costs fewer computing resources than the baselines. These results show that the BiMamba architecture is effective for solving key challenges in SELD tasks. The source code is publicly accessible at https://github.com/ alexandergwm/DCASE2025 TASK3 Stereo PSELD Mamba.
>
---
#### [new 026] Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model
- **分类: eess.AS; cs.SD**

- **简介: 该论文属于语音增强任务，旨在解决听力受损者在噪声环境下的语音可懂度和质量提升问题。现有方法缺乏灵活性或任务平衡。作者提出基于可微听觉模型的多任务学习框架，联合优化降噪与听力补偿，实现推理时动态调整两者平衡，提升综合性能。**

- **链接: [http://arxiv.org/pdf/2507.09372v1](http://arxiv.org/pdf/2507.09372v1)**

> **作者:** Philippe Gonzalez; Torsten Dau; Tobias May
>
> **备注:** Accepted to Clarity 2025 Workshop
>
> **摘要:** Deep learning-based hearing loss compensation (HLC) seeks to enhance speech intelligibility and quality for hearing impaired listeners using neural networks. One major challenge of HLC is the lack of a ground-truth target. Recent works have used neural networks to emulate non-differentiable auditory peripheral models in closed-loop frameworks, but this approach lacks flexibility. Alternatively, differentiable auditory models allow direct optimization, yet previous studies focused on individual listener profiles, or joint noise reduction (NR) and HLC without balancing each task. This work formulates NR and HLC as a multi-task learning problem, training a system to simultaneously predict denoised and compensated signals from noisy speech and audiograms using a differentiable auditory model. Results show the system achieves similar objective metric performance to systems trained for each task separately, while being able to adjust the balance between NR and HLC during inference.
>
---
#### [new 027] The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM Challenge
- **分类: eess.AS; cs.SD**

- **简介: 该论文参与MLC-SLM挑战赛任务2，旨在解决无先验说话人标签和时间边界的多说话人自动语音识别问题。作者基于Qwen2.5大语言模型构建了结合说话人嵌入和时间边界信息的系统，并通过微调语言适配器提升多语言性能，最终在开发集和测试集上分别取得23.56%和18.08%的tcpWER，显著优于基线系统。**

- **链接: [http://arxiv.org/pdf/2507.09499v1](http://arxiv.org/pdf/2507.09499v1)**

> **作者:** Yuke Lin; Ming Cheng; Ze Li; Ming Li
>
> **备注:** Technical Report for MLC-SLM Challenge in Interspeech2025
>
> **摘要:** We present the DKU system for Task 2 of the MLC-SLM Challenge, which aims to perform multi-speaker automatic speech recognition directly from raw audio without Oracle speaker labels or time boundaries. Our approach builds upon a diarization-aware framework integrating speaker embeddings and temporal utterance boundaries into a Qwen2.5-based large language model (LLM). Then, we enhance the system's multilingual performance by fine-tuning language-specific adapters and LoRA modules within the LLM decoder. Finally, our system achieves the tcpWER of 23.56\% and 18.08\% on the development and test sets of the MLC-SLM dataset, substantially outperforming the official baseline.
>
---
#### [new 028] Low-Rank Adaptation of Deep Prior Neural Networks For Room Impulse Response Reconstruction
- **分类: eess.AS; cs.SD**

- **简介: 论文研究在房间脉冲响应重建任务中应用深度先验网络的低秩适应方法。现有深度先验模型难以泛化到新的声学配置，需重复训练。作者引入LoRA进行迁移学习，实现对新测量数据的高效微调，尤其在声源位置变化时表现良好，提升了适应性和计算效率。**

- **链接: [http://arxiv.org/pdf/2507.09806v1](http://arxiv.org/pdf/2507.09806v1)**

> **作者:** Mirco Pezzoli; Federico Miotello; Shoichi Koyama; Fabio Antonacci
>
> **备注:** to appear in IEEE WASPAA
>
> **摘要:** The Deep Prior framework has emerged as a powerful generative tool which can be used for reconstructing sound fields in an environment from few sparse pressure measurements. It employs a neural network that is trained solely on a limited set of available data and acts as an implicit prior which guides the solution of the underlying optimization problem. However, a significant limitation of the Deep Prior approach is its inability to generalize to new acoustic configurations, such as changes in the position of a sound source. As a consequence, the network must be retrained from scratch for every new setup, which is both computationally intensive and time-consuming. To address this, we investigate transfer learning in Deep Prior via Low-Rank Adaptation (LoRA), which enables efficient fine-tuning of a pre-trained neural network by introducing a low-rank decomposition of trainable parameters, thus allowing the network to adapt to new measurement sets with minimal computational overhead. We embed LoRA into a MultiResUNet-based Deep Prior model and compare its adaptation performance against full fine-tuning of all parameters as well as classical retraining, particularly in scenarios where only a limited number of microphones are used. The results indicate that fine-tuning, whether done completely or via LoRA, is especially advantageous when the source location is the sole changing parameter, preserving high physical fidelity, and highlighting the value of transfer learning for acoustics applications.
>
---
#### [new 029] Can We Really Repurpose Multi-Speaker ASR Corpus for Speaker Diarization?
- **分类: eess.AS; cs.SD**

- **简介: 论文研究语音说话人日志任务，探讨是否可将多说话人自动语音识别（ASR）语料用于训练。问题在于ASR数据边界定义松散，影响日志系统的评估可靠性与模型泛化能力。作者通过强制对齐标准化边界进行训练，提升了日志性能和流式场景表现，并改善了ASR结果。**

- **链接: [http://arxiv.org/pdf/2507.09226v1](http://arxiv.org/pdf/2507.09226v1)**

> **作者:** Shota Horiguchi; Naohiro Tawara; Takanori Ashihara; Atsushi Ando; Marc Delcroix
>
> **摘要:** Neural speaker diarization is widely used for overlap-aware speaker diarization, but it requires large multi-speaker datasets for training. To meet this data requirement, large datasets are often constructed by combining multiple corpora, including those originally designed for multi-speaker automatic speech recognition (ASR). However, ASR datasets often feature loosely defined segment boundaries that do not align with the stricter conventions of diarization benchmarks. In this work, we show that such boundary looseness significantly impacts the diarization error rate, reducing evaluation reliability. We also reveal that models trained on data with varying boundary precision tend to learn dataset-specific looseness, leading to poor generalization across out-of-domain datasets. Training with standardized tight boundaries via forced alignment improves not only diarization performance, especially in streaming scenarios, but also ASR performance when combined with simple post-processing.
>
---
#### [new 030] Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction
- **分类: eess.AS; cs.CV; cs.SD**

- **简介: 该论文属于音频生成任务，旨在解决连续值音频建模问题。作者提出基于Transformer的因果语言模型，采用token-wise扩散和掩码下一token预测，提升生成质量，参数量更少却取得与SOTA扩散模型相当的效果。**

- **链接: [http://arxiv.org/pdf/2507.09834v1](http://arxiv.org/pdf/2507.09834v1)**

> **作者:** Shu-wen Yang; Byeonggeun Kim; Kuan-Po Huang; Qingming Tang; Huy Phan; Bo-Ru Lu; Harsha Sundar; Shalini Ghosh; Hung-yi Lee; Chieh-Chi Kao; Chao Wang
>
> **备注:** Accepted by ICML 2025. Project website: https://audiomntp.github.io/
>
> **摘要:** Autoregressive next-token prediction with the Transformer decoder has become a de facto standard in large language models (LLMs), achieving remarkable success in Natural Language Processing (NLP) at scale. Extending this paradigm to audio poses unique challenges due to its inherently continuous nature. We research audio generation with a causal language model (LM) without discrete tokens. We leverage token-wise diffusion to model the continuous distribution of the next continuous-valued token. Our approach delivers significant improvements over previous discrete solution, AudioGen, achieving 20% and 40% relative gains on AudioCaps in Frechet Audio Distance (FAD) and Kullback-Leibler (KL) divergence, respectively. Additionally, we propose a novel masked next-token prediction task that incorporates masked prediction into the causal LM framework. On AudioCaps, the innovation yields 41% and 33% relative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B) models, respectively, and is on par with the state-of-the-art (SOTA) diffusion models. Furthermore, we achieve these results with significantly fewer parameters -- 193M for our Base and 462M for our Large models.
>
---
#### [new 031] Large Language Models and Non-Negative Matrix Factorization for Bioacoustic Signal Decomposition
- **分类: eess.AS; cs.SD**

- **简介: 该论文属于生物声学信号分析任务，旨在解决临床录音中重叠生物声学信号分离的问题。通过结合非负矩阵分解与大语言模型，提出一种可解释的混合方法，无需标注数据即可将混合信号分解为不同生理信号，并关联特定声学模式与疾病，适用于智能诊断工具开发。**

- **链接: [http://arxiv.org/pdf/2507.09161v1](http://arxiv.org/pdf/2507.09161v1)**

> **作者:** Yasaman Torabi; Shahram Shirani; James P. Reilly
>
> **备注:** Presented at Queen's University Biological Station Seminars of Graduate Research in Ontario, Lake Shift Dissertation Camp (QUBS '25)
>
> **摘要:** Large language models have shown a remarkable ability to extract meaning from unstructured data, offering new ways to interpret biomedical signals beyond traditional numerical methods. In this study, we present a matrix factorization framework for bioacoustic signal analysis which is enhanced by large language models. The focus is on separating bioacoustic signals that commonly overlap in clinical recordings, using matrix factorization to decompose the mixture into interpretable components. A large language model is then applied to the separated signals to associate distinct acoustic patterns with potential medical conditions such as cardiac rhythm disturbances or respiratory abnormalities. Recordings were obtained from a digital stethoscope applied to a clinical manikin to ensure a controlled and high-fidelity acquisition environment. This hybrid approach does not require labeled data or prior knowledge of source types, and it provides a more interpretable and accessible framework for clinical decision support. The method demonstrates promise for integration into future intelligent diagnostic tools.
>
---
## 更新

#### [replaced 001] Human-CLAP: Human-perception-based contrastive language-audio pretraining
- **分类: eess.AS; cs.SD**

- **链接: [http://arxiv.org/pdf/2506.23553v2](http://arxiv.org/pdf/2506.23553v2)**

> **作者:** Taisei Takano; Yuki Okamoto; Yusuke Kanamori; Yuki Saito; Ryotaro Nagase; Hiroshi Saruwatari
>
> **备注:** Submitted to APSIPA ASC 2025
>
> **摘要:** Contrastive language-audio pretraining (CLAP) is widely used for audio generation and recognition tasks. For example, CLAPScore, which utilizes the similarity of CLAP embeddings, has been a major metric for the evaluation of the relevance between audio and text in text-to-audio. However, the relationship between CLAPScore and human subjective evaluation scores is still unclarified. We show that CLAPScore has a low correlation with human subjective evaluation scores. Additionally, we propose a human-perception-based CLAP called Human-CLAP by training a contrastive language-audio model using the subjective evaluation score. In our experiments, the results indicate that our Human-CLAP improved the Spearman's rank correlation coefficient (SRCC) between the CLAPScore and the subjective evaluation scores by more than 0.25 compared with the conventional CLAP.
>
---
#### [replaced 002] Hear-Your-Click: Interactive Object-Specific Video-to-Audio Generation
- **分类: cs.CV; cs.AI; cs.MM; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2507.04959v2](http://arxiv.org/pdf/2507.04959v2)**

> **作者:** Yingshan Liang; Keyu Fan; Zhicheng Du; Yiran Wang; Qingyang Shi; Xinyu Zhang; Jiasheng Lu; Peiwu Qin
>
> **摘要:** Video-to-audio (V2A) generation shows great potential in fields such as film production. Despite significant advances, current V2A methods relying on global video information struggle with complex scenes and generating audio tailored to specific objects. To address these limitations, we introduce Hear-Your-Click, an interactive V2A framework enabling users to generate sounds for specific objects by clicking on the frame. To achieve this, we propose Object-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided Visual Encoder (MVE) to obtain object-level visual features aligned with audio. Furthermore, we tailor two data augmentation strategies, Random Video Stitching (RVS) and Mask-guided Loudness Modulation (MLM), to enhance the model's sensitivity to segmented objects. To measure audio-visual correspondence, we designed a new evaluation metric, the CAV score. Extensive experiments demonstrate that our framework offers more precise control and improves generation performance across various metrics. Project Page: https://github.com/SynapGrid/Hear-Your-Click
>
---
#### [replaced 003] DeepGesture: A conversational gesture synthesis system based on emotions and semantics
- **分类: cs.HC; cs.CL; cs.LG; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2507.03147v2](http://arxiv.org/pdf/2507.03147v2)**

> **作者:** Thanh Hoang-Minh
>
> **备注:** Project page: https://deepgesture.github.io
>
> **摘要:** Along with the explosion of large language models, improvements in speech synthesis, advancements in hardware, and the evolution of computer graphics, the current bottleneck in creating digital humans lies in generating character movements that correspond naturally to text or speech inputs. In this work, we present DeepGesture, a diffusion-based gesture synthesis framework for generating expressive co-speech gestures conditioned on multimodal signals - text, speech, emotion, and seed motion. Built upon the DiffuseStyleGesture model, DeepGesture introduces novel architectural enhancements that improve semantic alignment and emotional expressiveness in generated gestures. Specifically, we integrate fast text transcriptions as semantic conditioning and implement emotion-guided classifier-free diffusion to support controllable gesture generation across affective states. To visualize results, we implement a full rendering pipeline in Unity based on BVH output from the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces gestures with improved human-likeness and contextual appropriateness. Our system supports interpolation between emotional states and demonstrates generalization to out-of-distribution speech, including synthetic voices - marking a step forward toward fully multimodal, emotionally aware digital humans. Project page: https://deepgesture.github.io
>
---
#### [replaced 004] Robust Localization of Partially Fake Speech: Metrics, Models, and Out-of-Domain Evaluation
- **分类: cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2507.03468v2](http://arxiv.org/pdf/2507.03468v2)**

> **作者:** Hieu-Thi Luong; Inbal Rimon; Haim Permuter; Kong Aik Lee; Eng Siong Chng
>
> **备注:** Submitted to APSIPA 2025
>
> **摘要:** Partial audio deepfake localization pose unique challenges and remain underexplored compared to full-utterance spoofing detection. While recent methods report strong in-domain performance, their real-world utility remains unclear. In this analysis, we critically examine the limitations of current evaluation practices, particularly the widespread use of Equal Error Rate (EER), which often obscures generalization and deployment readiness. We propose reframing the localization task as a sequential anomaly detection problem and advocate for the use of threshold-dependent metrics such as accuracy, precision, recall, and F1-score, which better reflect real-world behavior. Specifically, we analyze the performance of the open-source Coarse-to-Fine Proposal Refinement Framework (CFPRF), which achieves a 20-ms EER of 7.61% on the in-domain PartialSpoof evaluation set, but 43.25% and 27.59% on the LlamaPartialSpoof and Half-Truth out-of-domain test sets. Interestingly, our reproduced version of the same model performs worse on in-domain data (9.84%) but better on the out-of-domain sets (41.72% and 14.98%, respectively). This highlights the risks of over-optimizing for in-domain EER, which can lead to models that perform poorly in real-world scenarios. It also suggests that while deep learning models can be effective on in-domain data, they generalize poorly to out-of-domain scenarios, failing to detect novel synthetic samples and misclassifying unfamiliar bona fide audio. Finally, we observe that adding more bona fide or fully synthetic utterances to the training data often degrades performance, whereas adding partially fake utterances improves it.
>
---
#### [replaced 005] Tiny-Align: Bridging Automatic Speech Recognition and Large Language Model on the Edge
- **分类: cs.SD; cs.AI; eess.AS**

- **链接: [http://arxiv.org/pdf/2411.13766v4](http://arxiv.org/pdf/2411.13766v4)**

> **作者:** Ruiyang Qin; Dancheng Liu; Gelei Xu; Zheyu Yan; Chenhui Xu; Yuting Hu; Shaocong Wang; X. Sharon Hu; Jinjun Xiong; Yiyu Shi
>
> **备注:** Accepted by ICCAD'25
>
> **摘要:** The combination of Large Language Models (LLM) and Automatic Speech Recognition (ASR), when deployed on edge devices (called edge ASR-LLM), can serve as a powerful personalized assistant to enable audio-based interaction for users. Compared to text-based interaction, edge ASR-LLM allows accessible and natural audio interactions. Unfortunately, existing ASR-LLM models are mainly trained in high-performance computing environments and produce substantial model weights, making them difficult to deploy on edge devices. More importantly, to better serve users' personalized needs, the ASR-LLM must be able to learn from each distinct user, given that audio input often contains highly personalized characteristics that necessitate personalized on-device training. Since individually fine-tuning the ASR or LLM often leads to suboptimal results due to modality-specific limitations, end-to-end training ensures seamless integration of audio features and language understanding (cross-modal alignment), ultimately enabling a more personalized and efficient adaptation on edge devices. However, due to the complex training requirements and substantial computational demands of existing approaches, cross-modal alignment between ASR audio and LLM can be challenging on edge devices. In this work, we propose a resource-efficient cross-modal alignment framework that bridges ASR and LLMs on edge devices to handle personalized audio input. Our framework enables efficient ASR-LLM alignment on resource-constrained devices like NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while improving the alignment quality by more than 50\%. To the best of our knowledge, this is the first work to study efficient ASR-LLM alignment on resource-constrained edge devices.
>
---
#### [replaced 006] StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model
- **分类: cs.CL; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2507.07803v2](http://arxiv.org/pdf/2507.07803v2)**

> **作者:** Shoutao Guo; Xiang Li; Mengge Liu; Wei Chen; Yang Feng
>
> **备注:** The code is at https://github.com/ictnlp/StreamUni; The model is at https://huggingface.co/ICTNLP/StreamUni-Phi4
>
> **摘要:** Streaming speech translation (StreamST) requires determining appropriate timing, known as policy, to generate translations while continuously receiving source speech inputs, balancing low latency with high translation quality. However, existing StreamST methods typically operate on sentence-level speech segments, referred to as simultaneous speech translation (SimulST). In practice, they require collaboration with segmentation models to accomplish StreamST, where the truncated speech segments constrain SimulST models to make policy decisions and generate translations based on limited contextual information. Moreover, SimulST models struggle to learn effective policies due to the complexity of speech inputs and cross-lingual generation. To address these challenges, we propose StreamUni, which achieves StreamST through a unified Large Speech-Language Model (LSLM). Specifically, StreamUni incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate multi-stage outputs. Leveraging these multi-stage outputs, StreamUni simultaneously accomplishes speech segmentation, policy decision, and translation generation, completing StreamST without requiring massive policy-specific training. Additionally, we propose a streaming CoT training method that enhances low-latency policy decisions and generation capabilities using limited CoT data. Experiments demonstrate that our approach achieves state-of-the-art performance on StreamST tasks.
>
---
#### [replaced 007] AGAV-Rater: Adapting Large Multimodal Model for AI-Generated Audio-Visual Quality Assessment
- **分类: cs.MM; cs.CV; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2501.18314v2](http://arxiv.org/pdf/2501.18314v2)**

> **作者:** Yuqin Cao; Xiongkuo Min; Yixuan Gao; Wei Sun; Guangtao Zhai
>
> **摘要:** Many video-to-audio (VTA) methods have been proposed for dubbing silent AI-generated videos. An efficient quality assessment method for AI-generated audio-visual content (AGAV) is crucial for ensuring audio-visual quality. Existing audio-visual quality assessment methods struggle with unique distortions in AGAVs, such as unrealistic and inconsistent elements. To address this, we introduce AGAVQA-3k, the first large-scale AGAV quality assessment dataset, comprising $3,382$ AGAVs from $16$ VTA methods. AGAVQA-3k includes two subsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality, content consistency, and overall quality, and AGAVQA-Pair, designed for optimal AGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can score AGAVs, as well as audio and music generated from text, across multiple dimensions, and selects the best AGAV generated by VTA methods to present to the user. AGAV-Rater achieves state-of-the-art performance on AGAVQA-3k, Text-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that AGAV-Rater enhances VTA performance and user experience. The dataset and code is available at https://github.com/charlotte9524/AGAV-Rater.
>
---
#### [replaced 008] Token-based Audio Inpainting via Discrete Diffusion
- **分类: cs.SD; cs.AI; cs.IT; cs.LG; eess.AS; math.IT**

- **链接: [http://arxiv.org/pdf/2507.08333v2](http://arxiv.org/pdf/2507.08333v2)**

> **作者:** Tali Dror; Iftach Shoham; Moshe Buchris; Oren Gal; Haim Permuter; Gilad Katz; Eliya Nachmani
>
> **摘要:** Audio inpainting refers to the task of reconstructing missing segments in corrupted audio recordings. While prior approaches-including waveform and spectrogram-based diffusion models-have shown promising results for short gaps, they often degrade in quality when gaps exceed 100 milliseconds (ms). In this work, we introduce a novel inpainting method based on discrete diffusion modeling, which operates over tokenized audio representations produced by a pre-trained audio tokenizer. Our approach models the generative process directly in the discrete latent space, enabling stable and semantically coherent reconstruction of missing audio. We evaluate the method on the MusicNet dataset using both objective and perceptual metrics across gap durations up to 300 ms. We further evaluated our approach on the MTG dataset, extending the gap duration to 500 ms. Experimental results demonstrate that our method achieves competitive or superior performance compared to existing baselines, particularly for longer gaps, offering a robust solution for restoring degraded musical recordings. Audio examples of our proposed method can be found at https://iftach21.github.io/
>
---
#### [replaced 009] Handling Domain Shifts for Anomalous Sound Detection: A Review of DCASE-Related Work
- **分类: eess.AS; cs.SD**

- **链接: [http://arxiv.org/pdf/2503.10435v2](http://arxiv.org/pdf/2503.10435v2)**

> **作者:** Kevin Wilkinghoff; Takuya Fujimura; Keisuke Imoto; Jonathan Le Roux; Zheng-Hua Tan; Tomoki Toda
>
> **摘要:** When detecting anomalous sounds in complex environments, one of the main difficulties is that trained models must be sensitive to subtle differences in monitored target signals, while many practical applications also require them to be insensitive to changes in acoustic domains. Examples of such domain shifts include changing the type of microphone or the location of acoustic sensors, which can have a much stronger impact on the acoustic signal than subtle anomalies themselves. Moreover, users typically aim to train a model only on source domain data, which they may have a relatively large collection of, and they hope that such a trained model will be able to generalize well to an unseen target domain by providing only a minimal number of samples to characterize the acoustic signals in that domain. In this work, we review and discuss recent publications focusing on this domain generalization problem for anomalous sound detection in the context of the DCASE challenges on acoustic machine condition monitoring.
>
---
#### [replaced 010] A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment
- **分类: cs.RO; cs.CL; cs.SD**

- **链接: [http://arxiv.org/pdf/2503.06241v2](http://arxiv.org/pdf/2503.06241v2)**

> **作者:** Koji Inoue; Yuki Okafuji; Jun Baba; Yoshiki Ohira; Katsuya Hyodo; Tatsuya Kawahara
>
> **备注:** This paper has been accepted for presentation at IEEE/RSJ International Conference on Intelligent Robots and Systems 2025 (IROS 2025) and represents the author's version of the work
>
> **摘要:** Turn-taking is a crucial aspect of human-robot interaction, directly influencing conversational fluidity and user engagement. While previous research has explored turn-taking models in controlled environments, their robustness in real-world settings remains underexplored. In this study, we propose a noise-robust voice activity projection (VAP) model, based on a Transformer architecture, to enhance real-time turn-taking in dialogue robots. To evaluate the effectiveness of the proposed system, we conducted a field experiment in a shopping mall, comparing the VAP system with a conventional cloud-based speech recognition system. Our analysis covered both subjective user evaluations and objective behavioral analysis. The results showed that the proposed system significantly reduced response latency, leading to a more natural conversation where both the robot and users responded faster. The subjective evaluations suggested that faster responses contribute to a better interaction experience.
>
---
