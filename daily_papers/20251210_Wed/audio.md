# 音频 cs.SD;  eess.AS

- **最新发布 10 篇**

- **更新 3 篇**

## 最新发布

#### [new 001] LocaGen: Sub-Sample Time-Delay Learning for Beam Localization
- **分类: cs.SD; eess.AS; eess.SP**

- **简介: 该论文研究2D音频波束定位任务，旨在降低采样量化误差对定位精度的影响。提出LocaGen方法，利用仿真生成的合成数据训练模型，提升到达方向和位置估计精度，并在低功耗嵌入式系统上实现高效实时运行。**

- **链接: [https://arxiv.org/pdf/2512.07872v1](https://arxiv.org/pdf/2512.07872v1)**

> **作者:** Ishaan Kunwar; Henry Cantor; Tyler Rizzo; Ayaan Qayyum
>
> **备注:** 7 pages
>
> **摘要:** The goal of LocaGen is to improve the localization performance of audio signals in the 2-D beam localization problem. LocaGen reduces sampling quantization errors through machine learning models trained on realistic synthetic data generated by a simulation. The system increases the accuracy of both direction-of-arrival (DOA) and precise location estimation of an audio beam from an array of three microphones. We demonstrate LocaGen's efficacy on a low-powered embedded system with an increased localization accuracy with a minimal increase in real-time resource usage. LocaGen was demonstrated to reduce DOA error by approximately 67% even with a microphone array of only 10 kHz in audio processing.
>
---
#### [new 002] SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality
- **分类: cs.SD; cs.AI**

- **简介: 该论文属于语音质量评估任务，旨在解决传统方法无法支持自然语言交互和缺乏可解释性的问题。作者提出SpeechQualityLLM，结合音频编码器与大语言模型，通过文本问答形式实现多模态语音质量评估，支持灵活查询与多样化判断，减少对人工测试的依赖。**

- **链接: [https://arxiv.org/pdf/2512.08238v1](https://arxiv.org/pdf/2512.08238v1)**

> **作者:** Mahathir Monjur; Shahriar Nirjon
>
> **备注:** 9 pages, 5 figures, 8 tables
>
> **摘要:** Objective speech quality assessment is central to telephony, VoIP, and streaming systems, where large volumes of degraded audio must be monitored and optimized at scale. Classical metrics such as PESQ and POLQA approximate human mean opinion scores (MOS) but require carefully controlled conditions and expensive listening tests, while learning-based models such as NISQA regress MOS and multiple perceptual dimensions from waveforms or spectrograms, achieving high correlation with subjective ratings yet remaining rigid: they do not support interactive, natural-language queries and do not natively provide textual rationales. In this work, we introduce SpeechQualityLLM, a multimodal speech quality question-answering (QA) system that couples an audio encoder with a language model and is trained on the NISQA corpus using template-based question-answer pairs covering overall MOS and four perceptual dimensions (noisiness, coloration, discontinuity, and loudness) in both single-ended (degraded only) and double-ended (degraded plus clean reference) setups. Instead of directly regressing scores, our system is supervised to generate textual answers from which numeric predictions are parsed and evaluated with standard regression and ranking metrics; on held-out NISQA clips, the double-ended model attains a MOS mean absolute error (MAE) of 0.41 with Pearson correlation of 0.86, with competitive performance on dimension-wise tasks. Beyond these quantitative gains, it offers a flexible natural-language interface in which the language model acts as an audio quality expert: practitioners can query arbitrary aspects of degradations, prompt the model to emulate different listener profiles to capture human variability and produce diverse but plausible judgments rather than a single deterministic score, and thereby reduce reliance on large-scale crowdsourced tests and their monetary cost.
>
---
#### [new 003] Error-Resilient Semantic Communication for Speech Transmission over Packet-Loss Networks
- **分类: cs.SD**

- **简介: 该论文研究语音在丢包网络中的鲁棒语义通信任务，旨在解决传统方法难以兼顾抗丢包性与系统兼容性的问题。提出Glaris框架，通过生成隐空间编码和隐先验实现高效丢包隐蔽与错误恢复，在保持兼容性的同时提升传输鲁棒性。**

- **链接: [https://arxiv.org/pdf/2512.08203v1](https://arxiv.org/pdf/2512.08203v1)**

> **作者:** Zhuohang Han; Jincheng Dai; Shengshi Yao; Junyi Wang; Yanlong Li; Kai Niu; Wenjun Xu; Ping Zhang
>
> **备注:** submitted to IEEE in Nov. 2025
>
> **摘要:** Real-time speech communication over wireless networks remains challenging, as conventional channel protection mechanisms cannot effectively counter packet loss under stringent bandwidth and latency constraints. Semantic communication has emerged as a promising paradigm for enhancing the robustness of speech transmission by means of joint source-channel coding (JSCC). However, its cross-layer design hinders practical deployment due to the incompatibility with existing digital communication systems. In this case, the robustness of speech communication is consequently evaluated primarily by the error-resilience to packet loss over wireless networks. To address these challenges, we propose \emph{Glaris}, a generative latent-prior-based resilient speech semantic communication framework that performs resilient speech coding in the generative latent space. Generative latent priors enable high-quality packet loss concealment (PLC) at the receiver side, well-balancing semantic consistency and reconstruction fidelity. Additionally, an integrated error resilience mechanism is designed to mitigate the error propagation and improve the effectiveness of PLC. Compared with traditional packet-level forward error correction (FEC) strategies, our new method achieves enhanced robustness over dynamic wireless networks while reducing redundancy overhead significantly. Experimental results on the LibriSpeech dataset demonstrate that \emph{Glaris} consistently outperforms existing error-resilient codecs, achieving JSCC-level robustness while maintaining seamless compatibility with existing systems, and it also strikes a favorable balance between transmission efficiency and speech reconstruction quality.
>
---
#### [new 004] Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS
- **分类: cs.SD; cs.CL; eess.AS**

- **简介: 该论文属于语音合成任务，旨在解决实时TTS中音素转换质量与速度的权衡问题。提出一种面向服务的轻量级、上下文感知音素化框架，将复杂模块解耦为独立服务，在保证低延迟的同时提升发音准确性，适用于端侧离线应用。**

- **链接: [https://arxiv.org/pdf/2512.08006v1](https://arxiv.org/pdf/2512.08006v1)**

> **作者:** Mahta Fetrat; Donya Navabi; Zahra Dehghanian; Morteza Abolghasemi; Hamid R. Rabiee
>
> **摘要:** Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance. This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.
>
---
#### [new 005] BUT Systems for Environmental Sound Deepfake Detection in the ESDD 2026 Challenge
- **分类: eess.AS**

- **简介: 该论文参与ESDD 2026挑战赛的环境声音深度伪造检测任务，旨在提升对未知生成器的泛化能力。提出基于多种自监督学习模型与轻量注意力机制的融合框架，并引入特征域增强策略，仅用官方数据训练，在多个测试集上取得优异性能。**

- **链接: [https://arxiv.org/pdf/2512.08319v1](https://arxiv.org/pdf/2512.08319v1)**

> **作者:** Junyi Peng; Lin Zhang; Jin Li; Oldrich Plchot; Jan Cernocky
>
> **摘要:** This paper describes the BUT submission to the ESDD 2026 Challenge, specifically focusing on Track 1: Environmental Sound Deepfake Detection with Unseen Generators. To address the critical challenge of generalizing to audio generated by unseen synthesis algorithms, we propose a robust ensemble framework leveraging diverse Self-Supervised Learning (SSL) models. We conduct a comprehensive analysis of general audio SSL models (including BEATs, EAT, and Dasheng) and speech-specific SSLs. These front-ends are coupled with a lightweight Multi-Head Factorized Attention (MHFA) back-end to capture discriminative representations. Furthermore, we introduce a feature domain augmentation strategy based on distribution uncertainty modeling to enhance model robustness against unseen spectral distortions. All models are trained exclusively on the official EnvSDD data, without using any external resources. Experimental results demonstrate the effectiveness of our approach: our best single system achieved Equal Error Rates (EER) of 0.00\%, 4.60\%, and 4.80\% on the Development, Progress (Track 1), and Final Evaluation sets, respectively. The fusion system further improved generalization, yielding EERs of 0.00\%, 3.52\%, and 4.38\% across the same partitions.
>
---
#### [new 006] AudioScene: Integrating Object-Event Audio into 3D Scenes
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文属音频-空间场景理解任务，旨在解决现有音频数据缺乏空间上下文的问题。作者构建了两个融合音频与3D场景的数据集AudioScanNet和AudioRoboTHOR，结合大模型与人工验证实现音频事件的空间标注，推动音频引导的视觉定位与机器人导航研究。**

- **链接: [https://arxiv.org/pdf/2512.07845v1](https://arxiv.org/pdf/2512.07845v1)**

> **作者:** Shuaihang Yuan; Congcong Wen; Muhammad Shafique; Anthony Tzes; Yi Fang
>
> **摘要:** The rapid advances in audio analysis underscore its vast potential for humancomputer interaction, environmental monitoring, and public safety; yet, existing audioonly datasets often lack spatial context. To address this gap, we present two novel audiospatial scene datasets, AudioScanNet and AudioRoboTHOR, designed to explore audioconditioned tasks within 3D environments. By integrating audio clips with spatially aligned 3D scenes, our datasets enable research on how audio signals interact with spatial context. To associate audio events with corresponding spatial information, we leverage the common sense reasoning ability of large language models and supplement them with rigorous human verification, This approach offers greater scalability compared to purely manual annotation while maintaining high standards of accuracy, completeness, and diversity, quantified through inter annotator agreement and performance on two benchmark tasks audio based 3D visual grounding and audio based robotic zeroshot navigation. The results highlight the limitations of current audiocentric methods and underscore the practical challenges and significance of our datasets in advancing audio guided spatial learning.
>
---
#### [new 007] Emovectors: assessing emotional content in jazz improvisations for creativity evaluation
- **分类: cs.SD; cs.AI**

- **简介: 该论文旨在评估爵士即兴演奏的创造力，提出通过情感内容量化创造力。作者基于音乐情绪特征构建“emovectors”嵌入表示，分析其与创造力的关系，探索可扩展的自动化创造力评价指标，服务于生成式AI系统评估。**

- **链接: [https://arxiv.org/pdf/2512.08812v1](https://arxiv.org/pdf/2512.08812v1)**

> **作者:** Anna Jordanous
>
> **备注:** Presented at IEEE Big Data 2025 3rd Workshop on AI Music Generation (AIMG 2025). https://www.intellisky.org/workshops/AIMG2025/workshop_AIMG2025.html
>
> **摘要:** Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.
>
---
#### [new 008] DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components
- **分类: cs.SD**

- **简介: 该论文聚焦音频深度伪造检测，旨在提升模型在跨域和多任务场景下的泛化能力。通过优化音频编码器与文本大模型的组合，提出DFALLM架构，实现了检测、定位与溯源等多任务SOTA性能。**

- **链接: [https://arxiv.org/pdf/2512.08403v1](https://arxiv.org/pdf/2512.08403v1)**

> **作者:** Yupei Li; Li Wang; Yuxiang Wang; Lei Wang; Rizhao Cai; Jie Shi; Björn W. Schuller; Zhizheng Wu
>
> **摘要:** Audio deepfake detection has recently garnered public concern due to its implications for security and reliability. Traditional deep learning methods have been widely applied to this task but often lack generalisability when confronted with newly emerging spoofing techniques and more tasks such as spoof attribution recognition rather than simple binary classification. In principle, Large Language Models (LLMs) are considered to possess the needed generalisation capabilities. However, previous research on Audio LLMs (ALLMs) indicates a generalization bottleneck in audio deepfake detection performance, even when sufficient data is available. Consequently, this study investigates the model architecture and examines the effects of the primary components of ALLMs, namely the audio encoder and the text-based LLM. Our experiments demonstrate that the careful selection and combination of audio encoders and text-based LLMs are crucial for unlocking the deepfake detection potential of ALLMs. We further propose an ALLM structure capable of generalizing deepfake detection abilities to out-of-domain spoofing tests and other deepfake tasks, such as spoof positioning and spoof attribution recognition. Our proposed model architecture achieves state-of-the-art (SOTA) performance across multiple datasets, including ASVSpoof2019, InTheWild, and Demopage, with accuracy reaching up to 95.76% on average, and exhibits competitive capabilities in other deepfake detection tasks such as attribution, and localisation compared to SOTA audio understanding models. Data and codes are provided in supplementary materials.
>
---
#### [new 009] An Adaptive Method for Target Curve Selection
- **分类: eess.AS**

- **简介: 该论文将交互式差分进化（IDE）算法应用于音频领域，旨在从非专业听众中获取偏好的耳机频响目标曲线。通过自适应成对评分实验收集数据，验证了方法在收敛性和偏好提取上的有效性。**

- **链接: [https://arxiv.org/pdf/2512.08313v1](https://arxiv.org/pdf/2512.08313v1)**

> **作者:** Gabriele Ravizza; Julián Villegas; Christer P. Volk; Tore Stegenborg-Andersen; Yan Pei
>
> **备注:** 8 pages,6 figures. Accepted for presentation at the Audio Engineering Society (AES) International Conference on Headphone Technology, 2025
>
> **摘要:** In this paper, we introduce an adaptation of the "Interactive Differential Evolution" (IDE) algorithm to the audio domain for the task of identifying the preferred over-the-ear headphone frequency response target among consumers. The method is based on data collection using an adaptive paired rating listening test paradigm (paired comparison with a scale). The IDE algorithm and its parameters are explained in detail. Additionally, data collected from three listening experiments with more than 20 consumers is presented, and the algorithm's performance in this untested domain is investigated on the basis of two convergence measures. The results indicate that this method can converge and may ease the task of 'extracting' frequency response preference from untrained consumers.
>
---
#### [new 010] PAVAS: Physics-Aware Video-to-Audio Synthesis
- **分类: cs.CV; cs.MM; cs.SD**

- **简介: 该论文研究视频到音频生成任务，旨在解决现有方法忽略物理因素导致声音不真实的问题。提出PAVAS模型，通过引入物理参数估计和物理感知适配器，结合质量、运动轨迹等物理信息生成更符合真实物理规律的音频，并构建新基准与指标验证效果。**

- **链接: [https://arxiv.org/pdf/2512.08282v1](https://arxiv.org/pdf/2512.08282v1)**

> **作者:** Oh Hyun-Bin; Yuhta Takida; Toshimitsu Uesaka; Tae-Hyun Oh; Yuki Mitsufuji
>
> **摘要:** Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.
>
---
## 更新

#### [replaced 001] Direction-of-Arrival and Noise Covariance Matrix joint estimation for beamforming
- **分类: eess.AS; math.OC**

- **简介: 该论文研究波束成形中的到达方向（DoA）与噪声协方差矩阵（NCM）联合估计。针对传统方法计算复杂、抗噪差的问题，提出一种准线性求解方法和跨频带DoA估计技术，提升中高角度下估计精度与信号增强性能。**

- **链接: [https://arxiv.org/pdf/2511.10639v3](https://arxiv.org/pdf/2511.10639v3)**

> **作者:** Vitor Gelsleichter Probst Curtarelli; Stephan Paul; Anderson Wedderhoff Spengler
>
> **摘要:** We propose a joint estimation method for the Direction-of-Arrival (DoA) and the Noise Covariance Matrix (NCM) tailored for beamforming applications. Building upon an existing NCM framework, our approach simplifies the estimation procedure by deriving an quasi-linear solution, instead of the traditional exhaustive search. Additionally, we introduce a novel DoA estimation technique that operates across all frequency bins, improving robustness in reverberant environments. Simulation results demonstrate that our method outperforms classical techniques, such as MUSIC, in mid- to high-angle scenarios, achieving lower angular errors and superior signal enhancement through beamforming. The proposed framework was also fared against other techniques for signal enhancement, having better noise rejection and interference canceling capabilities. These improvements are validated using both theoretical and empirical performance metrics.
>
---
#### [replaced 002] Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research
- **分类: cs.RO; cs.HC; eess.AS**

- **简介: 该论文致力于延长老旧机器人研究寿命，解决因厂商停支持导致的功能落后问题。作者改造NAO机器人，集成新型传感器与计算单元，提升感知与对话能力，保持原有交互特性，在不增加延迟的前提下显著提高对话质量与用户偏好。**

- **链接: [https://arxiv.org/pdf/2509.17760v3](https://arxiv.org/pdf/2509.17760v3)**

> **作者:** Austin Wilson; Sahar Kapasi; Zane Greene; Alexis E. Block
>
> **摘要:** Legacy (unsupported) robotic platforms often lose research utility when manufacturer support ends, preventing integration of modern sensing, speech, and interaction capabilities. We present the Enhanced NAO, a revitalized version of Aldebaran's NAO robot featuring upgraded beamforming microphones, RGB-D and thermal cameras, and additional compute resources in a fully self-contained package. This system combines cloud-based and local models for perception and dialogue, while preserving the NAO's expressive body and behaviors. In a pilot user study validating conversational performance, the Enhanced NAO delivered significantly higher conversational quality and elicited stronger user preference compared to the NAO AI Edition, without increasing response latency. The added visual and thermal sensing modalities established a foundation for future perception-driven interaction. Beyond this implementation, our framework provides a platform-agnostic strategy for extending the lifespan and research utility of legacy robots, ensuring they remain valuable tools for human-robot interaction.
>
---
#### [replaced 003] VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning
- **分类: cs.SD; cs.AI; cs.CV; cs.MM; eess.AS**

- **简介: 该论文提出VoiceCloak，针对扩散模型驱动的语音克隆进行主动防御。通过在参考音频中引入对抗扰动，扰乱说话人身份表征与条件引导机制，并降低生成语音质量，有效遏制未经授权的语音克隆。**

- **链接: [https://arxiv.org/pdf/2505.12332v5](https://arxiv.org/pdf/2505.12332v5)**

> **作者:** Qianyue Hu; Junyan Wu; Wei Lu; Xiangyang Luo
>
> **备注:** 15 pages, 6 figures, 13 tables; Accepted by AAAI 2026
>
> **摘要:** Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at https://voice-cloak.github.io/VoiceCloak/.
>
---
