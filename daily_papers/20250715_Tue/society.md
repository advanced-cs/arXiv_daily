# 计算机与社会 cs.CY

- **最新发布 25 篇**

- **更新 13 篇**

## 最新发布

#### [new 001] Preliminary Analysis of Construction Work Zone on Roadways in Florida by Crash Severity
- **分类: cs.CY**

- **简介: 该论文分析了佛罗里达州道路施工区的事故严重性，旨在识别导致致命和重伤事故的关键因素。研究任务属于交通安全分析。通过多分类逻辑模型，评估了事故类型、天气、照明条件等因素的影响，并探讨利用机器学习提升预警系统以改善施工区安全。**

- **链接: [http://arxiv.org/pdf/2507.08869v1](http://arxiv.org/pdf/2507.08869v1)**

> **作者:** Tatiana Deslouches; Doreen Kobelo Regalado; Mohamed Khalafalla; Tejal Mulay
>
> **摘要:** Construction zones are inherently hazardous, posing significant risks to construction workers and motorists. Despite existing safety measures, construction zones continue to witness fatalities and serious injuries, imposing economic burdens. Addressing these issues requires understanding root causes and implementing preventive strategies centered around the 4Es (Engineering, Education, Enforcement, Emergency Response) and 4Is (Information Intelligence, Innovation, Insight into communities, Investment, and Policies). Proper safety management, integrating these strategic initiatives, aims to reduce and potentially eliminate fatalities and serious injuries in work zones. In Florida, road construction work zone fatalities and serious injuries remain a critical concern, especially in urban counties. Despite a 12 billion dollars infrastructure investment in 2022, Florida ranks eighth nationally for fatal work zone crashes involving commercial motor vehicles (CMVs). Analysis from 2019 to 2023 shows an average of 71 fatalities and 309 serious injuries annually in Florida work zones, reflecting a persistent safety challenge. High-risk counties include Orange, Broward, Duval, Hillsborough, Pasco, Miami-Dade, Seminole, Manatee, Palm Beach, and Lake. This study presents a preliminary analysis of work zone crashes in Broward, Duval, Hillsborough, and Orange counties. A multilogit model assessed attributes contributing to fatalities and serious injuries, such as crash type, weather and light conditions, work zone type, type of shoulder, presence of workers, and law enforcement. Results indicate significant contributing factors, highlighting opportunities to use machine learning for alerting drivers and construction managers, ultimately enhancing safety protocols and reducing fatalities.
>
---
#### [new 002] The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions
- **分类: cs.CY; cs.AI; cs.SI**

- **简介: 该论文研究大语言模型（LLM）在司法决策中的“一致性-可接受性偏离”问题，即技术一致性与社会接受度之间的差距。论文任务是分析该问题并提出解决方案，通过分析2023–2025年数据，提出双轨多角色协商治理框架（DTDMR-LJGF），实现任务分类和多方互动，以平衡技术效率与社会合法性。**

- **链接: [http://arxiv.org/pdf/2507.08881v1](http://arxiv.org/pdf/2507.08881v1)**

> **作者:** Zhang MingDa; Xu Qing
>
> **备注:** 12 pages,2 figures
>
> **摘要:** The integration of large language model (LLM) technology into judicial systems is fundamentally transforming legal practice worldwide. However, this global transformation has revealed an urgent paradox requiring immediate attention. This study introduces the concept of ``consistency-acceptability divergence'' for the first time, referring to the gap between technical consistency and social acceptance. While LLMs achieve high consistency at the technical level, this consistency demonstrates both positive and negative effects. Through comprehensive analysis of recent data on LLM judicial applications from 2023--2025, this study finds that addressing this challenge requires understanding both task and stakeholder dimensions. This study proposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance Framework (DTDMR-LJGF), which enables intelligent task classification and meaningful interaction among diverse stakeholders. This framework offers both theoretical insights and practical guidance for building an LLM judicial ecosystem that balances technical efficiency with social legitimacy.
>
---
#### [new 003] If open source is to win, it must go public
- **分类: cs.CY**

- **简介: 论文探讨了开源AI在推动人工智能普及中的局限性，提出需结合公共AI基础设施和机构，以确保模型的可访问性、可持续性及公共利益治理。任务是分析开源AI的挑战并提出解决方案，工作是论证公共AI的必要性。**

- **链接: [http://arxiv.org/pdf/2507.09296v1](http://arxiv.org/pdf/2507.09296v1)**

> **作者:** Joshua Tan; Nicholas Vincent; Katherine Elkins; Magnus Sahlgren
>
> **备注:** CodeML @ ICML 2025
>
> **摘要:** Open source projects have made incredible progress in producing transparent and widely usable machine learning models and systems, but open source alone will face challenges in fully democratizing access to AI. Unlike software, AI models require substantial resources for activation -- compute, post-training, deployment, and oversight -- which only a few actors can currently provide. This paper argues that open source AI must be complemented by public AI: infrastructure and institutions that ensure models are accessible, sustainable, and governed in the public interest. To achieve the full promise of AI models as prosocial public goods, we need to build public infrastructure to power and deliver open source software and models.
>
---
#### [new 004] ESG and the Cost of Capital: Insights from an AI-Assisted Systematic Literature Review
- **分类: cs.CY**

- **简介: 该论文属于文献综述任务，旨在解决传统文献筛选与分析效率低的问题。作者通过AI辅助工具识别、筛选和分析36篇关于ESG与资本成本关系的文献，总结关键发现与理论框架，展示了AI在学术研究中的应用价值。**

- **链接: [http://arxiv.org/pdf/2507.09020v1](http://arxiv.org/pdf/2507.09020v1)**

> **作者:** Ebenezer Asem; Ruijie Fan; Gloria Y. Tian
>
> **摘要:** This paper explores how AI-powered tools could be leveraged to streamline the process of identifying, screening, and analyzing relevant literature in academic research. More specifically, we examine the documented relationship between environmental, social, and governance (ESG) factors and the cost of capital (CoC). By applying an AI-assisted workflow, we identified 36 published studies, synthesized their key findings, and highlighted relevant theories, moderators, and methodological challenges. Our analyses demonstrate the value of AI tools in enhancing business research processes and also contribute to the growing literature on the importance of ESG in the field of corporate finance.
>
---
#### [new 005] Secondary Bounded Rationality: A Theory of How Algorithms Reproduce Structural Inequality in AI Hiring
- **分类: cs.CY**

- **简介: 该论文研究AI招聘系统如何通过“次级有限理性”理论加剧结构性不平等。任务是分析算法在招聘中继承并放大人类偏见的机制，解决问题为揭示AI看似客观却仍导致不公平的原因。工作包括结合布迪厄资本理论与西蒙有限理性理论，分析多模态招聘框架，并提出公平测试、审计和监管等缓解策略。**

- **链接: [http://arxiv.org/pdf/2507.09233v1](http://arxiv.org/pdf/2507.09233v1)**

> **作者:** Jia Xiao
>
> **摘要:** AI-driven recruitment systems, while promising efficiency and objectivity, often perpetuate systemic inequalities by encoding cultural and social capital disparities into algorithmic decision making. This article develops and defends a novel theory of secondary bounded rationality, arguing that AI systems, despite their computational power, inherit and amplify human cognitive and structural biases through technical and sociopolitical constraints. Analyzing multimodal recruitment frameworks, we demonstrate how algorithmic processes transform historical inequalities, such as elite credential privileging and network homophily, into ostensibly meritocratic outcomes. Using Bourdieusian capital theory and Simon's bounded rationality, we reveal a recursive cycle where AI entrenches exclusion by optimizing for legible yet biased proxies of competence. We propose mitigation strategies, including counterfactual fairness testing, capital-aware auditing, and regulatory interventions, to disrupt this self-reinforcing inequality.
>
---
#### [new 006] Can AI Rely on the Systematicity of Truth? The Challenge of Modelling Normative Domains
- **分类: cs.CY**

- **简介: 论文探讨大型语言模型（LLMs）能否依赖“真理的系统性”来准确建模世界，尤其关注规范性领域。它属于哲学与人工智能交叉任务，旨在分析LLMs在建模规范性领域时面临的挑战。作者指出，若规范领域缺乏系统性，则LLMs难以填补数据空白或纠正错误，从而限制其辅助人类实践推理的能力。**

- **链接: [http://arxiv.org/pdf/2507.09676v1](http://arxiv.org/pdf/2507.09676v1)**

> **作者:** Matthieu Queloz
>
> **摘要:** A key assumption fuelling optimism about the progress of large language models (LLMs) in accurately and comprehensively modelling the world is that the truth is systematic: true statements about the world form a whole that is not just consistent, in that it contains no contradictions, but coherent, in that the truths are inferentially interlinked. This holds out the prospect that LLMs might in principle rely on that systematicity to fill in gaps and correct inaccuracies in the training data: consistency and coherence promise to facilitate progress towards comprehensiveness in an LLM's representation of the world. However, philosophers have identified compelling reasons to doubt that the truth is systematic across all domains of thought, arguing that in normative domains, in particular, the truth is largely asystematic. I argue that insofar as the truth in normative domains is asystematic, this renders it correspondingly harder for LLMs to make progress, because they cannot then leverage the systematicity of truth. And the less LLMs can rely on the systematicity of truth, the less we can rely on them to do our practical deliberation for us, because the very asystematicity of normative domains requires human agency to play a greater role in practical thought.
>
---
#### [new 007] A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于内容审核任务，旨在应对深度伪造技术对民主社会的威胁。论文分析了现有标记、检测和标注深度伪造内容的方法及其在欧盟法规下的有效性，提出了一种多层级策略以提升可扩展性和实用性，并通过评分机制实现技术无关和情境适配的风险管理。**

- **链接: [http://arxiv.org/pdf/2507.08879v1](http://arxiv.org/pdf/2507.08879v1)**

> **作者:** Max-Paul Förster; Luca Deck; Raimund Weidlich; Niklas Kühl
>
> **摘要:** The growing availability and use of deepfake technologies increases risks for democratic societies, e.g., for political communication on online platforms. The EU has responded with transparency obligations for providers and deployers of Artificial Intelligence (AI) systems and online platforms. This includes marking deepfakes during generation and labeling deepfakes when they are shared. However, the lack of industry and enforcement standards poses an ongoing challenge. Through a multivocal literature review, we summarize methods for marking, detecting, and labeling deepfakes and assess their effectiveness under EU regulation. Our results indicate that individual methods fail to meet regulatory and practical requirements. Therefore, we propose a multi-level strategy combining the strengths of existing methods. To account for the masses of content on online platforms, our multi-level strategy provides scalability and practicality via a simple scoring mechanism. At the same time, it is agnostic to types of deepfake technology and allows for context-specific risk weighting.
>
---
#### [new 008] CALMA: A Process for Deriving Context-aligned Axes for Language Model Alignment
- **分类: cs.CY**

- **简介: 该论文属于自然语言处理任务中的模型对齐与评估。旨在解决现有对齐数据集忽视多样部署背景的问题。提出CALMA方法，通过参与式流程获取特定场景的评估维度，支持多元、透明和情境敏感的模型对齐。**

- **链接: [http://arxiv.org/pdf/2507.09060v1](http://arxiv.org/pdf/2507.09060v1)**

> **作者:** Prajna Soni; Deepika Raman; Dylan Hadfield-Menell
>
> **摘要:** Datasets play a central role in AI governance by enabling both evaluation (measuring capabilities) and alignment (enforcing values) along axes such as helpfulness, harmlessness, toxicity, quality, and more. However, most alignment and evaluation datasets depend on researcher-defined or developer-defined axes curated from non-representative samples. As a result, developers typically benchmark models against broad (often Western-centric) values that overlook the varied contexts of their real-world deployment. Consequently, models trained on such proxies can fail to meet the needs and expectations of diverse user communities within these deployment contexts. To bridge this gap, we introduce CALMA (Context-aligned Axes for Language Model Alignment), a grounded, participatory methodology for eliciting context-relevant axes for evaluation and alignment. In a pilot with two distinct communities, CALMA surfaced novel priorities that are absent from standard benchmarks. Our findings demonstrate the value of evaluation practices based on open-ended and use-case-driven processes. Our work advances the development of pluralistic, transparent, and context-sensitive alignment pipelines.
>
---
#### [new 009] The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations
- **分类: cs.CY; cs.ET; cs.LG**

- **简介: 该论文探讨如何在工程领域（尤其是建筑行业）建立法律框架，以规范机器学习技术的集成与应用。任务是解决当前工程行业中因采用ML技术而引发的法律责任、伦理和监管不确定性问题。作者通过分析相关法律先例、责任制度及其它领域的经验，提出利用类比推理将ML嵌入现有工程规范，并呼吁立法机构和标准组织提供明确指导，从而构建一个能评估责任、风险与收益的法律框架。**

- **链接: [http://arxiv.org/pdf/2507.08908v1](http://arxiv.org/pdf/2507.08908v1)**

> **作者:** M. Z. Naser
>
> **摘要:** Despite the widespread interest in machine learning (ML), the engineering industry has not yet fully adopted ML-based methods, which has left engineers and stakeholders uncertain about the legal and regulatory frameworks that govern their decisions. This gap remains unaddressed as an engineer's decision-making process, typically governed by professional ethics and practical guidelines, now intersects with complex algorithmic outputs. To bridge this gap, this paper explores how engineers can navigate legal principles and legislative justifications that support and/or contest the deployment of ML technologies. Drawing on recent precedents and experiences gained from other fields, this paper argues that analogical reasoning can provide a basis for embedding ML within existing engineering codes while maintaining professional accountability and meeting safety requirements. In exploring these issues, the discussion focuses on established liability doctrines, such as negligence and product liability, and highlights how courts have evaluated the use of predictive models. We further analyze how legislative bodies and standard-setting organizations can furnish explicit guidance equivalent to prior endorsements of emergent technologies. This exploration stresses the vitality of understanding the interplay between technical justifications and legal precedents for shaping an informed stance on ML's legitimacy in engineering practice. Finally, our analysis catalyzes a legal framework for integrating ML through which stakeholders can critically assess the responsibilities, liabilities, and benefits inherent in ML-driven engineering solutions.
>
---
#### [new 010] The Narrative Construction of Generative AI Efficacy by the Media: A Case Study of the Role of ChatGPT in Higher Education
- **分类: cs.CY**

- **简介: 该论文分析美国媒体如何构建生成式AI（以ChatGPT为例）在高等教育中效能的叙事。基于议程设置理论，研究通过LDA主题建模与情感分析198篇文章，探讨媒体对AI在教育中的应用所塑造的公众认知。旨在揭示媒体报道对AI教育采纳的影响，并为政策制定者、教育者和开发者提供参考。**

- **链接: [http://arxiv.org/pdf/2507.09239v1](http://arxiv.org/pdf/2507.09239v1)**

> **作者:** Yinan Sun; Ali Unlu; Aditya Johri
>
> **备注:** Final draft of article under review at a journal
>
> **摘要:** The societal role of technology, including artificial intelligence (AI), is often shaped by sociocultural narratives. This study examines how U.S. news media construct narratives about the efficacy of generative AI (GenAI), using ChatGPT in higher education as a case study. Grounded in Agenda Setting Theory, we analyzed 198 articles published between November 2022 and October 2024, employing LDA topic modeling and sentiment analysis. Our findings identify six key topics in the media discourse, with sentiment analysis revealing generally positive portrayals of ChatGPT's integration into higher education through policy, curriculum, teaching practices, collaborative decision-making, skill development, and human-centered learning. In contrast, media narratives express more negative sentiment regarding their impact on entry-level jobs and college admissions. This research highlights how media coverage can influence public perceptions of GenAI in education and provides actionable insights for policymakers, educators, and AI developers navigating its adoption and representation in public discourse.
>
---
#### [new 011] 'Teens Need to Be Educated on the Danger': Digital Access, Online Risks, and Safety Practices Among Nigerian Adolescents
- **分类: cs.HC; cs.CY**

- **简介: 该论文研究尼日利亚青少年的网络风险与安全行为，属于社会科学研究任务。旨在了解其网络使用情况、面临的风险及应对策略，通过调查409名中学生，分析影响其网络安全行为的因素，并提出教育、家长监督和技术改进等建议，以提升青少年的网络安全性。**

- **链接: [http://arxiv.org/pdf/2507.08914v1](http://arxiv.org/pdf/2507.08914v1)**

> **作者:** Munachimso B. Oguine; Ozioma C. Oguine; Karla Badillo-Urquiola; Oluwasogo Adekunle Okunade
>
> **备注:** 14 pages, 4 figures. Accepted to AfriCHI 2025
>
> **摘要:** Adolescents increasingly rely on online technologies to explore their identities, form social connections, and access information and entertainment. However, their growing digital engagement exposes them to significant online risks, particularly in underrepresented contexts like West Africa. This study investigates the online experiences of 409 secondary school adolescents in Nigeria's Federal Capital Territory (FCT), focusing on their access to technology, exposure to risks, coping strategies, key stakeholders influencing their online interactions, and recommendations for improving online safety. Using self-administered surveys, we found that while most adolescents reported moderate access to online technology and connectivity, those who encountered risks frequently reported exposure to inappropriate content and online scams. Blocking and reporting tools were the most commonly used strategies, though some adolescents responded with inaction due to limited resources or awareness. Parents emerged as the primary support network, though monitoring practices and communication varied widely. Guided by Protection Motivation Theory (PMT), our analysis interprets adolescents' online safety behaviors as shaped by both their threat perceptions and their confidence in available coping strategies. A thematic analysis of their recommendations highlights the need for greater awareness and education, parental mediation, enhanced safety tools, stricter age restrictions, improved content moderation, government accountability, and resilience-building initiatives. Our findings underscore the importance of culturally and contextually relevant interventions to empower adolescents in navigating the digital world, with implications for parents, educators, designers, and policymakers.
>
---
#### [new 012] Fair CCA for Fair Representation Learning: An ADNI Study
- **分类: cs.LG; cs.AI; cs.CY**

- **简介: 该论文属于表示学习任务，旨在解决多模态数据中公平性不足的问题。通过改进典型相关分析（CCA），提出一种新的公平表示学习方法，使投影特征与敏感属性无关，从而在保持高相关性的同时提升分类任务的公平性，并在ADNI数据集上验证了方法有效性。**

- **链接: [http://arxiv.org/pdf/2507.09382v1](http://arxiv.org/pdf/2507.09382v1)**

> **作者:** Bojian Hou; Zhanliang Wang; Zhuoping Zhou; Boning Tong; Zexuan Wang; Jingxuan Bao; Duy Duong-Tran; Qi Long; Li Shen
>
> **摘要:** Canonical correlation analysis (CCA) is a technique for finding correlations between different data modalities and learning low-dimensional representations. As fairness becomes crucial in machine learning, fair CCA has gained attention. However, previous approaches often overlook the impact on downstream classification tasks, limiting applicability. We propose a novel fair CCA method for fair representation learning, ensuring the projected features are independent of sensitive attributes, thus enhancing fairness without compromising accuracy. We validate our method on synthetic data and real-world data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating its ability to maintain high correlation analysis performance while improving fairness in classification tasks. Our work enables fair machine learning in neuroimaging studies where unbiased analysis is essential.
>
---
#### [new 013] Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)
- **分类: cs.SI; cs.AI; cs.CY; cs.LG; I.2.7; J.4**

- **简介: 该论文属于健康信息分类任务，旨在解决社交媒体上的健康虚假信息识别问题。作者基于ELM理论，结合CNN-LSTM模型，引入文本可读性、情感极性等特征，提升检测准确性和可靠性，实验表明模型性能显著提高。**

- **链接: [http://arxiv.org/pdf/2507.09149v1](http://arxiv.org/pdf/2507.09149v1)**

> **作者:** Mkululi Sikosana; Sean Maudsley-Barton; Oluwaseun Ajao
>
> **备注:** 11 Pages, 2 Figures, 3 Tables conference paper to appear in proceedings of International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA'25)
>
> **摘要:** Health misinformation during the COVID-19 pandemic has significantly challenged public health efforts globally. This study applies the Elaboration Likelihood Model (ELM) to enhance misinformation detection on social media using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) model. The model aims to enhance the detection accuracy and reliability of misinformation classification by integrating ELM-based features such as text readability, sentiment polarity, and heuristic cues (e.g., punctuation frequency). The enhanced model achieved an accuracy of 97.37%, precision of 96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined model incorporating feature engineering further improved performance, achieving a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of 99.80%. These findings highlight the value of ELM features in improving detection performance, offering valuable contextual information. This study demonstrates the practical application of psychological theories in developing advanced machine learning algorithms to address health misinformation effectively.
>
---
#### [new 014] Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications
- **分类: cs.SE; cs.CY**

- **简介: 该论文属于AI安全评估任务，旨在解决大语言模型（LLM）在实际应用中的安全性风险问题。论文提出了一套评估应用级安全风险的框架，包括定制化安全风险分类原则和评估实践，并通过实际部署验证其有效性，为组织提供可操作的安全测试指导。**

- **链接: [http://arxiv.org/pdf/2507.09820v1](http://arxiv.org/pdf/2507.09820v1)**

> **作者:** Jia Yi Goh; Shaun Khoo; Nyx Iskandar; Gabriel Chua; Leanne Tan; Jessica Foo
>
> **摘要:** Most safety testing efforts for large language models (LLMs) today focus on evaluating foundation models. However, there is a growing need to evaluate safety at the application level, as components such as system prompts, retrieval pipelines, and guardrails introduce additional factors that significantly influence the overall safety of LLM applications. In this paper, we introduce a practical framework for evaluating application-level safety in LLM systems, validated through real-world deployment across multiple use cases within our organization. The framework consists of two parts: (1) principles for developing customized safety risk taxonomies, and (2) practices for evaluating safety risks in LLM applications. We illustrate how the proposed framework was applied in our internal pilot, providing a reference point for organizations seeking to scale their safety testing efforts. This work aims to bridge the gap between theoretical concepts in AI safety and the operational realities of safeguarding LLM applications in practice, offering actionable guidance for safe and scalable deployment.
>
---
#### [new 015] Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond
- **分类: cs.LG; cs.CY; stat.ML**

- **简介: 该论文研究数据偏差对算法歧视的影响，旨在解决AI公平性问题。作者分析了三类常见数据偏差，提出“数据偏差画像”（DBP）用于系统识别偏差，并通过实验验证其预测歧视风险的能力，推动算法公平性研究与政策制定的结合。**

- **链接: [http://arxiv.org/pdf/2507.08866v1](http://arxiv.org/pdf/2507.08866v1)**

> **作者:** Marina Ceccon; Giandomenico Cornacchia; Davide Dalle Pezze; Alessandro Fabris; Gian Antonio Susto
>
> **备注:** Accepted in Expert Systems with Applications
>
> **摘要:** Undesirable biases encoded in the data are key drivers of algorithmic discrimination. Their importance is widely recognized in the algorithmic fairness literature, as well as legislation and standards on anti-discrimination in AI. Despite this recognition, data biases remain understudied, hindering the development of computational best practices for their detection and mitigation. In this work, we present three common data biases and study their individual and joint effect on algorithmic discrimination across a variety of datasets, models, and fairness measures. We find that underrepresentation of vulnerable populations in training sets is less conducive to discrimination than conventionally affirmed, while combinations of proxies and label bias can be far more critical. Consequently, we develop dedicated mechanisms to detect specific types of bias, and combine them into a preliminary construct we refer to as the Data Bias Profile (DBP). This initial formulation serves as a proof of concept for how different bias signals can be systematically documented. Through a case study with popular fairness datasets, we demonstrate the effectiveness of the DBP in predicting the risk of discriminatory outcomes and the utility of fairness-enhancing interventions. Overall, this article bridges algorithmic fairness research and anti-discrimination policy through a data-centric lens.
>
---
#### [new 016] A New Incentive Model For Content Trust
- **分类: cs.GT; cs.CY; econ.GN; q-fin.EC**

- **简介: 该论文属于内容可信度验证任务，旨在解决数字时代虚假信息泛滥问题。通过结合智能合约与数字身份，建立基于信任的激励机制，鼓励创作者质押资金以确保内容真实性，并由社区参与事实核查，推动形成去中心化的内容治理模式。**

- **链接: [http://arxiv.org/pdf/2507.09972v1](http://arxiv.org/pdf/2507.09972v1)**

> **作者:** Lucas Barbosa; Sam Kirshner; Rob Kopel; Eric Tze Kuan Lim; Tom Pagram
>
> **备注:** 20 pages, 6 figures and 2 tables
>
> **摘要:** This paper outlines an incentive-driven and decentralized approach to verifying the veracity of digital content at scale. Widespread misinformation, an explosion in AI-generated content and reduced reliance on traditional news sources demands a new approach for content authenticity and truth-seeking that is fit for a modern, digital world. By using smart contracts and digital identity to incorporate 'trust' into the reward function for published content, not just engagement, we believe that it could be possible to foster a self-propelling paradigm shift to combat misinformation through a community-based governance model. The approach described in this paper requires that content creators stake financial collateral on factual claims for an impartial jury to vet with a financial reward for contribution. We hypothesize that with the right financial and social incentive model users will be motivated to participate in crowdsourced fact-checking and content creators will place more care in their attestations. This is an exploratory paper and there are a number of open issues and questions that warrant further analysis and exploration.
>
---
#### [new 017] Technical Requirements for Halting Dangerous AI Activities
- **分类: cs.AI; cs.CY**

- **简介: 该论文属于AI治理任务，旨在应对AI快速发展带来的失控、滥用等风险。论文提出了关键技术干预措施，用于协调阻止危险AI活动，并为AI治理方案提供技术基础。**

- **链接: [http://arxiv.org/pdf/2507.09801v1](http://arxiv.org/pdf/2507.09801v1)**

> **作者:** Peter Barnett; Aaron Scher; David Abecassis
>
> **摘要:** The rapid development of AI systems poses unprecedented risks, including loss of control, misuse, geopolitical instability, and concentration of power. To navigate these risks and avoid worst-case outcomes, governments may proactively establish the capability for a coordinated halt on dangerous AI development and deployment. In this paper, we outline key technical interventions that could allow for a coordinated halt on dangerous AI activities. We discuss how these interventions may contribute to restricting various dangerous AI activities, and show how these interventions can form the technical foundation for potential AI governance plans.
>
---
#### [new 018] MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis
- **分类: cs.CL; cs.CY**

- **简介: 该论文构建了气候可视化隐喻数据库MetaClimage，分析其传播效果。任务是评估视觉隐喻在气候传播中的成本与效益。解决缺乏系统材料和影响研究的问题。工作包括收集图像、人员认知评分及自然语言处理分析，揭示视觉隐喻认知负荷高但审美更强，促进深度思考。**

- **链接: [http://arxiv.org/pdf/2507.09225v1](http://arxiv.org/pdf/2507.09225v1)**

> **作者:** Biagio Scalingi; Chiara Barattieri di San Pietro; Paolo Canal; Valentina Bambini
>
> **备注:** 27 pages, 5 figures
>
> **摘要:** Visual metaphors of climate change (e.g., melting glaciers depicted as a melting ice grenade) are regarded as valuable tools for addressing the complexity of environmental challenges. However, few studies have examined their impact on communication, also due to scattered availability of material. Here, we present a novel database of Metaphors of Climate Change in Images (MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal images and enriched with human ratings. For each image, we collected values of difficulty, efficacy, artistic quality, and emotional arousal from human rating, as well as number of tags generated by participants to summarize the message. Semantic and emotion variables were further derived from the tags via Natural Language Processing. Visual metaphors were rated as more difficult to understand, yet more aesthetically pleasant than literal images, but did not differ in efficacy and arousal. The latter for visual metaphors, however, was higher in participants with higher Need For Cognition. Furthermore, visual metaphors received more tags, often referring to entities not depicted in the image, and elicited words with more positive valence and greater dominance than literal images. These results evidence the greater cognitive load of visual metaphors, which nevertheless might induce positive effects such as deeper cognitive elaboration and abstraction compared to literal stimuli. Furthermore, while they are not deemed as more effective and arousing, visual metaphors seem to generate superior aesthetic appreciation and a more positively valenced experience. Overall, this study contributes to understanding the impact of visual metaphors of climate change both by offering a database for future research and by elucidating a cost-benefit trade-off to take into account when shaping environmental communication.
>
---
#### [new 019] Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen Critical Thinking
- **分类: cs.HC; cs.CY**

- **简介: 该论文提出“认知失调人工智能”（CD-AI），旨在通过维持不确定性而非提供答案，增强人类批判性思维。它挑战传统AI优化决策的范式，转而激发用户反思、辩证思考与认知适应力，应用于伦理、法律等领域，并探讨相关伦理风险。论文属于人工智能与认知科学交叉任务，解决算法过度确定性导致思维僵化的问题。**

- **链接: [http://arxiv.org/pdf/2507.08804v1](http://arxiv.org/pdf/2507.08804v1)**

> **作者:** Delia Deliu
>
> **备注:** Presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING
>
> **摘要:** AI-augmented systems are traditionally designed to streamline human decision-making by minimizing cognitive load, clarifying arguments, and optimizing efficiency. However, in a world where algorithmic certainty risks becoming an Orwellian tool of epistemic control, true intellectual growth demands not passive acceptance but active struggle. Drawing on the dystopian visions of George Orwell and Philip K. Dick - where reality is unstable, perception malleable, and truth contested - this paper introduces Cognitive Dissonance AI (CD-AI): a novel framework that deliberately sustains uncertainty rather than resolving it. CD-AI does not offer closure, but compels users to navigate contradictions, challenge biases, and wrestle with competing truths. By delaying resolution and promoting dialectical engagement, CD-AI enhances reflective reasoning, epistemic humility, critical thinking, and adaptability in complex decision-making. This paper examines the theoretical foundations of the approach, presents an implementation model, explores its application in domains such as ethics, law, politics, and science, and addresses key ethical concerns - including decision paralysis, erosion of user autonomy, cognitive manipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt rather than a deliverer of certainty, CD-AI challenges dominant paradigms of AI-augmented reasoning and offers a new vision - one in which AI sharpens the mind not by resolving conflict, but by sustaining it. Rather than reinforcing Huxleyan complacency or pacifying the user into intellectual conformity, CD-AI echoes Nietzsche's vision of the Uebermensch - urging users to transcend passive cognition through active epistemic struggle.
>
---
#### [new 020] CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers
- **分类: cs.AR; cs.CY; cs.PF; B.8.2; C.0; C.1.4; C.4; C.5.5; J.4; K.1; K.4.1; K.6.4**

- **简介: 该论文属于可持续计算任务，旨在解决数据中心碳排放与能源消耗问题。作者提出了CEO-DC模型，综合考虑成本、碳排放和计算需求，指导数据中心设备采购与升级策略，分析了硬件更新周期、经济激励及碳价对减排的影响，强调需多方协作实现绿色数据中心。**

- **链接: [http://arxiv.org/pdf/2507.08923v1](http://arxiv.org/pdf/2507.08923v1)**

> **作者:** Rubén Rodríguez Álvarez; Denisa-Andreea Constantinescu; Miguel Peón-Quirós; David Atienza
>
> **备注:** 15 pages, 11 figures, 2 tables
>
> **摘要:** The rapid expansion of data centers (DCs) to support large-scale AI and scientific workloads is driving unsustainable growth in energy consumption and greenhouse gas emissions. While successive generations of hardware platforms have improved performance and energy efficiency, the question remains whether new, more efficient platforms can realistically offset the rising emissions associated with increasing demand. Prior studies often overlook the complex trade-offs in such transitions by failing to account for both the economic incentives and the projected compute demand growth over the operational lifetime of the devices. In response, we present CEO-DC, an integrated model and decision-making methodology for Carbon and Economy Optimization in Data Centers. CEO-DC models the competing forces of cost, carbon, and compute demand to guide optimal platform procurement and replacement strategies. We propose metrics to steer procurement, platform design, and policy decisions toward sustainable DC technologies. Given current platform trends, our AI case study using CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces total emissions. However, these upgrades fail to scale with DC demand growth trends without increasing total emissions in over 44% of cases, and require economic incentives for adoption in over 72%. Furthermore, current carbon prices are insufficient to motivate upgrades in 9 out of the 14 countries with the highest number of DCs globally. We also find that optimizing platforms for energy efficiency at the expense of latency can increase the carbon price required to justify their adoption. In summary, CEO-DC provides actionable insights for DC architects, platform designers, and policymakers by timing legacy platform upgrades, constraining DC growth to sustainable levels, optimizing platform performance-to-cost ratios, and increasing incentives.
>
---
#### [new 021] Non-linear, Team-based VR Training for Cardiac Arrest Care with enhanced CRM Toolkit
- **分类: cs.HC; cs.CY; cs.GR**

- **简介: 论文提出iREACT，一种基于VR的非线性、团队协作的心脏骤停培训系统，旨在解决传统培训方法在动态场景模拟和CRM技能培养上的不足。系统通过多模态数据采集与实时反馈，提升团队协作、沟通与决策训练效果，适用于高风险场景的技能培训。**

- **链接: [http://arxiv.org/pdf/2507.08805v1](http://arxiv.org/pdf/2507.08805v1)**

> **作者:** Mike Kentros; Manos Kamarianakis; Michael Cole; Vitaliy Popov; Antonis Protopsaltis; George Papagiannakis
>
> **备注:** 4 pages, 3 figures, 1 table
>
> **摘要:** This paper introduces iREACT, a novel VR simulation addressing key limitations in traditional cardiac arrest (CA) training. Conventional methods struggle to replicate the dynamic nature of real CA events, hindering Crew Resource Management (CRM) skill development. iREACT provides a non-linear, collaborative environment where teams respond to changing patient states, mirroring real CA complexities. By capturing multi-modal data (user actions, cognitive load, visual gaze) and offering real-time and post-session feedback, iREACT enhances CRM assessment beyond traditional methods. A formative evaluation with medical experts underscores its usability and educational value, with potential applications in other high-stakes training scenarios to improve teamwork, communication, and decision-making.
>
---
#### [new 022] Knowledge Conceptualization Impacts RAG Efficacy
- **分类: cs.AI; cs.CY; cs.IR**

- **简介: 该论文研究知识表示方式对检索增强生成（RAG）系统效果的影响，属于AI可解释性与适应性任务。旨在解决如何设计兼具可解释性和跨领域适应性的神经符号AI系统问题。工作聚焦于分析不同知识结构和复杂度对大语言模型查询三元组存储库的效果影响，并评估其表现。**

- **链接: [http://arxiv.org/pdf/2507.09389v1](http://arxiv.org/pdf/2507.09389v1)**

> **作者:** Chris Davis Jaldi; Anmol Saini; Elham Ghiasi; O. Divine Eziolise; Cogan Shimizu
>
> **摘要:** Explainability and interpretability are cornerstones of frontier and next-generation artificial intelligence (AI) systems. This is especially true in recent systems, such as large language models (LLMs), and more broadly, generative AI. On the other hand, adaptability to new domains, contexts, or scenarios is also an important aspect for a successful system. As such, we are particularly interested in how we can merge these two efforts, that is, investigating the design of transferable and interpretable neurosymbolic AI systems. Specifically, we focus on a class of systems referred to as ''Agentic Retrieval-Augmented Generation'' systems, which actively select, interpret, and query knowledge sources in response to natural language prompts. In this paper, we systematically evaluate how different conceptualizations and representations of knowledge, particularly the structure and complexity, impact an AI agent (in this case, an LLM) in effectively querying a triplestore. We report our results, which show that there are impacts from both approaches, and we discuss their impact and implications.
>
---
#### [new 023] The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development
- **分类: cs.AI; cs.CY; 68T01**

- **简介: 该论文属于综述任务，旨在探讨人工智能在快速发展中带来的环境与伦理问题。论文聚焦四个关键领域：能耗、电子垃圾、算力分配不均以及网络安全的隐性能源负担，通过分析现有研究和报告，揭示AI发展中的系统性问题，提出推动可持续、透明和公平的AI发展模式。**

- **链接: [http://arxiv.org/pdf/2507.09611v1](http://arxiv.org/pdf/2507.09611v1)**

> **作者:** Jenis Winsta
>
> **备注:** 5 pages, 3 figures
>
> **摘要:** Artificial intelligence (AI) has made remarkable progress in recent years, yet its rapid expansion brings overlooked environmental and ethical challenges. This review explores four critical areas where AI's impact extends beyond performance: energy consumption, electronic waste (e-waste), inequality in compute access, and the hidden energy burden of cybersecurity systems. Drawing from recent studies and institutional reports, the paper highlights systemic issues such as high emissions from model training, rising hardware turnover, global infrastructure disparities, and the energy demands of securing AI. By connecting these concerns, the review contributes to Responsible AI discourse by identifying key research gaps and advocating for sustainable, transparent, and equitable development practices. Ultimately, it argues that AI's progress must align with ethical responsibility and environmental stewardship to ensure a more inclusive and sustainable technological future.
>
---
#### [new 024] Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives
- **分类: cs.CR; cs.AI; cs.CY; cs.DL; D.2.11, H.3.4, H.3.7, J.5**

- **简介: 论文提出Clio-X，一个结合隐私增强技术和Web3架构的去中心化数字档案解决方案，旨在支持AI访问敏感数据的同时保护隐私。它属于隐私保护与AI应用交叉任务，解决当前AI处理档案数据时的隐私与主权问题，并通过用户评估和创新扩散理论分析障碍，探索参与式设计和去中心化治理路径。**

- **链接: [http://arxiv.org/pdf/2507.08853v1](http://arxiv.org/pdf/2507.08853v1)**

> **作者:** Victoria L. Lemieux; Rosa Gil; Faith Molosiwa; Qihong Zhou; Binming Li; Roberto Garcia; Luis De La Torre Cubillo; Zehua Wang
>
> **备注:** 28 pages, 8 figures
>
> **摘要:** As archives turn to artificial intelligence to manage growing volumes of digital records, privacy risks inherent in current AI data practices raise critical concerns about data sovereignty and ethical accountability. This paper explores how privacy-enhancing technologies (PETs) and Web3 architectures can support archives to preserve control over sensitive content while still being able to make it available for access by researchers. We present Clio-X, a decentralized, privacy-first Web3 digital solution designed to embed PETs into archival workflows and support AI-enabled reference and access. Drawing on a user evaluation of a medium-fidelity prototype, the study reveals both interest in the potential of the solution and significant barriers to adoption related to trust, system opacity, economic concerns, and governance. Using Rogers' Diffusion of Innovation theory, we analyze the sociotechnical dimensions of these barriers and propose a path forward centered on participatory design and decentralized governance through a Clio-X Decentralized Autonomous Organization. By integrating technical safeguards with community-based oversight, Clio-X offers a novel model to ethically deploy AI in cultural heritage contexts.
>
---
#### [new 025] Central Bank Digital Currencies: A Survey
- **分类: econ.GN; cs.CE; cs.CY; cs.ET; q-fin.EC; 68M14; A.1; C.5**

- **简介: 该论文属于综述任务，旨在系统梳理全球央行数字货币（CBDC）的设计与实施进展。论文通过分析135篇文献，构建CBDC设计框架，比较26个现有系统，并提出未来研究方向。**

- **链接: [http://arxiv.org/pdf/2507.08880v1](http://arxiv.org/pdf/2507.08880v1)**

> **作者:** Qifeng Tang; Yain-Whar Si
>
> **备注:** 49 pages, 6 figures
>
> **摘要:** With the advancement of digital payment technologies, central banks worldwide have increasingly begun to explore the implementation of Central Bank Digital Currencies (CBDCs). This paper presents a comprehensive review of the latest developments in CBDC system design and implementation. By analyzing 135 research papers published between 2018 and 2025, the study provides an in-depth examination of CBDC design taxonomy and ecosystem frameworks. Grounded in the CBDC Design Pyramid, the paper refines and expands key architectural elements by thoroughly investigating innovations in ledger technologies, the selection of consensus mechanisms, and challenges associated with offline payments and digital wallet integration. Furthermore, it conceptualizes a CBDC ecosystem. A detailed comparative analysis of 26 existing CBDC systems is conducted across four dimensions: system architecture, ledger technology, access model, and application domain. The findings reveal that the most common configuration consists of a two-tier architecture, distributed ledger technology (DLT), and a token-based access model. However, no dominant trend has emerged regarding application domains. Notably, recent research shows a growing focus on leveraging CBDCs for cross-border payments to resolve inefficiencies and structural delays in current systems. Finally, the paper offers several forward-looking recommendations for future research.
>
---
## 更新

#### [replaced 001] Bounds and Bugs: The Limits of Symmetry Metrics to Detect Partisan Gerrymandering
- **分类: cs.CY; math.CO; physics.soc-ph; 97A40, 91F99; J.4**

- **链接: [http://arxiv.org/pdf/2406.12167v4](http://arxiv.org/pdf/2406.12167v4)**

> **作者:** Daryl DeFord; Ellen Veomett
>
> **备注:** To be published in Election Law Journal: Rules, Politics, and Policy. 59 pages, 45 figures, 7 tables
>
> **摘要:** We consider two symmetry metrics commonly used to analyze partisan gerrymandering: the Mean-Median Difference (MM) and Partisan Bias (PB). Our main results compare, for combinations of seats and votes achievable in districted elections, the number of districts won by each party to the extent of potential deviation from the ideal metric values, taking into account the political geography of the state. These comparisons are motivated by examples where the MM and PB have been used in efforts to detect when a districting plan awards extreme number of districts won by some party. These examples include expert testimony, public-facing apps, recommendations by experts to redistricting commissions, and public policy proposals. To achieve this goal we perform both theoretical and empirical analyses of the MM and PB. In our theoretical analysis, we consider vote-share, seat-share pairs (V, S) for which one can construct election data having vote share V and seat share S, and turnout is equal in each district. We calculate the range of values that MM and PB can achieve on that constructed election data. In the process, we find the range of (V,S) pairs that achieve MM = 0, and see that the corresponding range for PB is the same set of (V,S) pairs. We show how the set of such (V,S) pairs allowing for MM = 0 (and PB = 0) changes when turnout in each district is allowed to vary. By observing the results of this theoretical analysis, we can show that the values taken on by these metrics do not necessarily attain more extreme values in plans with more extreme numbers of districts won. We also analyze specific example elections, showing how these metrics can return unintuitive results. We follow this with an empirical study, where we show that on 18 different U.S. maps these metrics can fail to detect extreme seats outcomes.
>
---
#### [replaced 002] Discrimination-free Insurance Pricing with Privatized Sensitive Attributes
- **分类: stat.ML; cs.CY; cs.LG; q-fin.RM**

- **链接: [http://arxiv.org/pdf/2504.11775v2](http://arxiv.org/pdf/2504.11775v2)**

> **作者:** Tianhe Zhang; Suhan Liu; Peng Shi
>
> **摘要:** Fairness has emerged as a critical consideration in the landscape of machine learning algorithms, particularly as AI continues to transform decision-making across societal domains. To ensure that these algorithms are free from bias and do not discriminate against individuals based on sensitive attributes such as gender and race, the field of algorithmic bias has introduced various fairness concepts, along with methodologies to achieve these notions in different contexts. Despite the rapid advancement, not all sectors have embraced these fairness principles to the same extent. One specific sector that merits attention in this regard is insurance. Within the realm of insurance pricing, fairness is defined through a distinct and specialized framework. Consequently, achieving fairness according to established notions does not automatically ensure fair pricing in insurance. In particular, regulators are increasingly emphasizing transparency in pricing algorithms and imposing constraints on insurance companies on the collection and utilization of sensitive consumer attributes. These factors present additional challenges in the implementation of fairness in pricing algorithms. To address these complexities and comply with regulatory demands, we propose an efficient method for constructing fair models that are tailored to the insurance domain, using only privatized sensitive attributes. Notably, our approach ensures statistical guarantees, does not require direct access to sensitive attributes, and adapts to varying transparency requirements, addressing regulatory demands while ensuring fairness in insurance pricing.
>
---
#### [replaced 003] Unfair Learning: GenAI Exceptionalism and Copyright Law
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2504.00955v2](http://arxiv.org/pdf/2504.00955v2)**

> **作者:** David Atkinson
>
> **摘要:** This paper challenges the argument that generative artificial intelligence (GenAI) is entitled to broad immunity from copyright law for reproducing copyrighted works without authorization due to a fair use defense. It examines fair use legal arguments and eight distinct substantive arguments, contending that every legal and substantive argument favoring fair use for GenAI applies equally, if not more so, to humans. Therefore, granting GenAI exceptional privileges in this domain is legally and logically inconsistent with withholding broad fair use exemptions from individual humans. It would mean no human would need to pay for virtually any copyright work again. The solution is to take a circumspect view of any fair use claim for mass copyright reproduction by any entity and focus on the first principles of whether permitting such exceptionalism for GenAI promotes science and the arts.
>
---
#### [replaced 004] LLM Agents Are the Antidote to Walled Gardens
- **分类: cs.LG; cs.CL; cs.CY; cs.SI; 68T50, 68M10, 91B26; I.2.11; I.2.7; H.4.5**

- **链接: [http://arxiv.org/pdf/2506.23978v2](http://arxiv.org/pdf/2506.23978v2)**

> **作者:** Samuele Marro; Philip Torr
>
> **摘要:** While the Internet's core infrastructure was designed to be open and universal, today's application layer is dominated by closed, proprietary platforms. Open and interoperable APIs require significant investment, and market leaders have little incentive to enable data exchange that could erode their user lock-in. We argue that LLM-based agents fundamentally disrupt this status quo. Agents can automatically translate between data formats and interact with interfaces designed for humans: this makes interoperability dramatically cheaper and effectively unavoidable. We name this shift universal interoperability: the ability for any two digital services to exchange data seamlessly using AI-mediated adapters. Universal interoperability undermines monopolistic behaviours and promotes data portability. However, it can also lead to new security risks and technical debt. Our position is that the ML community should embrace this development while building the appropriate frameworks to mitigate the downsides. By acting now, we can harness AI to restore user freedom and competitive markets without sacrificing security.
>
---
#### [replaced 005] LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop
- **分类: cs.CY; cs.AI; cs.CL; cs.HC**

- **链接: [http://arxiv.org/pdf/2507.04295v2](http://arxiv.org/pdf/2507.04295v2)**

> **作者:** Runcong Zhao; Artem Bobrov; Jiazheng Li; Yulan He
>
> **摘要:** Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.
>
---
#### [replaced 006] Beyond classical and contemporary models: a transformative AI framework for student dropout prediction in distance learning using RAG, Prompt engineering, and Cross-modal fusion
- **分类: cs.CL; cs.AI; cs.CY; cs.IR; I.2.7; I.2.1; K.3.1**

- **链接: [http://arxiv.org/pdf/2507.05285v2](http://arxiv.org/pdf/2507.05285v2)**

> **作者:** Miloud Mihoubi; Meriem Zerkouk; Belkacem Chikhaoui
>
> **备注:** 13 pages, 8 figures, 1 Algorithms, 17th International Conference on Education and New Learning Technologies,: 30 June-2 July, 2025 Location: Palma, Spain
>
> **摘要:** Student dropout in distance learning remains a critical challenge, with profound societal and economic consequences. While classical machine learning models leverage structured socio-demographic and behavioral data, they often fail to capture the nuanced emotional and contextual factors embedded in unstructured student interactions. This paper introduces a transformative AI framework that redefines dropout prediction through three synergistic innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment analysis, prompt engineering to decode academic stressors,and cross-modal attention fusion to dynamically align textual, behavioral, and socio-demographic insights. By grounding sentiment analysis in a curated knowledge base of pedagogical content, our RAG-enhanced BERT model interprets student comments with unprecedented contextual relevance, while optimized prompts isolate indicators of academic distress (e.g., "isolation," "workload anxiety"). A cross-modal attention layer then fuses these insights with temporal engagement patterns, creating holistic risk pro-files. Evaluated on a longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and an F1-score of 0.88, outperforming conventional models by 7% and reducing false negatives by 21%. Beyond prediction, the system generates interpretable interventions by retrieving contextually aligned strategies (e.g., mentorship programs for isolated learners). This work bridges the gap between predictive analytics and actionable pedagogy, offering a scalable solution to mitigate dropout risks in global education systems
>
---
#### [replaced 007] Integrating Generative Artificial Intelligence in ADRD: A Roadmap for Streamlining Diagnosis and Care in Neurodegenerative Diseases
- **分类: cs.CY; cs.AI; I.2.1**

- **链接: [http://arxiv.org/pdf/2502.06842v2](http://arxiv.org/pdf/2502.06842v2)**

> **作者:** Andrew G. Breithaupt; Michael Weiner; Alice Tang; Katherine L. Possin; Marina Sirota; James Lah; Allan I. Levey; Pascal Van Hentenryck; Reza Zandehshahvar; Marilu Luisa Gorno-Tempini; Joseph Giorgio; Jingshen Wang; Andreas M. Rauschecker; Howard J. Rosen; Rachel L. Nosheny; Bruce L. Miller; Pedro Pinheiro-Chagas
>
> **备注:** 27 pages, 2 figures, 1 table
>
> **摘要:** Healthcare systems are struggling to meet the growing demand for neurological care, particularly in Alzheimer's disease and related dementias (ADRD). We propose that LLM-based generative AI systems can enhance clinician capabilities to approach specialist-level assessment and decision-making in ADRD care at scale. This article presents a comprehensive six-phase roadmap for responsible design and integration of such systems into ADRD care: (1) high-quality standardized data collection across modalities; (2) decision support; (3) clinical integration enhancing workflows; (4) rigorous validation and monitoring protocols; (5) continuous learning through clinical feedback; and (6) robust ethics and risk management frameworks. This human centered approach optimizes clinicians' capabilities in comprehensive data collection, interpretation of complex clinical information, and timely application of relevant medical knowledge while prioritizing patient safety, healthcare equity, and transparency. Though focused on ADRD, these principles offer broad applicability across medical specialties facing similar systemic challenges.
>
---
#### [replaced 008] Moodle Usability Assessment Methodology using the Universal Design for Learning perspective
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2403.10484v3](http://arxiv.org/pdf/2403.10484v3)**

> **作者:** Rosana Montes; Liliana Herrera; Emilio Crisol
>
> **备注:** final version
>
> **摘要:** The application of the Universal Design for Learning framework favors the creation of virtual educational environments for all. It requires developing accessible content, having a usable platform, and the use of flexible didactics and evaluations that promote constant student motivation. The present study aims to design a methodology to evaluate the usability of the Moodle platform based on the principles of Universal Design for Learning, recognizing the importance of accessibility, usability and the availability of Assistive Technologies. We developed and applied a methodology to assess the usability level of Moodle platforms, taking into consideration that they integrate Assistive Technologies or are used for MOOC contexts. We provide the results of a use case that assesses two instances for the respective Moodle v.2.x and v.3.x family versions. We employed the framework of mixed design research in order to assess a MOOC-type educational program devised under the principles of Universal Design for Learning. As a result of the assessment of Moodle v.2.x and v.3.x, we conclude that the platforms must improve some key elements (e.g. contrasting colors, incorporation of alternative text and links) in order to comply with international accessibility standards. With respect to usability, we can confirm that the principles and guidelines of Universal Design for Learning are applicable to MOOC-type Virtual Learning Environments, are positively valued by students, and have a positive impact on certification rates.
>
---
#### [replaced 009] Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy
- **分类: cs.MA; cs.AI; cs.CL; cs.CY; cs.HC**

- **链接: [http://arxiv.org/pdf/2503.09639v4](http://arxiv.org/pdf/2503.09639v4)**

> **作者:** Abe Bohan Hou; Hongru Du; Yichen Wang; Jingyu Zhang; Zixiao Wang; Paul Pu Liang; Daniel Khashabi; Lauren Gardner; Tianxing He
>
> **备注:** Accepted to COLM 2025
>
> **摘要:** Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents' attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.
>
---
#### [replaced 010] Insuring Uninsurable Risks from AI: Government as Insurer of Last Resort
- **分类: cs.CY; cs.AI; cs.LG; q-fin.RM**

- **链接: [http://arxiv.org/pdf/2409.06672v3](http://arxiv.org/pdf/2409.06672v3)**

> **作者:** Cristian Trout
>
> **备注:** Accepted to Generative AI and Law Workshop at the International Conference on Machine Learning (ICML 2024)
>
> **摘要:** Many experts believe that AI systems will sooner or later pose uninsurable risks, including existential risks. This creates an extreme judgment-proof problem: few if any parties can be held accountable ex post in the event of such a catastrophe. This paper proposes a novel solution: a government-provided, mandatory indemnification program for AI developers. The program uses risk-priced indemnity fees to induce socially optimal levels of care. Risk-estimates are determined by surveying experts, including indemnified developers. The Bayesian Truth Serum mechanism is employed to incent honest and effortful responses. Compared to alternatives, this approach arguably better leverages all private information, and provides a clearer signal to indemnified developers regarding what risks they must mitigate to lower their fees. It's recommended that collected fees be used to help fund the safety research developers need, employing a fund matching mechanism (Quadratic Financing) to induce an optimal supply of this public good. Under Quadratic Financing, safety research projects would compete for private contributions from developers, signaling how much each is to be supplemented with public funds.
>
---
#### [replaced 011] LLMs' Leaning in European Elections
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.13554v2](http://arxiv.org/pdf/2503.13554v2)**

> **作者:** Federico Ricciuti
>
> **摘要:** Many studies suggest that LLMs have left wing leans. The article extends previous analysis of US presidential elections considering several virtual elections in multiple European countries. The analysis considers multiple LLMs and the results confirm the extent of the leaning. Furthermore, the results show that the leaning is not uniform between countries. Sometimes, models refuse to take a position in the virtual elections, but the refusal rate itself is not uniform between countries.
>
---
#### [replaced 012] Practical Principles for AI Cost and Compute Accounting
- **分类: cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2502.15873v2](http://arxiv.org/pdf/2502.15873v2)**

> **作者:** Stephen Casper; Luke Bailey; Tim Schreier
>
> **摘要:** Policymakers increasingly use development cost and compute as proxies for AI capabilities and risks. Recent laws have introduced regulatory requirements that are contingent on specific thresholds. However, technical ambiguities in how to perform this accounting create loopholes that can undermine regulatory effectiveness. We propose seven principles for designing AI cost and compute accounting standards that (1) reduce opportunities for strategic gaming, (2) avoid disincentivizing responsible risk mitigation, and (3) enable consistent implementation across companies and jurisdictions.
>
---
#### [replaced 013] An Epistemic and Aleatoric Decomposition of Arbitrariness to Constrain the Set of Good Models
- **分类: cs.LG; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2302.04525v2](http://arxiv.org/pdf/2302.04525v2)**

> **作者:** Falaah Arif Khan; Denys Herasymuk; Nazar Protsiv; Julia Stoyanovich
>
> **摘要:** Recent research reveals that machine learning (ML) models are highly sensitive to minor changes in their training procedure, such as the inclusion or exclusion of a single data point, leading to conflicting predictions on individual data points; a property termed as arbitrariness or instability in ML pipelines in prior work. Drawing from the uncertainty literature, we show that stability decomposes into epistemic and aleatoric components, capturing the consistency and confidence in prediction, respectively. We use this decomposition to provide two main contributions. Our first contribution is an extensive empirical evaluation. We find that (i) epistemic instability can be reduced with more training data whereas aleatoric instability cannot; (ii) state-of-the-art ML models have aleatoric instability as high as 79% and aleatoric instability disparities among demographic groups as high as 29% in popular fairness benchmarks; and (iii) fairness pre-processing interventions generally increase aleatoric instability more than in-processing interventions, and both epistemic and aleatoric instability are highly sensitive to data-processing interventions and model architecture. Our second contribution is a practical solution to the problem of systematic arbitrariness. We propose a model selection procedure that includes epistemic and aleatoric criteria alongside existing accuracy and fairness criteria, and show that it successfully narrows down a large set of good models (50-100 on our datasets) to a handful of stable, fair and accurate ones. We built and publicly released a python library to measure epistemic and aleatoric multiplicity in any ML pipeline alongside existing confusion-matrix-based metrics, providing practitioners with a rich suite of evaluation metrics to use to define a more precise criterion during model selection.
>
---
