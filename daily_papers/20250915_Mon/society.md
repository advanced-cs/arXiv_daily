# 计算机与社会 cs.CY

- **最新发布 11 篇**

- **更新 11 篇**

## 最新发布

#### [new 001] A Taxonomy of Response Strategies to Toxic Online Content: Evaluating the Evidence
- **分类: cs.CY**

- **简介: 该论文提出在线言论应对策略分类体系（ODE），旨在系统梳理25种应对网络有害内容的方法，归纳为五类，并综述相关研究证据，以提升构建积极网络讨论的实证基础。**

- **链接: [http://arxiv.org/pdf/2509.09921v1](http://arxiv.org/pdf/2509.09921v1)**

> **作者:** Lisa Schirch; Kristina Radivojevic; Cathy Buerger
>
> **摘要:** Toxic Online Content (TOC) includes messages on digital platforms that are harmful, hostile, or damaging to constructive public discourse. Individuals, organizations, and LLMs respond to TOC through counterspeech or counternarrative initiatives. There is a wide variation in their goals, terminology, response strategies, and methods of evaluating impact. This paper identifies a taxonomy of online response strategies, which we call Online Discourse Engagement (ODE), to include any type of online speech to build healthier online public discourse. The literature on ODE makes contradictory assumptions about ODE goals and rarely distinguishes between them or rigorously evaluates their effectiveness. This paper categorizes 25 distinct ODE strategies, from humor and distraction to empathy, solidarity, and fact-based rebuttals, and groups these into a taxonomy of five response categories: defusing and distracting, engaging the speaker's perspective, identifying shared values, upstanding for victims, and information and fact-building. The paper then systematically reviews the evidence base for each of these categories. By clarifying definitions, cataloging response strategies, and providing a meta-analysis of research papers on these strategies, this article aims to bring coherence to the study of ODE and to strengthen evidence-informed approaches for fostering constructive ODE.
>
---
#### [new 002] We Need a New Ethics for a World of AI Agents
- **分类: cs.CY; cs.AI; I.2.0; K.4.1**

- **简介: 论文探讨AI代理普及带来的伦理挑战，呼吁多方合作确保人机及代理间互动的益处。属于伦理研究任务，旨在解决AI代理引发的安全与社会协调问题，提出需加强跨领域协作以应对新挑战。**

- **链接: [http://arxiv.org/pdf/2509.10289v1](http://arxiv.org/pdf/2509.10289v1)**

> **作者:** Iason Gabriel; Geoff Keeling; Arianna Manzini; James Evans
>
> **备注:** 6 pages, no figures
>
> **摘要:** The deployment of capable AI agents raises fresh questions about safety, human-machine relationships and social coordination. We argue for greater engagement by scientists, scholars, engineers and policymakers with the implications of a world increasingly populated by AI agents. We explore key challenges that must be addressed to ensure that interactions between humans and agents, and among agents themselves, remain broadly beneficial.
>
---
#### [new 003] When Your Reviewer is an LLM: Biases, Divergence, and Prompt Injection Risks in Peer Review
- **分类: cs.CY; cs.CR**

- **简介: 该论文评估LLM作为学术审稿人的表现，分析其评分偏差、一致性及对提示注入的脆弱性。通过对比人类审稿人，揭示LLM在提升弱论文评分和受特定指令影响方面的风险，探讨其在同行评审中的潜力与隐患。**

- **链接: [http://arxiv.org/pdf/2509.09912v1](http://arxiv.org/pdf/2509.09912v1)**

> **作者:** Changjia Zhu; Junjie Xiong; Renkai Ma; Zhicong Lu; Yao Liu; Lingyao Li
>
> **摘要:** Peer review is the cornerstone of academic publishing, yet the process is increasingly strained by rising submission volumes, reviewer overload, and expertise mismatches. Large language models (LLMs) are now being used as "reviewer aids," raising concerns about their fairness, consistency, and robustness against indirect prompt injection attacks. This paper presents a systematic evaluation of LLMs as academic reviewers. Using a curated dataset of 1,441 papers from ICLR 2023 and NeurIPS 2022, we evaluate GPT-5-mini against human reviewers across ratings, strengths, and weaknesses. The evaluation employs structured prompting with reference paper calibration, topic modeling, and similarity analysis to compare review content. We further embed covert instructions into PDF submissions to assess LLMs' susceptibility to prompt injection. Our findings show that LLMs consistently inflate ratings for weaker papers while aligning more closely with human judgments on stronger contributions. Moreover, while overarching malicious prompts induce only minor shifts in topical focus, explicitly field-specific instructions successfully manipulate specific aspects of LLM-generated reviews. This study underscores both the promises and perils of integrating LLMs into peer review and points to the importance of designing safeguards that ensure integrity and trust in future review processes.
>
---
#### [new 004] The Hierarchical Morphotope Classification: A Theory-Driven Framework for Large-Scale Analysis of Built Form
- **分类: cs.CY**

- **简介: 论文提出HiMoC方法，用于理论驱动的大规模建成环境分类。通过SA3算法划分具有独特形态的“morphotope”，构建分层分类体系，解决现有方法缺乏理论基础、可扩展性差的问题，适用于多国城市形态分析。**

- **链接: [http://arxiv.org/pdf/2509.10083v1](http://arxiv.org/pdf/2509.10083v1)**

> **作者:** Martin Fleischmann; Krasen Samardzhiev; Anna Brázdová; Daniela Dančejová; Lisa Winkler
>
> **摘要:** Built environment, formed of a plethora of patterns of building, streets, and plots, has a profound impact on how cities are perceived and function. While various methods exist to classify urban patterns, they often lack a strong theoretical foundation, are not scalable beyond a local level, or sacrifice detail for broader application. This paper introduces the Hierarchical Morphotope Classification (HiMoC), a novel, theory-driven, and computationally scalable method of classification of built form. HiMoC operationalises the idea of a morphotope - the smallest locality with a distinctive character - using a bespoke regionalisation method SA3 (Spatial Agglomerative Adaptive Aggregation), to delineate contiguous, morphologically distinct localities. These are further organised into a hierarchical taxonomic tree reflecting their dissimilarity based on morphometric profile derived from buildings and streets retrieved from open data, allowing flexible, interpretable classification of built fabric, that can be applied beyond a scale of a single country. The method is tested on a subset of countries of Central Europe, grouping over 90 million building footprints into over 500,000 morphotopes. The method extends the capabilities of available morphometric analyses, while offering a complementary perspective to existing large scale data products, which are focusing primarily on land use or use conceptual definition of urban fabric types. This theory-grounded, reproducible, unsupervised and scalable method facilitates a nuanced understanding of urban structure, with broad applications in urban planning, environmental analysis, and socio-spatial studies.
>
---
#### [new 005] Openness in AI and downstream governance: A global value chain approach
- **分类: cs.CY; cs.AI; K.4.1; K.4.3**

- **简介: 论文探讨AI开放性对全球价值链及下游治理的影响，分析其作为新型企业间关系的特性，构建框架连接基础AI与下游价值链，旨在理解AI产业权力结构与技术扩散潜力。**

- **链接: [http://arxiv.org/pdf/2509.10220v1](http://arxiv.org/pdf/2509.10220v1)**

> **作者:** Christopher Foster
>
> **摘要:** The rise of AI has been rapid, becoming a leading sector for investment and promising disruptive impacts across the economy. Within the critical analysis of the economic impacts, AI has been aligned to the critical literature on data power and platform capitalism - further concentrating power and value capture amongst a small number of "big tech" leaders. The equally rapid rise of openness in AI (here taken to be claims made by AI firms about openness, "open source" and free provision) signals an interesting development. It highlights an emerging ecosystem of open AI models, datasets and toolchains, involving massive capital investment. It poses questions as to whether open resources can support technological transfer and the ability for catch-up, even in the face of AI industry power. This work seeks to add conceptual clarity to these debates by conceptualising openness in AI as a unique type of interfirm relation and therefore amenable to value chain analysis. This approach then allows consideration of the capitalist dynamics of "outsourcing" of foundational firms in value chains, and consequently the types of governance and control that might emerge downstream as AI is adopted. This work, therefore, extends previous mapping of AI value chains to build a framework which links foundational AI with downstream value chains. Overall, this work extends our understanding of AI as a productive sector. While the work remains critical of the power of leading AI firms, openness in AI may lead to potential spillovers stemming from the intense competition for global technological leadership in AI.
>
---
#### [new 006] Combining Textual and Spectral Features for Robust Classification of Pilot Communications
- **分类: cs.SD; cs.CY; eess.AS**

- **简介: 论文提出一种双通道机器学习框架，结合文本和频谱特征分类飞行员通信，解决非塔台机场航空操作识别难题。使用真实音频数据训练模型，取得超过91%的F1分数，方法具备可扩展性和低成本优势。**

- **链接: [http://arxiv.org/pdf/2509.09752v1](http://arxiv.org/pdf/2509.09752v1)**

> **作者:** Abdullah All Tanvir; Chenyu Huang; Moe Alahmad; Chuyang Yang; Xin Zhong
>
> **摘要:** Accurate estimation of aircraft operations, such as takeoffs and landings, is critical for effective airport management, yet remains challenging, especially at non-towered facilities lacking dedicated surveillance infrastructure. This paper presents a novel dual pipeline machine learning framework that classifies pilot radio communications using both textual and spectral features. Audio data collected from a non-towered U.S. airport was annotated by certified pilots with operational intent labels and preprocessed through automatic speech recognition and Mel-spectrogram extraction. We evaluate a wide range of traditional classifiers and deep learning models, including ensemble methods, LSTM, and CNN across both pipelines. To our knowledge, this is the first system to classify operational aircraft intent using a dual-pipeline ML framework on real-world air traffic audio. Our results demonstrate that spectral features combined with deep architectures consistently yield superior classification performance, with F1-scores exceeding 91%. Data augmentation further improves robustness to real-world audio variability. The proposed approach is scalable, cost-effective, and deployable without additional infrastructure, offering a practical solution for air traffic monitoring at general aviation airports.
>
---
#### [new 007] The Role of Follow Networks and Twitter's Content Recommender on Partisan Skew and Rumor Exposure during the 2022 U.S. Midterm Election
- **分类: cs.SI; cs.CY**

- **简介: 该论文研究Twitter算法推荐和社交网络对2022年美国中期选举期间用户信息曝光、政治偏见和谣言传播的影响。通过自动化账号分析，发现算法与社交网络共同作用，加剧政治极化和低质量信息传播，揭示了算法在民主进程中的关键影响。**

- **链接: [http://arxiv.org/pdf/2509.09826v1](http://arxiv.org/pdf/2509.09826v1)**

> **作者:** Kayla Duskin; Joseph S. Schafer; Alexandros Efstratiou; Jevin D. West; Emma S. Spiro
>
> **备注:** Accepted at the AAAI International Conference on Web and Social Media (ICWSM) 2026
>
> **摘要:** Social media platforms shape users' experiences through the algorithmic systems they deploy. In this study, we examine to what extent Twitter's content recommender, in conjunction with a user's social network, impacts the topic, political skew, and reliability of information served on the platform during a high-stakes election. We utilize automated accounts to document Twitter's algorithmically curated and reverse chronological timelines throughout the U.S. 2022 midterm election. We find that the algorithmic timeline measurably influences exposure to election content, partisan skew, and the prevalence of low-quality information and election rumors. Critically, these impacts are mediated by the partisan makeup of one's personal social network, which often exerts greater influence than the algorithm alone. We find that the algorithmic feed decreases the proportion of election content shown to left-leaning accounts, and that it skews content toward right-leaning sources when compared to the reverse chronological feed. We additionally find evidence that the algorithmic system increases the prevalence of election-related rumors for right-leaning accounts, and has mixed effects on the prevalence of low-quality information sources. Our work provides insight into the outcomes of Twitter's complex recommender system at a crucial time period before controversial changes to the platform and in the midst of nationwide elections and highlights the need for ongoing study of algorithmic systems and their role in democratic processes.
>
---
#### [new 008] Temporal Preferences in Language Models for Long-Horizon Assistance
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 论文研究语言模型在时间选择中的倾向，探讨其未来或现在导向偏好及可操控性。通过实验对比人类与模型决策，提出MTO指标，分析模型在不同提示下的时间偏好变化，旨在为长周期AI助手设计提供参考。**

- **链接: [http://arxiv.org/pdf/2509.09704v1](http://arxiv.org/pdf/2509.09704v1)**

> **作者:** Ali Mazyaki; Mohammad Naghizadeh; Samaneh Ranjkhah Zonouzaghi; Hossein Setareh
>
> **摘要:** We study whether language models (LMs) exhibit future- versus present-oriented preferences in intertemporal choice and whether those preferences can be systematically manipulated. Using adapted human experimental protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them against a sample of human decision makers. We introduce an operational metric, the Manipulability of Time Orientation (MTO), defined as the change in an LM's revealed time preference between future- and present-oriented prompts. In our tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini) choose later options under future-oriented prompts but only partially personalize decisions across identities or geographies. Moreover, models that correctly reason about time orientation internalize a future orientation for themselves as AI decision makers. We discuss design implications for AI assistants that should align with heterogeneous, long-horizon goals and outline a research agenda on personalized contextual calibration and socially aware deployment.
>
---
#### [new 009] Why Data Anonymization Has Not Taken Off
- **分类: cs.CR; cs.CY**

- **简介: 论文探讨数据匿名化在实践中未普及的原因，指出其实施复杂且需定制化。属于数据隐私研究任务，旨在分析匿名化方法的局限性，并提出综合隐私策略的重要性。**

- **链接: [http://arxiv.org/pdf/2509.10165v1](http://arxiv.org/pdf/2509.10165v1)**

> **作者:** Matthew J. Schneider; James Bailie; Dawn Iacobucci
>
> **备注:** 15 pages
>
> **摘要:** Companies are looking to data anonymization research $\unicode{x2013}$ including differential private and synthetic data methods $\unicode{x2013}$ for simple and straightforward compliance solutions. But data anonymization has not taken off in practice because it is anything but simple to implement. For one, it requires making complex choices which are case dependent, such as the domain of the dataset to anonymize; the units to protect; the scope where the data protection should extend to; and the standard of protection. Each variation of these choices changes the very meaning, as well as the practical implications, of differential privacy (or of any other measure of data anonymization). Yet differential privacy is frequently being branded as the same privacy guarantee regardless of variations in these choices. Some data anonymization methods can be effective, but only when the insights required are much larger than the unit of protection. Given that businesses care about profitability, any solution must preserve the patterns between a firm's data and that profitability. As a result, data anonymization solutions usually need to be bespoke and case-specific, which reduces their scalability. Companies should not expect easy wins, but rather recognize that anonymization is just one approach to data privacy with its own particular advantages and drawbacks, while the best strategies jointly leverage the full range of approaches to data privacy and security in combination.
>
---
#### [new 010] Evolution of Coordination Through Institutional Incentives: An Evolutionary Game Theory Approach
- **分类: cs.GT; cs.CY**

- **简介: 该论文运用演化博弈论，研究制度激励在促进协调中的作用，解决如何分配有限资源以提升参与度与承诺遵守的问题。通过模型分析发现奖励机制比惩罚更有效，为新技术推广提供政策设计新思路。**

- **链接: [http://arxiv.org/pdf/2509.10112v1](http://arxiv.org/pdf/2509.10112v1)**

> **作者:** Ndidi Bianca Ogbo; Zhao Song; The Anh Han
>
> **备注:** 16 pages, 5 figures
>
> **摘要:** There is a broad recognition that commitment-based mechanisms can promote coordination and cooperative behaviours in both biological populations and self-organised multi-agent systems by making individuals' intentions explicit prior to engagement. Yet their effectiveness depends on sustained compliance supported by institutions, especially in one-off interactions. Despite advances in quantitative studies of cooperation and commitment, most applied analyses and policy debates remain largely qualitative, with limited attention to the allocation of scarce institutional resources between enhancing participation and ensuring commitment compliance. Herein, we develop an evolutionary game-theoretic model that explicitly examines the strategic distribution of a limited budget for institutional incentives, namely rewards or punishments, aimed at these two critical objectives within pre-commitment frameworks. Our findings reveal that a reward-based incentive approach consistently yields greater coordination success than a punishment-based approach, with optimal outcomes arising when resources are appropriately distributed between participation promotion and compliance assurance. These findings offer novel insights for designing institutional incentives to promote broad, coordinated adoption of new technologies.
>
---
#### [new 011] Who Decides How Knowing Becomes Doing? Redistributing Authority in Human-AI Music Co-Creation
- **分类: cs.HC; cs.CY**

- **简介: 该论文探讨人机协作音乐创作中权威分配问题，提出基于可争议性、主体性和多元性的框架，通过实证研究重塑人机关系，激发人类创造力，属于人机协同创作领域的权威再分配任务。**

- **链接: [http://arxiv.org/pdf/2509.10331v1](http://arxiv.org/pdf/2509.10331v1)**

> **作者:** Zhejing Hu; Yan Liu; Zhi Zhang; Gong Chen; Bruce X. B. Yu; Jiannong Cao
>
> **摘要:** In the era of human-AI co-creation, the maxim "knowing is easy, doing is hard" is redefined. AI has the potential to ease execution, yet the essence of "hard" lies in who governs the translation from knowing to doing. Mainstream tools often centralize interpretive authority and homogenize expression, suppressing marginal voices. To address these challenges, we introduce the first systematic framework for redistributing authority in the knowing-doing cycle, built on three principles, namely contestability, agency, and plurality. Through interactive studies with 180 music practitioners, complemented by in-depth interviews, we demonstrate that these principles reshape human-AI authority relations and reactivate human creative expression. The findings establish a new paradigm for critical computing and human-AI co-creation that advances from critique to practice.
>
---
## 更新

#### [replaced 001] The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?
- **分类: cs.CY; cs.AI; econ.GN; q-fin.EC**

- **链接: [http://arxiv.org/pdf/2505.02846v2](http://arxiv.org/pdf/2505.02846v2)**

> **作者:** Kim Kaivanto
>
> **备注:** 47 pages
>
> **摘要:** In policy debates concerning the governance and regulation of Artificial Intelligence (AI), both the Precautionary Principle (PP) and the Innovation Principle (IP) are advocated by their respective interest groups. Do these principles offer wholly incompatible and contradictory guidance? Does one necessarily negate the other? I argue here that provided attention is restricted to weak-form PP and IP, the answer to both of these questions is "No." The essence of these weak formulations is the requirement to fully account for type-I error costs arising from erroneously preventing the innovation's diffusion through society (i.e. mistaken regulatory red-lighting) as well as the type-II error costs arising from erroneously allowing the innovation to diffuse through society (i.e. mistaken regulatory green-lighting). Within the Signal Detection Theory (SDT) model developed here, weak-PP red-light (weak-IP green-light) determinations are optimal for sufficiently small (large) ratios of expected type-I to type-II error costs. For intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy is optimal. Regulatory sandbox instruments allow AI testing and experimentation to take place within a structured environment of limited duration and societal scale, whereby the expected cost ratio falls within the 'wait-and-monitor' range. Through sandboxing regulators and innovating firms learn more about the expected cost ratio, and what respective adaptations -- of regulation, of technical solution, of business model, or combination thereof, if any -- are needed to keep the ratio out of the weak-PP red-light zone. Nevertheless AI foundation models are ill-suited for regulatory sandboxing as their general-purpose nature precludes credible identification of misclassification costs.
>
---
#### [replaced 002] Toward Responsible and Beneficial AI: Comparing Regulatory and Guidance-Based Approaches -A Comprehensive Comparative Analysis of Artificial Intelligence Governance Frameworks across the European Union, United States, China, and IEEE
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2508.00868v3](http://arxiv.org/pdf/2508.00868v3)**

> **作者:** Jian Du
>
> **备注:** PhD thesis
>
> **摘要:** This dissertation presents a comprehensive comparative analysis of artificial intelligence governance frameworks across the European Union, United States, China, and IEEE technical standards, examining how different jurisdictions and organizations approach the challenge of promoting responsible and beneficial AI development. Using a qualitative research design based on systematic content analysis, the study identifies distinctive patterns in regulatory philosophy, implementation mechanisms, and global engagement strategies across these major AI governance ecosystems.
>
---
#### [replaced 003] Web3 x AI Agents: Landscape, Integrations, and Foundational Challenges
- **分类: cs.CY; cs.AI; econ.GN; q-fin.EC**

- **链接: [http://arxiv.org/pdf/2508.02773v3](http://arxiv.org/pdf/2508.02773v3)**

> **作者:** Yiming Shen; Jiashuo Zhang; Zhenzhe Shao; Wenxuan Luo; Yanlin Wang; Ting Chen; Zibin Zheng; Jiachi Chen
>
> **摘要:** The convergence of Web3 technologies and AI agents represents a rapidly evolving frontier poised to reshape decentralized ecosystems. This paper presents the first and most comprehensive analysis of the intersection between Web3 and AI agents, examining five critical dimensions: landscape, economics, governance, security, and trust mechanisms. Through an analysis of 133 existing projects, we first develop a taxonomy and systematically map the current market landscape (RQ1), identifying distinct patterns in project distribution and capitalization. Building upon these findings, we further investigate four key integrations: (1) the role of AI agents in participating in and optimizing decentralized finance (RQ2); (2) their contribution to enhancing Web3 governance mechanisms (RQ3); (3) their capacity to strengthen Web3 security via intelligent vulnerability detection and automated smart contract auditing (RQ4); and (4) the establishment of robust reliability frameworks for AI agent operations leveraging Web3's inherent trust infrastructure (RQ5). By synthesizing these dimensions, we identify key integration patterns, highlight foundational challenges related to scalability, security, and ethics, and outline critical considerations for future research toward building robust, intelligent, and trustworthy decentralized systems with effective AI agent interactions.
>
---
#### [replaced 004] Exploring a Gamified Personality Assessment Method through Interaction with LLM Agents Embodying Different Personalities
- **分类: cs.HC; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.04005v3](http://arxiv.org/pdf/2507.04005v3)**

> **作者:** Baiqiao Zhang; Xiangxian Li; Chao Zhou; Xinyu Gai; Juan Liu; Xue Yang; Xiaojuan Ma; Yong-jin Liu; Yulong Bian
>
> **摘要:** The low-intrusion and automated personality assessment is receiving increasing attention in psychology and human-computer interaction fields. This study explores an interactive approach for personality assessment, focusing on the multiplicity of personality representation. We propose a framework of Gamified Personality Assessment through Multi-Personality Representations (Multi-PR GPA). The framework leverages Large Language Models to empower virtual agents with different personalities. These agents elicit multifaceted human personality representations through engaging in interactive games. Drawing upon the multi-type textual data generated throughout the interaction, it achieves two modes of personality assessment (i.e., Direct Assessment and Questionnaire-based Assessment) and provides interpretable insights. Grounded in the classic Big Five personality theory, we developed a prototype system and conducted a user study to evaluate the efficacy of Multi-PR GPA. The results affirm the effectiveness of our approach in personality assessment and demonstrate its superior performance when considering the multiplicity of personality representation.
>
---
#### [replaced 005] Quantum-Enhanced Forecasting for Deep Reinforcement Learning in Algorithmic Trading
- **分类: cs.LG; cs.CY**

- **链接: [http://arxiv.org/pdf/2509.09176v2](http://arxiv.org/pdf/2509.09176v2)**

> **作者:** Jun-Hao Chen; Yu-Chien Huang; Yun-Cheng Tsai; Samuel Yen-Chi Chen
>
> **摘要:** The convergence of quantum-inspired neural networks and deep reinforcement learning offers a promising avenue for financial trading. We implemented a trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM) for short-term trend prediction with Quantum Asynchronous Advantage Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the long-only agent achieves 11.87\% return over around 5 years with 0.92\% max drawdown, outperforming several currency ETFs. We detail state design (QLSTM features and indicators), reward function for trend-following/risk control, and multi-core training. Results show hybrid models yield competitive FX trading performance. Implications include QLSTM's effectiveness for small-profit trades with tight risk and future enhancements. Key hyperparameters: QLSTM sequence length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and simplified strategy. \footnote{The views expressed in this article are those of the authors and do not represent the views of Wells Fargo. This article is for informational purposes only. Nothing contained in this article should be construed as investment advice. Wells Fargo makes no express or implied warranties and expressly disclaims all legal, tax, and accounting implications related to this article.
>
---
#### [replaced 006] Prompt Programming: A Platform for Dialogue-based Computational Problem Solving with Generative AI Models
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.04267v2](http://arxiv.org/pdf/2503.04267v2)**

> **作者:** Victor-Alexandru Pădurean; Paul Denny; Alkis Gotovos; Adish Singla
>
> **备注:** ITiCSE'25 paper
>
> **摘要:** Computing students increasingly rely on generative AI tools for programming assistance, often without formal instruction or guidance. This highlights a need to teach students how to effectively interact with AI models, particularly through natural language prompts, to generate and critically evaluate code for solving computational tasks. To address this, we developed a novel platform for prompt programming that enables authentic dialogue-based interactions, supports problems involving multiple interdependent functions, and offers on-request execution of generated code. Data analysis from over 900 students in an introductory programming course revealed high engagement, with the majority of prompts occurring within multi-turn dialogues. Problems with multiple interdependent functions encouraged iterative refinement, with progression graphs highlighting several common strategies. Students were highly selective about the code they chose to test, suggesting that on-request execution of generated code promoted critical thinking. Given the growing importance of learning dialogue-based programming with AI, we provide this tool as a publicly accessible resource, accompanied by a corpus of programming problems for educational use.
>
---
#### [replaced 007] A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of Solutions and Directions for Future Research
- **分类: cs.LG; cs.AI; cs.CY; 68T01; I.2.6; I.5.1; K.4.1**

- **链接: [http://arxiv.org/pdf/2410.03855v2](http://arxiv.org/pdf/2410.03855v2)**

> **作者:** Teresa Salazar; Helder Araújo; Alberto Cano; Pedro Henriques Abreu
>
> **摘要:** Group fairness in machine learning is an important area of research focused on achieving equitable outcomes across different groups defined by sensitive attributes such as race or gender. Federated Learning, a decentralized approach to training machine learning models across multiple clients, amplifies the need for fairness methodologies due to its inherent heterogeneous data distributions that can exacerbate biases. The intersection of Federated Learning and group fairness has attracted significant interest, with 48 research works specifically dedicated to addressing this issue. However, no comprehensive survey has specifically focused on group fairness in Federated Learning. In this work, we analyze the key challenges of this topic, propose practices for its identification and benchmarking, and create a novel taxonomy based on criteria such as data partitioning, location, and strategy. Furthermore, we analyze broader concerns, review how different approaches handle the complexities of various sensitive attributes, examine common datasets and applications, and discuss the ethical, legal, and policy implications of group fairness in FL. We conclude by highlighting key areas for future research, emphasizing the need for more methods to address the complexities of achieving group fairness in federated systems.
>
---
#### [replaced 008] The Architecture of AI Transformation: Four Strategic Patterns and an Emerging Frontier
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.02853v3](http://arxiv.org/pdf/2509.02853v3)**

> **作者:** Diana A. Wolfe; Alice Choe; Fergus Kidd
>
> **备注:** 59 pages, 2 tables, 4 figures
>
> **摘要:** Despite extensive investment in artificial intelligence, 95% of enterprises report no measurable profit impact from AI deployments (MIT, 2025). In this theoretical paper, we argue that this gap reflects paradigmatic lock-in that channels AI into incremental optimization rather than structural transformation. Using a cross-case analysis, we propose a 2x2 framework that reconceptualizes AI strategy along two independent dimensions: the degree of transformation achieved (incremental to transformational) and the treatment of human contribution (reduced to amplified). The framework surfaces four patterns now dominant in practice: individual augmentation, process automation, workforce substitution, and a less deployed frontier of collaborative intelligence. Evidence shows that the first three dimensions reinforce legacy work models and yield localized gains without durable value capture. Realizing collaborative intelligence requires three mechanisms: complementarity (pairing distinct human and machine strengths), co-evolution (mutual adaptation through interaction), and boundary-setting (human determination of ethical and strategic parameters). Complementarity and boundary-setting are observable in regulated and high-stakes domains; co-evolution is largely absent, which helps explain limited system-level impact. Our findings in a case study analysis illustrated that advancing toward collaborative intelligence requires material restructuring of roles, governance, and data architecture rather than additional tools. The framework reframes AI transformation as an organizational design challenge: moving from optimizing the division of labor between humans and machines to architecting their convergence, with implications for operating models, workforce development, and the future of work.
>
---
#### [replaced 009] Exercising the CCPA Opt-out Right on Android: Legally Mandated but Practically Challenging
- **分类: cs.CR; cs.CY**

- **链接: [http://arxiv.org/pdf/2407.14938v4](http://arxiv.org/pdf/2407.14938v4)**

> **作者:** Sebastian Zimmeck; Nishant Aggarwal; Zachary Liu; Sage Altman; Konrad Kollnig
>
> **摘要:** Many mobile apps' business model is based on generating revenue from sharing user data with ad networks and other companies to deliver personalized ads. The California Consumer Privacy Act (CCPA) gives consumers a right to opt out of the selling and sharing of their personal information. In two experiments we evaluate to which extent popular apps on the Android platform enable users to exercise their CCPA opt-out right. In our first experiment -- manually opting out via app-level UIs for a set of 100 apps -- we find that despite this legal requirement, only 48 apps implement such legally mandated setting suggesting a broad level of non-compliance. In our second experiment -- opting out by sending Global Privacy Control (GPC) signals and disabling the AdID -- we automate a dynamic analysis for an app dataset of 1,811 apps to evaluate whether platform-level opt-out settings are effective to exercise the CCPA opt-out right. While we estimate with 95% confidence that 62%--81% of apps in our app dataset must honor the CCPA's opt-out right, many apps do not do so. For example, when sending GPC signals and disabling apps' access to the AdID, 338 apps still had the `ccpa status` of the ad network Vungle set to `opted in` while only 26 had set it to `opted out`. Overall, our results suggest a compliance gap as Android users have no effective way of exercising their CCPA opt-out right; neither at the app- nor at the platform-level. We think that re-purposing the Android AdID setting as an opt-out right setting with legal meaning could resolve this compliance gap and improve users' privacy on the platform overall.
>
---
#### [replaced 010] Agentic Vehicles for Human-Centered Mobility Systems
- **分类: cs.CY; cs.CE; cs.CL; cs.HC; cs.RO**

- **链接: [http://arxiv.org/pdf/2507.04996v5](http://arxiv.org/pdf/2507.04996v5)**

> **作者:** Jiangbo Yu
>
> **摘要:** Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Autonomous vehicles (AuVs) are therefore understood as systems that perceive their environment and execute pre-programmed tasks independently of external input, consistent with the SAE levels of automated driving. Yet recent research and real-world deployments have begun to showcase vehicles that exhibit behaviors outside the scope of this definition. These include natural language interaction with humans, goal adaptation, contextual reasoning, external tool use, and the handling of unforeseen ethical dilemmas, enabled in part by multimodal large language models (LLMs). These developments highlight not only a gap between technical autonomy and the broader cognitive and social capacities required for human-centered mobility, but also the emergence of a form of vehicle intelligence that currently lacks a clear designation. To address this gap, the paper introduces the concept of agentic vehicles (AgVs): vehicles that integrate agentic AI systems to reason, adapt, and interact within complex environments. It synthesizes recent advances in agentic systems and suggests how AgVs can complement and even reshape conventional autonomy to ensure mobility services are aligned with user and societal needs. The paper concludes by outlining key challenges in the development and governance of AgVs and their potential role in shaping future agentic transportation systems.
>
---
#### [replaced 011] Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models
- **分类: cs.AI; cs.CL; cs.CY; cs.HC; cs.SC**

- **链接: [http://arxiv.org/pdf/2509.01909v4](http://arxiv.org/pdf/2509.01909v4)**

> **作者:** Ranjie Duan; Jiexi Liu; Xiaojun Jia; Shiji Zhao; Ruoxi Cheng; Fengxiang Wang; Cheng Wei; Yong Xie; Chang Liu; Defeng Li; Yinpeng Dong; Yichi Zhang; Yuefeng Chen; Chongwen Wang; Xingjun Ma; Xingxing Wei; Yang Liu; Hang Su; Jun Zhu; Xinfeng Li; Yitong Sun; Jie Zhang; Jinzhao Hu; Sha Xu; Yitong Yang; Jialing Tao; Hui Xue
>
> **备注:** Technical Report Code & Model weights available: https://github.com/Alibaba-AAIG/Oyster
>
> **摘要:** Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.
>
---
