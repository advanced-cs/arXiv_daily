# 计算机与社会 cs.CY

- **最新发布 22 篇**

- **更新 8 篇**

## 最新发布

#### [new 001] Surviving the Narrative Collapse: Sustainability and Justice in Computing Within Limits
- **分类: cs.CY**

- **简介: 论文探讨后真相时代可持续计算研究的生存策略，通过Fictomorphosis叙事方法重构争议议题，以应对 misinformation、极化等挑战，探索其适应路径。**

- **链接: [http://arxiv.org/pdf/2508.05992v1](http://arxiv.org/pdf/2508.05992v1)**

> **作者:** Dave Guruge; Samuel Mann; Ruth Myers; Oliver Bates; Mikey Goldweber; Andy Williamson; Jon Lasenby; Ian Brooks
>
> **备注:** Post-proceedings paper presented at LIMITS 2025: 11th Workshop on Computing within Limits, 2025-06-26/27, Online
>
> **摘要:** Sustainability-driven computing research - encompassing equity, diversity, climate change, and social justice - is increasingly dismissed as woke or even dangerous in many sociopolitical contexts. As misinformation, ideological polarisation, deliberate ignorance and reactionary narratives gain ground, how can sustainability research in computing continue to exist and make an impact? This paper explores these tensions through Fictomorphosis, a creative story retelling method that reframes contested topics through different genres and perspectives. By engaging computing researchers in structured narrative transformations, we investigate how sustainability-oriented computing research is perceived, contested, and can adapt in a post-truth world.
>
---
#### [new 002] Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks
- **分类: cs.CY; cs.AI; cs.LG**

- **简介: 该论文提出多维度风险特征分析与路径建模方法，解决AI风险缺乏系统框架问题，通过七个维度和步骤模型识别风险及干预措施。**

- **链接: [http://arxiv.org/pdf/2508.06411v1](http://arxiv.org/pdf/2508.06411v1)**

> **作者:** Ze Shen Chin
>
> **备注:** 24 pages including references, 6 figures. To be presented in Technical AI Governance Forum 2025
>
> **摘要:** Although discourse around the risks of Artificial Intelligence (AI) has grown, it often lacks a comprehensive, multidimensional framework, and concrete causal pathways mapping hazard to harm. This paper aims to bridge this gap by examining six commonly discussed AI catastrophic risks: CBRN, cyber offense, sudden loss of control, gradual loss of control, environmental risk, and geopolitical risk. First, we characterize these risks across seven key dimensions, namely intent, competency, entity, polarity, linearity, reach, and order. Next, we conduct risk pathway modeling by mapping step-by-step progressions from the initial hazard to the resulting harms. The dimensional approach supports systematic risk identification and generalizable mitigation strategies, while risk pathway models help identify scenario-specific interventions. Together, these methods offer a more structured and actionable foundation for managing catastrophic AI risks across the value chain.
>
---
#### [new 003] Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems
- **分类: cs.CY; cs.AI; cs.HC; cs.LG; cs.RO; 68T01, 68T40; K.7.4; K.4.1; I.2.9; H.1.2**

- **简介: 论文提出透明伦理AI框架，解决机器人系统伦理问题，通过标准化、可解释AI和用户界面提升透明度，强化问责与信任，推动负责任AI发展。**

- **链接: [http://arxiv.org/pdf/2508.05846v1](http://arxiv.org/pdf/2508.05846v1)**

> **作者:** Ahmad Farooq; Kamran Iqbal
>
> **备注:** Published in the Proceedings of the 2025 3rd International Conference on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 tables
>
> **摘要:** As artificial intelligence (AI) and robotics increasingly permeate society, ensuring the ethical behavior of these systems has become paramount. This paper contends that transparency in AI decision-making processes is fundamental to developing trustworthy and ethically aligned robotic systems. We explore how transparency facilitates accountability, enables informed consent, and supports the debugging of ethical algorithms. The paper outlines technical, ethical, and practical challenges in implementing transparency and proposes novel approaches to enhance it, including standardized metrics, explainable AI techniques, and user-friendly interfaces. This paper introduces a framework that connects technical implementation with ethical considerations in robotic systems, focusing on the specific challenges of achieving transparency in dynamic, real-world contexts. We analyze how prioritizing transparency can impact public trust, regulatory policies, and avenues for future research. By positioning transparency as a fundamental element in ethical AI system design, we aim to add to the ongoing discussion on responsible AI and robotics, providing direction for future advancements in this vital field.
>
---
#### [new 004] The Problem of Atypicality in LLM-Powered Psychiatry
- **分类: cs.CY**

- **简介: 论文探讨LLM在精神医学中引发的“atypicality”伦理问题，提出动态上下文认证（DCC）框架作为可逆、上下文敏感的部署方案，强调解释性安全而非静态性能指标。**

- **链接: [http://arxiv.org/pdf/2508.06479v1](http://arxiv.org/pdf/2508.06479v1)**

> **作者:** Bosco Garcia; Eugene Y. S. Chua; Harman Singh Brah
>
> **备注:** Preprint of 8/8/2025 -- please cite published version. This article has been published in the Journal of Medical Ethics (2025) following peer review and can also be viewed on the journal's website at 10.1136/jme-2025-110972
>
> **摘要:** Large language models (LLMs) are increasingly proposed as scalable solutions to the global mental health crisis. But their deployment in psychiatric contexts raises a distinctive ethical concern: the problem of atypicality. Because LLMs generate outputs based on population-level statistical regularities, their responses -- while typically appropriate for general users -- may be dangerously inappropriate when interpreted by psychiatric patients, who often exhibit atypical cognitive or interpretive patterns. We argue that standard mitigation strategies, such as prompt engineering or fine-tuning, are insufficient to resolve this structural risk. Instead, we propose dynamic contextual certification (DCC): a staged, reversible and context-sensitive framework for deploying LLMs in psychiatry, inspired by clinical translation and dynamic safety models from artificial intelligence governance. DCC reframes chatbot deployment as an ongoing epistemic and ethical process that prioritises interpretive safety over static performance benchmarks. Atypicality, we argue, cannot be eliminated -- but it can, and must, be proactively managed.
>
---
#### [new 005] The Memory Wars: AI Memory, Network Effects, and the Geopolitics of Cognitive Sovereignty
- **分类: cs.CY; K.4.1; H.3.0**

- **简介: 论文探讨AI记忆对认知主权的挑战，提出“网络效应2.0”模型及政策框架，分析心理风险与地缘政治影响，强调记忆可移植性与透明度的重要性。**

- **链接: [http://arxiv.org/pdf/2508.05867v1](http://arxiv.org/pdf/2508.05867v1)**

> **作者:** Mario Brcic
>
> **备注:** Policy analysis. 9 pages, 1 figure, 2 tables
>
> **摘要:** The advent of continuously learning Artificial Intelligence (AI) assistants marks a paradigm shift from episodic interactions to persistent, memory-driven relationships. This paper introduces the concept of "Cognitive Sovereignty", the ability of individuals, groups, and nations to maintain autonomous thought and preserve identity in the age of powerful AI systems, especially those that hold their deep personal memory. It argues that the primary risk of these technologies transcends traditional data privacy to become an issue of cognitive and geopolitical control. We propose "Network Effect 2.0," a model where value scales with the depth of personalized memory, creating powerful cognitive moats and unprecedented user lock-in. We analyze the psychological risks of such systems, including cognitive offloading and identity dependency, by drawing on the "extended mind" thesis. These individual-level risks scale to geopolitical threats, such as a new form of digital colonialism and subtle shifting of public discourse. To counter these threats, we propose a policy framework centered on memory portability, transparency, sovereign cognitive infrastructure, and strategic alliances. This work reframes the discourse on AI assistants in an era of increasingly intimate machines, pointing to challenges to individual and national sovereignty.
>
---
#### [new 006] Sprouting technology otherwise, hospicing negative commons -- Rethinking technology in the transition to sustainability-oriented futures
- **分类: cs.CY**

- **简介: 论文探讨技术转型，提出框架应对ICT环境危害，结合commons与极限理论，通过四类概念分析计算材料与文化对可持续未来的潜在影响。**

- **链接: [http://arxiv.org/pdf/2508.05860v1](http://arxiv.org/pdf/2508.05860v1)**

> **作者:** Martin Deron
>
> **备注:** Post-proceedings paper presented at LIMITS 2025: 11th Workshop on Computing within Limits, 2025-06-26/27, Online
>
> **摘要:** Due to its significant and growing environmental harms, both directly through its materiality and indirectly through its pervasive integration into unsustainable economic systems, ICT will need to be radically redirected to align with sustainability-oriented futures. While the role of ICT in such futures will likely diverge significantly from current dynamics, it will probably not be entirely disconnected from the present. Instead, such transition involves complex dynamics of continuity, adaptation and rupture. Drawing from recent work in transition studies, the commons (particularly "negative commons"), as well as some of the Limits literature, this article proposes a conceptual framework for navigating this redirection. The framework attempts to bring together the disentanglement from sociotechnical elements incompatible with long-term sustainability and the support of existing practices that may serve as foundations for alternative technological paths. It introduces four categories: ruins, ghosts, seeds and visions, to examine how material and cultural aspects of computing may become obsolete, persist in latent or reinterpreted forms, or contribute to sustainability-oriented futures. Through both empirical and speculative examples, I intend to show how this lens can help researchers and practitioners engage more concretely with the tensions, inheritances, and opportunities involved in redirecting computing towards more sustainable and equitable futures.
>
---
#### [new 007] Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of LLM-generated Educational Feedback via LLM Feedback Evaluators
- **分类: cs.CY**

- **简介: 论文提出使用LLM反馈评估者（DeanLLMs）自动评估LLM生成的教育反馈，解决其随机性和幻觉问题，通过构建六维评估框架与2000份反馈对比，提升教育反馈质量。**

- **链接: [http://arxiv.org/pdf/2508.05952v1](http://arxiv.org/pdf/2508.05952v1)**

> **作者:** Keyang Qian; Yixin Cheng; Rui Guan; Wei Dai; Flora Jin; Kaixun Yang; Sadia Nawaz; Zachari Swiecki; Guanliang Chen; Lixiang Yan; Dragan Gašević
>
> **摘要:** The use of LLM tutors to provide automated educational feedback to students on student assignment submissions has received much attention in the AI in Education field. However, the stochastic nature and tendency for hallucinations in LLMs can undermine both quality of learning experience and adherence to ethical standards. To address this concern, we propose a method that uses LLM feedback evaluators (DeanLLMs) to automatically and comprehensively evaluate feedback generated by LLM tutor for submissions on university assignments before it is delivered to students. This allows low-quality feedback to be rejected and enables LLM tutors to improve the feedback they generated based on the evaluation results. We first proposed a comprehensive evaluation framework for LLM-generated educational feedback, comprising six dimensions for feedback content, seven for feedback effectiveness, and three for hallucination types. Next, we generated a virtual assignment submission dataset covering 85 university assignments from 43 computer science courses using eight commonly used commercial LLMs. We labelled and open-sourced the assignment dataset to support the fine-tuning and evaluation of LLM feedback evaluators. Our findings show that o3-pro demonstrated the best performance in zero-shot labelling of feedback while o4-mini demonstrated the best performance in few-shot labelling of feedback. Moreover, GPT-4.1 achieved human expert level performance after fine-tuning (Accuracy 79.8%, F1-score 79.4%; human average Accuracy 78.3%, F1-score 82.6%). Finally, we used our best-performance model to evaluate 2,000 assignment feedback instances generated by 10 common commercial LLMs, 200 each, to compare the quality of feedback generated by different LLMs. Our LLM feedback evaluator method advances our ability to automatically provide high-quality and reliable educational feedback to students.
>
---
#### [new 008] Analysis and Constructive Criticism of the Official Data Protection Impact Assessment of the German Corona-Warn-App
- **分类: cs.CY; 68P27, 68-00; K.4.1; K.4.2; K.4.3; H.4.m; H.1.m; C.5.m; K.4; J.3; H.1; J.4; K.5**

- **简介: 论文分析德国新冠预警应用官方DPIA的缺陷，指出其仅关注应用而非全流程及基础设施，缺乏对攻击者和安全措施的全面评估，提出改进建议，部分建议已纳入更新版。任务为优化DPIA质量，解决初始缺陷。**

- **链接: [http://arxiv.org/pdf/2508.06267v1](http://arxiv.org/pdf/2508.06267v1)**

> **作者:** Rainer Rehak; Christian R. Kühne; Kirsten Bock
>
> **备注:** 16 pages
>
> **摘要:** On June 15, 2020, the official data protection impact assessment (DPIA) for the German Corona-Warn-App (CWA) was made publicly available. Shortly thereafter, the app was made available for download in the app stores. However, the first version of the DPIA had significant weaknesses, as this paper argues. However since then, the quality of the official DPIA increased immensely due to interventions and interactions such as an alternative DPIA produced by external experts and extensive public discussions. To illustrate the development and improvement, the initial weaknesses of the official DPIA are documented and analyzed here. For this paper to meaningfully do this, first the purpose of a DPIA is briefly summarized. According to Article 35 of the GDPR, it consists primarily of identifying the risks to the fundamental rights and freedoms of natural persons. This paper documents at least specific methodological, technical and legal shortcomings of the initial DPIA of the CWA: 1) It only focused on the app itself, neither on the whole processing procedure nor on the infrastructure used. 2) It only briefly touched on the main data protection specific attacker, the processing organization itself. And 3) The discussion of effective safeguards to all risks including such as the ones posed by Google and Apple has only insufficiently been worked out. Finally, this paper outlines the constructive criticism and suggestions uttered, also by the authors of this paper, regarding the initial release. As of now, some of those constructive contributions have been worked into the current DPIA, such as 1) and 2), but some central ones still haven't, such as 3). This paper aims to provide an opportunity to improve the practical knowledge and academic discourse regarding high-quality DPIAs.
>
---
#### [new 009] SCALEFeedback: A Large-Scale Dataset of Synthetic Computer Science Assignments for LLM-generated Educational Feedback Research
- **分类: cs.CY**

- **简介: 论文构建了SCALEFeedback数据集，通过SAM框架生成合成计算机科学作业，解决缺乏大规模教育反馈数据的问题，实现高精度生成与隐私保护，提升LLM生成反馈效果。**

- **链接: [http://arxiv.org/pdf/2508.05953v1](http://arxiv.org/pdf/2508.05953v1)**

> **作者:** Keyang Qian; Kaixun Yang; Wei Dai; Flora Jin; Yixin Cheng; Rui Guan; Sadia Nawaz; Zachari Swiecki; Guanliang Chen; Lixiang Yan; Dragan Gašević
>
> **摘要:** Using LLMs to give educational feedback to students for their assignments has attracted much attention in the AI in Education field. Yet, there is currently no large-scale open-source dataset of student assignments that includes detailed assignment descriptions, rubrics, and student submissions across various courses. As a result, research on generalisable methodology for automatic generation of effective and responsible educational feedback remains limited. In the current study, we constructed a large-scale dataset of Synthetic Computer science Assignments for LLM-generated Educational Feedback research (SCALEFeedback). We proposed a Sophisticated Assignment Mimicry (SAM) framework to generate the synthetic dataset by one-to-one LLM-based imitation from real assignment descriptions, student submissions to produce their synthetic versions. Our open-source dataset contains 10,000 synthetic student submissions spanning 155 assignments across 59 university-level computer science courses. Our synthetic submissions achieved BERTScore F1 0.84, PCC of 0.62 for assignment marks and 0.85 for length, compared to the corresponding real-world assignment dataset, while ensuring perfect protection of student private information. All these results of our SAM framework outperformed results of a naive mimicry method baseline. The LLM-generated feedback for our synthetic assignments demonstrated the same level of effectiveness compared to that of real-world assignment dataset. Our research showed that one-to-one LLM imitation is a promising method for generating open-source synthetic educational datasets that preserve the original dataset's semantic meaning and student data distribution, while protecting student privacy and institutional copyright. SCALEFeedback enhances our ability to develop LLM-based generalisable methods for offering high-quality, automated educational feedback in a scalable way.
>
---
#### [new 010] Energy Experience Design
- **分类: cs.CY; cs.ET**

- **简介: 论文提出“能量体验”概念，设计无电池可持续电子设备原型，解决矿物资源消耗与可持续性问题，探讨其对主流系统的启示。**

- **链接: [http://arxiv.org/pdf/2508.05869v1](http://arxiv.org/pdf/2508.05869v1)**

> **作者:** Brian Sutherland
>
> **备注:** Post-proceedings paper presented at LIMITS 2024: 10th Workshop on Computing within Limits, 2024-06-19/20, Online
>
> **摘要:** The material footprint of information and communications technology (ICT) systems is both significant and growing, inspiring a variety of conversations around sustainability and climate justice. In part this effort has been catalysed by past scholarship and analysis from the LIMITS community. This paper examines energy storage systems for computing, particularly batteries -- which are discarded at the rate of 15 billion a year worldwide. The International Energy Agency (IEA) is now referring to the energy transition toward low carbon systems as a critical mineral problem, and countries are speaking openly of 'mineral security' in policy documents. In this paper I 1) present a definition for energy experience and what this means for the design and making of devices, interactions and experiences. I also 2) explore a series of electronics device prototypes converted to run from batteryless sustainable energy that are extremely long lasting, and make limited use of critical minerals. As transitional energy experience device-design experiments, what do prototypes like these suggest for more mainstream, mass-manufactured systems?
>
---
#### [new 011] Towards Reliable Generative AI-Driven Scaffolding: Reducing Hallucinations and Enhancing Quality in Self-Regulated Learning Support
- **分类: cs.CY**

- **简介: 论文提出两种评估方法，通过多智能体系统和LLM作为评判者，解决GenAI生成个性化支架的幻觉问题，提升质量并优化自调节学习支持系统。**

- **链接: [http://arxiv.org/pdf/2508.05929v1](http://arxiv.org/pdf/2508.05929v1)**

> **作者:** Keyang Qian; Shiqi Liu; Tongguang Li; Mladen Raković; Xinyu Li; Rui Guan; Inge Molenaar; Sadia Nawaz; Zachari Swiecki; Lixiang Yan; Dragan Gašević
>
> **摘要:** Generative Artificial Intelligence (GenAI) holds a potential to advance existing educational technologies with capabilities to automatically generate personalised scaffolds that support students' self-regulated learning (SRL). While advancements in large language models (LLMs) promise improvements in the adaptability and quality of educational technologies for SRL, there remain concerns about the hallucinations in content generated by LLMs, which can compromise both the learning experience and ethical standards. To address these challenges, we proposed GenAI-enabled approaches for evaluating personalised SRL scaffolds before they are presented to students, aiming for reducing hallucinations and improving the overall quality of LLM-generated personalised scaffolds. Specifically, two approaches are investigated. The first approach involved developing a multi-agent system approach for reliability evaluation to assess the extent to which LLM-generated scaffolds accurately target relevant SRL processes. The second approach utilised the "LLM-as-a-Judge" technique for quality evaluation that evaluates LLM-generated scaffolds for their helpfulness in supporting students. We constructed evaluation datasets, and compared our results with single-agent LLM systems and machine learning approach baselines. Our findings indicate that the reliability evaluation approach is highly effective and outperforms the baselines, showing almost perfect alignment with human experts' evaluations. Moreover, both proposed evaluation approaches can be harnessed to effectively reduce hallucinations. Additionally, we identified and discussed bias limitations of the "LLM-as-a-Judge" technique in evaluating LLM-generated scaffolds. We suggest incorporating these approaches into GenAI-powered personalised SRL scaffolding systems to mitigate hallucination issues and improve the overall scaffolding quality.
>
---
#### [new 012] Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 论文提出让学生作为教师教LLM，解决被动学习问题，设计Socrates系统提升学生表现。**

- **链接: [http://arxiv.org/pdf/2508.05979v1](http://arxiv.org/pdf/2508.05979v1)**

> **作者:** Xinming Yang; Haasil Pujara; Jun Li
>
> **备注:** Published at COLM 2025
>
> **摘要:** While Large Language Models (LLMs) are often used as virtual tutors in computer science (CS) education, this approach can foster passive learning and over-reliance. This paper presents a novel pedagogical paradigm that inverts this model: students act as instructors who must teach an LLM to solve problems. To facilitate this, we developed strategies for designing questions with engineered knowledge gaps that only a student can bridge, and we introduce Socrates, a system for deploying this method with minimal overhead. We evaluated our approach in an undergraduate course and found that this active-learning method led to statistically significant improvements in student performance compared to historical cohorts. Our work demonstrates a practical, cost-effective framework for using LLMs to deepen student engagement and mastery.
>
---
#### [new 013] Generative AI and the Future of the Digital Commons: Five Open Questions and Knowledge Gaps
- **分类: cs.CY; K.4.1; K.4.2; I.2.0**

- **简介: 论文探讨生成式AI与数字公共领域的互动，提出五个核心问题及解决方案，强调责任AI发展与公共知识治理。**

- **链接: [http://arxiv.org/pdf/2508.06470v1](http://arxiv.org/pdf/2508.06470v1)**

> **作者:** Arman Noroozian; Lorena Aldana; Marta Arisi; Hadi Asghari; Renata Avila; Pietro Giovanni Bizzaro; Ramya Chandrasekhar; Cristian Consonni; Deborah De Angelis; Francesca De Chiara; Maria del Rio-Chanona; Melanie Dulong de Rosnay; Maria Eriksson; Frederic Font; Emilia Gomez; Valérian Guillier; Lisa Gutermuth; David Hartmann; Lucie-Aimée Kaffee; Paul Keller; Felix Stalder; Joao Vinagre; Denny Vrandečić; Amanda Wasielewski
>
> **摘要:** The rapid advancement of Generative AI (GenAI) relies heavily on the digital commons, a vast collection of free and open online content that is created, shared, and maintained by communities. However, this relationship is becoming increasingly strained due to financial burdens, decreased contributions, and misalignment between AI models and community norms. As we move deeper into the GenAI era, it is essential to examine the interdependent relationship between GenAI, the long-term sustainability of the digital commons, and the equity of current AI development practices. We highlight five critical questions that require urgent attention: 1. How can we prevent the digital commons from being threatened by undersupply as individuals cease contributing to the commons and turn to Generative AI for information? 2. How can we mitigate the risk of the open web closing due to restrictions on access to curb AI crawlers? 3. How can technical standards and legal frameworks be updated to reflect the evolving needs of organizations hosting common content? 4. What are the effects of increased synthetic content in open knowledge databases, and how can we ensure their integrity? 5. How can we account for and distribute the infrastructural and environmental costs of providing data for AI training? We emphasize the need for more responsible practices in AI development, recognizing the digital commons not only as content but as a collaborative and decentralized form of knowledge governance, which relies on the practice of "commoning" - making, maintaining, and protecting shared and open resources. Ultimately, our goal is to stimulate discussion and research on the intersection of Generative AI and the digital commons, with the aim of developing an "AI commons" and public infrastructures for AI development that support the long-term health of the digital commons.
>
---
#### [new 014] Public support for misinformation interventions depends on perceived fairness, effectiveness, and intrusiveness
- **分类: cs.CY**

- **简介: 论文研究公众对反虚假信息干预措施的支持因素，发现公平性、有效性、侵入性是关键变量，揭示不同群体对干预方式的偏好，强调理解支持机制对政策实施的重要性。**

- **链接: [http://arxiv.org/pdf/2508.05849v1](http://arxiv.org/pdf/2508.05849v1)**

> **作者:** Catherine King; Samantha C. Phillips; Kathleen M. Carley
>
> **备注:** 23 pages, 3 figures, to be submitted to Social Media + Society
>
> **摘要:** The proliferation of misinformation on social media has concerning possible consequences, such as the degradation of democratic norms. While recent research on countering misinformation has largely focused on analyzing the effectiveness of interventions, the factors associated with public support for these interventions have received little attention. We asked 1,010 American social media users to rate their support for and perceptions of ten misinformation interventions implemented by the government or social media companies. Our results indicate that the perceived fairness of the intervention is the most important factor in determining support, followed by the perceived effectiveness of that intervention and then the intrusiveness. Interventions that supported user agency and transparency, such as labeling content or fact-checking ads, were more popular than those that involved moderating or removing content or accounts. We found some demographic differences in support levels, with Democrats and women supporting interventions more and finding them more fair, more effective, and less intrusive than Republicans and men, respectively. It is critical to understand which interventions are supported and why, as public opinion can play a key role in the rollout and effectiveness of policies.
>
---
#### [new 015] Dirty Bits in Low-Earth Orbit: The Carbon Footprint of Launching Computers
- **分类: cs.CY**

- **简介: 论文研究低轨卫星发射计算机的碳足迹，开发工具评估发射与轨道运营排放，对比技术方案，提出碳意识设计和监管建议。**

- **链接: [http://arxiv.org/pdf/2508.06250v1](http://arxiv.org/pdf/2508.06250v1)**

> **作者:** Robin Ohs; Gregory F. Stock; Andreas Schmidt; Juan A. Fraire; Holger Hermanns
>
> **备注:** This is the authors' version of a paper that was originally presented at the 4th Workshop on Sustainable Computer Systems (HotCarbon'25) and subsequently published in the ACM SIGENERGY Energy Informatics Review, see https://doi.org/10.1145/3757892.3757896
>
> **摘要:** Low-Earth Orbit (LEO) satellites are increasingly proposed for communication and in-orbit computing, achieving low-latency global services. However, their sustainability remains largely unexamined. This paper investigates the carbon footprint of computing in space, focusing on lifecycle emissions from launch over orbital operation to re-entry. We present ESpaS, a lightweight tool for estimating carbon intensities across CPU usage, memory, and networking in orbital vs. terrestrial settings. Three worked examples compare (i) launch technologies (state-of-the-art rocket vs. potential next generation) and (ii) operational emissions of data center workloads in orbit and on the ground. Results show that, even under optimistic assumptions, in-orbit systems incur significantly higher carbon costs - up to an order of magnitude more than terrestrial equivalents - primarily due to embodied emissions from launch and re-entry. Our findings advocate for carbon-aware design principles and regulatory oversight in developing sustainable digital infrastructure in orbit.
>
---
#### [new 016] Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale
- **分类: cs.CL; cs.AI; cs.CY; I.2.7; K.4**

- **简介: 论文提出一种面向玩家游戏聊天的利他行为检测方法，解决缺乏明确定义和标注数据的问题，通过三阶段流水线结合人类与AI协作，提升标注效率与精度，降低推理成本。**

- **链接: [http://arxiv.org/pdf/2508.05938v1](http://arxiv.org/pdf/2508.05938v1)**

> **作者:** Rafal Kocielnik; Min Kim; Penphob; Boonyarungsrit; Fereshteh Soltani; Deshawn Sambrano; Animashree Anandkumar; R. Michael Alvarez
>
> **备注:** 9 pages, 4 figures, 4 tables
>
> **摘要:** Detecting prosociality in text--communication intended to affirm, support, or improve others' behavior--is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment. We present a practical, three-stage pipeline that enables scalable, high-precision prosocial content classification while minimizing human labeling effort and inference costs. First, we identify the best LLM-based labeling strategy using a small seed set of human-labeled examples. We then introduce a human-AI refinement loop, where annotators review high-disagreement cases between GPT-4 and humans to iteratively clarify and expand the task definition-a critical step for emerging annotation tasks like prosociality. This process results in improved label quality and definition alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train a two-stage inference system: a lightweight classifier handles high-confidence predictions, while only $\sim$35\% of ambiguous instances are escalated to GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI interaction, careful task formulation, and deployment-aware architecture design can unlock scalable solutions for novel responsible AI tasks.
>
---
#### [new 017] Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?
- **分类: cs.CR; cs.AI; cs.CY; cs.GT**

- **简介: 论文探讨LLMs是否能通过博弈论场景模拟网络安全，解决语言偏倚问题，构建可重复框架测试零和博弈与囚徒困境，分析语言选择对结果的影响，并提出量化指标评估模型稳定性。**

- **链接: [http://arxiv.org/pdf/2508.05670v1](http://arxiv.org/pdf/2508.05670v1)**

> **作者:** Daniele Proverbio; Alessio Buscemi; Alessandro Di Stefano; The Anh Han; German Castignani; Pietro Liò
>
> **摘要:** Game theory has long served as a foundational tool in cybersecurity to test, predict, and design strategic interactions between attackers and defenders. The recent advent of Large Language Models (LLMs) offers new tools and challenges for the security of computer systems; In this work, we investigate whether classical game-theoretic frameworks can effectively capture the behaviours of LLM-driven actors and bots. Using a reproducible framework for game-theoretic LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to expected outcomes or exhibit deviations due to embedded biases. Our experiments involve four state-of-the-art LLMs and span five natural languages, English, French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic sensitivity. For both games, we observe that the final payoffs are influenced by agents characteristics such as personality traits or knowledge of repeated rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to the choice of languages, which should warn against indiscriminate application of LLMs in cybersecurity applications and call for in-depth studies, as LLMs may behave differently when deployed in different countries. We also employ quantitative metrics to evaluate the internal consistency and cross-language stability of LLM agents, to help guide the selection of the most stable LLMs and optimising models for secure applications.
>
---
#### [new 018] ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge
- **分类: cs.CV; cs.AI; cs.CY**

- **简介: 论文提出基于多模态特征融合的EMR-SEMI挑战解决方案，通过预训练模型提取视觉、文本等模态特征，结合自注意力机制与残差连接实现动态权重分配，优化噪声标签处理，取得87.49%的F-score成绩。**

- **链接: [http://arxiv.org/pdf/2508.05991v1](http://arxiv.org/pdf/2508.05991v1)**

> **作者:** Juewen Hu; Yexin Li; Jiulin Li; Shuo Chen; Pring Wong
>
> **摘要:** Emotion recognition plays a vital role in enhancing human-computer interaction. In this study, we tackle the MER-SEMI challenge of the MER2025 competition by proposing a novel multimodal emotion recognition framework. To address the issue of data scarcity, we leverage large-scale pre-trained models to extract informative features from visual, audio, and textual modalities. Specifically, for the visual modality, we design a dual-branch visual encoder that captures both global frame-level features and localized facial representations. For the textual modality, we introduce a context-enriched method that employs large language models to enrich emotional cues within the input text. To effectively integrate these multimodal features, we propose a fusion strategy comprising two key components, i.e., self-attention mechanisms for dynamic modality weighting, and residual connections to preserve original representations. Beyond architectural design, we further refine noisy labels in the training set by a multi-source labeling strategy. Our approach achieves a substantial performance improvement over the official baseline on the MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to 78.63%, thereby validating the effectiveness of the proposed framework.
>
---
#### [new 019] "Mirror" Language AI Models of Depression are Criterion-Contaminated
- **分类: cs.CL; cs.CY**

- **简介: 论文探讨Mirror语言模型在抑郁症预测中的Criterion Contamination问题，通过对比Non-Mirror模型，发现其效果夸大且泛化性差，建议使用Non-Mirror模型获取更可靠的特征。**

- **链接: [http://arxiv.org/pdf/2508.05830v1](http://arxiv.org/pdf/2508.05830v1)**

> **作者:** Tong Li; Rasiq Hussain; Mehak Gupta; Joshua R. Oltmanns
>
> **备注:** 39 pages, 9 figures
>
> **摘要:** A growing number of studies show near-perfect LLM language-based prediction of depression assessment scores (up to R2 of .70). However, many develop these models directly from language responses to depression assessments. These "Mirror models" suffer from "criterion contamination", which arises when a predicted score depends in part on the predictors themselves. This causes artificial effect size inflation which reduces model generalizability. The present study compares the performance of Mirror models versus "Non-Mirror models", which are developed from language that does not mirror the assessment they are developed to predict. N = 110 research participants completed two different interviews: structured diagnostic and life history interviews. GPT-4, GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic interview depression scores from the two transcripts separately. Mirror models (using structured diagnostic data) showed very large effect sizes (e.g., R2 = .80). As expected, NonMirror models (using life history data) demonstrated smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror and Non-Mirror model-predicted structured interview depression scores were correlated with self-reported depression symptoms, Mirror and NonMirror performed the same (e.g., r = ~.54), indicating that Mirror models contain bias perhaps due to criterion contamination. Topic modeling identified clusters across Mirror and Non-Mirror models, as well as between true-positive and false-positive predictions. In this head-to-head comparison study, Mirror language AI models of depression showed artificially inflated effect sizes and less generalizability. As language AI models for depression continue to evolve, incorporating Non-Mirror models may identify interpretable, and generalizable semantic features that have unique utility in real-world psychological assessment.
>
---
#### [new 020] Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation
- **分类: cs.CL; cs.CY**

- **简介: 本文综述LLMs有害内容生成与安全缓解，提出统一伤害分类及防御策略，分析攻击与缓解方法，总结未来方向。**

- **链接: [http://arxiv.org/pdf/2508.05775v1](http://arxiv.org/pdf/2508.05775v1)**

> **作者:** Chi Zhang; Changjia Zhu; Junjie Xiong; Xiaoran Xu; Lingyao Li; Yao Liu; Zhuo Lu
>
> **摘要:** Large Language Models (LLMs) have revolutionized content creation across digital platforms, offering unprecedented capabilities in natural language generation and understanding. These models enable beneficial applications such as content generation, question and answering (Q&A), programming, and code reasoning. Meanwhile, they also pose serious risks by inadvertently or intentionally producing toxic, offensive, or biased content. This dual role of LLMs, both as powerful tools for solving real-world problems and as potential sources of harmful language, presents a pressing sociotechnical challenge. In this survey, we systematically review recent studies spanning unintentional toxicity, adversarial jailbreaking attacks, and content moderation techniques. We propose a unified taxonomy of LLM-related harms and defenses, analyze emerging multimodal and LLM-assisted jailbreak strategies, and assess mitigation efforts, including reinforcement learning with human feedback (RLHF), prompt engineering, and safety alignment. Our synthesis highlights the evolving landscape of LLM safety, identifies limitations in current evaluation methodologies, and outlines future research directions to guide the development of robust and ethically aligned language technologies.
>
---
#### [new 021] The Fair Game: Auditing & Debiasing AI Algorithms Over Time
- **分类: cs.AI; cs.CY; cs.ET; cs.GT**

- **简介: 论文提出“Fair Game”机制，通过强化学习动态审计与去偏，解决传统Fair ML在动态社会中的偏见适应问题。**

- **链接: [http://arxiv.org/pdf/2508.06443v1](http://arxiv.org/pdf/2508.06443v1)**

> **作者:** Debabrota Basu; Udvas Das
>
> **摘要:** An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify different types of bias (also known as unfairness) exhibited in the predictions of ML algorithms, and to design new algorithms to mitigate them. Often, the definitions of bias used in the literature are observational, i.e. they use the input and output of a pre-trained algorithm to quantify a bias under concern. In reality,these definitions are often conflicting in nature and can only be deployed if either the ground truth is known or only in retrospect after deploying the algorithm. Thus,there is a gap between what we want Fair ML to achieve and what it does in a dynamic social environment. Hence, we propose an alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions of an ML algorithm and to adapt its predictions as the society interacts with the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing algorithm in a loop around an ML algorithm. The "Fair Game" puts these two components in a loop by leveraging Reinforcement Learning (RL). RL algorithms interact with an environment to take decisions, which yields new observations (also known as data/feedback) from the environment and in turn, adapts future decisions. RL is already used in algorithms with pre-fixed long-term fairness goals. "Fair Game" provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system. This allows us to develop a flexible and adaptive-over-time framework to build Fair ML systems pre- and post-deployment.
>
---
#### [new 022] Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional Resonance
- **分类: cs.HC; cs.CY**

- **简介: 该论文研究Telegram上emoji反应的通信功能，探讨其是否反映社会认可而非情感共鸣。通过分析65万条消息的语义与反应关系，发现正向反应主导，模式稳定，表明emoji反应可能传递社会认可而非情感共振，对情感分析方法提出新见解。**

- **链接: [http://arxiv.org/pdf/2508.06349v1](http://arxiv.org/pdf/2508.06349v1)**

> **作者:** Serena Tardelli; Lorenzo Alvisi; Lorenzo Cima; Stefano Cresci; Maurizio Tesconi
>
> **摘要:** Emoji reactions are a frequently used feature of messaging platforms. Prior work mainly interpreted emojis as indicators of emotional resonance or user sentiment. However, emoji reactions may instead reflect broader social dynamics. Here, we investigate the communicative function of emoji reactions on Telegram by analyzing the relationship between the emotional and rhetorical content of messages and the emoji reactions they receive. We collect and analyze over 650k Telegram messages that received at least one emoji reaction. We annotate each message with sentiment, emotion, persuasion strategy, and speech act labels, and infer the sentiment and emotion of emoji reactions using both lexicons and large languages. We find a systematic mismatch between message sentiment and reaction sentiment, with positive reactions dominating even when the message is neutral or negative. We show that this pattern remains consistent across rhetorical strategies and emotional tones, suggesting that emoji reactions may signal a degree of social approval rather than reflecting emotional resonance. Finally, we shed light on the communicative strategies that predict greater emoji engagement. These findings have methodological implications for sentiment analysis, as interpreting emoji reactions as direct proxies for emotional response may be misleading.
>
---
## 更新

#### [replaced 001] Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks
- **分类: cs.HC; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.02819v2](http://arxiv.org/pdf/2507.02819v2)**

> **作者:** Luke Guerdan; Devansh Saxena; Stevie Chancellor; Zhiwei Steven Wu; Kenneth Holstein
>
> **备注:** CSCW 2025
>
> **摘要:** Data scientists often formulate predictive modeling tasks involving fuzzy, hard-to-define concepts, such as the "authenticity" of student writing or the "healthcare need" of a patient. Yet the process by which data scientists translate fuzzy concepts into a concrete, proxy target variable remains poorly understood. We interview fifteen data scientists in education (N=8) and healthcare (N=7) to understand how they construct target variables for predictive modeling tasks. Our findings suggest that data scientists construct target variables through a bricolage process, involving iterative negotiation between high-level measurement objectives and low-level practical constraints. Data scientists attempt to satisfy five major criteria for a target variable through bricolage: validity, simplicity, predictability, portability, and resource requirements. To achieve this, data scientists adaptively use problem (re)formulation strategies, such as swapping out one candidate target variable for another when the first fails to meet certain criteria (e.g., predictability), or composing multiple outcomes into a single target variable to capture a more holistic set of modeling objectives. Based on our findings, we present opportunities for future HCI, CSCW, and ML research to better support the art and science of target variable construction.
>
---
#### [replaced 002] Learning AI Auditing: A Case Study of Teenagers Auditing a Generative AI Model
- **分类: cs.HC; cs.CY; H.5.0; K.3.2**

- **链接: [http://arxiv.org/pdf/2508.04902v2](http://arxiv.org/pdf/2508.04902v2)**

> **作者:** Luis Morales-Navarro; Michelle Gan; Evelyn Yu; Lauren Vogelstein; Yasmin B. Kafai; Danaé Metaxa
>
> **摘要:** This study investigates how high school-aged youth engage in algorithm auditing to identify and understand biases in artificial intelligence and machine learning (AI/ML) tools they encounter daily. With AI/ML technologies being increasingly integrated into young people's lives, there is an urgent need to equip teenagers with AI literacies that build both technical knowledge and awareness of social impacts. Algorithm audits (also called AI audits) have traditionally been employed by experts to assess potential harmful biases, but recent research suggests that non-expert users can also participate productively in auditing. We conducted a two-week participatory design workshop with 14 teenagers (ages 14-15), where they audited the generative AI model behind TikTok's Effect House, a tool for creating interactive TikTok filters. We present a case study describing how teenagers approached the audit, from deciding what to audit to analyzing data using diverse strategies and communicating their results. Our findings show that participants were engaged and creative throughout the activities, independently raising and exploring new considerations, such as age-related biases, that are uncommon in professional audits. We drew on our expertise in algorithm auditing to triangulate their findings as a way to examine if the workshop supported participants to reach coherent conclusions in their audit. Although the resulting number of changes in race, gender, and age representation uncovered by the teens were slightly different from ours, we reached similar conclusions. This study highlights the potential for auditing to inspire learning activities to foster AI literacies, empower teenagers to critically examine AI systems, and contribute fresh perspectives to the study of algorithmic harms.
>
---
#### [replaced 003] Don't Trust A Single Gerrymandering Metric
- **分类: physics.soc-ph; cs.CY; 60J20, 91C99, 91F99; J.4**

- **链接: [http://arxiv.org/pdf/2409.17186v3](http://arxiv.org/pdf/2409.17186v3)**

> **作者:** Thomas Ratliff; Stephanie Somersille; Ellen Veomett
>
> **备注:** 46 pages, 27 figures
>
> **摘要:** In recent years, in an effort to promote fairness in the election process, a wide variety of techniques and metrics have been proposed to determine whether a map is a partisan gerrymander. The most accessible measures, requiring easily obtained data, are metrics such as the Mean-Median Difference, Efficiency Gap, Declination, and GEO metric. But for most of these metrics, researchers have struggled to describe, given no additional information, how a value of that metric on a single map indicates the presence or absence of gerrymandering. Our main result is that each of these metrics is gameable when used as a single, isolated quantity to detect gerrymandering (or the lack thereof). That is, for each of the four metrics, we can find district plans for a given state with an extremely large number of Democratic-won (or Republican-won) districts while the metric value of that plan falls within a reasonable, predetermined bound. We do this by using a hill-climbing method to generate district plans that are constrained by the bounds on the metric but also maximize or nearly maximize the number of districts won by a party. In addition, extreme values of the Mean-Median Difference do not necessarily correspond to maps with an extreme number of districts won. Thus, the Mean- Median Difference metric is particularly misleading, as it cannot distinguish more extreme maps from less extreme maps. The other metrics are more nuanced, but when assessed on an ensemble, none perform substantially differently from simply measuring number of districts won by a fixed party. One clear consequence of these results is that they demonstrate the folly of specifying a priori bounds on a metric that a redistricting commission must meet in order to avoid gerrymandering.
>
---
#### [replaced 004] Web3 x AI Agents: Landscape, Integrations, and Foundational Challenges
- **分类: cs.CY; cs.AI; econ.GN; q-fin.EC**

- **链接: [http://arxiv.org/pdf/2508.02773v2](http://arxiv.org/pdf/2508.02773v2)**

> **作者:** Yiming Shen; Jiashuo Zhang; Zhenzhe Shao; Wenxuan Luo; Yanlin Wang; Ting Chen; Zibin Zheng; Jiachi Chen
>
> **摘要:** The convergence of Web3 technologies and AI agents represents a rapidly evolving frontier poised to reshape decentralized ecosystems. This paper presents the first and most comprehensive analysis of the intersection between Web3 and AI agents, examining five critical dimensions: landscape, economics, governance, security, and trust mechanisms. Through an analysis of 133 existing projects, we first develop a taxonomy and systematically map the current market landscape (RQ1), identifying distinct patterns in project distribution and capitalization. Building upon these findings, we further investigate four key integrations: (1) the role of AI agents in participating in and optimizing decentralized finance (RQ2); (2) their contribution to enhancing Web3 governance mechanisms (RQ3); (3) their capacity to strengthen Web3 security via intelligent vulnerability detection and automated smart contract auditing (RQ4); and (4) the establishment of robust reliability frameworks for AI agent operations leveraging Web3's inherent trust infrastructure (RQ5). By synthesizing these dimensions, we identify key integration patterns, highlight foundational challenges related to scalability, security, and ethics, and outline critical considerations for future research toward building robust, intelligent, and trustworthy decentralized systems with effective AI agent interactions.
>
---
#### [replaced 005] Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective
- **分类: cs.CL; cs.AI; cs.CY; 68T50; I.2.7**

- **链接: [http://arxiv.org/pdf/2506.19028v3](http://arxiv.org/pdf/2506.19028v3)**

> **作者:** Weijie Xu; Yiwen Wang; Chi Xue; Xiangkun Hu; Xi Fang; Guimin Dong; Chandan K. Reddy
>
> **备注:** 29 pages, 9 figures, 15 tables
>
> **摘要:** Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo(Fine-grained Semantic Computation), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSco more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.
>
---
#### [replaced 006] Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction
- **分类: cs.AI; cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2508.02622v2](http://arxiv.org/pdf/2508.02622v2)**

> **作者:** Enrico De Santis; Antonello Rizzi
>
> **备注:** This version has been extensively revised and revisited in light of feedback and further research. Several sections have been expanded or improved for greater clarity and completeness. Specifically, new clarification on complex system foundation related to Noosemia has been added (Secs. "2.4 and "2.5")
>
> **摘要:** This paper introduces and formalizes Noosem\`ia, a novel cognitive-phenomenological pattern emerging from human interaction with generative AI systems, particularly those enabling dialogic or multimodal exchanges. We propose a multidisciplinary framework to explain how, under certain conditions, users attribute intentionality, agency, and even interiority to these systems - a process grounded not in physical resemblance, but in linguistic performance, epistemic opacity, and emergent technological complexity. By linking an LLM declination of meaning holism to our technical notion of the LLM Contextual Cognitive Field, we clarify how LLMs construct meaning relationally and how coherence and a simulacrum of agency arise at the human-AI interface. The analysis situates noosemia alongside pareidolia, animism, the intentional stance and the uncanny valley, distinguishing its unique characteristics. We also introduce a-noosemia to describe the phenomenological withdrawal of such projections. The paper concludes with reflections on the broader philosophical, epistemological and social implications of noosemic dynamics and directions for future research.
>
---
#### [replaced 007] From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems
- **分类: cs.CY; cs.CE; cs.CL; cs.HC; cs.RO**

- **链接: [http://arxiv.org/pdf/2507.04996v2](http://arxiv.org/pdf/2507.04996v2)**

> **作者:** Jiangbo Yu
>
> **摘要:** Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are defined as systems capable of perceiving their environment and executing preprogrammed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 1 to 6), such as interaction with humans and machines, goal adaptation, contextual reasoning, external tool use, and long-term planning, particularly with the integration of large language models (LLMs) and agentic AI systems. These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this, we introduce the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and interact within complex environments. This paper presents a systems-level framework to characterize AgVs, focusing on their cognitive and communicative layers and differentiating them from conventional AuVs. It synthesizes relevant advances in agentic AI, robotics, multi-agent systems, and human-machine interaction, and highlights how agentic AI, through high-level reasoning and tool use, can function not merely as computational tools but as interactive agents embedded in mobility ecosystems. The paper concludes by identifying key challenges in the development and governance of AgVs, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.
>
---
#### [replaced 008] Spatial Association Between Near-Misses and Accident Blackspots in Sydney, Australia: A Getis-Ord $G_i^*$ Analysis
- **分类: eess.SY; cs.CY; cs.SY; stat.AP**

- **链接: [http://arxiv.org/pdf/2506.03356v2](http://arxiv.org/pdf/2506.03356v2)**

> **作者:** Artur Grigorev; David Lillo-Trynes; Adriana-Simona Mihaita
>
> **摘要:** Conventional road safety management is inherently reactive, relying on analysis of sparse and lagged historical crash data to identify hazardous locations, or crash blackspots. The proliferation of vehicle telematics presents an opportunity for a paradigm shift towards proactive safety, using high-frequency, high-resolution near-miss data as a leading indicator of crash risk. This paper presents a spatial-statistical framework to systematically analyze the concordance and discordance between official crash records and near-miss events within urban environment. A Getis-Ord statistic is first applied to both reported crashes and near-miss events to identify statistically significant local clusters of each type. Subsequently, Bivariate Local Moran's I assesses spatial relationships between crash counts and High-G event counts, classifying grid cells into distinct profiles: High-High (coincident risk), High-Low and Low-High. Our analysis reveals significant amount of Low-Crash, High-Near-Miss clusters representing high-risk areas that remain unobservable when relying solely on historical crash data. Feature importance analysis is performed using contextual Point of Interest data to identify the different infrastructure factors that characterize difference between spatial clusters. The results provide a data-driven methodology for transport authorities to transition from a reactive to a proactive safety management strategy, allowing targeted interventions before severe crashes occur.
>
---
