# 音频 cs.SD;  eess.SP

- **最新发布 22 篇**

- **更新 4 篇**

## 最新发布

#### [new 001] SynthVC: Leveraging Synthetic Data for End-to-End Low Latency Streaming Voice Conversion
- **分类: cs.SD**

- **简介: 论文提出SynthVC，一种端到端流式语音转换框架，旨在低延迟下实现高质量语音转换。该任务属于语音转换（Voice Conversion），目标是在保持语言内容不变的同时转换说话人音色。现有方法因高延迟或依赖ASR模块难以实现实时应用。SynthVC利用合成数据训练，无需显式分离内容与说话人信息，降低了延迟并提升了自然度与音色相似性，实现了77.1毫秒的端到端延迟。**

- **链接: [http://arxiv.org/pdf/2510.09245v1](http://arxiv.org/pdf/2510.09245v1)**

> **作者:** Zhao Guo; Ziqian Ning; Guobin Ma; Lei Xie
>
> **备注:** Accepted by NCMMSC2025
>
> **摘要:** Voice Conversion (VC) aims to modify a speaker's timbre while preserving linguistic content. While recent VC models achieve strong performance, most struggle in real-time streaming scenarios due to high latency, dependence on ASR modules, or complex speaker disentanglement, which often results in timbre leakage or degraded naturalness. We present SynthVC, a streaming end-to-end VC framework that directly learns speaker timbre transformation from synthetic parallel data generated by a pre-trained zero-shot VC model. This design eliminates the need for explicit content-speaker separation or recognition modules. Built upon a neural audio codec architecture, SynthVC supports low-latency streaming inference with high output fidelity. Experimental results show that SynthVC outperforms baseline streaming VC systems in both naturalness and speaker similarity, achieving an end-to-end latency of just 77.1 ms.
>
---
#### [new 002] MMAudioSep: Taming Video-to-Audio Generative Model Towards Video/Text-Queried Sound Separation
- **分类: cs.SD; cs.CV; cs.LG; eess.AS**

- **简介: 该论文属于音视频生成与分离任务，旨在解决视频/文本引导下的声音分离问题。作者基于预训练视频-音频模型提出MMAudioSep，通过微调使其具备声音分离能力，同时保留原有视频生成音频功能。实验表明该方法优于现有分离模型。**

- **链接: [http://arxiv.org/pdf/2510.09065v1](http://arxiv.org/pdf/2510.09065v1)**

> **作者:** Akira Takahashi; Shusuke Takahashi; Yuki Mitsufuji
>
> **备注:** 4 pages, 4 figures, 2 tables
>
> **摘要:** We introduce MMAudioSep, a generative model for video/text-queried sound separation that is founded on a pretrained video-to-audio model. By leveraging knowledge about the relationship between video/text and audio learned through a pretrained audio generative model, we can train the model more efficiently, i.e., the model does not need to be trained from scratch. We evaluate the performance of MMAudioSep by comparing it to existing separation models, including models based on both deterministic and generative approaches, and find it is superior to the baseline models. Furthermore, we demonstrate that even after acquiring functionality for sound separation via fine-tuning, the model retains the ability for original video-to-audio generation. This highlights the potential of foundational sound generation models to be adopted for sound-related downstream tasks. Our code is available at https://github.com/sony/mmaudiosep.
>
---
#### [new 003] Emotion-Disentangled Embedding Alignment for Noise-Robust and Cross-Corpus Speech Emotion Recognition
- **分类: cs.SD; cs.AI; cs.HC; cs.LG**

- **简介: 该论文属于语音情感识别任务，旨在解决真实场景中噪声干扰和跨数据集差异导致的识别效果下降问题。论文提出EDRL和MEA两步方法，学习更具鲁棒性和泛化能力的情感表征，提升在噪声和跨数据集场景下的识别性能。**

- **链接: [http://arxiv.org/pdf/2510.09072v1](http://arxiv.org/pdf/2510.09072v1)**

> **作者:** Upasana Tiwari; Rupayan Chakraborty; Sunil Kumar Kopparapu
>
> **备注:** 13 pages, 1 figure
>
> **摘要:** Effectiveness of speech emotion recognition in real-world scenarios is often hindered by noisy environments and variability across datasets. This paper introduces a two-step approach to enhance the robustness and generalization of speech emotion recognition models through improved representation learning. First, our model employs EDRL (Emotion-Disentangled Representation Learning) to extract class-specific discriminative features while preserving shared similarities across emotion categories. Next, MEA (Multiblock Embedding Alignment) refines these representations by projecting them into a joint discriminative latent subspace that maximizes covariance with the original speech input. The learned EDRL-MEA embeddings are subsequently used to train an emotion classifier using clean samples from publicly available datasets, and are evaluated on unseen noisy and cross-corpus speech samples. Improved performance under these challenging conditions demonstrates the effectiveness of the proposed method.
>
---
#### [new 004] DiTSinger: Scaling Singing Voice Synthesis with Diffusion Transformer and Implicit Alignment
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文属于歌唱语音合成任务，旨在解决数据稀缺和模型可扩展性问题。作者构建了一个小型高质量数据集，并提出DiTSinger模型，结合扩散变压器与隐式对齐机制，实现高保真、无需音素级标注的歌唱合成。**

- **链接: [http://arxiv.org/pdf/2510.09016v1](http://arxiv.org/pdf/2510.09016v1)**

> **作者:** Zongcai Du; Guilin Deng; Xiaofeng Guo; Xin Gao; Linke Li; Kaichang Cheng; Fubo Han; Siyu Yang; Peng Liu; Pan Zhong; Qiang Fu
>
> **备注:** under review
>
> **摘要:** Recent progress in diffusion-based Singing Voice Synthesis (SVS) demonstrates strong expressiveness but remains limited by data scarcity and model scalability. We introduce a two-stage pipeline: a compact seed set of human-sung recordings is constructed by pairing fixed melodies with diverse LLM-generated lyrics, and melody-specific models are trained to synthesize over 500 hours of high-quality Chinese singing data. Building on this corpus, we propose DiTSinger, a Diffusion Transformer with RoPE and qk-norm, systematically scaled in depth, width, and resolution for enhanced fidelity. Furthermore, we design an implicit alignment mechanism that obviates phoneme-level duration labels by constraining phoneme-to-acoustic attention within character-level spans, thereby improving robustness under noisy or uncertain alignments. Extensive experiments validate that our approach enables scalable, alignment-free, and high-fidelity SVS.
>
---
#### [new 005] EGSTalker: Real-Time Audio-Driven Talking Head Generation with Efficient Gaussian Deformation
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文属于音频驱动的说话人头部生成任务，旨在解决实时生成高质量、唇形同步的面部动画问题。论文提出了EGSTalker框架，利用3D高斯变形和音频-空间注意力模块，在少量训练数据下实现快速推理与高渲染质量。**

- **链接: [http://arxiv.org/pdf/2510.08587v1](http://arxiv.org/pdf/2510.08587v1)**

> **作者:** Tianheng Zhu; Yinfeng Yu; Liejun Wang; Fuchun Sun; Wendong Zheng
>
> **备注:** Main paper (6 pages). Accepted for publication by IEEE International Conference on Systems, Man, and Cybernetics 2025
>
> **摘要:** This paper presents EGSTalker, a real-time audio-driven talking head generation framework based on 3D Gaussian Splatting (3DGS). Designed to enhance both speed and visual fidelity, EGSTalker requires only 3-5 minutes of training video to synthesize high-quality facial animations. The framework comprises two key stages: static Gaussian initialization and audio-driven deformation. In the first stage, a multi-resolution hash triplane and a Kolmogorov-Arnold Network (KAN) are used to extract spatial features and construct a compact 3D Gaussian representation. In the second stage, we propose an Efficient Spatial-Audio Attention (ESAA) module to fuse audio and spatial cues, while KAN predicts the corresponding Gaussian deformations. Extensive experiments demonstrate that EGSTalker achieves rendering quality and lip-sync accuracy comparable to state-of-the-art methods, while significantly outperforming them in inference speed. These results highlight EGSTalker's potential for real-time multimedia applications.
>
---
#### [new 006] VM-UNSSOR: Unsupervised Neural Speech Separation Enhanced by Higher-SNR Virtual Microphone Arrays
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于语音分离任务，旨在解决麦克风数量较少时无监督语音分离性能下降的问题。作者提出VM-UNSSOR方法，通过引入高信噪比的虚拟麦克风信号，增强了原有模型的分离效果和鲁棒性。**

- **链接: [http://arxiv.org/pdf/2510.08914v1](http://arxiv.org/pdf/2510.08914v1)**

> **作者:** Shulin He; Zhong-Qiu Wang
>
> **摘要:** Blind speech separation (BSS) aims to recover multiple speech sources from multi-channel, multi-speaker mixtures under unknown array geometry and room impulse responses. In unsupervised setup where clean target speech is not available for model training, UNSSOR proposes a mixture consistency (MC) loss for training deep neural networks (DNN) on over-determined training mixtures to realize unsupervised speech separation. However, when the number of microphones of the training mixtures decreases, the MC constraint weakens and the separation performance falls dramatically. To address this, we propose VM-UNSSOR, augmenting the observed training mixture signals recorded by a limited number of microphones with several higher-SNR virtual-microphone (VM) signals, which are obtained by applying linear spatial demixers (such as IVA and spatial clustering) to the observed training mixtures. As linear projections of the observed mixtures, the virtual-microphone signals can typically increase the SNR of each source and can be leveraged to compute extra MC losses to improve UNSSOR and address the frequency permutation problem in UNSSOR. On the SMS-WSJ dataset, in the over-determined six-microphone, two-speaker separation setup, VM-UNSSOR reaches 17.1 dB SI-SDR, while UNSSOR only obtains 14.7 dB; and in the determined two-microphone, two-speaker case, UNSSOR collapses to -2.7 dB SI-SDR, while VM-UNSSOR achieves 10.7 dB.
>
---
#### [new 007] Déréverbération non-supervisée de la parole par modèle hybride
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文属于语音去混响任务，旨在解决缺乏配对干净/混响语音数据的问题。作者提出一种仅使用混响语音和少量声学信息（如RT60）的无监督训练策略，提升了去混响系统的性能。**

- **链接: [http://arxiv.org/pdf/2510.09025v1](http://arxiv.org/pdf/2510.09025v1)**

> **作者:** Louis Bahrman; Mathieu Fontaine; Gaël Richard
>
> **备注:** in French language
>
> **摘要:** This paper introduces a new training strategy to improve speech dereverberation systems in an unsupervised manner using only reverberant speech. Most existing algorithms rely on paired dry/reverberant data, which is difficult to obtain. Our approach uses limited acoustic information, like the reverberation time (RT60), to train a dereverberation system. Experimental results demonstrate that our method achieves more consistent performance across various objective metrics than the state-of-the-art.
>
---
#### [new 008] O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice Conversion
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于语音转换任务，旨在解决传统方法中难以分离说话人身份与语言内容导致的信息损失问题。论文利用预训练的高质量多说话人文本到语音模型生成合成数据，通过共享语言内容、不同说话人的数据对训练模型，实现源语音到目标语音的直接映射，提升了语音转换效果和对未见说话人及新语言的适应能力。**

- **链接: [http://arxiv.org/pdf/2510.09061v1](http://arxiv.org/pdf/2510.09061v1)**

> **作者:** Huu Tuong Tu; Huan Vu; cuong tien nguyen; Dien Hy Ngo; Nguyen Thi Thu Trang
>
> **备注:** EMNLP 2025
>
> **摘要:** Traditional voice conversion (VC) methods typically attempt to separate speaker identity and linguistic information into distinct representations, which are then combined to reconstruct the audio. However, effectively disentangling these factors remains challenging, often leading to information loss during training. In this paper, we propose a new approach that leverages synthetic speech data generated by a high-quality, pretrained multispeaker text-to-speech (TTS) model. Specifically, synthetic data pairs that share the same linguistic content but differ in speaker identity are used as input-output pairs to train the voice conversion model. This enables the model to learn a direct mapping between source and target voices, effectively capturing speaker-specific characteristics while preserving linguistic content. Additionally, we introduce a flexible training strategy for any-to-any voice conversion that generalizes well to unseen speakers and new languages, enhancing adaptability and performance in zero-shot scenarios. Our experiments show that our proposed method achieves a 16.35% relative reduction in word error rate and a 5.91% improvement in speaker cosine similarity, outperforming several state-of-the-art methods. Voice conversion samples can be accessed at: https://oovc-emnlp-2025.github.io/
>
---
#### [new 009] LadderSym: A Multimodal Interleaved Transformer for Music Practice Error Detection
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文属于音乐错误检测任务，旨在解决音乐练习中音符错误识别的问题。论文提出LadderSym模型，通过双流编码器和跨模态对齐模块提升音频对比能力，并利用符号谱作为解码提示减少频谱歧义。实验表明，该方法在多个数据集上显著提升了错误检测的F1分数。**

- **链接: [http://arxiv.org/pdf/2510.08580v1](http://arxiv.org/pdf/2510.08580v1)**

> **作者:** Benjamin Shiue-Hal Chou; Purvish Jajal; Nick John Eliopoulos; James C. Davis; George K. Thiruvathukal; Kristen Yeon-Ji Yun; Yung-Hsiang Lu
>
> **备注:** Under Submission
>
> **摘要:** Music learners can greatly benefit from tools that accurately detect errors in their practice. Existing approaches typically compare audio recordings to music scores using heuristics or learnable models. This paper introduces \textit{LadderSym}, a novel Transformer-based method for music error detection. \textit{LadderSym} is guided by two key observations about the state-of-the-art approaches: (1) late fusion limits inter-stream alignment and cross-modality comparison capability; and (2) reliance on score audio introduces ambiguity in the frequency spectrum, degrading performance in music with concurrent notes. To address these limitations, \textit{LadderSym} introduces (1) a two-stream encoder with inter-stream alignment modules to improve audio comparison capabilities and error detection F1 scores, and (2) a multimodal strategy that leverages both audio and symbolic scores by incorporating symbolic representations as decoder prompts, reducing ambiguity and improving F1 scores. We evaluate our method on the \textit{MAESTRO-E} and \textit{CocoChorales-E} datasets by measuring the F1 score for each note category. Compared to the previous state of the art, \textit{LadderSym} more than doubles F1 for missed notes on \textit{MAESTRO-E} (26.8\% $\rightarrow$ 56.3\%) and improves extra note detection by 14.4 points (72.0\% $\rightarrow$ 86.4\%). Similar gains are observed on \textit{CocoChorales-E}. This work introduces general insights about comparison models that could inform sequence evaluation tasks for reinforcement Learning, human skill assessment, and model evaluation.
>
---
#### [new 010] Evaluating Hallucinations in Multimodal LLMs with Spoken Queries under Diverse Acoustic Conditions
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文研究语音输入对多模态大语言模型“幻觉”问题的影响，属于自然语言处理与语音识别交叉任务。为解决语音驱动接口中幻觉现象未被充分探索的问题，作者构建了RePOPE-Spk基准，在不同声学条件下测试模型表现。结果显示，语音输入加剧幻觉，噪声环境下错误率显著上升。**

- **链接: [http://arxiv.org/pdf/2510.08581v1](http://arxiv.org/pdf/2510.08581v1)**

> **作者:** Hansol Park; Hoseong Ahn; Junwon Moon; Yejin Lee; Kyuhong Shim
>
> **摘要:** Hallucinations in vision-language models have been extensively studied using benchmarks that probe reliability in image-text settings. In contrast, the effect of spoken queries on multimodal hallucinations remains largely unexplored, despite the growing role of voice-driven interfaces. In this work, we investigate how spoken input influences hallucinations in multimodal large language models. We present RePOPE-Spk, an audio-augmented extension of the RePOPE benchmark, where queries are provided as speech under diverse acoustic conditions. Using RePOPE-Spk, we systematically evaluate both proprietary and open-source models. Experimental results show that hallucinations escalate when queries are spoken rather than written: error rates increase by 3% under clean speech and by up to 20% with environmental noise. Input order and query length further affect robustness, while strategies such as many-shot prompting and chain-of-thought reasoning offer partial but insufficient mitigation. These findings highlight a critical and underexplored challenge, opening new directions for building reliable voice interface systems.
>
---
#### [new 011] ControlAudio: Tackling Text-Guided, Timing-Indicated and Intelligible Audio Generation via Progressive Diffusion Modeling
- **分类: cs.SD; cs.AI; cs.CL; eess.AS**

- **简介: 该论文属于文本到音频生成任务，旨在解决精细控制信号下音频生成效果差的问题。作者提出ControlAudio方法，通过渐进式扩散建模，融合文本、时序和音素特征，提升了生成音频的时间准确性和语音清晰度，达到最优性能。**

- **链接: [http://arxiv.org/pdf/2510.08878v1](http://arxiv.org/pdf/2510.08878v1)**

> **作者:** Yuxuan Jiang; Zehua Chen; Zeqian Ju; Yusheng Dai; Weibei Dou; Jun Zhu
>
> **备注:** 18 pages, 8 tables, 5 figures
>
> **摘要:** Text-to-audio (TTA) generation with fine-grained control signals, e.g., precise timing control or intelligible speech content, has been explored in recent works. However, constrained by data scarcity, their generation performance at scale is still compromised. In this study, we recast controllable TTA generation as a multi-task learning problem and introduce a progressive diffusion modeling approach, ControlAudio. Our method adeptly fits distributions conditioned on more fine-grained information, including text, timing, and phoneme features, through a step-by-step strategy. First, we propose a data construction method spanning both annotation and simulation, augmenting condition information in the sequence of text, timing, and phoneme. Second, at the model training stage, we pretrain a diffusion transformer (DiT) on large-scale text-audio pairs, achieving scalable TTA generation, and then incrementally integrate the timing and phoneme features with unified semantic representations, expanding controllability. Finally, at the inference stage, we propose progressively guided generation, which sequentially emphasizes more fine-grained information, aligning inherently with the coarse-to-fine sampling nature of DiT. Extensive experiments show that ControlAudio achieves state-of-the-art performance in terms of temporal accuracy and speech clarity, significantly outperforming existing methods on both objective and subjective evaluations. Demo samples are available at: https://control-audio.github.io/Control-Audio.
>
---
#### [new 012] WildElder: A Chinese Elderly Speech Dataset from the Wild with Fine-Grained Manual Annotations
- **分类: cs.SD**

- **简介: 该论文属于语音数据集构建任务，旨在解决现有中文老年人语音数据多样性不足、缺乏真实场景的问题。作者收集了来自网络视频的真实老年语音，提供细粒度人工标注，构建了WildElder数据集，用于提升老年人语音识别与说话人分析的研究。**

- **链接: [http://arxiv.org/pdf/2510.09344v1](http://arxiv.org/pdf/2510.09344v1)**

> **作者:** Hui Wang; Jiaming Zhou; Jiabei He; Haoqin Sun; Yong Qin
>
> **摘要:** Elderly speech poses unique challenges for automatic processing due to age-related changes such as slower articulation and vocal tremors. Existing Chinese datasets are mostly recorded in controlled environments, limiting their diversity and real-world applicability. To address this gap, we present WildElder, a Mandarin elderly speech corpus collected from online videos and enriched with fine-grained manual annotations, including transcription, speaker age, gender, and accent strength. Combining the realism of in-the-wild data with expert curation, WildElder enables robust research on automatic speech recognition and speaker profiling. Experimental results reveal both the difficulties of elderly speech recognition and the potential of WildElder as a challenging new benchmark. The dataset and code are available at https://github.com/NKU-HLT/WildElder.
>
---
#### [new 013] Audible Networks: Deconstructing and Manipulating Sounds with Deep Non-Negative Autoencoders
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于音频信号处理任务，旨在解决声音的可解释性分解与创造性编辑问题。作者提出使用深度非负自编码器（NAEs）对声音进行多层级分解，实现声音事件的可视化、编辑与重组，支持用户引导的声音操作。工作包括设计NAE结构、提出新编辑操作及展示实际应用效果。**

- **链接: [http://arxiv.org/pdf/2510.08816v1](http://arxiv.org/pdf/2510.08816v1)**

> **作者:** Juan José Burred; Carmine-Emanuele Cella
>
> **摘要:** We propose the use of Non-Negative Autoencoders (NAEs) for sound deconstruction and user-guided manipulation of sounds for creative purposes. NAEs offer a versatile and scalable extension of traditional Non-Negative Matrix Factorization (NMF)-based approaches for interpretable audio decomposition. By enforcing non-negativity constraints through projected gradient descent, we obtain decompositions where internal weights and activations can be directly interpreted as spectral shapes and temporal envelopes, and where components can themselves be listened to as individual sound events. In particular, multi-layer Deep NAE architectures enable hierarchical representations with an adjustable level of granularity, allowing sounds to be deconstructed at multiple levels of abstraction: from high-level note envelopes down to fine-grained spectral details. This framework enables a wide new range of expressive, controllable, and randomized sound transformations. We introduce novel manipulation operations including cross-component and cross-layer synthesis, hierarchical deconstructions, and several randomization strategies that control timbre and event density. Through visualizations and resynthesis of practical examples, we demonstrate how NAEs can serve as flexible and interpretable tools for object-based sound editing.
>
---
#### [new 014] Effects of automotive microphone frequency response characteristics and noise conditions on speech and ASR quality -- an experimental evaluation
- **分类: eess.AS; cs.SD**

- **简介: 该论文研究汽车麦克风频率响应特性及噪声条件对语音通信和自动语音识别（ASR）质量的影响。属于语音信号处理任务，旨在解决麦克风选型缺乏数据支持的问题。通过实车噪声信号实验，评估麦克风带宽与频率响应对语音质量和ASR性能（WER）的影响，提供选型依据。**

- **链接: [http://arxiv.org/pdf/2510.09236v1](http://arxiv.org/pdf/2510.09236v1)**

> **作者:** Michele Buccoli; Yu Du; Jacob Soendergaard; Simone Shawn Cazzaniga
>
> **摘要:** Upon choosing microphones for automotive hands-free communication or Automatic Speech Recognition (ASR) applications, OEMs typically specify wideband, super wideband or even fullband requirements following established standard recommendations (e.g., ITU-P.1110, ITU-P.1120). In practice, it is often challenging to achieve the preferred bandwidth for an automotive microphone when considering limitations and constraints on microphone placement inside the cabin, and the automotive grade environmental robustness requirements. On the other hand, there seems to be no consensus or sufficient data on the effect of each microphone characteristic on the actual performance. As an attempt to answer this question, we used noise signals recorded in real vehicles and under various driving conditions to experimentally study the relationship between the microphones' characteristics and the final audio quality of speech communication and performance of ASR engines. We focus on how variations in microphone bandwidth and amplitude frequency response shapes affect the perceptual speech quality. The speech quality results are compared by using ETSI TS 103 281 metrics (S-MOS, N-MOS, G-MOS) and ancillary metrics such as SNR. The ASR results are evaluated with standard metrics such as Word Error Rate (WER). Findings from this study provide knowledge in the understanding of what microphone frequency response characteristics are more relevant for audio quality and choice of proper microphone specifications, particularly for automotive applications.
>
---
#### [new 015] Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech
- **分类: cs.CL; cs.AI; cs.SD; eess.AS**

- **简介: 该论文属于语音抑郁检测任务，旨在解决传统方法依赖单一语音特征、难以捕捉抑郁时序特征的问题。论文提出HAREN-CTC模型，融合多层自监督语音特征，引入跨模态注意力与CTC损失，提升抑郁检测效果，取得SOTA性能。**

- **链接: [http://arxiv.org/pdf/2510.08593v1](http://arxiv.org/pdf/2510.08593v1)**

> **作者:** Yuxin Li; Eng Siong Chng; Cuntai Guan
>
> **摘要:** Speech-based depression detection (SDD) is a promising, non-invasive alternative to traditional clinical assessments. However, it remains limited by the difficulty of extracting meaningful features and capturing sparse, heterogeneous depressive cues over time. Pretrained self-supervised learning (SSL) models such as WavLM provide rich, multi-layer speech representations, yet most existing SDD methods rely only on the final layer or search for a single best-performing one. These approaches often overfit to specific datasets and fail to leverage the full hierarchical structure needed to detect subtle and persistent depression signals. To address this challenge, we propose HAREN-CTC, a novel architecture that integrates multi-layer SSL features using cross-attention within a multitask learning framework, combined with Connectionist Temporal Classification loss to handle sparse temporal supervision. HAREN-CTC comprises two key modules: a Hierarchical Adaptive Clustering module that reorganizes SSL features into complementary embeddings, and a Cross-Modal Fusion module that models inter-layer dependencies through cross-attention. The CTC objective enables alignment-aware training, allowing the model to track irregular temporal patterns of depressive speech cues. We evaluate HAREN-CTC under both an upper-bound setting with standard data splits and a generalization setting using five-fold cross-validation. The model achieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on MODMA, outperforming prior methods across both evaluation scenarios.
>
---
#### [new 016] BaldWhisper: Faster Whisper with Head Shearing and Layer Merging
- **分类: eess.AS; cs.AI; cs.CL; cs.SD**

- **简介: 该论文属于模型压缩任务，旨在解决低资源语言语音识别模型在数据稀缺情况下难以压缩的问题。通过提出一种新的压缩方法，包括嵌入压缩和层融合，实现在有限数据下减少模型大小和提升推理速度。**

- **链接: [http://arxiv.org/pdf/2510.08599v1](http://arxiv.org/pdf/2510.08599v1)**

> **作者:** Yaya Sy; Christophe Cerisara; Irina Illina
>
> **摘要:** Pruning large pre-trained transformers for low-resource languages is challenging, as it often requires massive retraining data to recover performance. For instance, Distill-Whisper prunes Whisper by 40% and retrains on 21,000 hours of speech, far beyond what is available for most languages. Can Whisper be made lighter and faster for edge devices in data-scarce settings? Focusing on Bambara with only 32h of speech-to-text data, we propose a new pruning recipe. Instead of vocabulary pruning, which is unsuitable due to frequent code-switching by Bambara speakers, we compress the embeddings with low-rank decomposition and feature distillation. Rather than removing layers, we merge them to limit performance loss. The final model preserves 90% of the original performance while being 48% smaller and 2.15x faster on a MacBook Air M1.
>
---
#### [new 017] FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms
- **分类: cs.LG; cs.SD; eess.AS**

- **简介: 论文提出FLToP CTC，属于语音识别任务中的解码优化工作。旨在解决CTC解码在资源受限环境下的计算与内存瓶颈。通过帧级令牌剪枝，动态剔除低概率令牌，显著提升解码效率并降低内存占用，适用于多种平台。**

- **链接: [http://arxiv.org/pdf/2510.09085v1](http://arxiv.org/pdf/2510.09085v1)**

> **作者:** Atul Shree; Harshith Jupuru
>
> **备注:** 5 pages, 5 figures
>
> **摘要:** CTC-based ASR systems face computational and memory bottlenecks in resource-limited environments. Traditional CTC decoders, requiring up to 90% of processing time in systems (e.g., wav2vec2-large on L4 GPUs), face inefficiencies due to exhaustive token-level operations. This paper introduces Frame Level Token Pruning for Connectionist Temporal Classification (FLToP CTC), a novel decoding algorithm that employs frame-level token pruning guided by a relative threshold probability. By dynamically eliminating low-probability tokens per frame, FLToP CTC reduces compute and memory demands while maintaining negligible WER degradation. On LibriSpeech, FLToP CTC achieves a 10.5x runtime speedup and 2.78x memory reduction versus standard CTC decoders. Its simplicity enables seamless integration into CTC decoders across platforms (CPUs, GPUs, etc.). FLToP CTC addresses CTC bottlenecks, offering scalability for resource-limited environments and realtime applications, enhancing speech recognition accessibility and efficiency.
>
---
#### [new 018] Accent-Invariant Automatic Speech Recognition via Saliency-Driven Spectrogram Masking
- **分类: cs.CL; cs.SD; eess.AS**

- **简介: 该论文属于语音识别任务，旨在解决口音和方言差异导致的识别准确率下降问题。作者提出了一种基于显著性驱动的频谱图掩码方法，通过训练口音分类器并掩码其关键区域，增强语音识别模型对口音变化的鲁棒性。实验表明该方法有效降低了英语和波斯语的词错误率。**

- **链接: [http://arxiv.org/pdf/2510.09528v1](http://arxiv.org/pdf/2510.09528v1)**

> **作者:** Mohammad Hossein Sameti; Sepehr Harfi Moridani; Ali Zarean; Hossein Sameti
>
> **备注:** Submitted to ICASSP 2026
>
> **摘要:** Pre-trained transformer-based models have significantly advanced automatic speech recognition (ASR), yet they remain sensitive to accent and dialectal variations, resulting in elevated word error rates (WER) in linguistically diverse languages such as English and Persian. To address this challenge, we propose an accent-invariant ASR framework that integrates accent and dialect classification into the recognition pipeline. Our approach involves training a spectrogram-based classifier to capture accent-specific cues, masking the regions most influential to its predictions, and using the masked spectrograms for data augmentation. This enhances the robustness of ASR models against accent variability. We evaluate the method using both English and Persian speech. For Persian, we introduce a newly collected dataset spanning multiple regional accents, establishing the first systematic benchmark for accent variation in Persian ASR that fills a critical gap in multilingual speech research and provides a foundation for future studies on low-resource, linguistically diverse languages. Experimental results with the Whisper model demonstrate that our masking and augmentation strategy yields substantial WER reductions in both English and Persian settings, confirming the effectiveness of the approach. This research advances the development of multilingual ASR systems that are resilient to accent and dialect diversity. Code and dataset are publicly available at: https://github.com/MH-Sameti/Accent_invariant_ASR
>
---
#### [new 019] Look before Transcription: End-to-End SlideASR with Visually-Anchored Policy Optimization
- **分类: eess.AS; cs.CV; cs.SD**

- **简介: 该论文属于语音识别任务（SlideASR），旨在通过结合演示文稿的视觉信息提升语音识别准确率，尤其针对专业术语识别难题。论文提出VAPO方法，利用视觉锚定策略优化模型推理过程，并构建了新数据集SlideASR-Bench。**

- **链接: [http://arxiv.org/pdf/2510.08618v1](http://arxiv.org/pdf/2510.08618v1)**

> **作者:** Rui Hu; Delai Qiu; Yining Wang; Shengping Liu; Jitao Sang
>
> **摘要:** Automatic speech recognition (ASR) systems often struggle with domain-specific terminology, especially in specialized settings such as academic lectures. To address this, we define the SlideASR task, which leverages the rich visual information from presentation slides to improve transcription accuracy. Existing pipeline methods for this task tend to be complex and underperform. Although omni-modal large language models (OLLMs) provide a promising end-to-end framework, they frequently fail in practice by degenerating into simple optical character recognition (OCR) systems. To overcome this, we propose Visually-Anchored Policy Optimization (VAPO), a novel post-training method designed to control the model's reasoning process. Drawing on the Chain-of-Thought reasoning paradigm, VAPO enforces a structured "Look before Transcription" procedure using a <think><answer> format. Specifically, the model first performs OCR on the slide content within the think step, then generates the transcription by referencing this recognized visual information in the answer step. This reasoning process is optimized via reinforcement learning with four distinct rewards targeting format compliance, OCR accuracy, ASR quality, and visual anchoring consistency. To support further research, we construct SlideASR-Bench, a new entity-rich benchmark consisting of a synthetic dataset for training and testing, and a challenging real-world set for evaluation. Extensive experiments demonstrate that VAPO significantly improves recognition of domain-specific terms, establishing an effective end-to-end paradigm for SlideASR.
>
---
#### [new 020] Dynamic Stress Detection: A Study of Temporal Progression Modelling of Stress in Speech
- **分类: eess.AS; cs.AI; cs.CL; cs.SD**

- **简介: 该论文属于语音中的压力检测任务，旨在解决传统方法将压力视为静态标签的问题。作者提出动态标注策略，利用情感标签生成细粒度压力标注，并采用基于交叉注意力的序列模型（如单向LSTM和Transformer编码器）捕捉压力随时间的演变。实验表明，该方法在MuSE和StressID数据集上分别提升了5%和18%的准确率，并能泛化到真实场景数据。**

- **链接: [http://arxiv.org/pdf/2510.08586v1](http://arxiv.org/pdf/2510.08586v1)**

> **作者:** Vishakha Lall; Yisi Liu
>
> **备注:** Accepted at IEEE CogMI 2025
>
> **摘要:** Detecting psychological stress from speech is critical in high-pressure settings. While prior work has leveraged acoustic features for stress detection, most treat stress as a static label. In this work, we model stress as a temporally evolving phenomenon influenced by historical emotional state. We propose a dynamic labelling strategy that derives fine-grained stress annotations from emotional labels and introduce cross-attention-based sequential models, a Unidirectional LSTM and a Transformer Encoder, to capture temporal stress progression. Our approach achieves notable accuracy gains on MuSE (+5%) and StressID (+18%) over existing baselines, and generalises well to a custom real-world dataset. These results highlight the value of modelling stress as a dynamic construct in speech.
>
---
#### [new 021] Unsupervised lexicon learning from speech is limited by representations rather than clustering
- **分类: eess.AS; cs.CL; cs.SD**

- **简介: 该论文属于零资源语音词分割任务，旨在无文本标注情况下从语音中提取词单元。论文探讨了系统性能瓶颈，分析了表征方法与聚类方法的影响，发现同一词类片段的表征差异是主要限制因素，而非聚类方法本身。**

- **链接: [http://arxiv.org/pdf/2510.09225v1](http://arxiv.org/pdf/2510.09225v1)**

> **作者:** Danel Adendorff; Simon Malan; Herman Kamper
>
> **备注:** Submitted to ICASSP 2026
>
> **摘要:** Zero-resource word segmentation and clustering systems aim to tokenise speech into word-like units without access to text labels. Despite progress, the induced lexicons are still far from perfect. In an idealised setting with gold word boundaries, we ask whether performance is limited by the representation of word segments, or by the clustering methods that group them into word-like types. We combine a range of self-supervised speech features (continuous/discrete, frame/word-level) with different clustering methods (K-means, hierarchical, graph-based) on English and Mandarin data. The best system uses graph clustering with dynamic time warping on continuous features. Faster alternatives use graph clustering with cosine distance on averaged continuous features or edit distance on discrete unit sequences. Through controlled experiments that isolate either the representations or the clustering method, we demonstrate that representation variability across segments of the same word type -- rather than clustering -- is the primary factor limiting performance.
>
---
#### [new 022] Articulation-Informed ASR: Integrating Articulatory Features into ASR via Auxiliary Speech Inversion and Cross-Attention Fusion
- **分类: eess.AS; cs.AI; cs.CL; cs.SD**

- **简介: 该论文属于语音识别任务，旨在提升自动语音识别（ASR）性能。它通过引入发音特征作为辅助任务和伪输入，利用语音反演和交叉注意力机制，将发音信息融合到深度学习模型中。实验表明，该方法在LibriSpeech数据集上优于强基准模型，尤其适用于资源有限的情况。**

- **链接: [http://arxiv.org/pdf/2510.08585v1](http://arxiv.org/pdf/2510.08585v1)**

> **作者:** Ahmed Adel Attia; Jing Liu; Carol Espy Wilson
>
> **摘要:** Prior works have investigated the use of articulatory features as complementary representations for automatic speech recognition (ASR), but their use was largely confined to shallow acoustic models. In this work, we revisit articulatory information in the era of deep learning and propose a framework that leverages articulatory representations both as an auxiliary task and as a pseudo-input to the recognition model. Specifically, we employ speech inversion as an auxiliary prediction task, and the predicted articulatory features are injected into the model as a query stream in a cross-attention module with acoustic embeddings as keys and values. Experiments on LibriSpeech demonstrate that our approach yields consistent improvements over strong transformer-based baselines, particularly under low-resource conditions. These findings suggest that articulatory features, once sidelined in ASR research, can provide meaningful benefits when reintroduced with modern architectures.
>
---
## 更新

#### [replaced 001] Direction Estimation of Sound Sources Using Microphone Arrays and Signal Strength
- **分类: cs.SD; cs.SY; eess.AS; eess.SY**

- **链接: [http://arxiv.org/pdf/2507.03466v3](http://arxiv.org/pdf/2507.03466v3)**

> **作者:** Mahdi Ali Pour; Zahra Habibzadeh
>
> **备注:** Accepted to the 32nd International Conference on Systems Engineering (ICSEng'2025)
>
> **摘要:** Sound-tracking refers to the process of determining the direction from which a sound originates, making it a fundamental component of sound source localization. This capability is essential in a variety of applications, including security systems, acoustic monitoring, and speaker tracking, where accurately identifying the direction of a sound source enables real-time responses, efficient resource allocation, and improved situational awareness. While sound-tracking is closely related to localization, it specifically focuses on identifying the direction of the sound source rather than estimating its exact position in space. Despite its utility, sound-tracking systems face several challenges, such as maintaining directional accuracy and precision, along with the need for sophisticated hardware configurations and complex signal processing algorithms. This paper presents a sound-tracking method using three electret microphones. We estimate the direction of a sound source using a lightweight method that analyzes signals from three strategically placed microphones. By comparing the average power of the received signals, the system infers the most probable direction of the sound. The results indicate that the power level from each microphone effectively determines the sound source direction. Our system employs a straightforward and cost-effective hardware design, ensuring simplicity and affordability in implementation. It achieves a localization error of less than 6 degrees and a precision of 98%. Additionally, its effortless integration with various systems makes it versatile and adaptable. Consequently, this technique presents a robust and reliable solution for sound-tracking and localization, with potential applications spanning diverse domains such as security systems, smart homes, and acoustic monitoring.
>
---
#### [replaced 002] From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for Speech Separation
- **分类: cs.SD**

- **链接: [http://arxiv.org/pdf/2509.22425v2](http://arxiv.org/pdf/2509.22425v2)**

> **作者:** Ke Xue; Rongfei Fan; Lixin; Dawei Zhao; Chao Zhu; Han Hu
>
> **摘要:** Audio-visual speech separation aims to isolate each speaker's clean voice from mixtures by leveraging visual cues such as lip movements and facial features. While visual information provides complementary semantic guidance, existing methods often underexploit its potential by relying on static visual representations. In this paper, we propose CSFNet, a Coarse-to-Separate-Fine Network that introduces a recursive semantic enhancement paradigm for more effective separation. CSFNet operates in two stages: (1) Coarse Separation, where a first-pass estimation reconstructs a coarse audio waveform from the mixture and visual input; and (2) Fine Separation, where the coarse audio is fed back into an audio-visual speech recognition (AVSR) model together with the visual stream. This recursive process produces more discriminative semantic representations, which are then used to extract refined audio. To further exploit these semantics, we design a speaker-aware perceptual fusion block to encode speaker identity across modalities, and a multi-range spectro-temporal separation network to capture both local and global time-frequency patterns. Extensive experiments on three benchmark datasets and two noisy datasets show that CSFNet achieves state-of-the-art (SOTA) performance, with substantial coarse-to-fine improvements, validating the necessity and effectiveness of our recursive semantic enhancement framework.
>
---
#### [replaced 003] Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time Text-to-Speech
- **分类: cs.CL; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2506.12311v2](http://arxiv.org/pdf/2506.12311v2)**

> **作者:** Yakov Kolani; Maxim Melichov; Cobi Calev; Morris Alper
>
> **备注:** Project page: https://phonikud.github.io
>
> **摘要:** Real-time text-to-speech (TTS) for Modern Hebrew is challenging due to the language's orthographic complexity. Existing solutions ignore crucial phonetic features such as stress that remain underspecified even when vowel marks are added. To address these limitations, we introduce Phonikud, a lightweight, open-source Hebrew grapheme-to-phoneme (G2P) system that outputs fully-specified IPA transcriptions. Our approach adapts an existing diacritization model with lightweight adaptors, incurring negligible additional latency. We also contribute the ILSpeech dataset of transcribed Hebrew speech with IPA annotations, serving as a benchmark for Hebrew G2P, as training data for TTS systems, and enabling audio-to-IPA for evaluating TTS performance while capturing important phonetic details. Our results demonstrate that Phonikud G2P conversion more accurately predicts phonemes from Hebrew text compared to prior methods, and that this enables training of effective real-time Hebrew TTS models with superior speed-accuracy trade-offs. We release our code, data, and models at https: //phonikud.github.io.
>
---
#### [replaced 004] TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis
- **分类: cs.SD; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2504.05684v3](http://arxiv.org/pdf/2504.05684v3)**

> **作者:** Tri Ton; Ji Woo Hong; Chang D. Yoo
>
> **备注:** Accepted to ICCV 2025. Please visit our project page at https://triton99.github.io/taro-site/
>
> **摘要:** This paper introduces Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning (TARO), a novel framework for high-fidelity and temporally coherent video-to-audio synthesis. Built upon flow-based transformers, which offer stable training and continuous transformations for enhanced synchronization and audio quality, TARO introduces two key innovations: (1) Timestep-Adaptive Representation Alignment (TRA), which dynamically aligns latent representations by adjusting alignment strength based on the noise schedule, ensuring smooth evolution and improved fidelity, and (2) Onset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp event-driven markers of audio-relevant visual moments to enhance synchronization with dynamic visual events. Extensive experiments on the VGGSound and Landscape datasets demonstrate that TARO outperforms prior methods, achieving relatively 53% lower Frechet Distance (FD), 29% lower Frechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its superior audio quality and synchronization precision.
>
---
