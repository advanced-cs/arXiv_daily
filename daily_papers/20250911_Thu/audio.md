# 音频 cs.SD;  eess.SP

- **最新发布 10 篇**

- **更新 3 篇**

## 最新发布

#### [new 001] LatentVoiceGrad: Nonparallel Voice Conversion with Latent Diffusion/Flow-Matching Models
- **分类: cs.SD**

- **简介: 该论文提出LatentVoiceGrad，改进非平行语音转换任务。针对原VoiceGrad存在的音频质量与速度问题，引入潜空间扩散模型和流匹配模型，提升语音质量并加快转换速度。**

- **链接: [http://arxiv.org/pdf/2509.08379v1](http://arxiv.org/pdf/2509.08379v1)**

> **作者:** Hirokazu Kameoka; Takuhiro Kaneko; Kou Tanaka; Yuto Kondo
>
> **备注:** Submitted to IEEE-TASLP
>
> **摘要:** Previously, we introduced VoiceGrad, a nonparallel voice conversion (VC) technique enabling mel-spectrogram conversion from source to target speakers using a score-based diffusion model. The concept involves training a score network to predict the gradient of the log density of mel-spectrograms from various speakers. VC is executed by iteratively adjusting an input mel-spectrogram until resembling the target speaker's. However, challenges persist: audio quality needs improvement, and conversion is slower compared to modern VC methods designed to operate at very high speeds. To address these, we introduce latent diffusion models into VoiceGrad, proposing an improved version with reverse diffusion in the autoencoder bottleneck. Additionally, we propose using a flow matching model as an alternative to the diffusion model to further speed up the conversion process without compromising the conversion quality. Experimental results show enhanced speech quality and accelerated conversion compared to the original.
>
---
#### [new 002] Explainability of CNN Based Classification Models for Acoustic Signal
- **分类: cs.SD; cs.AI; cs.LG**

- **简介: 该论文研究基于CNN的声学信号分类模型的可解释性，针对北美某鸟类叫声进行分类，使用XAI方法（如LIME、SHAP、DeepLIFT、Grad-CAM）分析模型决策过程，提升模型可信度与跨领域适用性。**

- **链接: [http://arxiv.org/pdf/2509.08717v1](http://arxiv.org/pdf/2509.08717v1)**

> **作者:** Zubair Faruqui; Mackenzie S. McIntire; Rahul Dubey; Jay McEntee
>
> **备注:** Accepted in IEEE ICTAI 2025
>
> **摘要:** Explainable Artificial Intelligence (XAI) has emerged as a critical tool for interpreting the predictions of complex deep learning models. While XAI has been increasingly applied in various domains within acoustics, its use in bioacoustics, which involves analyzing audio signals from living organisms, remains relatively underexplored. In this paper, we investigate the vocalizations of a bird species with strong geographic variation throughout its range in North America. Audio recordings were converted into spectrogram images and used to train a deep Convolutional Neural Network (CNN) for classification, achieving an accuracy of 94.8\%. To interpret the model's predictions, we applied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT, Grad-CAM) XAI techniques. These techniques produced different but complementary explanations, and when their explanations were considered together, they provided more complete and interpretable insights into the model's decision-making. This work highlights the importance of using a combination of XAI techniques to improve trust and interoperability, not only in broader acoustics signal analysis but also argues for broader applicability in different domain specific tasks.
>
---
#### [new 003] Segment Transformer: AI-Generated Music Detection via Music Structural Analysis
- **分类: cs.SD; cs.AI**

- **简介: 该论文属于AI生成音乐检测任务，旨在解决AI与人类创作音乐的区分问题。研究提出Segment Transformer框架，结合预训练模型提取音乐片段特征，并分析段间关系，提升检测准确率与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2509.08283v1](http://arxiv.org/pdf/2509.08283v1)**

> **作者:** Yumin Kim; Seonghyeon Go
>
> **摘要:** Audio and music generation systems have been remarkably developed in the music information retrieval (MIR) research field. The advancement of these technologies raises copyright concerns, as ownership and authorship of AI-generated music (AIGM) remain unclear. Also, it can be difficult to determine whether a piece was generated by AI or composed by humans clearly. To address these challenges, we aim to improve the accuracy of AIGM detection by analyzing the structural patterns of music segments. Specifically, to extract musical features from short audio clips, we integrated various pre-trained models, including self-supervised learning (SSL) models or an audio effect encoder, each within our suggested transformer-based framework. Furthermore, for long audio, we developed a segment transformer that divides music into segments and learns inter-segment relationships. We used the FakeMusicCaps and SONICS datasets, achieving high accuracy in both the short-audio and full-audio detection experiments. These findings suggest that integrating segment-level musical features into long-range temporal analysis can effectively enhance both the performance and robustness of AIGM detection systems.
>
---
#### [new 004] Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper for Speech Emotion Recognition
- **分类: cs.SD; cs.LG; eess.AS**

- **简介: 论文研究LoRA在Whisper语音情绪识别中的机制可解释性，揭示其延迟专业化和矩阵动态机制，为高效适配大模型提供理论支持。属于语音情绪识别任务，解决LoRA适应机制不明确的问题。**

- **链接: [http://arxiv.org/pdf/2509.08454v1](http://arxiv.org/pdf/2509.08454v1)**

> **作者:** Yujian Ma; Jinqiu Sang; Ruizhe Li
>
> **备注:** Work in process
>
> **摘要:** Large pre-trained speech models such as Whisper offer strong generalization but pose significant challenges for resource-efficient adaptation. Low-Rank Adaptation (LoRA) has become a popular parameter-efficient fine-tuning method, yet its underlying mechanisms in speech tasks remain poorly understood. In this work, we conduct the first systematic mechanistic interpretability study of LoRA within the Whisper encoder for speech emotion recognition (SER). Using a suite of analytical tools, including layer contribution probing, logit-lens inspection, and representational similarity via singular value decomposition (SVD) and centered kernel alignment (CKA), we reveal two key mechanisms: a delayed specialization process that preserves general features in early layers before consolidating task-specific information, and a forward alignment, backward differentiation dynamic between LoRA's matrices. Our findings clarify how LoRA reshapes encoder hierarchies, providing both empirical insights and a deeper mechanistic understanding for designing efficient and interpretable adaptation strategies in large speech models.
>
---
#### [new 005] PianoVAM: A Multimodal Piano Performance Dataset
- **分类: cs.SD; cs.AI; cs.CV; cs.MM; eess.AS**

- **简介: 该论文提出PianoVAM数据集，用于多模态钢琴演奏分析。任务是解决音乐信息检索中跨模态对齐与标注问题。工作包括采集视频、音频、MIDI及手部关键点数据，并建立标注方法与基准测试。**

- **链接: [http://arxiv.org/pdf/2509.08800v1](http://arxiv.org/pdf/2509.08800v1)**

> **作者:** Yonghyun Kim; Junhyung Park; Joonhyung Bae; Kirak Kim; Taegyun Kwon; Alexander Lerch; Juhan Nam
>
> **备注:** Accepted to the 26th International Society for Music Information Retrieval (ISMIR) Conference, 2025
>
> **摘要:** The multimodal nature of music performance has driven increasing interest in data beyond the audio domain within the music information retrieval (MIR) community. This paper introduces PianoVAM, a comprehensive piano performance dataset that includes videos, audio, MIDI, hand landmarks, fingering labels, and rich metadata. The dataset was recorded using a Disklavier piano, capturing audio and MIDI from amateur pianists during their daily practice sessions, alongside synchronized top-view videos in realistic and varied performance conditions. Hand landmarks and fingering labels were extracted using a pretrained hand pose estimation model and a semi-automated fingering annotation algorithm. We discuss the challenges encountered during data collection and the alignment process across different modalities. Additionally, we describe our fingering annotation method based on hand landmarks extracted from videos. Finally, we present benchmarking results for both audio-only and audio-visual piano transcription using the PianoVAM dataset and discuss additional potential applications.
>
---
#### [new 006] LALM-Eval: An Open-Source Toolkit for Holistic Evaluation of Large Audio Language Models
- **分类: cs.SD; cs.AI; cs.LG; eess.AS**

- **简介: 该论文提出LALM-Eval，用于全面评估大音频语言模型。解决现有工具效率低、提示不一致和任务覆盖窄的问题，通过优化处理提升速度，引入新评估类别，揭示模型在时序理解和复杂推理上的不足。**

- **链接: [http://arxiv.org/pdf/2509.08031v1](http://arxiv.org/pdf/2509.08031v1)**

> **作者:** Sidharth Surapaneni; Hoang Nguyen; Jash Mehta; Aman Tiwari; Oluwanifemi Bamgbose; Akshay Kalkunte; Sai Rajeswar; Sathwik Tejaswi Madhusudhan
>
> **摘要:** Large Audio Language Models (LALMs) are rapidly advancing, but evaluating them remains challenging due to inefficient toolkits that limit fair comparison and systematic assessment. Current frameworks suffer from three critical issues: slow processing that bottlenecks large-scale studies, inconsistent prompting that hurts reproducibility, and narrow task coverage that misses important audio reasoning capabilities. We introduce LALM-Eval, an efficient and comprehensive evaluation framework for LALMs. Our system achieves a speedup of up to 127% over existing toolkits through optimized batch processing and parallel execution, enabling large-scale evaluations previously impractical. We provide standardized prompting protocols and flexible configurations for fair model comparison across diverse scenarios. Additionally, we introduce two new evaluation categories: LLM-Adaptive Diarization for temporal audio understanding and Spoken Language Reasoning for complex audio-based cognitive tasks. Through evaluation across 380+ tasks, we reveal significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning tasks. Our findings also highlight a lack of standardization in instruction modality existent across audio benchmarks, which can lead up performance differences up to 9.5 absolute points on the challenging complex instruction following downstream tasks. LALM-Eval provides both practical evaluation tools and insights into model limitations, advancing systematic LALM development.
>
---
#### [new 007] Context-Aware Query Refinement for Target Sound Extraction: Handling Partially Matched Queries
- **分类: eess.AS; cs.SD**

- **简介: 论文研究目标声提取（TSE）任务，解决部分匹配查询（PMQ）条件下性能下降问题。提出基于上下文的查询优化方法，在推理时剔除不活跃声类，提升模型鲁棒性。**

- **链接: [http://arxiv.org/pdf/2509.08292v1](http://arxiv.org/pdf/2509.08292v1)**

> **作者:** Ryo Sato; Chiho Haruta; Nobuhiko Hiruma; Keisuke Imoto
>
> **备注:** Accepted to IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) 2025
>
> **摘要:** Target sound extraction (TSE) is the task of extracting a target sound specified by a query from an audio mixture. Much prior research has focused on the problem setting under the Fully Matched Query (FMQ) condition, where the query specifies only active sounds present in the mixture. However, in real-world scenarios, queries may include inactive sounds that are not present in the mixture. This leads to scenarios such as the Fully Unmatched Query (FUQ) condition, where only inactive sounds are specified in the query, and the Partially Matched Query (PMQ) condition, where both active and inactive sounds are specified. Among these conditions, the performance degradation under the PMQ condition has been largely overlooked. To achieve robust TSE under the PMQ condition, we propose context-aware query refinement. This method eliminates inactive classes from the query during inference based on the estimated sound class activity. Experimental results demonstrate that while conventional methods suffer from performance degradation under the PMQ condition, the proposed method effectively mitigates this degradation and achieves high robustness under diverse query conditions.
>
---
#### [new 008] Real-world Music Plagiarism Detection With Music Segment Transcription System
- **分类: cs.AI; cs.SD; eess.AS**

- **简介: 该论文提出一种结合MIR技术的音乐抄袭检测系统，通过音乐片段转录和多特征相似度计算实现跨格式抄袭检测，并构建了真实案例的SMP数据集。属于音乐版权保护任务，解决音乐抄袭识别问题。**

- **链接: [http://arxiv.org/pdf/2509.08282v1](http://arxiv.org/pdf/2509.08282v1)**

> **作者:** Seonghyeon Go
>
> **备注:** Accepted in APSIPA 2025 but not published yet(will be published in 2 month..), Arxiv preprint ready for references in future-works
>
> **摘要:** As a result of continuous advances in Music Information Retrieval (MIR) technology, generating and distributing music has become more diverse and accessible. In this context, interest in music intellectual property protection is increasing to safeguard individual music copyrights. In this work, we propose a system for detecting music plagiarism by combining various MIR technologies. We developed a music segment transcription system that extracts musically meaningful segments from audio recordings to detect plagiarism across different musical formats. With this system, we compute similarity scores based on multiple musical features that can be evaluated through comprehensive musical analysis. Our approach demonstrated promising results in music plagiarism detection experiments, and the proposed method can be applied to real-world music scenarios. We also collected a Similar Music Pair (SMP) dataset for musical similarity research using real-world cases. The dataset are publicly available.
>
---
#### [new 009] CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework
- **分类: cs.CL; cs.MM; cs.SD**

- **简介: 论文提出CommonVoice-SpeechRE数据集和RPG-MoGe框架，解决语音关系抽取任务中数据不足与模型性能受限问题，通过多阶生成策略和跨模态对齐提升效果。**

- **链接: [http://arxiv.org/pdf/2509.08438v1](http://arxiv.org/pdf/2509.08438v1)**

> **作者:** Jinzhong Ning; Paerhati Tulajiang; Yingying Le; Yijia Zhang; Yuanyuan Sun; Hongfei Lin; Haifeng Liu
>
> **摘要:** Speech Relation Extraction (SpeechRE) aims to extract relation triplets directly from speech. However, existing benchmark datasets rely heavily on synthetic data, lacking sufficient quantity and diversity of real human speech. Moreover, existing models also suffer from rigid single-order generation templates and weak semantic alignment, substantially limiting their performance. To address these challenges, we introduce CommonVoice-SpeechRE, a large-scale dataset comprising nearly 20,000 real-human speech samples from diverse speakers, establishing a new benchmark for SpeechRE research. Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet generation ensemble strategy, leveraging data diversity through diverse element orders during both training and inference, and (2) CNN-based latent relation prediction heads that generate explicit relation prompts to guide cross-modal alignment and accurate triplet generation. Experiments show our approach outperforms state-of-the-art methods, providing both a benchmark dataset and an effective solution for real-world SpeechRE. The source code and dataset are publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.
>
---
#### [new 010] Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer Layer Caching
- **分类: eess.AS; cs.SD**

- **简介: 该论文属于文本到语音合成任务，旨在加速扩散Transformer（DiT）模型的推理过程。通过引入SmoothCache机制缓存Transformer层输出，减少冗余计算，提升效率，同时保持合成质量。**

- **链接: [http://arxiv.org/pdf/2509.08696v1](http://arxiv.org/pdf/2509.08696v1)**

> **作者:** Siratish Sakpiboonchit
>
> **备注:** 9 pages, 2 tables, 5 figures
>
> **摘要:** This paper presents a method to accelerate the inference process of diffusion transformer (DiT)-based text-to-speech (TTS) models by applying a selective caching mechanism to transformer layers. Specifically, I integrate SmoothCache into the F5-TTS architecture, focusing on caching outputs of self-attention and feed-forward network layers to reduce redundant computations during the denoising process. A calibration phase is introduced to analyze L1 relative errors between timesteps, guiding the selection of cache schedules that minimize quality degradation. To address the problem of inter-layer dependency, a unified caching schedule is adopted, applying the cache pattern derived from self-attention layers to both layer types. Experiments on LibriSpeech-PC and Seed-TTS datasets evaluate various cache thresholds and denoising step configurations. Results show that caching at higher denoising steps reduces inference time without compromising output quality, whereas caching at lower steps can negatively impact synthesis quality similarly to reducing the total number of denoising steps. Objective and subjective metrics confirm the effectiveness of SmoothCache in maintaining performance while improving computational efficiency. Comparisons between cached inference and reduced-step inference further highlight the benefits of selective caching, especially under high-step configurations. This work demonstrates that transformer layer caching is a practical solution for optimizing diffusion transformer-based TTS models without requiring architectural changes or retraining. Example inference results can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .
>
---
## 更新

#### [replaced 001] Alternating Minimization Schemes for Computing Rate-Distortion-Perception Functions with $f$-Divergence Perception Constraints
- **分类: cs.IT; cs.CV; eess.SP; math.IT**

- **链接: [http://arxiv.org/pdf/2408.15015v2](http://arxiv.org/pdf/2408.15015v2)**

> **作者:** Giuseppe Serra; Photios A. Stavrou; Marios Kountouris
>
> **备注:** This work has been submitted for possible publication
>
> **摘要:** We study the computation of the rate-distortion-perception function (RDPF) for discrete memoryless sources subject to a single-letter average distortion constraint and a perception constraint belonging to the family of $f$-divergences. In this setting, the RDPF forms a convex programming problem for which we characterize optimal parametric solutions. We employ the developed solutions in an alternating minimization scheme, namely Optimal Alternating Minimization (OAM), for which we provide convergence guarantees. Nevertheless, the OAM scheme does not lead to a direct implementation of a generalized Blahut-Arimoto (BA) type of algorithm due to implicit equations in the iteration's structure. To overcome this difficulty, we propose two alternative minimization approaches whose applicability depends on the smoothness of the used perception metric: a Newton-based Alternating Minimization (NAM) scheme, relying on Newton's root-finding method for the approximation of the optimal solution of the iteration, and a Relaxed Alternating Minimization (RAM) scheme, based on relaxing the OAM iterates. We show, by deriving necessary and sufficient conditions, that both schemes guarantee convergence to a globally optimal solution. We also provide sufficient conditions on the distortion and perception constraints, which guarantee that the proposed algorithms converge exponentially fast in the number of iteration steps. We corroborate our theoretical results with numerical simulations and establish connections with existing results.
>
---
#### [replaced 002] QR-VC: Leveraging Quantization Residuals for Linear Disentanglement in Zero-Shot Voice Conversion
- **分类: cs.SD; cs.AI; eess.AS**

- **链接: [http://arxiv.org/pdf/2411.16147v2](http://arxiv.org/pdf/2411.16147v2)**

> **作者:** Youngjun Sim; Jinsung Yoon; Wooyeol Jeong; Young-Joo Suh
>
> **备注:** 5 pages. Accepted to EUSIPCO 2025 (Paper #1938)
>
> **摘要:** Zero-shot voice conversion is a technique that alters the speaker identity of an input speech to match a target speaker using only a single reference utterance, without requiring additional training. Recent approaches extensively utilize self-supervised learning features with K-means quantization to extract high-quality content representations while removing speaker identity. However, this quantization process also eliminates fine-grained phonetic and prosodic variations, degrading intelligibility and prosody preservation. While prior works have primarily focused on quantized representations, quantization residuals remain underutilized and deserve further exploration. In this paper, we introduce a novel approach that fully utilizes quantization residuals by leveraging temporal properties of speech components. This facilitates the disentanglement of speaker identity and the recovery of phonetic and prosodic details lost during quantization. By applying only K-means quantization and linear projections, our method achieves simple yet effective disentanglement, without requiring complex architectures or explicit supervision. This allows for high-fidelity voice conversion trained solely with reconstruction losses. Experiments show that the proposed model outperforms existing methods across both subjective and objective metrics. It achieves superior intelligibility and speaker similarity, along with improved prosody preservation, highlighting the impact of our Linear Disentangler module.
>
---
#### [replaced 003] Neural-Enhanced Dynamic Range Compression Inversion: A Hybrid Approach for Restoring Audio Dynamics
- **分类: cs.SD; cs.AI; eess.AS**

- **链接: [http://arxiv.org/pdf/2411.04337v2](http://arxiv.org/pdf/2411.04337v2)**

> **作者:** Haoran Sun; Dominique Fourer; Hichem Maaref
>
> **备注:** This work has been submitted to the IEEE for possible publication
>
> **摘要:** Dynamic Range Compression (DRC) is a widely used audio effect that adjusts signal dynamics for applications in music production, broadcasting, and speech processing. Inverting DRC is of broad importance for restoring the original dynamics, enabling remixing, and enhancing the overall audio quality. Existing DRC inversion methods either overlook key parameters or rely on precise parameter values, which can be challenging to estimate accurately. To address this limitation, we introduce a hybrid approach that combines model-based DRC inversion with neural networks to achieve robust DRC parameter estimation and audio restoration simultaneously. Our method uses tailored neural network architectures (classification and regression), which are then integrated into a model-based inversion framework to reconstruct the original signal. Experimental evaluations on various music and speech datasets confirm the effectiveness and robustness of our approach, outperforming several state-of-the-art techniques.
>
---
