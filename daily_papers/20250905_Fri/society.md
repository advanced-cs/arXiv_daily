# 计算机与社会 cs.CY

- **最新发布 13 篇**

- **更新 13 篇**

## 最新发布

#### [new 001] A software security review on Uganda's Mobile Money Services: Dr. Jim Spire's tweets sentiment analysis
- **分类: cs.CY; cs.AI; cs.CR**

- **简介: 该论文通过分析Dr. Jim Spire推文引发的公众舆论，研究乌干达移动支付的安全漏洞与用户不满，旨在揭示安全机制缺陷并提出改进建议。属情感分析与安全审查任务，解决移动金融安全问题。**

- **链接: [http://arxiv.org/pdf/2509.03545v1](http://arxiv.org/pdf/2509.03545v1)**

> **作者:** Nsengiyumva Wilberforce
>
> **备注:** 16 pages, 3 figures
>
> **摘要:** The proliferation of mobile money in Uganda has been a cornerstone of financial inclusion, yet its security mechanisms remain a critical concern. This study investigates a significant public response to perceived security failures: the #StopAirtelThefty Twitter campaign of August 2025 Sparked by an incident publicized by Dr. Jim Spire Ssentongo where a phone thief accessed a victim's account, withdrew funds, and procured a loan, the campaign revealed deep seated public anxiety over the safety of mobile money. This research employs qualitative analysis to systematically examine the complaints raised during this campaign, extracting key themes related to security vulnerabilities and user dissatisfaction. By synthesizing these public sentiments, the paper provides crucial insights into the specific security gaps experienced by users and situates these findings within the larger framework of Uganda's mobile money regulatory and operational environment. The study concludes with implications for providers, policymakers, and the future of secure digital finance in Uganda.
>
---
#### [new 002] No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in Resume Screening
- **分类: cs.CY; cs.AI; cs.CL; cs.HC; K.4.2**

- **简介: 该研究通过简历筛选实验，探讨AI种族偏见对人类决策的影响。发现人类在AI偏见下倾向跟随其推荐，隐性偏见测试显示决策受种族刻板印象影响。结果强调AI-HITL场景中人类自主性的局限，呼吁政策制定者关注系统设计与监管。**

- **链接: [http://arxiv.org/pdf/2509.04404v1](http://arxiv.org/pdf/2509.04404v1)**

> **作者:** Kyra Wilson; Mattea Sim; Anna-Maria Gueorguieva; Aylin Caliskan
>
> **备注:** Published in Proceedings of the 2025 AAAI/ACM Conference on AI, Ethics, and Society; code available at https://github.com/kyrawilson/No-Thoughts-Just-AI
>
> **摘要:** In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate people's preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for people's autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight.
>
---
#### [new 003] Are LLM Agents the New RPA? A Comparative Study with RPA Across Enterprise Workflows
- **分类: cs.CY; cs.MA**

- **简介: 该论文比较LLM代理（AACU）与RPA在企业工作流中的性能，通过实验评估其在数据录入、监控和文档提取任务中的速度、可靠性和开发效率，探讨AACU是否可替代RPA。**

- **链接: [http://arxiv.org/pdf/2509.04198v1](http://arxiv.org/pdf/2509.04198v1)**

> **作者:** Petr Průcha; Michaela Matoušková; Jan Strnad
>
> **摘要:** The emergence of large language models (LLMs) has introduced a new paradigm in automation: LLM agents or Agentic Automation with Computer Use (AACU). Unlike traditional Robotic Process Automation (RPA), which relies on rule-based workflows and scripting, AACU enables intelligent agents to perform tasks through natural language instructions and autonomous interaction with user interfaces. This study investigates whether AACU can serve as a viable alternative to RPA in enterprise workflow automation. We conducted controlled experiments across three standard RPA challenges data entry, monitoring, and document extraction comparing RPA (via UiPath) and AACU (via Anthropic's Computer Use Agent) in terms of speed, reliability, and development effort. Results indicate that RPA outperforms AACU in execution speed and reliability, particularly in repetitive, stable environments. However, AACU significantly reduces development time and adapts more flexibly to dynamic interfaces. While current AACU implementations are not yet production-ready, their promise in rapid prototyping and lightweight automation is evident. Future research should explore multi-agent orchestration, hybrid RPA-AACU architectures, and more robust evaluation across industries and platforms.
>
---
#### [new 004] The human biological advantage over AI
- **分类: cs.AI; cs.CY; I.2.0**

- **简介: 该论文探讨人类与AI的差异，解决AI是否将超越人类的问题。通过分析中枢神经系统的生物特性，论证其不可复制性，强调人类在伦理领导力上的优势。**

- **链接: [http://arxiv.org/pdf/2509.04130v1](http://arxiv.org/pdf/2509.04130v1)**

> **作者:** William Stewart
>
> **备注:** 12 pages
>
> **摘要:** Recent advances in AI raise the possibility that AI systems will one day be able to do anything humans can do, only better. If artificial general intelligence (AGI) is achieved, AI systems may be able to understand, reason, problem solve, create, and evolve at a level and speed that humans will increasingly be unable to match, or even understand. These possibilities raise a natural question as to whether AI will eventually become superior to humans, a successor "digital species", with a rightful claim to assume leadership of the universe. However, a deeper consideration suggests the overlooked differentiator between human beings and AI is not the brain, but the central nervous system (CNS), providing us with an immersive integration with physical reality. It is our CNS that enables us to experience emotion including pain, joy, suffering, and love, and therefore to fully appreciate the consequences of our actions on the world around us. And that emotional understanding of the consequences of our actions is what is required to be able to develop sustainable ethical systems, and so be fully qualified to be the leaders of the universe. A CNS cannot be manufactured or simulated; it must be grown as a biological construct. And so, even the development of consciousness will not be sufficient to make AI systems superior to humans. AI systems may become more capable than humans on almost every measure and transform our society. However, the best foundation for leadership of our universe will always be DNA, not silicon.
>
---
#### [new 005] The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process
- **分类: cs.CL; cs.CY; cs.LG**

- **简介: 该论文构建ProLiFIC数据集，解决法律过程挖掘数据不足问题，通过LLMs结构化非结构化数据，为法律PM提供基准。**

- **链接: [http://arxiv.org/pdf/2509.03528v1](http://arxiv.org/pdf/2509.03528v1)**

> **作者:** Matilde Contestabile; Chiara Ferrara; Alberto Giovannetti; Giovanni Parrillo; Andrea Vandin
>
> **摘要:** Process Mining (PM), initially developed for industrial and business contexts, has recently been applied to social systems, including legal ones. However, PM's efficacy in the legal domain is limited by the accessibility and quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in Italian Chambers), a comprehensive event log of the Italian lawmaking process from 1987 to 2022. Created from unstructured data from the Normattiva portal and structured using large language models (LLMs), ProLiFIC aligns with recent efforts in integrating PM with LLMs. We exemplify preliminary analyses and propose ProLiFIC as a benchmark for legal PM, fostering new developments.
>
---
#### [new 006] Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias Model
- **分类: cs.SI; cs.AI; cs.CY**

- **简介: 该论文提出基于LLM的确认偏误模型，扩展重力井模型以动态捕捉用户偏误，提升回音室检测准确性。通过分析用户行为验证，有效识别高风险信息环境。**

- **链接: [http://arxiv.org/pdf/2509.03832v1](http://arxiv.org/pdf/2509.03832v1)**

> **作者:** Joseph Jackson; Georgiy Lapin; Jeremy E. Thompson
>
> **摘要:** Social media echo chambers play a central role in the spread of misinformation, yet existing models often overlook the influence of individual confirmation bias. An existing model of echo chambers is the "gravity well" model, which creates an analog between echo chambers and spatial gravity wells. We extend this established model by introducing a dynamic confirmation bias variable that adjusts the strength of pull based on a user's susceptibility to belief-reinforcing content. This variable is calculated for each user through comparisons between their posting history and their responses to posts of a wide range of viewpoints. Incorporating this factor produces a confirmation-bias-integrated gravity well model that more accurately identifies echo chambers and reveals community-level markers of information health. We validated the approach on nineteen Reddit communities, demonstrating improved detection of echo chambers. Our contribution is a framework for systematically capturing the role of confirmation bias in online group dynamics, enabling more effective identification of echo chambers. By flagging these high-risk environments, the model supports efforts to curb the spread of misinformation at its most common points of amplification.
>
---
#### [new 007] A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai
- **分类: cs.AI; cs.CV; cs.CY**

- **简介: 该论文提出多维AI框架，通过社交媒体多模态数据分析游客对历史城区的感知，解决城市规划中游客体验评估问题。工作包括视觉焦点提取、色彩主题分析、情感挖掘，揭示空间审美差异与感知偏差，支持文旅与遗产保护决策。**

- **链接: [http://arxiv.org/pdf/2509.03830v1](http://arxiv.org/pdf/2509.03830v1)**

> **作者:** Kaizhen Tan; Yufan Wu; Yuxuan Liu; Haoran Zeng
>
> **摘要:** Historic urban quarters play a vital role in preserving cultural heritage while serving as vibrant spaces for tourism and everyday life. Understanding how tourists perceive these environments is essential for sustainable, human-centered urban planning. This study proposes a multidimensional AI-powered framework for analyzing tourist perception in historic urban quarters using multimodal data from social media. Applied to twelve historic quarters in central Shanghai, the framework integrates focal point extraction, color theme analysis, and sentiment mining. Visual focus areas are identified from tourist-shared photos using a fine-tuned semantic segmentation model. To assess aesthetic preferences, dominant colors are extracted using a clustering method, and their spatial distribution across quarters is analyzed. Color themes are further compared between social media photos and real-world street views, revealing notable shifts. This divergence highlights potential gaps between visual expectations and the built environment, reflecting both stylistic preferences and perceptual bias. Tourist reviews are evaluated through a hybrid sentiment analysis approach combining a rule-based method and a multi-task BERT model. Satisfaction is assessed across four dimensions: tourist activities, built environment, service facilities, and business formats. The results reveal spatial variations in aesthetic appeal and emotional response. Rather than focusing on a single technical innovation, this framework offers an integrated, data-driven approach to decoding tourist perception and contributes to informed decision-making in tourism, heritage conservation, and the design of aesthetically engaging public spaces.
>
---
#### [new 008] Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights from a Human-Labeled Dataset and AI Modeling
- **分类: cs.CL; cs.CY; cs.LG**

- **简介: 本研究构建KPoEM数据集，解决韩语现代诗歌隐喻与文化差异导致的情感分析难题。通过微调模型提升情感分类性能，并结合计算与文学分析，推动诗歌情感量化研究。**

- **链接: [http://arxiv.org/pdf/2509.03932v1](http://arxiv.org/pdf/2509.03932v1)**

> **作者:** Iro Lim; Haein Ji; Byungjun Kim
>
> **备注:** 30 pages, 13 tables, 2 figures, Digital Humanities and Social Sciences Korea Conference, James Joo-Jin Kim Center for Korean Studies, University of Pennsylvania, Philadelphia, USA
>
> **摘要:** This study introduces KPoEM (Korean Poetry Emotion Mapping) , a novel dataset for computational emotion analysis in modern Korean poetry. Despite remarkable progress in text-based emotion classification using large language models, poetry-particularly Korean poetry-remains underexplored due to its figurative language and cultural specificity. We built a multi-label emotion dataset of 7,662 entries, including 7,007 line-level entries from 483 poems and 615 work-level entries, annotated with 44 fine-grained emotion categories from five influential Korean poets. A state-of-the-art Korean language model fine-tuned on this dataset significantly outperformed previous models, achieving 0.60 F1-micro compared to 0.34 from models trained on general corpora. The KPoEM model, trained through sequential fine-tuning-first on general corpora and then on the KPoEM dataset-demonstrates not only an enhanced ability to identify temporally and culturally specific emotional expressions, but also a strong capacity to preserve the core sentiments of modern Korean poetry. This study bridges computational methods and literary analysis, presenting new possibilities for the quantitative exploration of poetic emotions through structured data that faithfully retains the emotional and cultural nuances of Korean literature.
>
---
#### [new 009] The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs
- **分类: cs.AI; cs.CL; cs.CY; cs.LG; stat.ML**

- **简介: 该论文研究LLM性格特征，揭示自我报告与行为的脱节。通过分析训练阶段特质演变、自我报告预测有效性及干预措施影响，发现指令对齐稳定特质表达，但自我报告无法可靠预测行为，挑战LLM人格假设。**

- **链接: [http://arxiv.org/pdf/2509.03730v1](http://arxiv.org/pdf/2509.03730v1)**

> **作者:** Pengrui Han; Rafal Kocielnik; Peiyang Song; Ramit Debnath; Dean Mobbs; Anima Anandkumar; R. Michael Alvarez
>
> **备注:** We make public all code and source data at https://github.com/psychology-of-AI/Personality-Illusion
>
> **摘要:** Personality traits have long been studied as predictors of human behavior.Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.
>
---
#### [new 010] Psychologically Enhanced AI Agents
- **分类: cs.AI; cs.CL; cs.CY; cs.HC; cs.MA**

- **简介: 论文提出MBTI-in-Thoughts框架，通过心理人格调用增强AI代理行为，提升任务表现。解决AI代理心理适配性问题，支持多智能体通信，验证方法可扩展至其他心理模型。**

- **链接: [http://arxiv.org/pdf/2509.04343v1](http://arxiv.org/pdf/2509.04343v1)**

> **作者:** Maciej Besta; Shriram Chandran; Robert Gerstenberger; Mathis Lindner; Marcin Chrapek; Sebastian Hermann Martschat; Taraneh Ghandi; Patrick Iff; Hubert Niewiadomski; Piotr Nyczyk; Jürgen Müller; Torsten Hoefler
>
> **摘要:** We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.
>
---
#### [new 011] Who Pays for Fairness? Rethinking Recourse under Social Burden
- **分类: cs.LG; cs.CY**

- **简介: 该论文针对算法救济中的公平性问题，提出基于社会负担的新框架及算法MISOB，减少各群体社会负担的同时保持分类准确。**

- **链接: [http://arxiv.org/pdf/2509.04128v1](http://arxiv.org/pdf/2509.04128v1)**

> **作者:** Ainhize Barrainkua; Giovanni De Toni; Jose Antonio Lozano; Novi Quadrianto
>
> **摘要:** Machine learning based predictions are increasingly used in sensitive decision-making applications that directly affect our lives. This has led to extensive research into ensuring the fairness of classifiers. Beyond just fair classification, emerging legislation now mandates that when a classifier delivers a negative decision, it must also offer actionable steps an individual can take to reverse that outcome. This concept is known as algorithmic recourse. Nevertheless, many researchers have expressed concerns about the fairness guarantees within the recourse process itself. In this work, we provide a holistic theoretical characterization of unfairness in algorithmic recourse, formally linking fairness guarantees in recourse and classification, and highlighting limitations of the standard equal cost paradigm. We then introduce a novel fairness framework based on social burden, along with a practical algorithm (MISOB), broadly applicable under real-world conditions. Empirical results on real-world datasets show that MISOB reduces the social burden across all groups without compromising overall classifier accuracy.
>
---
#### [new 012] Strategic Analysis of Dissent and Self-Censorship
- **分类: physics.soc-ph; cs.CY; cs.SI**

- **简介: 该论文通过建立模型分析个体在数字时代如何策略性地在表达异议与自我审查间权衡，研究权威政策对压制异议的影响，揭示抗惩罚意愿对压制效果的关键作用。**

- **链接: [http://arxiv.org/pdf/2509.03731v1](http://arxiv.org/pdf/2509.03731v1)**

> **作者:** Joshua J. Daymude; Robert Axelrod; Stephanie Forrest
>
> **备注:** 20 pages, 8 figures
>
> **摘要:** Expressions of dissent against authority are an important feature of most societies, and efforts to suppress such expressions are common. Modern digital communications, social media, and Internet surveillance and censorship technologies are changing the landscape of public speech and dissent. Especially in authoritarian settings, individuals must assess the risk of voicing their true opinions or choose self-censorship, voluntarily moderating their behavior to comply with authority. We present a model in which individuals strategically manage the tradeoff between expressing dissent and avoiding punishment through self-censorship while an authority adapts its policies to minimize both total expressed dissent and punishment costs. We study the model analytically and in simulation to derive conditions separating defiant individuals who express their desired dissent in spite of punishment from self-censoring individuals who fully or partially limit their expression. We find that for any population, there exists an authority policy that leads to total self-censorship. However, the probability and time for an initially moderate, locally-adaptive authority to suppress dissent depend critically on the population's willingness to withstand punishment early on, which can deter the authority from adopting more extreme policies.
>
---
#### [new 013] A Primer on Causal and Statistical Dataset Biases for Fair and Robust Image Analysis
- **分类: cs.LG; cs.CY; stat.ML**

- **简介: 该论文综述了因果和统计数据偏差对图像分析公平性与鲁棒性的影响，提出“no fair lunch”和“subgroup separability”两个新问题，分析现有方法不足，并探索解决路径。**

- **链接: [http://arxiv.org/pdf/2509.04295v1](http://arxiv.org/pdf/2509.04295v1)**

> **作者:** Charles Jones; Ben Glocker
>
> **备注:** Excerpt from C. Jones' PhD thesis. Winner of the G-Research PhD prize 2025
>
> **摘要:** Machine learning methods often fail when deployed in the real world. Worse still, they fail in high-stakes situations and across socially sensitive lines. These issues have a chilling effect on the adoption of machine learning methods in settings such as medical diagnosis, where they are arguably best-placed to provide benefits if safely deployed. In this primer, we introduce the causal and statistical structures which induce failure in machine learning methods for image analysis. We highlight two previously overlooked problems, which we call the \textit{no fair lunch} problem and the \textit{subgroup separability} problem. We elucidate why today's fair representation learning methods fail to adequately solve them and propose potential paths forward for the field.
>
---
## 更新

#### [replaced 001] Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models
- **分类: cs.AI; cs.CL; cs.CY; cs.HC; cs.SC**

- **链接: [http://arxiv.org/pdf/2509.01909v2](http://arxiv.org/pdf/2509.01909v2)**

> **作者:** Ranjie Duan; Jiexi Liu; Xiaojun Jia; Shiji Zhao; Ruoxi Cheng; Fengxiang Wang; Cheng Wei; Yong Xie; Chang Liu; Defeng Li; Yinpeng Dong; Yichi Zhang; Yuefeng Chen; Chongwen Wang; Xingjun Ma; Xingxing Wei; Yang Liu; Hang Su; Jun Zhu; Xinfeng Li; Yitong Sun; Jie Zhang; Jinzhao Hu; Sha Xu; Yitong Yang; Jialing Tao; Hui Xue
>
> **备注:** Technical Report Code & Model weights available: https://github.com/Alibaba-AAIG/Oyster
>
> **摘要:** Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.
>
---
#### [replaced 002] (Ir)rationality in AI: State of the Art, Research Challenges and Open Questions
- **分类: cs.AI; cs.CY; cs.HC; cs.LG; cs.MA**

- **链接: [http://arxiv.org/pdf/2311.17165v4](http://arxiv.org/pdf/2311.17165v4)**

> **作者:** Olivia Macmillan-Scott; Mirco Musolesi
>
> **摘要:** The concept of rationality is central to the field of artificial intelligence (AI). Whether we are seeking to simulate human reasoning, or trying to achieve bounded optimality, our goal is generally to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in AI, and sets out the open questions in this area. We consider how the understanding of rationality in other fields has influenced its conception within AI, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we examine irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area remains limited. Methods that have up to now been developed for other purposes, namely adversarial scenarios, may be adapted to suit interactions with artificial agents. We further discuss the interplay between human and artificial agents, and the role that rationality plays within this interaction; many questions remain in this area, relating to potentially irrational behaviour of both humans and artificial agents.
>
---
#### [replaced 003] Revealing the empirical flexibility of gas units through deep clustering
- **分类: cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2504.16943v2](http://arxiv.org/pdf/2504.16943v2)**

> **作者:** Chiara Fusar Bassini; Alice Lixuan Xu; Jorge Sánchez Canales; Lion Hirth; Lynn H. Kaack
>
> **备注:** 19 pages, 4 figures, 3 tables
>
> **摘要:** The flexibility of a power generation unit determines how quickly and often it can ramp up or down. In energy models, it depends on assumptions on the technical characteristics of the unit, such as its installed capacity or turbine technology. In this paper, we learn the empirical flexibility of gas units from their electricity generation, revealing how real-world limitations can lead to substantial differences between units with similar technical characteristics. Using a novel deep clustering approach, we transform 5 years (2019-2023) of unit-level hourly generation data for 49 German units from 100 MWp of installed capacity into low-dimensional embeddings. Our unsupervised approach identifies two clusters of peaker units (high flexibility) and two clusters of non-peaker units (low flexibility). The estimated ramp rates of non-peakers, which constitute half of the sample, display a low empirical flexibility, comparable to coal units. Non-peakers, predominantly owned by industry and municipal utilities, show limited response to low residual load and negative prices, generating on average 1.3 GWh during those hours. As the transition to renewables increases market variability, regulatory changes will be needed to unlock this flexibility potential.
>
---
#### [replaced 004] Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms
- **分类: cs.CY; cs.AI; cs.HC; stat.AP**

- **链接: [http://arxiv.org/pdf/2509.00167v2](http://arxiv.org/pdf/2509.00167v2)**

> **作者:** W. F. Lamberti; S. R. Lawrence; D. White; S. Kim; S. Abdullah
>
> **摘要:** Generative AI (GAI) tools have seen rapid adoption in educational settings, yet their role in fostering critical thinking remains underexplored. While previous studies have examined GAI as a tutor for specific lessons or as a tool for completing assignments, few have addressed how students critically evaluate the accuracy and appropriateness of GAI-generated responses. This pilot study investigates students' ability to apply structured critical thinking when assessing Generative AI outputs in introductory Computational and Data Science courses. Given that GAI tools often produce contextually flawed or factually incorrect answers, we designed learning activities that require students to analyze, critique, and revise AI-generated solutions. Our findings offer initial insights into students' ability to engage critically with GAI content and lay the groundwork for more comprehensive studies in future semesters.
>
---
#### [replaced 005] SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts--Extended Version
- **分类: cs.CL; cs.CY; cs.PF**

- **链接: [http://arxiv.org/pdf/2508.15478v2](http://arxiv.org/pdf/2508.15478v2)**

> **作者:** Nghiem Thanh Pham; Tung Kieu; Duc-Manh Nguyen; Son Ha Xuan; Nghia Duong-Trung; Danh Le-Phuoc
>
> **备注:** 24 pages. An extended version of "SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts" accepted at EMNLP 2025
>
> **摘要:** Small Language Models (SLMs) offer computational efficiency and accessibility, yet a systematic evaluation of their performance and environmental impact remains lacking. We introduce SLM-Bench, the first benchmark specifically designed to assess SLMs across multiple dimensions, including accuracy, computational efficiency, and sustainability metrics. SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14 domains. The evaluation is conducted on 4 hardware configurations, providing a rigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench quantifies 11 metrics across correctness, computation, and consumption, enabling a holistic assessment of efficiency trade-offs. Our evaluation considers controlled hardware conditions, ensuring fair comparisons across models. We develop an open-source benchmarking pipeline with standardized evaluation protocols to facilitate reproducibility and further research. Our findings highlight the diverse trade-offs among SLMs, where some models excel in accuracy while others achieve superior energy efficiency. SLM-Bench sets a new standard for SLM evaluation, bridging the gap between resource efficiency and real-world applicability.
>
---
#### [replaced 006] That is Unacceptable: the Moral Foundations of Canceling
- **分类: cs.CY; cs.CL**

- **链接: [http://arxiv.org/pdf/2503.05720v3](http://arxiv.org/pdf/2503.05720v3)**

> **作者:** Soda Marem Lo; Oscar Araque; Rajesh Sharma; Marco Antonio Stranisci
>
> **摘要:** Canceling is a morally-driven phenomenon that hinders the development of safe social media platforms and contributes to ideological polarization. To address this issue we present the Canceling Attitudes Detection (CADE) dataset, an annotated corpus of canceling incidents aimed at exploring the factors of disagreements in evaluating people canceling attitudes on social media. Specifically, we study the impact of annotators' morality in their perception of canceling, showing that morality is an independent axis for the explanation of disagreement on this phenomenon. Annotator's judgments heavily depend on the type of controversial events and involved celebrities. This shows the need to develop more event-centric datasets to better understand how harms are perpetrated in social media and to develop more aware technologies for their detection.
>
---
#### [replaced 007] Autonomation, Not Automation: Activities and Needs of European Fact-checkers as a Basis for Designing Human-Centered AI Systems
- **分类: cs.CY; cs.AI; cs.HC**

- **链接: [http://arxiv.org/pdf/2211.12143v3](http://arxiv.org/pdf/2211.12143v3)**

> **作者:** Andrea Hrckova; Robert Moro; Ivan Srba; Jakub Simko; Maria Bielikova
>
> **备注:** 44 pages, 13 figures, 2 annexes. Accepted to ACM Journal on Responsible Computing
>
> **摘要:** To mitigate the negative effects of false information more effectively, the development of Artificial Intelligence (AI) systems to assist fact-checkers is needed. Nevertheless, the lack of focus on the needs of these stakeholders results in their limited acceptance and skepticism toward automating the whole fact-checking process. In this study, we conducted semi-structured in-depth interviews with Central European fact-checkers. Their activities and problems were analyzed using iterative content analysis. The most significant problems were validated with a survey of European fact-checkers, in which we collected 24 responses from 20 countries, i.e., 62% of active European signatories of the International Fact-Checking Network (IFCN). Our contributions include an in-depth examination of the variability of fact-checking work in non-English-speaking regions, which still remained largely uncovered. By aligning them with the knowledge from prior studies, we created conceptual models that help to understand the fact-checking processes. In addition, we mapped our findings on the fact-checkers' activities and needs to the relevant tasks for AI research, while providing a discussion on three AI tasks that were not covered by previous similar studies. The new opportunities identified for AI researchers and developers have implications for the focus of AI research in this domain.
>
---
#### [replaced 008] Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?
- **分类: cs.DL; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2508.20117v2](http://arxiv.org/pdf/2508.20117v2)**

> **作者:** Liang Li; Yuntian Li; Wenxin Zhao; Shan Ye; Yun Lu
>
> **备注:** miscommunication in the authorization process from the first author
>
> **摘要:** Through bibliometric analysis and topic modeling, we find that artificial intelligence (AI) is positively transforming geosciences research, with a notable increase in AI-related scientific output in recent years. We are encouraged to observe that earth scientists from developing countries have gained better visibility in the recent AI for Science (AI4S) paradigm and that AI is also improving the landscape of international collaboration in geoscience-related research.
>
---
#### [replaced 009] Computational Basis of LLM's Decision Making in Social Simulation
- **分类: cs.AI; cs.CY; cs.LG; econ.GN; q-fin.EC**

- **链接: [http://arxiv.org/pdf/2504.11671v2](http://arxiv.org/pdf/2504.11671v2)**

> **作者:** Ji Ma
>
> **摘要:** Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications, strengthening sociological theory and measurement.
>
---
#### [replaced 010] When the Past Misleads: Rethinking Training Data Expansion Under Temporal Distribution Shifts
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2509.01060v2](http://arxiv.org/pdf/2509.01060v2)**

> **作者:** Chengyuan Yao; Yunxuan Tang; Christopher Brooks; Rene F. Kizilcec; Renzhe Yu
>
> **备注:** Accepted by the Eighth AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)
>
> **摘要:** Predictive models are typically trained on historical data to predict future outcomes. While it is commonly assumed that training on more historical data would improve model performance and robustness, data distribution shifts over time may undermine these benefits. This study examines how expanding historical data training windows under covariate shifts (changes in feature distributions) and concept shifts (changes in feature-outcome relationships) affects the performance and algorithmic fairness of predictive models. First, we perform a simulation study to explore scenarios with varying degrees of covariate and concept shifts in training data. Absent distribution shifts, we observe performance gains from longer training windows though they reach a plateau quickly; in the presence of concept shift, performance may actually decline. Covariate shifts alone do not significantly affect model performance, but may complicate the impact of concept shifts. In terms of fairness, models produce more biased predictions when the magnitude of concept shifts differs across sociodemographic groups; for intersectional groups, these effects are more complex and not simply additive. Second, we conduct an empirical case study of student retention prediction, a common machine learning application in education, using 12 years of student records from 23 minority-serving community colleges in the United States. We find concept shifts to be a key contributor to performance degradation when expanding the training window. Moreover, model fairness is compromised when marginalized populations have distinct data distribution shift patterns from their peers. Overall, our findings caution against conventional wisdom that "more data is better" and underscore the importance of using historical data judiciously, especially when it may be subject to data distribution shifts, to improve model performance and fairness.
>
---
#### [replaced 011] Partnering with AI: A Pedagogical Feedback System for LLM Integration into Programming Education
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2507.00406v3](http://arxiv.org/pdf/2507.00406v3)**

> **作者:** Niklas Scholz; Manh Hung Nguyen; Adish Singla; Tomohiro Nagashima
>
> **备注:** ECTEL 2025 Preprint. This is an extended version of a poster paper accepted and published at ECTEL-2025
>
> **摘要:** Feedback is one of the most crucial components to facilitate effective learning. With the rise of large language models (LLMs) in recent years, research in programming education has increasingly focused on automated feedback generation to help teachers provide timely support to every student. However, prior studies often overlook key pedagogical principles, such as mastery and progress adaptation, that shape effective feedback strategies. This paper introduces a novel pedagogical framework for LLM-driven feedback generation derived from established feedback models and local insights from secondary school teachers. To evaluate this framework, we implemented a web-based application for Python programming with LLM-based feedback that follows the framework and conducted a mixed-method evaluation with eight secondary-school computer science teachers. Our findings suggest that teachers consider that, when aligned with the framework, LLMs can effectively support students and even outperform human teachers in certain scenarios through instant and precise feedback. However, we also found several limitations, such as its inability to adapt feedback to dynamic classroom contexts. Such a limitation highlights the need to complement LLM-generated feedback with human expertise to ensure effective student learning. This work demonstrates an effective way to use LLMs for feedback while adhering to pedagogical standards and highlights important considerations for future systems.
>
---
#### [replaced 012] Street-Level AI: Are Large Language Models Ready for Real-World Judgments?
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2508.08193v2](http://arxiv.org/pdf/2508.08193v2)**

> **作者:** Gaurab Pokharel; Shafkat Farabi; Patrick J. Fowler; Sanmay Das
>
> **备注:** This work has been accepted for publication as a full paper at the AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)
>
> **摘要:** A surge of recent work explores the ethical and societal implications of large-scale AI models that make "moral" judgments. Much of this literature focuses either on alignment with human judgments through various thought experiments or on the group fairness implications of AI judgments. However, the most immediate and likely use of AI is to help or fully replace the so-called street-level bureaucrats, the individuals deciding to allocate scarce social resources or approve benefits. There is a rich history underlying how principles of local justice determine how society decides on prioritization mechanisms in such domains. In this paper, we examine how well LLM judgments align with human judgments, as well as with socially and politically determined vulnerability scoring systems currently used in the domain of homelessness resource allocation. Crucially, we use real data on those needing services (maintaining strict confidentiality by only using local large models) to perform our analyses. We find that LLM prioritizations are extremely inconsistent in several ways: internally on different runs, between different LLMs, and between LLMs and the vulnerability scoring systems. At the same time, LLMs demonstrate qualitative consistency with lay human judgments in pairwise testing. Findings call into question the readiness of current generation AI systems for naive integration in high-stakes societal decision-making.
>
---
#### [replaced 013] EigenBench: A Comparative Behavioral Measure of Value Alignment
- **分类: cs.AI; cs.CL; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2509.01938v2](http://arxiv.org/pdf/2509.01938v2)**

> **作者:** Jonathn Chang; Leonhard Piff; Suvadip Sana; Jasmine X. Li; Lionel Levine
>
> **摘要:** Aligning AI with human values is a pressing unsolved problem. To address the lack of quantitative metrics for value alignment, we propose EigenBench: a black-box method for comparatively benchmarking language models' values. Given an ensemble of models, a constitution describing a value system, and a dataset of scenarios, our method returns a vector of scores quantifying each model's alignment to the given constitution. To produce these scores, each model judges the outputs of other models across many scenarios, and these judgments are aggregated with EigenTrust (Kamvar et al, 2003), yielding scores that reflect a weighted-average judgment of the whole ensemble. EigenBench uses no ground truth labels, as it is designed to quantify traits for which reasonable judges may disagree on the correct label. Using prompted personas, we test whether EigenBench scores are more sensitive to the model or the prompt: we find that most of the variance is explained by the prompt, but a small residual quantifies the disposition of the model itself.
>
---
