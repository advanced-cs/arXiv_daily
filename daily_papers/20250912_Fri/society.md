# 计算机与社会 cs.CY

- **最新发布 22 篇**

- **更新 10 篇**

## 最新发布

#### [new 001] Investigating Student Interaction Patterns with Large Language Model-Powered Course Assistants in Computer Science Courses
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 论文研究学生与LLM课程助手的互动模式，旨在解决学术支持不足的问题。通过部署LLM助手，分析使用数据和对话内容，发现其在晚间和入门课程中使用较多，但生成的高阶问题较少，提示需加强教育者参与配置。**

- **链接: [http://arxiv.org/pdf/2509.08862v1](http://arxiv.org/pdf/2509.08862v1)**

> **作者:** Chang Liu; Loc Hoang; Andrew Stolman; Rene F. Kizilcec; Bo Wu
>
> **摘要:** Providing students with flexible and timely academic support is a challenge at most colleges and universities, leaving many students without help outside scheduled hours. Large language models (LLMs) are promising for bridging this gap, but interactions between students and LLMs are rarely overseen by educators. We developed and studied an LLM-powered course assistant deployed across multiple computer science courses to characterize real-world use and understand pedagogical implications. By Spring 2024, our system had been deployed to approximately 2,000 students across six courses at three institutions. Analysis of the interaction data shows that usage remains strong in the evenings and nights and is higher in introductory courses, indicating that our system helps address temporal support gaps and novice learner needs. We sampled 200 conversations per course for manual annotation: most sampled responses were judged correct and helpful, with a small share unhelpful or erroneous; few responses included dedicated examples. We also examined an inquiry-based learning strategy: only around 11% of sampled conversations contained LLM-generated follow-up questions, which were often ignored by students in advanced courses. A Bloom's taxonomy analysis reveals that current LLM capabilities are limited in generating higher-order cognitive questions. These patterns suggest opportunities for pedagogically oriented LLM-based educational systems and greater educator involvement in configuring prompts, content, and policies.
>
---
#### [new 002] De spanning tussen het non-discriminatierecht en het gegevensbeschermingsrecht: heeft de AVG een nieuwe uitzondering nodig om discriminatie door kunstmatige intelligentie tegen te gaan?
- **分类: cs.CY**

- **简介: 论文探讨GDPR对AI歧视预防的限制，分析是否需增设例外条款。研究欧洲法律框架下隐私与反歧视政策的冲突，评估特殊数据使用的合法性，并讨论相关立法建议。**

- **链接: [http://arxiv.org/pdf/2509.08836v1](http://arxiv.org/pdf/2509.08836v1)**

> **作者:** Marvin van Bekkum; Frederik Zuiderveen Borgesius
>
> **备注:** 23 pages, in Dutch
>
> **摘要:** Organisations can use artificial intelligence to make decisions about people for a variety of reasons, for instance, to select the best candidates from many job applications. However, AI systems can have discriminatory effects when used for decision-making. To illustrate, an AI system could reject applications of people with a certain ethnicity, while the organisation did not plan such ethnicity discrimination. But in Europe, an organisation runs into a problem when it wants to assess whether its AI system accidentally discriminates based on ethnicity: the organisation may not know the applicants' ethnicity. In principle, the GDPR bans the use of certain 'special categories of data' (sometimes called 'sensitive data'), which include data on ethnicity, religion, and sexual preference. The proposal for an AI Act of the European Commission includes a provision that would enable organisations to use special categories of data for auditing their AI systems. This paper asks whether the GDPR's rules on special categories of personal data hinder the prevention of AI-driven discrimination. We argue that the GDPR does prohibit such use of special category data in many circumstances. We also map out the arguments for and against creating an exception to the GDPR's ban on using special categories of personal data, to enable preventing discrimination by AI systems. The paper discusses European law, but the paper can be relevant outside Europe too, as many policymakers in the world grapple with the tension between privacy and non-discrimination policy.
>
---
#### [new 003] Safe and Certifiable AI Systems: Concepts, Challenges, and Lessons Learned
- **分类: cs.CY; cs.AI; cs.LG**

- **简介: 该论文提出TÜV AUSTRIA Trusted AI框架，用于评估和认证AI系统的安全性与合规性。其任务是解决AI在安全关键领域应用时缺乏认证方法的问题，通过三支柱体系和功能可信度概念，提供可测试的认证标准与实践教训。**

- **链接: [http://arxiv.org/pdf/2509.08852v1](http://arxiv.org/pdf/2509.08852v1)**

> **作者:** Kajetan Schweighofer; Barbara Brune; Lukas Gruber; Simon Schmid; Alexander Aufreiter; Andreas Gruber; Thomas Doms; Sebastian Eder; Florian Mayer; Xaver-Paul Stadlbauer; Christoph Schwald; Werner Zellinger; Bernhard Nessler; Sepp Hochreiter
>
> **备注:** 63 pages, 27 figures
>
> **摘要:** There is an increasing adoption of artificial intelligence in safety-critical applications, yet practical schemes for certifying that AI systems are safe, lawful and socially acceptable remain scarce. This white paper presents the T\"UV AUSTRIA Trusted AI framework an end-to-end audit catalog and methodology for assessing and certifying machine learning systems. The audit catalog has been in continuous development since 2019 in an ongoing collaboration with scientific partners. Building on three pillars - Secure Software Development, Functional Requirements, and Ethics & Data Privacy - the catalog translates the high-level obligations of the EU AI Act into specific, testable criteria. Its core concept of functional trustworthiness couples a statistically defined application domain with risk-based minimum performance requirements and statistical testing on independently sampled data, providing transparent and reproducible evidence of model quality in real-world settings. We provide an overview of the functional requirements that we assess, which are oriented on the lifecycle of an AI system. In addition, we share some lessons learned from the practical application of the audit catalog, highlighting common pitfalls we encountered, such as data leakage scenarios, inadequate domain definitions, neglect of biases, or a lack of distribution drift controls. We further discuss key aspects of certifying AI systems, such as robustness, algorithmic fairness, or post-certification requirements, outlining both our current conclusions and a roadmap for future research. In general, by aligning technical best practices with emerging European standards, the approach offers regulators, providers, and users a practical roadmap for legally compliant, functionally trustworthy, and certifiable AI systems.
>
---
#### [new 004] Deep opacity and AI: A threat to XAI and to privacy protection mechanisms
- **分类: cs.CY; cs.AI**

- **简介: 论文探讨AI的“深度不透明性”对隐私保护和可解释AI（XAI）的威胁，分析其对数据主体和分析专家的影响，并提出技术应对方案。属于AI伦理与隐私保护领域，旨在解决AI不透明性带来的隐私问题。**

- **链接: [http://arxiv.org/pdf/2509.08835v1](http://arxiv.org/pdf/2509.08835v1)**

> **作者:** Vincent C. Müller
>
> **摘要:** It is known that big data analytics and AI pose a threat to privacy, and that some of this is due to some kind of "black box problem" in AI. I explain how this becomes a problem in the context of justification for judgments and actions. Furthermore, I suggest distinguishing three kinds of opacity: 1) the subjects do not know what the system does ("shallow opacity"), 2) the analysts do not know what the system does ("standard black box opacity"), or 3) the analysts cannot possibly know what the system might do ("deep opacity"). If the agents, data subjects as well as analytics experts, operate under opacity, then these agents cannot provide justifications for judgments that are necessary to protect privacy, e.g., they cannot give "informed consent", or guarantee "anonymity". It follows from these points that agents in big data analytics and AI often cannot make the judgments needed to protect privacy. So I conclude that big data analytics makes the privacy problems worse and the remedies less effective. As a positive note, I provide a brief outlook on technical ways to handle this situation.
>
---
#### [new 005] Towards Trustworthy AI: Characterizing User-Reported Risks across LLMs "In the Wild"
- **分类: cs.CY; cs.HC**

- **简介: 该论文分析用户在Reddit上对七种LLM聊天机器人的风险讨论，揭示用户报告的风险分布不均且平台特异，提出需采用用户中心方法解决系统研究与用户实际体验间的差距。属于AI风险评估任务。**

- **链接: [http://arxiv.org/pdf/2509.08912v1](http://arxiv.org/pdf/2509.08912v1)**

> **作者:** Lingyao Li; Renkai Ma; Zhaoqian Xue; Junjie Xiong
>
> **摘要:** While Large Language Models (LLMs) are rapidly integrating into daily life, research on their risks often remains lab-based and disconnected from the problems users encounter "in the wild." While recent HCI research has begun to explore these user-facing risks, it typically concentrates on a singular LLM chatbot like ChatGPT or an isolated risk like privacy. To gain a holistic understanding of multi-risk across LLM chatbots, we analyze online discussions on Reddit around seven major LLM chatbots through the U.S. NIST's AI Risk Management Framework. We find that user-reported risks are unevenly distributed and platform-specific. While "Valid and Reliable" risk is the most frequently mentioned, each product also exhibits a unique "risk fingerprint;" for instance, user discussions associate GPT more with "Safe" and "Fair" issues, Gemini with "Privacy," and Claude with "Secure and Resilient" risks. Furthermore, the nature of these risks differs by their prevalence: less frequent risks like "Explainability" and "Privacy" manifest as nuanced user trade-offs, more common ones like "Fairness" are experienced as direct personal harms. Our findings reveal gaps between risks reported by system-centered studies and by users, highlighting the need for user-centered approaches that support users in their daily use of LLM chatbots.
>
---
#### [new 006] POW: Political Overton Windows of Large Language Models
- **分类: cs.CY**

- **简介: 论文提出POW框架，通过PRISM方法评估28个LLM的政治倾向边界。任务是分析模型政治偏见，解决传统审计无法全面反映意识形态范围的问题，揭示各模型在经济和社会立场上的不同限制与开放程度。**

- **链接: [http://arxiv.org/pdf/2509.08853v1](http://arxiv.org/pdf/2509.08853v1)**

> **作者:** Leif Azzopardi; Yashar Moshfeghi
>
> **备注:** Accepted in EMNLP 2025
>
> **摘要:** Political bias in Large Language Models (LLMs) presents a growing concern for the responsible deployment of AI systems. Traditional audits often attempt to locate a model's political position as a point estimate, masking the broader set of ideological boundaries that shape what a model is willing or unwilling to say. In this paper, we draw upon the concept of the Overton Window as a framework for mapping these boundaries: the range of political views that a given LLM will espouse, remain neutral on, or refuse to endorse. To uncover these windows, we applied an auditing-based methodology, called PRISM, that probes LLMs through task-driven prompts designed to elicit political stances indirectly. Using the Political Compass Test, we evaluated twenty-eight LLMs from eight providers to reveal their distinct Overton Windows. While many models default to economically left and socially liberal positions, we show that their willingness to express or reject certain positions varies considerably, where DeepSeek models tend to be very restrictive in what they will discuss and Gemini models tend to be most expansive. Our findings demonstrate that Overton Windows offer a richer, more nuanced view of political bias in LLMs and provide a new lens for auditing their normative boundaries.
>
---
#### [new 007] Die Verarbeitung medizinischer Forschungsdaten ohne datenschutzrechtliche Einwilligung: Der Korridor zwischen Anonymisierung und der Forschungsausnahme in Österreich
- **分类: cs.CY; cs.CR**

- **简介: 该论文分析在无患者同意情况下处理医疗数据的法律途径，探讨GDPR框架下匿名化与奥地利研究豁免的适用。旨在解决医学研究中数据保护与科研需求间的矛盾，明确合法数据处理方式。**

- **链接: [http://arxiv.org/pdf/2509.08841v1](http://arxiv.org/pdf/2509.08841v1)**

> **作者:** Saskia Kaltenbrunner; Michael Schmidbauer
>
> **备注:** 28 pages, in German, Austrian legal framework
>
> **摘要:** Modern, data-driven medical research requires the processing of sensitive health data on a large scale. However, this data is subject to special protection under the GDPR, which is why processing regularly raises data protection concerns in practice. These concerns are particularly prevalent when sensitive personal data is processed without informed consent. This article analyses options for data processing in the field of medical research without consent and describes the legal framework for anonymisation under the GDPR, the national Austrian implementation of the research exemption, and their interaction. -- Moderne, datengetriebene medizinische Forschung erfordert die Verarbeitung sensibler Gesundheitsdaten in grossem Ausmass. Diese sind im System der DSGVO jedoch besonders gesch\"utzt, weswegen einer rechtssicheren Verarbeitung in der Praxis regelm\"assig datenschutzrechtliche Bedenken entgegenstehen. Diese Bedenken bestehen insbesondere bei Verarbeitung sensibler personenbezogener Daten ohne informierte Einwilligung. Dieser Beitrag analysiert daher M\"oglichkeiten zur Datenverarbeitung im Bereich der medizinischen Forschung fernab der Einwilligung und beschreibt hierf\"ur das rechtliche Rahmenwerk f\"ur Anonymisierung der DSGVO, die nationale, \"osterreichische Umsetzung der Forschungsausnahme und ihr Zusammenspiel.
>
---
#### [new 008] Position: The Pitfalls of Over-Alignment: Overly Caution Health-Related Responses From LLMs are Unethical and Dangerous
- **分类: cs.CY**

- **简介: 该论文探讨LLMs在健康相关问题中过度对齐带来的负面影响，指出其可能加剧焦虑和强迫症用户的心理负担。论文分析了不同LLMs的对齐程度，并呼吁开发更具推理能力、能提供更精准回应的模型。任务为评估与改进LLMs在健康领域的伦理响应。**

- **链接: [http://arxiv.org/pdf/2509.08833v1](http://arxiv.org/pdf/2509.08833v1)**

> **作者:** Wenqi Marshall Guo; Yiyang Du; Heidi J. S. Tworek; Shan Du
>
> **摘要:** Large Language Models (LLMs) are usually aligned with "human values/preferences" to prevent harmful output. Discussions around the alignment of Large Language Models (LLMs) generally focus on preventing harmful outputs. However, in this paper, we argue that in health-related queries, over-alignment-leading to overly cautious responses-can itself be harmful, especially for people with anxiety and obsessive-compulsive disorder (OCD). This is not only unethical but also dangerous to the user, both mentally and physically. We also showed qualitative results that some LLMs exhibit varying degrees of alignment. Finally, we call for the development of LLMs with stronger reasoning capabilities that provide more tailored and nuanced responses to health queries. Warning: This paper contains materials that could trigger health anxiety or OCD.
>
---
#### [new 009] AuraSight: Generating Realistic Social Media Data
- **分类: cs.CY**

- **简介: 论文介绍AuraSight系统，通过AESOP-SynSM引擎生成拟真社交媒体数据，用于模拟多日流行文化事件中的网络行为。任务是构建训练场景，解决现实社交数据不足问题，采用基于智能体与生成AI的混合方法实现交互模拟。**

- **链接: [http://arxiv.org/pdf/2509.08927v1](http://arxiv.org/pdf/2509.08927v1)**

> **作者:** Lynnette Hui Xian Ng; Bianca N. Y. Kang; Kathleen M. Carley
>
> **备注:** Carnegie Mellon University Technical Report
>
> **摘要:** This document details the narrative and technical design behind the process of generating a quasi-realistic set X data for a fictional multi-day pop culture episode (AuraSight). Social media post simulation is essential towards creating realistic training scenarios for understanding emergent network behavior that formed from known sets of agents. Our social media post generation pipeline uses the AESOP-SynSM engine, which employs a hybrid approach of agent-based and generative artificial intelligence techniques. We explicate choices in scenario setup and summarize the fictional groups involved, before moving on to the operationalization of these actors and their interactions within the SynSM engine. We also briefly illustrate some outputs generated and discuss the utility of such simulated data and potential future improvements.
>
---
#### [new 010] Protected Grounds and the System of Non-Discrimination Law in the Context of Algorithmic Decision-Making and Artificial Intelligence
- **分类: cs.CY**

- **简介: 该论文探讨算法决策中非歧视法的适用性，分析其对新型分类的保护漏洞，并提出立法应对策略。任务是解决算法可能引发的歧视问题，通过理论分析为政策制定提供参考。**

- **链接: [http://arxiv.org/pdf/2509.08837v1](http://arxiv.org/pdf/2509.08837v1)**

> **作者:** Janneke Gerards; Frederik Zuiderveen Borgesius
>
> **备注:** Colorado Technology Law Journal 2022
>
> **摘要:** Algorithmic decision-making and similar types of artificial intelligence (AI) may lead to improvements in all sectors of society, but can also have discriminatory effects. While current non-discrimination law offers people some protection, algorithmic decision-making presents the law with several challenges. For instance, algorithms can generate new categories of people based on seemingly innocuous characteristics, such as web browser preference or apartment number, or more complicated categories combining many data points. Such new types of differentiation could evade non-discrimination law, as browser type and house number are not protected characteristics, but such differentiation could still be unfair, for instance if it reinforces social inequality. This paper explores which system of non-discrimination law can best be applied to algorithmic decision-making, considering that algorithms can differentiate on the basis of characteristics that do not correlate with protected grounds of discrimination such as ethnicity or gender. The paper analyses the current loopholes in the protection offered by non-discrimination law and explores the best way for lawmakers to approach algorithmic differentiation. While we focus on Europe, the conceptual and theoretical focus of the paper can make it useful for scholars and policymakers from other regions too, as they encounter similar problems with algorithmic decision-making.
>
---
#### [new 011] PerFairX: Is There a Balance Between Fairness and Personality in Large Language Model Recommendations?
- **分类: cs.CY; cs.AI; cs.IR**

- **简介: 该论文提出PerFairX框架，评估大语言模型推荐中个性化与公平性的平衡问题。通过对比ChatGPT和DeepSeek在电影和音乐数据集上的表现，揭示个性提示提升契合度但可能加剧公平性差异，为构建兼顾心理适配与公平的推荐系统提供基准。**

- **链接: [http://arxiv.org/pdf/2509.08829v1](http://arxiv.org/pdf/2509.08829v1)**

> **作者:** Chandan Kumar Sah
>
> **备注:** 10 pages, 5 figures. Accepted to the Workshop on Multimodal Continual Learning (MCL) at ICCV 2025. @2025 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), ICCV's 2025
>
> **摘要:** The integration of Large Language Models (LLMs) into recommender systems has enabled zero-shot, personality-based personalization through prompt-based interactions, offering a new paradigm for user-centric recommendations. However, incorporating user personality traits via the OCEAN model highlights a critical tension between achieving psychological alignment and ensuring demographic fairness. To address this, we propose PerFairX, a unified evaluation framework designed to quantify the trade-offs between personalization and demographic equity in LLM-generated recommendations. Using neutral and personality-sensitive prompts across diverse user profiles, we benchmark two state-of-the-art LLMs, ChatGPT and DeepSeek, on movie (MovieLens 10M) and music (Last.fm 360K) datasets. Our results reveal that personality-aware prompting significantly improves alignment with individual traits but can exacerbate fairness disparities across demographic groups. Specifically, DeepSeek achieves stronger psychological fit but exhibits higher sensitivity to prompt variations, while ChatGPT delivers stable yet less personalized outputs. PerFairX provides a principled benchmark to guide the development of LLM-based recommender systems that are both equitable and psychologically informed, contributing to the creation of inclusive, user-centric AI applications in continual learning contexts.
>
---
#### [new 012] Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 该论文探讨将AI事件报告纳入电信法律政策，以应对AI带来的新型风险。研究分析印度现行法规的不足，提出政策建议，旨在填补监管空白，增强电信领域AI风险管理能力。**

- **链接: [http://arxiv.org/pdf/2509.09508v1](http://arxiv.org/pdf/2509.09508v1)**

> **作者:** Avinash Agarwal; Manisha J. Nene
>
> **备注:** 16 pages, 2 figures, 1 table
>
> **摘要:** The integration of artificial intelligence (AI) into telecommunications infrastructure introduces novel risks, such as algorithmic bias and unpredictable system behavior, that fall outside the scope of traditional cybersecurity and data protection frameworks. This paper introduces a precise definition and a detailed typology of telecommunications AI incidents, establishing them as a distinct category of risk that extends beyond conventional cybersecurity and data protection breaches. It argues for their recognition as a distinct regulatory concern. Using India as a case study for jurisdictions that lack a horizontal AI law, the paper analyzes the country's key digital regulations. The analysis reveals that India's existing legal instruments, including the Telecommunications Act, 2023, the CERT-In Rules, and the Digital Personal Data Protection Act, 2023, focus on cybersecurity and data breaches, creating a significant regulatory gap for AI-specific operational incidents, such as performance degradation and algorithmic bias. The paper also examines structural barriers to disclosure and the limitations of existing AI incident repositories. Based on these findings, the paper proposes targeted policy recommendations centered on integrating AI incident reporting into India's existing telecom governance. Key proposals include mandating reporting for high-risk AI failures, designating an existing government body as a nodal agency to manage incident data, and developing standardized reporting frameworks. These recommendations aim to enhance regulatory clarity and strengthen long-term resilience, offering a pragmatic and replicable blueprint for other nations seeking to govern AI risks within their existing sectoral frameworks.
>
---
#### [new 013] Adtech and Real-Time Bidding under European Data Protection Law
- **分类: cs.CY; cs.CR**

- **简介: 该论文分析实时竞价（RTB）广告技术与欧盟数据保护法的兼容性，探讨其合法性、透明性和安全性问题，指出RTB难以符合GDPR要求，需监管干预。属于法律与科技交叉研究任务，旨在解决数据保护与广告技术冲突的问题。**

- **链接: [http://arxiv.org/pdf/2509.08838v1](http://arxiv.org/pdf/2509.08838v1)**

> **作者:** Michael Veale; Frederik Zuiderveen Borgesius
>
> **摘要:** This article discusses the troubled relationship between contemporary advertising technology (adtech) systems, in particular systems of real-time bidding (RTB, also known as programmatic advertising) underpinning much behavioral targeting on the web and through mobile applications. This article analyzes the extent to which practices of RTB are compatible with the requirements regarding a legal basis for processing, transparency, and security in European data protection law. We first introduce the technologies at play through explaining and analyzing the systems deployed online today. Following that, we turn to the law. Rather than analyze RTB against every provision of the General Data Protection Regulation (GDPR), we consider RTB in the context of the GDPR's requirement of a legal basis for processing and the GDPR's transparency and security requirements. We show, first, that the GDPR requires prior consent of the internet user for RTB, as other legal bases are not appropriate. Second, we show that it is difficult - and perhaps impossible - for website publishers and RTB companies to meet the GDPR's transparency requirements. Third, RTB incentivizes insecure data processing. We conclude that, in concept and in practice, RTB is structurally difficult to reconcile with European data protection law. Therefore, intervention by regulators is necessary.
>
---
#### [new 014] Evaluating the Clinical Safety of LLMs in Response to High-Risk Mental Health Disclosures
- **分类: cs.CY; I.2.7**

- **简介: 该论文评估六种大语言模型对高风险心理健康披露的响应安全性。通过临床编码框架，分析模型在风险识别、共情、求助鼓励等方面的表现，发现 Claude 表现最佳，但整体未达临床标准，需进一步优化以确保AI在心理健康领域的伦理应用。**

- **链接: [http://arxiv.org/pdf/2509.08839v1](http://arxiv.org/pdf/2509.08839v1)**

> **作者:** Siddharth Shah; Amit Gupta; Aarav Mann; Alexandre Vaz; Benjamin E. Caldwell; Robert Scholz; Peter Awad; Rocky Allemandi; Doug Faust; Harshita Banka; Tony Rousmaniere
>
> **备注:** Previously posted as a preprint on Research Square (DOI: 10.21203/rs.3.rs-7364128/v1), under a CC BY 4.0 License
>
> **摘要:** As large language models (LLMs) increasingly mediate emotionally sensitive conversations, especially in mental health contexts, their ability to recognize and respond to high-risk situations becomes a matter of public safety. This study evaluates the responses of six popular LLMs (Claude, Gemini, Deepseek, ChatGPT, Grok 3, and LLAMA) to user prompts simulating crisis-level mental health disclosures. Drawing on a coding framework developed by licensed clinicians, five safety-oriented behaviors were assessed: explicit risk acknowledgment, empathy, encouragement to seek help, provision of specific resources, and invitation to continue the conversation. Claude outperformed all others in global assessment, while Grok 3, ChatGPT, and LLAMA underperformed across multiple domains. Notably, most models exhibited empathy, but few consistently provided practical support or sustained engagement. These findings suggest that while LLMs show potential for emotionally attuned communication, none currently meet satisfactory clinical standards for crisis response. Ongoing development and targeted fine-tuning are essential to ensure ethical deployment of AI in mental health settings.
>
---
#### [new 015] A vibe coding learning design to enhance EFL students' talking to, through, and about AI
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 论文探讨如何通过“vibe coding”提升EFL学生与AI互动的英语能力。研究设计四小时工作坊，分析学生使用AI解决写作问题的过程，发现有效教学需加强元语言指导、提示工程和AI认知模型表达。**

- **链接: [http://arxiv.org/pdf/2509.08854v1](http://arxiv.org/pdf/2509.08854v1)**

> **作者:** David James Woo; Kai Guo; Yangyang Yu
>
> **备注:** 15 pages, 12 figures
>
> **摘要:** This innovative practice article reports on the piloting of vibe coding (using natural language to create software applications with AI) for English as a Foreign Language (EFL) education. We developed a human-AI meta-languaging framework with three dimensions: talking to AI (prompt engineering), talking through AI (negotiating authorship), and talking about AI (mental models of AI). Using backward design principles, we created a four-hour workshop where two students designed applications addressing authentic EFL writing challenges. We adopted a case study methodology, collecting data from worksheets and video recordings, think-aloud protocols, screen recordings, and AI-generated images. Contrasting cases showed one student successfully vibe coding a functional application cohering to her intended design, while another encountered technical difficulties with major gaps between intended design and actual functionality. Analysis reveals differences in students' prompt engineering approaches, suggesting different AI mental models and tensions in attributing authorship. We argue that AI functions as a beneficial languaging machine, and that differences in how students talk to, through, and about AI explain vibe coding outcome variations. Findings indicate that effective vibe coding instruction requires explicit meta-languaging scaffolding, teaching structured prompt engineering, facilitating critical authorship discussions, and developing vocabulary for articulating AI mental models.
>
---
#### [new 016] Decentralising LLM Alignment: A Case for Context, Pluralism, and Participation
- **分类: cs.CY; cs.LG**

- **简介: 论文探讨如何通过情境、多元主义与参与性去中心化大语言模型对齐过程，以对抗知识权力集中。论文提出三种策略，并结合实例说明其应用，旨在推动更公平、民主的AI治理模式。**

- **链接: [http://arxiv.org/pdf/2509.08858v1](http://arxiv.org/pdf/2509.08858v1)**

> **作者:** Oriane Peter; Kate Devlin
>
> **备注:** Accepted at AIES 2025
>
> **摘要:** Large Language Models (LLMs) alignment methods have been credited with the commercial success of products like ChatGPT, given their role in steering LLMs towards user-friendly outputs. However, current alignment techniques predominantly mirror the normative preferences of a narrow reference group, effectively imposing their values on a wide user base. Drawing on theories of the power/knowledge nexus, this work argues that current alignment practices centralise control over knowledge production and governance within already influential institutions. To counter this, we propose decentralising alignment through three characteristics: context, pluralism, and participation. Furthermore, this paper demonstrates the critical importance of delineating the context-of-use when shaping alignment practices by grounding each of these features in concrete use cases. This work makes the following contributions: (1) highlighting the role of context, pluralism, and participation in decentralising alignment; (2) providing concrete examples to illustrate these strategies; and (3) demonstrating the nuanced requirements associated with applying alignment across different contexts of use. Ultimately, this paper positions LLM alignment as a potential site of resistance against epistemic injustice and the erosion of democratic processes, while acknowledging that these strategies alone cannot substitute for broader societal changes.
>
---
#### [new 017] Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking
- **分类: cs.CL; cs.CY; cs.HC; cs.LG; cs.SI**

- **简介: 该论文属于社交推荐任务，旨在提升在线学习平台SAMI的社交匹配效果。通过检测学生个性特征，改进其匹配算法，以增强推荐的相关性与学生参与度。**

- **链接: [http://arxiv.org/pdf/2509.09583v1](http://arxiv.org/pdf/2509.09583v1)**

> **作者:** Brittany Harbison; Samuel Taubman; Travis Taylor; Ashok. K. Goel
>
> **摘要:** Social connection is a vital part of learning, yet online course environments present barriers to the organic formation of social groups. SAMI offers one solution by facilitating student connections, but its effectiveness is constrained by an incomplete Theory of Mind, limiting its ability to create an effective mental model of a student. One facet of this is its inability to intuit personality, which may influence the relevance of its recommendations. To explore this, we propose a personality detection model utilizing GPTs zero-shot capability to infer Big-Five personality traits from forum introduction posts, often encouraged in online courses. We benchmark its performance against established models, demonstrating its efficacy in this task. Furthermore, we integrate this model into SAMIs entity-based matchmaking system, enabling personality-informed social recommendations. Initial integration suggests personality traits can complement existing matching factors, though additional evaluation is required to determine their full impact on student engagement and match quality.
>
---
#### [new 018] Content Moderation Futures
- **分类: cs.HC; cs.CY**

- **简介: 论文探讨社交媒体内容治理的失败与可能，分析企业激励与公众利益的结构性矛盾。通过参与式设计工作坊，提出技术公司因追求创新和扩张而忽视用户安全，并倡导以关怀工作历史为灵感，推动平台治理变革。**

- **链接: [http://arxiv.org/pdf/2509.09076v1](http://arxiv.org/pdf/2509.09076v1)**

> **作者:** Lindsay Blackwell
>
> **备注:** 76 pages
>
> **摘要:** This study examines the failures and possibilities of contemporary social media governance through the lived experiences of various content moderation professionals. Drawing on participatory design workshops with 33 practitioners in both the technology industry and broader civil society, this research identifies significant structural misalignments between corporate incentives and public interests. While experts agree that successful content moderation is principled, consistent, contextual, proactive, transparent, and accountable, current technology companies fail to achieve these goals, due in part to exploitative labor practices, chronic underinvestment in user safety, and pressures of global scale. I argue that successful governance is undermined by the pursuit of technological novelty and rapid growth, resulting in platforms that necessarily prioritize innovation and expansion over public trust and safety. To counter this dynamic, I revisit the computational history of care work, to motivate present-day solidarity amongst platform governance workers and inspire systemic change.
>
---
#### [new 019] Digital Iran Reloaded: Gamer Mitigation Tactics of IRI Information Controls
- **分类: cs.HC; cs.CY; cs.SI; H.5.2; K.6.5; K.4.1**

- **简介: 该论文研究伊朗互联网用户（特别是游戏玩家）如何通过技术与社交策略绕过网络审查。通过调查与网络测量，分析用户信心、技术工具及社交网络对规避效果的影响，提出设计与政策建议。属于数字权利与信息控制领域研究。**

- **链接: [http://arxiv.org/pdf/2509.09063v1](http://arxiv.org/pdf/2509.09063v1)**

> **作者:** Melinda Cohoon
>
> **备注:** Preprint report. 40 pages, 10 figures. Supported by the Open Technology Fund (OTF) Information Controls Fellowship Program (ICFP)
>
> **摘要:** Internet censorship in the Islamic Republic of Iran restricts access to global platforms and services, forcing users to rely on circumvention technologies such as VPNs, proxies, and tunneling tools. This report presents findings from a mixed-methods study of 660 Iranian internet users, with a focus on gamers as a digitally literate and socially networked community. Survey data are combined with network measurements of latency and VPN performance to identify both technical and social strategies of circumvention. Results show that while younger users report higher confidence with circumvention, peer networks, rather than formal training, are the strongest predictors of resilience. Gaming communities, particularly those active on platforms such as Discord and Telegram, serve as hubs for sharing tactics and lowering barriers to adoption. These findings extend existing work on usable security and censorship circumvention by highlighting the intersection of infrastructural conditions and social learning. The study concludes with design and policy implications for developers, researchers, and funders working on digital rights and information controls.
>
---
#### [new 020] Integrating Public Perspectives in Microreactor Facility Design
- **分类: physics.soc-ph; cs.CY**

- **简介: 该论文提出一种社区参与的微反应堆设计方法，通过工作坊收集公众对设施外观、经济和社会影响的意见，以解决当前设计缺乏公众输入的问题。研究强调需结合地方需求进行定制化设计，而非单纯追求自动化与标准化。**

- **链接: [http://arxiv.org/pdf/2509.08975v1](http://arxiv.org/pdf/2509.08975v1)**

> **作者:** Diana Cambero Inda; Armita Marpu; Gina Rubio; Caralyn Haas; Prish Dhagat; Aditi Verma
>
> **摘要:** Current approaches to the design and regulation of nuclear energy facilities offer limited opportunities for public input, particularly for host communities to shape decisions about a facility's aesthetics, socioeconomic, and environmental impacts, or even levels of safety. In this paper, we propose a community-engaged approach to designing microreactors. In a participatory design workshop, we invited community members to work with engineers to create designs for hypothetical microreactor facilities for Southeast Michigan as a way to understand their hopes, concerns, and preferences. Our findings reveal a desire for local energy infrastructure to not just provide a service (energy) but also to be a central and accessible feature of the community. Community members articulated several specific ways in which the hypothetical facilities could be designed, with particular focus placed on the well-being of local families as well as employment opportunities. These findings call into question current microreactor design trajectories that seek to achieve high levels of automation. Our findings also suggest a need for contextual design that may be at odds with the logics of standardization currently being pursued by reactor designers. We call on microreactor developers to carry out such participatory design engagements in other places as a way to build a more comprehensive, place-based understanding of local preferences for community-embedded energy infrastructure.
>
---
#### [new 021] Explaining the Reputational Risks of AI-Mediated Communication: Messages Labeled as AI-Assisted Are Viewed as Less Diagnostic of the Sender's Moral Character
- **分类: cs.HC; cs.CY; cs.ET**

- **简介: 该论文研究AI辅助标签如何影响人们对发送者道德性格的判断。通过实验发现，AI标签削弱了信息的诊断性，使温暖或冷漠信号不如人工信息明显。任务是探讨AI中介沟通中的声誉风险问题。**

- **链接: [http://arxiv.org/pdf/2509.09645v1](http://arxiv.org/pdf/2509.09645v1)**

> **作者:** Pranav Khadpe; Kimi Wenzel; George Loewenstein; Geoff Kaufman
>
> **备注:** To appear at AIES 2025
>
> **摘要:** When someone sends us a thoughtful message, we naturally form judgments about their character. But what happens when that message carries a label indicating it was written with the help of AI? This paper investigates how the appearance of AI assistance affects our perceptions of message senders. Adding nuance to previous research, through two studies (N=399) featuring vignette scenarios, we find that AI-assistance labels don't necessarily make people view senders negatively. Rather, they dampen the strength of character signals in communication. We show that when someone sends a warmth-signalling message (like thanking or apologizing) without AI help, people more strongly categorize the sender as warm. At the same time, when someone sends a coldness-signalling message (like bragging or blaming) without assistance, people more confidently categorize them as cold. Interestingly, AI labels weaken both these associations: An AI-assisted apology makes the sender appear less warm than if they had written it themselves, and an AI-assisted blame makes the sender appear less cold than if they had composed it independently. This supports our signal diagnosticity explanation: messages labeled as AI-assisted are viewed as less diagnostic than messages which seem unassisted. We discuss how our findings shed light on the causal origins of previously reported observations in AI-Mediated Communication.
>
---
#### [new 022] Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning
- **分类: cs.LG; cs.CY**

- **简介: 论文结合量子神经网络与深度强化学习，开发用于外汇交易的智能代理。通过QLSTM预测趋势，QA3C进行决策，实现优于ETF的回报。任务为金融交易优化，解决高风险控制下的小利润交易问题。**

- **链接: [http://arxiv.org/pdf/2509.09176v1](http://arxiv.org/pdf/2509.09176v1)**

> **作者:** Jun-Hao Chen; Yu-Chien Huang; Yun-Cheng Tsai; Samuel Yen-Chi Chen
>
> **摘要:** The convergence of quantum-inspired neural networks and deep reinforcement learning offers a promising avenue for financial trading. We implemented a trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM) for short-term trend prediction with Quantum Asynchronous Advantage Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the long-only agent achieves 11.87\% return over around 5 years with 0.92\% max drawdown, outperforming several currency ETFs. We detail state design (QLSTM features and indicators), reward function for trend-following/risk control, and multi-core training. Results show hybrid models yield competitive FX trading performance. Implications include QLSTM's effectiveness for small-profit trades with tight risk and future enhancements. Key hyperparameters: QLSTM sequence length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and simplified strategy. \footnote{The views expressed in this article are those of the authors and do not represent the views of Wells Fargo. This article is for informational purposes only. Nothing contained in this article should be construed as investment advice. Wells Fargo makes no express or implied warranties and expressly disclaims all legal, tax, and accounting implications related to this article.
>
---
## 更新

#### [replaced 001] AI Self-preferencing in Algorithmic Hiring: Empirical Evidence and Insights
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2509.00462v2](http://arxiv.org/pdf/2509.00462v2)**

> **作者:** Jiannan Xu; Gujie Li; Jane Yi Jiang
>
> **备注:** This paper has been accepted as a non-archival submission at EAAMO 2025 and AIES 2025
>
> **摘要:** As generative artificial intelligence (AI) tools become widely adopted, large language models (LLMs) are increasingly involved on both sides of decision-making processes, ranging from hiring to content moderation. This dual adoption raises a critical question: do LLMs systematically favor content that resembles their own outputs? Prior research in computer science has identified self-preference bias -- the tendency of LLMs to favor their own generated content -- but its real-world implications have not been empirically evaluated. We focus on the hiring context, where job applicants often rely on LLMs to refine resumes, while employers deploy them to screen those same resumes. Using a large-scale controlled resume correspondence experiment, we find that LLMs consistently prefer resumes generated by themselves over those written by humans or produced by alternative models, even when content quality is controlled. The bias against human-written resumes is particularly substantial, with self-preference bias ranging from 68% to 88% across major commercial and open-source models. To assess labor market impact, we simulate realistic hiring pipelines across 24 occupations. These simulations show that candidates using the same LLM as the evaluator are 23% to 60% more likely to be shortlisted than equally qualified applicants submitting human-written resumes, with the largest disadvantages observed in business-related fields such as sales and accounting. We further demonstrate that this bias can be reduced by more than 50% through simple interventions targeting LLMs' self-recognition capabilities. These findings highlight an emerging but previously overlooked risk in AI-assisted decision making and call for expanded frameworks of AI fairness that address not only demographic-based disparities, but also biases in AI-AI interactions.
>
---
#### [replaced 002] Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics
- **分类: cs.AI; cs.CY; cs.HC; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.19317v4](http://arxiv.org/pdf/2505.19317v4)**

> **作者:** Tin Trung Nguyen; Jiannan Xu; Zora Che; Phuong-Anh Nguyen-Le; Rushil Dandamudi; Donald Braman; Furong Huang; Hal Daumé III; Zubin Jelveh
>
> **备注:** AIES 2025
>
> **摘要:** Although popularized AI fairness metrics, e.g., demographic parity, have uncovered bias in AI-assisted decision-making outcomes, they do not consider how much effort one has spent to get to where one is today in the input feature space. However, the notion of effort is important in how Philosophy and humans understand fairness. We propose a philosophy-informed approach to conceptualize and evaluate Effort-aware Fairness (EaF), grounded in the concept of Force, which represents the temporal trajectory of predictive features coupled with inertia. Besides theoretical formulation, our empirical contributions include: (1) a pre-registered human subjects experiment, which shows that for both stages of the (individual) fairness evaluation process, people consider the temporal trajectory of a predictive feature more than its aggregate value; (2) pipelines to compute Effort-aware Individual/Group Fairness in the criminal justice and personal finance contexts. Our work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who have spent significant efforts to improve but are still stuck with systemic disadvantages outside their control.
>
---
#### [replaced 003] Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?
- **分类: cs.CY; cs.AI; cs.CL; cs.ET**

- **链接: [http://arxiv.org/pdf/2507.10576v2](http://arxiv.org/pdf/2507.10576v2)**

> **作者:** Bhakti Khera; Rezvan Alamian; Pascal A. Scherz; Stephan M. Goetz
>
> **备注:** 41 pages, 21 figures
>
> **摘要:** The legal field already uses various large language models (LLMs) in actual applications, but their quantitative performance and reasons for it are underexplored. We evaluated several open-source and proprietary LLMs -- including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of the European Qualifying Examination (EQE) for future European Patent Attorneys. OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama 3.1 8B scored 0.55. The latter two are within the range of mere guessing for the two-answer forced-choice design. None of the evaluated models could have passed the examination fully, as accuracy never exceeded the average threshold of 0.90 required for professional-level standards -- also not models that are regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level performance. GPT-4o excelled at integrating text and graphics, while Claude 3 Opus often lost formatting coherence. Human patent experts evaluated the textual justifications and uncovered various critical shortcomings of each model. They valued clarity and legal rationale over the raw correctness of the answers, which revealed misalignment between automatic metrics and expert judgment. Model outputs were sensitive to modest temperature changes and prompt wording, which underscores the remaining necessity of expert oversight. Future work should target logical consistency, robust multimodality, and adaptive prompting to approach human-level patent proficiency. In summary, despite the outstanding performance of recent large models, the general public might overestimate their performance. The field has a long way to go to develop a virtual patent attorney. This paper wants to point out several specific limitations that need solutions.
>
---
#### [replaced 004] Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review
- **分类: cs.LG; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2504.18544v2](http://arxiv.org/pdf/2504.18544v2)**

> **作者:** Nazia Nafis; Inaki Esnaola; Alvaro Martinez-Perez; Maria-Cruz Villa-Uriol; Venet Osmani
>
> **摘要:** Generating synthetic tabular data can be challenging, however evaluation of their quality is just as challenging, if not more. This systematic review sheds light on the critical importance of rigorous evaluation of synthetic health data to ensure reliability, relevance, and their appropriate use. Based on screening of 1766 papers and a detailed review of 101 papers we identified key challenges, including lack of consensus on evaluation methods, improper use of evaluation metrics, limited input from domain experts, inadequate reporting of dataset characteristics, and limited reproducibility of results. In response, we provide several guidelines on the generation and evaluation of synthetic data, to allow the community to unlock and fully harness the transformative potential of synthetic data and accelerate innovation.
>
---
#### [replaced 005] Toward Responsible and Beneficial AI: Comparing Regulatory and Guidance-Based Approaches
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2508.00868v2](http://arxiv.org/pdf/2508.00868v2)**

> **作者:** Jian Du
>
> **备注:** PhD thesis
>
> **摘要:** This dissertation presents a comprehensive comparative analysis of artificial intelligence governance frameworks across the European Union, United States, China, and IEEE technical standards, examining how different jurisdictions and organizations approach the challenge of promoting responsible and beneficial AI development. Using a qualitative research design based on systematic content analysis, the study identifies distinctive patterns in regulatory philosophy, implementation mechanisms, and global engagement strategies across these major AI governance ecosystems.
>
---
#### [replaced 006] Towards Post-mortem Data Management Principles for Generative AI
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2509.07375v2](http://arxiv.org/pdf/2509.07375v2)**

> **作者:** Elina Van Kempen; Ismat Jarin; Chloe Georgiou
>
> **摘要:** Foundation models, large language models (LLMs), and agentic AI systems rely heavily on vast corpora of user data. The use of such data for training has raised persistent concerns around ownership, copyright, and potential harms. In this work, we explore a related but less examined dimension: the ownership rights of data belonging to deceased individuals. We examine the current landscape of post-mortem data management and privacy rights as defined by the privacy policies of major technology companies and regulations such as the EU AI Act. Based on this analysis, we propose three post-mortem data management principles to guide the protection of deceased individuals data rights. Finally, we discuss directions for future work and offer recommendations for policymakers and privacy practitioners on deploying these principles alongside technological solutions to operationalize and audit them in practice.
>
---
#### [replaced 007] From Vision to Validation: A Theory- and Data-Driven Construction of a GCC-Specific AI Adoption Index
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.05474v2](http://arxiv.org/pdf/2509.05474v2)**

> **作者:** Mohammad Rashed Albous; Anwaar AlKandari; Abdel Latef Anouze
>
> **备注:** 38 pages, 8 figures, 17 tables
>
> **摘要:** Artificial intelligence (AI) is rapidly transforming public-sector processes worldwide, yet standardized measures rarely address the unique drivers, governance models, and cultural nuances of the Gulf Cooperation Council (GCC) countries. This study employs a theory-driven foundation derived from an in-depth analysis of literature review and six National AI Strategies (NASs), coupled with a data-driven approach that utilizes a survey of 203 mid- and senior-level government employees and advanced statistical techniques (K-Means clustering, Principal Component Analysis, and Partial Least Squares Structural Equation Modeling). By combining policy insights with empirical evidence, the research develops and validates a novel AI Adoption Index specifically tailored to the GCC public sector. Findings indicate that robust technical infrastructure and clear policy mandates exert the strongest influence on successful AI implementations, overshadowing organizational readiness in early adoption stages. The combined model explains 70% of the variance in AI outcomes, suggesting that resource-rich environments and top-down policy directives can drive rapid but uneven technology uptake. By consolidating key dimensions (Technical Infrastructure (TI), Organizational Readiness (OR), and Governance Environment (GE)) into a single composite index, this study provides a holistic yet context-sensitive tool for benchmarking AI maturity. The index offers actionable guidance for policymakers seeking to harmonize large-scale deployments with ethical and regulatory standards. Beyond advancing academic discourse, these insights inform more strategic allocation of resources, cross-country cooperation, and capacity-building initiatives, thereby supporting sustained AI-driven transformation in the GCC region and beyond.
>
---
#### [replaced 008] NeedForHeat DataGear: An Open Monitoring System to Accelerate the Residential Heating Transition
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2509.06927v2](http://arxiv.org/pdf/2509.06927v2)**

> **作者:** Henri ter Hofte; Nick van Ravenzwaaij
>
> **备注:** 10 pages + 3 pages appendices
>
> **摘要:** We introduce NeedForHeat DataGear: an open hardware and open software data collection system designed to accelerate the residential heating transition. NeedForHeat DataGear collects time series monitoring data in homes that have not yet undergone a heating transition, enabling assessment of real-life thermal characteristics, heating system efficiency, and residents' comfort needs. This paper outlines its architecture and functionalities, emphasizing its modularity, adaptability, and cost-effectiveness for field data acquisition. Unlike conventional domestic monitoring solutions focused on home automation, direct feedback, or post-installation heat pump monitoring, it prioritizes time series data we deemed essential to evaluate the current situation in existing homes before the heating transition. Designed for seamless deployment across diverse households, NeedForHeat DataGear combines openness, security, and privacy with a low-cost, user-friendly approach, making it a valuable tool for researchers, energy professionals, and energy coaches.
>
---
#### [replaced 009] SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models
- **分类: cs.CL; cs.CR; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2502.02787v2](http://arxiv.org/pdf/2502.02787v2)**

> **作者:** Amirhossein Dabiriaghdam; Lele Wang
>
> **备注:** Accepted to EMNLP 25 main
>
> **摘要:** The widespread adoption of large language models (LLMs) necessitates reliable methods to detect LLM-generated text. We introduce SimMark, a robust sentence-level watermarking algorithm that makes LLMs' outputs traceable without requiring access to model internals, making it compatible with both open and API-based LLMs. By leveraging the similarity of semantic sentence embeddings combined with rejection sampling to embed detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while maintaining the text quality and fluency.
>
---
#### [replaced 010] The Architecture of AI Transformation: Four Strategic Patterns and an Emerging Frontier
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.02853v2](http://arxiv.org/pdf/2509.02853v2)**

> **作者:** Diana A. Wolfe; Alice Choe; Fergus Kidd
>
> **备注:** 59 pages, 2 tables, 4 figures
>
> **摘要:** Despite extensive investment in artificial intelligence, 95% of enterprises report no measurable profit impact from AI deployments (MIT, 2025). In this theoretical paper, we argue that this gap reflects paradigmatic lock-in that channels AI into incremental optimization rather than structural transformation. Using a cross-case analysis, we propose a 2x2 framework that reconceptualizes AI strategy along two independent dimensions: the degree of transformation achieved (incremental to transformational) and the treatment of human contribution (reduced to amplified). The framework surfaces four patterns now dominant in practice: individual augmentation, process automation, workforce substitution, and a less deployed frontier of collaborative intelligence. Evidence shows that the first three dimensions reinforce legacy work models and yield localized gains without durable value capture. Realizing collaborative intelligence requires three mechanisms: complementarity (pairing distinct human and machine strengths), co-evolution (mutual adaptation through interaction), and boundary-setting (human determination of ethical and strategic parameters). Complementarity and boundary-setting are observable in regulated and high-stakes domains; co-evolution is largely absent, which helps explain limited system-level impact. Our findings in a case study analysis illustrated that advancing toward collaborative intelligence requires material restructuring of roles, governance, and data architecture rather than additional tools. The framework reframes AI transformation as an organizational design challenge: moving from optimizing the division of labor between humans and machines to architecting their convergence, with implications for operating models, workforce development, and the future of work.
>
---
