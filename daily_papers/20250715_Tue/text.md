# 自然语言处理 cs.CL

- **最新发布 96 篇**

- **更新 92 篇**

## 最新发布

#### [new 001] Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于信息检索与自然语言处理任务，旨在解决传统RAG系统中语义分割不足的问题。作者提出一种结合层次文本分割和聚类的新框架，生成更语义连贯的文本块，并在多个数据集上验证了其有效性。**

- **链接: [http://arxiv.org/pdf/2507.09935v1](http://arxiv.org/pdf/2507.09935v1)**

> **作者:** Hai Toan Nguyen; Tien Dat Nguyen; Viet Ha Nguyen
>
> **摘要:** Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies for retrieval, which enhance large language models (LLMs) by enabling them to access external knowledge, ensuring that the retrieved information is up-to-date and domain-specific. However, traditional methods often fail to create chunks that capture sufficient semantic meaning, as they do not account for the underlying textual structure. This paper proposes a novel framework that enhances RAG by integrating hierarchical text segmentation and clustering to generate more meaningful and semantically coherent chunks. During inference, the framework retrieves information by leveraging both segment-level and cluster-level vector representations, thereby increasing the likelihood of retrieving more precise and contextually relevant information. Evaluations on the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method achieved improved results compared to traditional chunking techniques.
>
---
#### [new 002] CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks
- **分类: cs.CL; cs.AI; cs.SE**

- **简介: 该论文属于代码评估任务，旨在解决当前缺乏专门评估LLM作为代码评判模型效果的基准问题。作者构建了CodeJudgeBench基准，涵盖代码生成、修复和测试生成三个任务，评估26个LLM-as-a-Judge模型表现，发现思维链模型优于普通模型，但存在判断随机性和顺序敏感问题，并探讨了优化提示策略。**

- **链接: [http://arxiv.org/pdf/2507.10535v1](http://arxiv.org/pdf/2507.10535v1)**

> **作者:** Hongchao Jiang; Yiming Chen; Yushi Cao; Hung-yi Lee; Robby T. Tan
>
> **备注:** Dataset is available at https://huggingface.co/datasets/mattymchen/codejudgebench
>
> **摘要:** Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.
>
---
#### [new 003] SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于多语言安全对齐任务，旨在解决现有英文主导的LLM系统在东南亚低资源语言中对不安全和越狱提示检测效果差的问题。作者提出了SEALGuard，通过LoRA微调多语言模型，并构建了包含10种语言、26万条数据的SEALSBench数据集进行评估，结果显示SEALGuard在检测多语言不安全提示方面显著优于LlamaGuard。**

- **链接: [http://arxiv.org/pdf/2507.08898v1](http://arxiv.org/pdf/2507.08898v1)**

> **作者:** Wenliang Shan; Michael Fu; Rui Yang; Chakkrit; Tantithamthavorn
>
> **备注:** Under Review at Information and Software Technology
>
> **摘要:** Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''), they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia. This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems. We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases. We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on this benchmark. Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score. Our ablation study further reveals the contributions of adaptation strategies and model size to the overall performance of SEALGuard. SEALGuard advances the safety alignment of LLM systems by introducing an effective multilingual guardrail.
>
---
#### [new 004] Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery
- **分类: cs.CL; q-bio.NC; q-bio.QM**

- **简介: 该论文研究不同视觉想象能力个体在Ganzflicker诱导下的幻觉内容差异。利用自然语言处理分析4000多人的自由文本描述，发现强想象者看到更复杂、自然的内容，弱想象者多见几何图案。结合视觉语言模型，揭示了高低层级视觉区域协调性的个体差异，属于认知神经科学任务。**

- **链接: [http://arxiv.org/pdf/2507.09011v1](http://arxiv.org/pdf/2507.09011v1)**

> **作者:** Ana Chkhaidze; Reshanne R. Reeder; Connor Gag; Anastasia Kiyonaga; Seana Coulson
>
> **摘要:** A rapidly alternating red and black display known as Ganzflicker induces visual hallucinations that reflect the generative capacity of the visual system. Recent proposals regarding the imagery spectrum, that is, differences in the visual system of individuals with absent imagery, typical imagery, and vivid imagery, suggest these differences should impact the complexity of other internally generated visual experiences. Here, we used tools from natural language processing to analyze free-text descriptions of hallucinations from over 4,000 participants, asking whether people with different imagery phenotypes see different things in their mind's eye during Ganzflicker-induced hallucinations. Strong imagers described complex, naturalistic content, while weak imagers reported simple geometric patterns. Embeddings from vision language models better captured these differences than text-only language models, and participants with stronger imagery used language with richer sensorimotor associations. These findings may reflect individual variation in coordination between early visual areas and higher-order regions relevant for the imagery spectrum.
>
---
#### [new 005] Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models
- **分类: cs.CL; cs.LG**

- **简介: 该论文属于模型优化任务，旨在解决大语言模型在特定数据集上过拟合并影响泛化的问题。通过引入一种基于集成梯度的神经元剪枝方法，识别并移除对高置信度预测贡献大的非泛化神经元，从而提升模型在新任务上的表现。**

- **链接: [http://arxiv.org/pdf/2507.09185v1](http://arxiv.org/pdf/2507.09185v1)**

> **作者:** Ameen Ali; Shahar Katz; Lior Wolf; Ivan Titov
>
> **摘要:** Large language models (LLMs) often develop learned mechanisms specialized to specific datasets, such as reliance on domain-specific correlations, which yield high-confidence predictions without generalizable reasoning. While beneficial in one setting, these dataset-specific mechanisms typically degrade performance when models encounter novel tasks or distributions. In this work, we introduce a fine-tuning approach designed to enhance generalization by identifying and pruning neurons associated with dataset-specific mechanisms in transformer-based LLMs. Our method employs Integrated Gradients to quantify each neuron's influence on high-confidence predictions, pinpointing those that disproportionately contribute to dataset-specific performance without supporting robust, transferable reasoning. Selectively pruning these neurons compels the model to depend on generalizable representations. Evaluated across multiple-choice benchmarks, our pruning-based fine-tuning significantly enhances performance, surpassing prior (non-pruning) adaptation methods.
>
---
#### [new 006] Lizard: An Efficient Linearization Framework for Large Language Models
- **分类: cs.CL; cs.LG**

- **简介: 论文提出Lizard，一种高效的大型语言模型线性化框架，旨在解决Transformer模型在长上下文生成中的内存和计算瓶颈。通过结合门控线性注意力与滑动窗口注意力机制，实现亚二次复杂度的注意力计算，同时保持输出质量。相比以往方法，Lizard支持更灵活的模型设计，并在多项任务上表现出优越性能。**

- **链接: [http://arxiv.org/pdf/2507.09025v1](http://arxiv.org/pdf/2507.09025v1)**

> **作者:** Chien Van Nguyen; Ruiyi Zhang; Hanieh Deilamsalehy; Puneet Mathur; Viet Dac Lai; Haoliang Wang; Jayakumar Subramanian; Ryan A. Rossi; Trung Bui; Nikos Vlassis; Franck Dernoncourt; Thien Huu Nguyen
>
> **备注:** 15 pages
>
> **摘要:** We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.
>
---
#### [new 007] OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique
- **分类: cs.CL**

- **简介: 该论文属于代码生成与批判任务，旨在解决缺乏大规模高质量数据的问题。作者构建了包含250万问题-解答-批判三元组的OpenCodeReasoning-II数据集，并采用两阶段微调策略训练模型，在代码生成和批判方面取得显著成果，并扩展了支持C++的LiveCodeBench基准。**

- **链接: [http://arxiv.org/pdf/2507.09075v1](http://arxiv.org/pdf/2507.09075v1)**

> **作者:** Wasi Uddin Ahmad; Somshubra Majumdar; Aleksander Ficek; Sean Narenthiran; Mehrzad Samadi; Jocelyn Huang; Siddhartha Jain; Vahid Noroozi; Boris Ginsburg
>
> **备注:** work in progress
>
> **摘要:** Recent advancements in reasoning-based Large Language Models (LLMs), particularly their potential through test-time scaling, have created significant opportunities for distillation in code generation and critique. However, progress in both areas fundamentally depends on large-scale, high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a dataset consists of 2.5M question-solution-critique triples (approx. 35K unique programming questions), making it nearly twice the size of the previous largest publicly available code reasoning dataset. In this work, we employ a two-stage supervised fine-tuning strategy. The first stage focuses on fine-tuning for code generation, while the second stage involves the joint training of models for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct models achieve performance in code generation that either exceeds or equals the best prior open-weight distilled models. Notably, the integration of our code generation and critique models leads to significant improvements in competitive coding performance. Furthermore, we present an extension of the LiveCodeBench benchmark to specifically support the C++ programming language, thereby facilitating more comprehensive LLM evaluation using this benchmark.
>
---
#### [new 008] TextOmics-Guided Diffusion for Hit-like Molecular Generation
- **分类: cs.CL**

- **简介: 该论文属于药物发现任务，旨在解决靶向药物分子生成中缺乏多源数据整合的问题。作者构建了TextOmics数据集，建立了分子描述与组学表达的对应关系，并提出ToDi框架，结合组学和文本信息生成高质量分子。**

- **链接: [http://arxiv.org/pdf/2507.09982v1](http://arxiv.org/pdf/2507.09982v1)**

> **作者:** Hang Yuan; Chen Li; Wenjun Ma; Yuncheng Jiang
>
> **摘要:** Hit-like molecular generation with therapeutic potential is essential for target-specific drug discovery. However, the field lacks heterogeneous data and unified frameworks for integrating diverse molecular representations. To bridge this gap, we introduce TextOmics, a pioneering benchmark that establishes one-to-one correspondences between omics expressions and molecular textual descriptions. TextOmics provides a heterogeneous dataset that facilitates molecular generation through representations alignment. Built upon this foundation, we propose ToDi, a generative framework that jointly conditions on omics expressions and molecular textual descriptions to produce biologically relevant, chemically valid, hit-like molecules. ToDi leverages two encoders (OmicsEn and TextEn) to capture multi-level biological and semantic associations, and develops conditional diffusion (DiffGen) for controllable generation. Extensive experiments confirm the effectiveness of TextOmics and demonstrate ToDi outperforms existing state-of-the-art approaches, while also showcasing remarkable potential in zero-shot therapeutic molecular generation. Sources are available at: https://github.com/hala-ToDi.
>
---
#### [new 009] Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media
- **分类: cs.CL**

- **简介: 该论文属于心理健康预测任务，旨在解决社交媒体上自杀风险的动态预测问题。现有方法忽视了保护性因素的作用且无法捕捉风险随时间的变化。为此，作者构建了一个包含风险与保护因素的新数据集，并提出了动态影响学习方法，有效提升预测性能并提供可解释性。**

- **链接: [http://arxiv.org/pdf/2507.10008v1](http://arxiv.org/pdf/2507.10008v1)**

> **作者:** Jun Li; Xiangmeng Wang; Haoyang Li; Yifei Yan; Hong Va Leong; Ling Feng; Nancy Xiaonan Yu; Qing Li
>
> **摘要:** Suicide is a critical global health issue that requires urgent attention. Even though prior work has revealed valuable insights into detecting current suicide risk on social media, little attention has been paid to developing models that can predict subsequent suicide risk over time, limiting their ability to capture rapid fluctuations in individuals' mental state transitions. In addition, existing work ignores protective factors that play a crucial role in suicide risk prediction, focusing predominantly on risk factors alone. Protective factors such as social support and coping strategies can mitigate suicide risk by moderating the impact of risk factors. Therefore, this study proposes a novel framework for predicting subsequent suicide risk by jointly learning the dynamic influence of both risk factors and protective factors on users' suicide risk transitions. We propose a novel Protective Factor-Aware Dataset, which is built from 12 years of Reddit posts along with comprehensive annotations of suicide risk and both risk and protective factors. We also introduce a Dynamic Factors Influence Learning approach that captures the varying impact of risk and protective factors on suicide risk transitions, recognizing that suicide risk fluctuates over time according to established psychological theories. Our thorough experiments demonstrate that the proposed model significantly outperforms state-of-the-art models and large language models across three datasets. In addition, the proposed Dynamic Factors Influence Learning provides interpretable weights, helping clinicians better understand suicidal patterns and enabling more targeted intervention strategies.
>
---
#### [new 010] Task-Based Flexible Feature Distillation for LLMs
- **分类: cs.CL**

- **简介: 该论文属于知识蒸馏任务，旨在解决教师与学生模型隐藏层维度不匹配的问题。传统方法需引入额外参数进行特征空间对齐，影响性能。本文提出一种无需新参数的任务导向特征蒸馏方法，通过识别教师模型中与任务最相关的隐藏单元并将其激活值直接蒸馏至学生模型，实现跨维度知识迁移，提升分类、指令跟随和摘要等任务的性能。**

- **链接: [http://arxiv.org/pdf/2507.10155v1](http://arxiv.org/pdf/2507.10155v1)**

> **作者:** Khouloud Saadi; Di Wang
>
> **摘要:** Knowledge Distillation (KD) in general and feature distillation in particular are promising techniques for reducing the high computational demand of large language models (LLMs). However, traditional feature KD methods typically assume that the teacher and the student share the same hidden size, limiting the flexibility of the student's architecture. A common solution to this problem involves training a linear projector to align their feature spaces, but this introduces additional parameters that must be learned from scratch and often degrades performance on downstream tasks, especially in generative settings. To address this issue, in this work, we propose a novel task-based feature distillation method that enables knowledge transfer between teacher and student models with different hidden layer dimensions, without introducing any new parameters. Leveraging the insight that only a subset of LLM components contribute significantly to a specific downstream task, our approach identifies the most task-relevant hidden units in the teacher and directly distills their activations to the student. Our method is flexible and easily integrates with other distillation frameworks. Empirical results show consistent improvements over prior approaches across diverse tasks, including classification, instruction-following, and summarization, achieving up to a 3\% performance gain over the linear projection baseline.
>
---
#### [new 011] Evaluating LLMs in Medicine: A Call for Rigor, Transparency
- **分类: cs.CL**

- **简介: 该论文属于医学与人工智能交叉任务，旨在评估大语言模型（LLMs）在医疗问答中的表现。论文聚焦现有评估数据集的不足，如缺乏临床真实性、透明度和验证。作者审查了多个常用数据集，并探索替代方案，呼吁建立更严谨、全面的标准框架，以推动医学领域LLM的可靠发展。**

- **链接: [http://arxiv.org/pdf/2507.08916v1](http://arxiv.org/pdf/2507.08916v1)**

> **作者:** Mahmoud Alwakeel; Aditya Nagori; Vijay Krishnamoorthy; Rishikesan Kamaleswaran
>
> **摘要:** Objectives: To evaluate the current limitations of large language models (LLMs) in medical question answering, focusing on the quality of datasets used for their evaluation. Materials and Methods: Widely-used benchmark datasets, including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor, transparency, and relevance to clinical scenarios. Alternatives, such as challenge questions in medical journals, were also analyzed to identify their potential as unbiased evaluation tools. Results: Most existing datasets lack clinical realism, transparency, and robust validation processes. Publicly available challenge questions offer some benefits but are limited by their small size, narrow scope, and exposure to LLM training. These gaps highlight the need for secure, comprehensive, and representative datasets. Conclusion: A standardized framework is critical for evaluating LLMs in medicine. Collaborative efforts among institutions and policymakers are needed to ensure datasets and methodologies are rigorous, unbiased, and reflective of clinical complexities.
>
---
#### [new 012] Psychology-Driven Enhancement of Humour Translation
- **分类: cs.CL**

- **简介: 该论文属于 humour translation 任务，旨在解决 humour 翻译中幽默缺失和语言干扰问题。作者提出了一种受心理学启发的 Humour Decomposition Mechanism (HDM)，结合 Chain-of-Thought 和 humour 理论，提升翻译文本的幽默性、流畅性和连贯性。**

- **链接: [http://arxiv.org/pdf/2507.09259v1](http://arxiv.org/pdf/2507.09259v1)**

> **作者:** Yuchen Su; Yonghua Zhu; Yang Chen; Diana Benavides-Prado; Michael Witbrock
>
> **摘要:** Humour translation plays a vital role as a bridge between different cultures, fostering understanding and communication. Although most existing Large Language Models (LLMs) are capable of general translation tasks, these models still struggle with humour translation, which is especially reflected through linguistic interference and lacking humour in translated text. In this paper, we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that utilises Chain-of-Thought (CoT) to imitate the ability of the human thought process, stimulating LLMs to optimise the readability of translated humorous texts. Moreover, we integrate humour theory in HDM to further enhance the humorous elements in the translated text. Our automatic evaluation experiments on open-source humour datasets demonstrate that our method significantly improves the quality of humour translation, yielding average gains of 7.75\% in humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.
>
---
#### [new 013] PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning
- **分类: cs.CL**

- **简介: 该论文属于欺骗检测任务，旨在解决外交对话中数据不平衡（少于5%的欺骗消息）带来的挑战。作者提出PU-Lie模型，结合冻结BERT嵌入、语言与游戏特征及正-未标记学习方法，在减少参数的同时提升检测效果，强调识别欺骗信息优先于识别真实信息。**

- **链接: [http://arxiv.org/pdf/2507.09157v1](http://arxiv.org/pdf/2507.09157v1)**

> **作者:** Bhavinkumar Vinodbhai Kuwar; Bikrant Bikram Pratap Maurya; Priyanshu Gupta; Nitin Choudhury
>
> **摘要:** Detecting deception in strategic dialogues is a complex and high-stakes task due to the subtlety of language and extreme class imbalance between deceptive and truthful communications. In this work, we revisit deception detection in the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We introduce a lightweight yet effective model combining frozen BERT embeddings, interpretable linguistic and game-specific features, and a Positive-Unlabeled (PU) learning objective. Unlike traditional binary classifiers, PU-Lie is tailored for situations where only a small portion of deceptive messages are labeled, and the majority are unlabeled. Our model achieves a new best macro F1 of 0.60 while reducing trainable parameters by over 650x. Through comprehensive evaluations and ablation studies across seven models, we demonstrate the value of PU learning, linguistic interpretability, and speaker-aware representations. Notably, we emphasize that in this problem setting, accurately detecting deception is more critical than identifying truthful messages. This priority guides our choice of PU learning, which explicitly models the rare but vital deceptive class.
>
---
#### [new 014] Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding
- **分类: cs.CL; cs.LG; I.2.7; I.2.6**

- **简介: 该论文属于自然语言处理任务，旨在解决传统课程学习中人工定义难度不准确的问题。通过预训练模型自身预测样本难度，自适应调整训练顺序，提升模型收敛速度与性能。实验验证了方法在多个NLU任务上的有效性。**

- **链接: [http://arxiv.org/pdf/2507.09758v1](http://arxiv.org/pdf/2507.09758v1)**

> **作者:** Qi Feng; Yihong Liu; Hinrich Schütze
>
> **备注:** 18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop (SRW)
>
> **摘要:** Curriculum learning is a widely adopted training strategy in natural language processing (NLP), where models are exposed to examples organized by increasing difficulty to enhance learning efficiency and performance. However, most existing approaches rely on manually defined difficulty metrics -- such as text length -- which may not accurately reflect the model's own perspective. To overcome this limitation, we present a self-adaptive curriculum learning paradigm that prioritizes fine-tuning examples based on difficulty scores predicted by pre-trained language models (PLMs) themselves. Building on these scores, we explore various training strategies that differ in the ordering of examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed sampling. We evaluate our method on four natural language understanding (NLU) datasets covering both binary and multi-class classification tasks. Experimental results show that our approach leads to faster convergence and improved performance compared to standard random sampling.
>
---
#### [new 015] OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering
- **分类: cs.CL; cs.AI; 68T50, 68T07**

- **简介: 该论文提出OPENXRD，一个面向X射线衍射（XRD）问答任务的开放知识框架。它通过GPT-4.5生成的领域文本辅助小模型理解晶体学知识，解决了版权问题并提升问答准确率。论文评估了多种模型在有无辅助文本下的表现，验证了开放知识的有效性，为材料科学中的NLP应用提供了新思路。**

- **链接: [http://arxiv.org/pdf/2507.09155v1](http://arxiv.org/pdf/2507.09155v1)**

> **作者:** Ali Vosoughi; Ayoub Shahnazari; Yufeng Xi; Zeliang Zhang; Griffin Hess; Chenliang Xu; Niaz Abdolrahim
>
> **备注:** 10 pages, 6 figures, 5 tables. Code and dataset available at https://github.com/niaz60/OpenXRD. Project webpage: https://niaz60.github.io/OpenXRD/
>
> **摘要:** This work presents OPENXRD, an open-book pipeline designed for crystallography question answering, which integrates textual prompts with concise supporting content generated by GPT-4.5. Instead of using scanned textbooks, which may lead to copyright issues, OPENXRD generates compact, domain-specific references that help smaller models understand key concepts in X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217 expert-level XRD questions by comparing different vision-language models, including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN, under both closed-book (without supporting material) and open-book (with supporting material) conditions. Our experimental results show significant accuracy improvements in models that use the GPT-4.5-generated summaries, particularly those with limited prior training in crystallography. OPENXRD uses knowledge from larger models to fill knowledge gaps in crystallography and shows that AI-generated texts can help smaller models reason more effectively in scientific tasks. While the current version of OPENXRD focuses on text-based inputs, we also explore future extensions such as adding real crystal diagrams or diffraction patterns to improve interpretation in specialized materials science contexts. Overall, OPENXRD shows that specialized open-book systems can be useful in materials science and provides a foundation for broader natural language processing (NLP) tools in critical scientific fields.
>
---
#### [new 016] Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models
- **分类: cs.CL; cs.AI; 68T07**

- **简介: 该论文属于临床文本分类任务，旨在提升医学案例描述的二分类效果。通过优化DRAGON Longformer模型，调整序列长度、学习率、训练轮数等，并加入医学术语处理。最终在验证集上显著提升了各项指标，证明了模型在医疗领域的有效性与应用潜力。**

- **链接: [http://arxiv.org/pdf/2507.09470v1](http://arxiv.org/pdf/2507.09470v1)**

> **作者:** Mingchuan Yang; Ziyuan Huang
>
> **备注:** 29 pages, 5 tables
>
> **摘要:** This study explores the optimization of the DRAGON Longformer base model for clinical text classification, specifically targeting the binary classification of medical case descriptions. A dataset of 500 clinical cases containing structured medical observations was used, with 400 cases for training and 100 for validation. Enhancements to the pre-trained joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter tuning, domain-specific preprocessing, and architectural adjustments. Key modifications involved increasing sequence length from 512 to 1024 tokens, adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5 to 8, and incorporating specialized medical terminology. The optimized model achieved notable performance gains: accuracy improved from 72.0% to 85.2%, precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from 71.0% to 85.2%. Statistical analysis confirmed the significance of these improvements (p < .001). The model demonstrated enhanced capability in interpreting medical terminology, anatomical measurements, and clinical observations. These findings contribute to domain-specific language model research and offer practical implications for clinical natural language processing applications. The optimized model's strong performance across diverse medical conditions underscores its potential for broad use in healthcare settings.
>
---
#### [new 017] Fusing Large Language Models with Temporal Transformers for Time Series Forecasting
- **分类: cs.CL**

- **简介: 该论文属于时间序列预测任务，旨在解决大型语言模型（LLMs）在连续数值时间序列建模上的不足。通过融合LLM的高语义表达与Transformer的时间建模能力，提出一种新架构，实现更精准预测。**

- **链接: [http://arxiv.org/pdf/2507.10098v1](http://arxiv.org/pdf/2507.10098v1)**

> **作者:** Chen Su; Yuanhe Tian; Qinyu Liu; Jun Zhang; Yan Song
>
> **摘要:** Recently, large language models (LLMs) have demonstrated powerful capabilities in performing various tasks and thus are applied by recent studies to time series forecasting (TSF) tasks, which predict future values with the given historical time series. Existing LLM-based approaches transfer knowledge learned from text data to time series prediction using prompting or fine-tuning strategies. However, LLMs are proficient at reasoning over discrete tokens and semantic patterns but are not initially designed to model continuous numerical time series data. The gaps between text and time series data lead LLMs to achieve inferior performance to a vanilla Transformer model that is directly trained on TSF data. However, the vanilla Transformers often struggle to learn high-level semantic patterns. In this paper, we design a novel Transformer-based architecture that complementarily leverages LLMs and vanilla Transformers, so as to integrate the high-level semantic representations learned by LLMs into the temporal information encoded by time series Transformers, where a hybrid representation is obtained by fusing the representations from the LLM and the Transformer. The resulting fused representation contains both historical temporal dynamics and semantic variation patterns, allowing our model to predict more accurate future values. Experiments on benchmark datasets demonstrate the effectiveness of the proposed approach.
>
---
#### [new 018] Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决罗马化僧伽罗语到僧伽罗语的转写问题。作者构建了Swa-bhasha资源中心，提供相关数据集和算法，推动转写模型研究与应用发展。**

- **链接: [http://arxiv.org/pdf/2507.09245v1](http://arxiv.org/pdf/2507.09245v1)**

> **作者:** Deshan Sumanathilaka; Sameera Perera; Sachithya Dharmasiri; Maneesha Athukorala; Anuja Dilrukshi Herath; Rukshan Dias; Pasindu Gamage; Ruvan Weerasinghe; Y. H. P. P. Priyadarshana
>
> **备注:** 13 pages, 3 Tables, 3 figures
>
> **摘要:** The Swa-bhasha Resource Hub provides a comprehensive collection of data resources and algorithms developed for Romanized Sinhala to Sinhala transliteration between 2020 and 2025. These resources have played a significant role in advancing research in Sinhala Natural Language Processing (NLP), particularly in training transliteration models and developing applications involving Romanized Sinhala. The current openly accessible data sets and corresponding tools are made publicly available through this hub. This paper presents a detailed overview of the resources contributed by the authors and includes a comparative analysis of existing transliteration applications in the domain.
>
---
#### [new 019] Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?
- **分类: cs.CL**

- **简介: 该论文属于法律问答任务，旨在提升泰语法律问答中引用准确性和回复质量。针对现有RAG系统在复杂法律推理上的不足，作者提出GRPO方法，利用BGE-M3嵌入作为奖励机制优化LLM策略，显著提高了性能并降低了计算成本。**

- **链接: [http://arxiv.org/pdf/2507.09638v1](http://arxiv.org/pdf/2507.09638v1)**

> **作者:** Pawitsapak Akarajaradwong; Chompakorn Chaksangchaichot; Pirat Pothavorn; Attapol Thamrongrattanarit-Rutherford; Ekapol Chuangsuwanich; Sarana Nutanong
>
> **摘要:** The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal question answering is still limited, especially for questions requiring extensive, complex legal reasoning. To address these limitations, we introduce an approach aligning LLMs toward improved law citation accuracy and better response quality using Group-Relative Policy Optimization (GRPO). Our approach leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward, significantly reducing computational expenses up to 2.5x compared to large language model judges. Experiments on the NitiBench benchmark demonstrate substantial improvements: GRPO achieves up to 90% citation-F1 gains from the base model and a 31% increase in joint quality metrics over instruction tuning. Crucially, our method shows enhanced robustness on complex legal reasoning tasks compared to instruction tuning, providing an effective and resource-efficient solution for enhancing Thai legal LLMs.
>
---
#### [new 020] Referential ambiguity and clarification requests: comparing human and LLM behaviour
- **分类: cs.CL; cs.AI**

- **简介: 该论文研究任务导向对话中人类与大语言模型（LLM）提出澄清问题的行为差异。它结合了Minecraft对话语料库的两种注释，分析指代表述模糊与澄清请求之间的关系。论文发现人类较少因指代歧义提问，而LLM则更倾向于对此提问，并探索了推理能力对LLM提问行为的影响。**

- **链接: [http://arxiv.org/pdf/2507.10445v1](http://arxiv.org/pdf/2507.10445v1)**

> **作者:** Chris Madge; Matthew Purver; Massimo Poesio
>
> **摘要:** In this work we examine LLMs' ability to ask clarification questions in task-oriented dialogues that follow the asynchronous instruction-giver/instruction-follower format. We present a new corpus that combines two existing annotations of the Minecraft Dialogue Corpus -- one for reference and ambiguity in reference, and one for SDRT including clarifications -- into a single common format providing the necessary information to experiment with clarifications and their relation to ambiguity. With this corpus we compare LLM actions with original human-generated clarification questions, examining how both humans and LLMs act in the case of ambiguity. We find that there is only a weak link between ambiguity and humans producing clarification questions in these dialogues, and low correlation between humans and LLMs. Humans hardly ever produce clarification questions for referential ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce more clarification questions for referential ambiguity, but less so for task uncertainty. We question if LLMs' ability to ask clarification questions is predicated on their recent ability to simulate reasoning, and test this with different reasoning approaches, finding that reasoning does appear to increase question frequency and relevancy.
>
---
#### [new 021] Using AI to replicate human experimental results: a motion study
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理与心理语言学交叉任务，旨在验证大语言模型（LLMs）能否可靠复现人类在语言实验中的判断。研究聚焦情感意义在运动动词时间表达中的浮现，通过四个人类实验及对应LLM实验，分析AI与人类反应的一致性，解决LLM在语言研究中可信度不足的问题。**

- **链接: [http://arxiv.org/pdf/2507.10342v1](http://arxiv.org/pdf/2507.10342v1)**

> **作者:** Rosa Illan Castillo; Javier Valenzuela
>
> **摘要:** This paper explores the potential of large language models (LLMs) as reliable analytical tools in linguistic research, focusing on the emergence of affective meanings in temporal expressions involving manner-of-motion verbs. While LLMs like GPT-4 have shown promise across a range of tasks, their ability to replicate nuanced human judgements remains under scrutiny. We conducted four psycholinguistic studies (on emergent meanings, valence shifts, verb choice in emotional contexts, and sentence-emoji associations) first with human participants and then replicated the same tasks using an LLM. Results across all studies show a striking convergence between human and AI responses, with statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong correlations in both rating patterns and categorical choices. While minor divergences were observed in some cases, these did not alter the overall interpretative outcomes. These findings offer compelling evidence that LLMs can augment traditional human-based experimentation, enabling broader-scale studies without compromising interpretative validity. This convergence not only strengthens the empirical foundation of prior human-based findings but also opens possibilities for hypothesis generation and data expansion through AI. Ultimately, our study supports the use of LLMs as credible and informative collaborators in linguistic inquiry.
>
---
#### [new 022] Meanings are like Onions: a Layered Approach to Metaphor Processing
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决比喻意义的深层理解问题。论文提出三层模型：内容分析、概念融合与语用意图，以构建更贴近认知机制的比喻处理框架，提升计算系统对隐含意义的理解能力。**

- **链接: [http://arxiv.org/pdf/2507.10354v1](http://arxiv.org/pdf/2507.10354v1)**

> **作者:** Silvia Cappa; Anna Sofia Lippolis; Stefano Zoia
>
> **摘要:** Metaphorical meaning is not a flat mapping between concepts, but a complex cognitive phenomenon that integrates multiple levels of interpretation. In this paper, we propose a stratified model of metaphor processing that treats meaning as an onion: a multi-layered structure comprising (1) content analysis, (2) conceptual blending, and (3) pragmatic intentionality. This three-dimensional framework allows for a richer and more cognitively grounded approach to metaphor interpretation in computational systems. At the first level, metaphors are annotated through basic conceptual elements. At the second level, we model conceptual combinations, linking components to emergent meanings. Finally, at the third level, we introduce a pragmatic vocabulary to capture speaker intent, communicative function, and contextual effects, aligning metaphor understanding with pragmatic theories. By unifying these layers into a single formal framework, our model lays the groundwork for computational methods capable of representing metaphorical meaning beyond surface associations, toward deeper, more context-sensitive reasoning.
>
---
#### [new 023] MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis
- **分类: cs.CL; cs.CY**

- **简介: 该论文构建了气候可视化隐喻数据库MetaClimage，分析其传播效果。任务是评估视觉隐喻在气候传播中的成本与效益。解决缺乏系统材料和影响研究的问题。工作包括收集图像、人员认知评分及自然语言处理分析，揭示视觉隐喻认知负荷高但审美更强，促进深度思考。**

- **链接: [http://arxiv.org/pdf/2507.09225v1](http://arxiv.org/pdf/2507.09225v1)**

> **作者:** Biagio Scalingi; Chiara Barattieri di San Pietro; Paolo Canal; Valentina Bambini
>
> **备注:** 27 pages, 5 figures
>
> **摘要:** Visual metaphors of climate change (e.g., melting glaciers depicted as a melting ice grenade) are regarded as valuable tools for addressing the complexity of environmental challenges. However, few studies have examined their impact on communication, also due to scattered availability of material. Here, we present a novel database of Metaphors of Climate Change in Images (MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal images and enriched with human ratings. For each image, we collected values of difficulty, efficacy, artistic quality, and emotional arousal from human rating, as well as number of tags generated by participants to summarize the message. Semantic and emotion variables were further derived from the tags via Natural Language Processing. Visual metaphors were rated as more difficult to understand, yet more aesthetically pleasant than literal images, but did not differ in efficacy and arousal. The latter for visual metaphors, however, was higher in participants with higher Need For Cognition. Furthermore, visual metaphors received more tags, often referring to entities not depicted in the image, and elicited words with more positive valence and greater dominance than literal images. These results evidence the greater cognitive load of visual metaphors, which nevertheless might induce positive effects such as deeper cognitive elaboration and abstraction compared to literal stimuli. Furthermore, while they are not deemed as more effective and arousing, visual metaphors seem to generate superior aesthetic appreciation and a more positively valenced experience. Overall, this study contributes to understanding the impact of visual metaphors of climate change both by offering a database for future research and by elucidating a cost-benefit trade-off to take into account when shaping environmental communication.
>
---
#### [new 024] Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News
- **分类: cs.CL; cs.SI**

- **简介: 该论文属于自然语言处理任务，旨在解决西班牙语新闻中点击诱饵（clickbait）识别问题。作者重新定义了clickbait，强调其通过制造好奇心缺口吸引点击的核心特征，并提出了一个更客观的标注标准。基于此，构建了首个西班牙语clickbait检测公开数据集TA1C，包含3500条推文，标注一致性高。实验表明基线模型已取得良好效果。**

- **链接: [http://arxiv.org/pdf/2507.09777v1](http://arxiv.org/pdf/2507.09777v1)**

> **作者:** Gabriel Mordecki; Guillermo Moncecchi; Javier Couto
>
> **摘要:** We revise the definition of clickbait, which lacks current consensus, and argue that the creation of a curiosity gap is the key concept that distinguishes clickbait from other related phenomena such as sensationalism and headlines that do not deliver what they promise or diverge from the article. Therefore, we propose a new definition: clickbait is a technique for generating headlines and teasers that deliberately omit part of the information with the goal of raising the readers' curiosity, capturing their attention and enticing them to click. We introduce a new approach to clickbait detection datasets creation, by refining the concept limits and annotations criteria, minimizing the subjectivity in the decision as much as possible. Following it, we created and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the first open source dataset for clickbait detection in Spanish. It consists of 3,500 tweets coming from 18 well known media sources, manually annotated and reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong baselines that achieve 0.84 in F1-score.
>
---
#### [new 025] Tiny Reward Models
- **分类: cs.CL; cs.AI**

- **简介: 论文提出TinyRM，一种小型双向掩码语言模型，用于解决大型解码器模型在推理成本上的问题。属于偏好建模任务，旨在降低资源消耗同时保持性能。通过FLAN式提示、方向低秩适配和层冻结技术，在少量参数下实现高效奖励建模。**

- **链接: [http://arxiv.org/pdf/2507.09973v1](http://arxiv.org/pdf/2507.09973v1)**

> **作者:** Sarah Pan
>
> **备注:** 2025 ICML Efficient Systems for Foundation Models Workshop
>
> **摘要:** Large decoder-based language models have become the dominant architecture for reward modeling in reinforcement learning from human feedback (RLHF). However, as reward models are increasingly deployed in test-time strategies, their inference costs become a growing concern. We present TinyRM, a family of small, bidirectional masked language models (MLMs) with as few as 400 million parameters, that rival the capabilities of models over 175 times larger on reasoning and safety preference modeling tasks. TinyRM combines FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to achieve strong performance on RewardBench, despite using significantly fewer resources. Our experiments suggest that small models benefit from domain-specific tuning strategies, particularly in reasoning, where lightweight finetuning methods are especially effective. While challenges remain in building generalist models and conversational preference modeling, our preliminary results highlight the promise of lightweight bidirectional architectures as efficient, scalable alternatives for preference modeling.
>
---
#### [new 026] NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance
- **分类: cs.CL; cs.AI; q-fin.CP**

- **简介: 该论文属于自然语言处理中的跨语言句子表示学习任务，旨在解决通用模型在金融领域低资源语言（如韩语）中表现不佳的问题。作者提出了NMIXX模型，通过专业金融数据微调，并发布韩语金融语义相似度基准KorFinSTS，提升了跨语言金融语义理解效果。**

- **链接: [http://arxiv.org/pdf/2507.09601v1](http://arxiv.org/pdf/2507.09601v1)**

> **作者:** Hanwool Lee; Sara Yu; Yewon Hwang; Jonghyun Choi; Heejae Ahn; Sungbum Jung; Youngjae Yu
>
> **备注:** Under Review
>
> **摘要:** General-purpose sentence embedding models often struggle to capture specialized financial semantics, especially in low-resource languages like Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual embedding models fine-tuned with 18.8K high-confidence triplets that pair in-domain paraphrases, hard negatives derived from a semantic-shift typology, and exact Korean-English translations. Concurrently, we release KorFinSTS, a 1,921-pair Korean financial STS benchmark spanning news, disclosures, research reports, and regulations, designed to expose nuances that general benchmarks miss. When evaluated against seven open-license baselines, NMIXX's multilingual bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and +0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing other models by the largest margin, while revealing a modest trade-off in general STS performance. Our analysis further shows that models with richer Korean token coverage adapt more effectively, underscoring the importance of tokenizer design in low-resource, cross-lingual settings. By making both models and the benchmark publicly available, we provide the community with robust tools for domain-adapted, multilingual representation learning in finance.
>
---
#### [new 027] Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在评估长文本语言模型（LCLMs）在长上下文引用方面的能力。论文提出了Ref-Long基准，包含三个子集，要求模型识别引用特定关键词的文档。实验发现包括GPT-4o在内的模型在该任务上表现不佳，作者进一步通过多种分析探讨挑战所在。**

- **链接: [http://arxiv.org/pdf/2507.09506v1](http://arxiv.org/pdf/2507.09506v1)**

> **作者:** Junjie Wu; Gefei Gu; Yanan Zheng; Dit-Yan Yeung; Arman Cohan
>
> **备注:** ACL 2025 Main Conference. First 2 authors contributed equally
>
> **摘要:** Long-context language models (LCLMs) have exhibited impressive capabilities in long-context understanding tasks. Among these, long-context referencing -- a crucial task that requires LCLMs to attribute items of interest to specific parts of long-context data -- remains underexplored. To bridge this gap, this paper proposes Referencing Evaluation for Long-context Language Models (Ref-Long), a novel benchmark designed to assess the long-context referencing capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the indexes of documents that reference a specific key, emphasizing contextual relationships between the key and the documents over simple retrieval. Based on the task design, we construct three subsets ranging from synthetic to realistic scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs reveal significant shortcomings in long-context referencing, even among advanced models like GPT-4o. To further investigate these challenges, we conduct comprehensive analyses, including human evaluations, task format adjustments, fine-tuning experiments, and error analyses, leading to several key insights. Our data and code can be found in https://github. com/wujunjie1998/Ref-Long.
>
---
#### [new 028] GeLaCo: An Evolutionary Approach to Layer Compression
- **分类: cs.CL**

- **简介: 该论文属于模型压缩任务，旨在解决大语言模型因计算需求高而难以部署的问题。论文提出GeLaCo方法，通过进化算法进行层压缩，利用种群搜索和模块相似性适应度函数优化压缩方案，并支持单/多目标优化，建立压缩与质量的帕累托前沿，验证显示其性能优于现有方法。**

- **链接: [http://arxiv.org/pdf/2507.10059v1](http://arxiv.org/pdf/2507.10059v1)**

> **作者:** David Ponce; Thierry Etchegoyhen; Javier Del Ser
>
> **摘要:** Large Language Models (LLM) have achieved remarkable performance across a large number of tasks, but face critical deployment and usage barriers due to substantial computational requirements. Model compression methods, which aim to reduce model size while preserving its capacity, are an important means to mitigate these issues. Promising approaches along these lines, such as structured pruning, typically require costly empirical search for optimal variants and may run the risk of ignoring better solutions. In this work we introduce GeLaCo, an evolutionary approach to LLM compression via layer collapse. Our approach supports an efficient exploration of the compression solution space via population-based search and a module-wise similarity fitness function capturing attention, feed-forward, and hidden state representations. GeLaCo also supports both single and multi-objective evolutionary compression search, establishing the first Pareto frontier along compression and quality axes. We evaluate GeLaCo solutions via both perplexity-based and generative evaluations over foundational and instruction-tuned models, outperforming state-of-the-art alternatives.
>
---
#### [new 029] An Exploration of Knowledge Editing for Arabic
- **分类: cs.CL**

- **简介: 该论文研究阿拉伯语的知识编辑（KE）任务，解决其在形态丰富语言中的应用问题。作者评估了四种方法在阿拉伯语基准数据上的表现，发现参数方法跨语言泛化能力弱，而指令调优方法更稳健。他们扩展了LTE方法至多语言设置，提升了阿拉伯语和英语的编辑与迁移效果，并公开了相关数据与模型。**

- **链接: [http://arxiv.org/pdf/2507.09629v1](http://arxiv.org/pdf/2507.09629v1)**

> **作者:** Basel Mousi; Nadir Durrani; Fahim Dalvi
>
> **摘要:** While Knowledge Editing (KE) has been widely explored in English, its behavior in morphologically rich languages like Arabic remains underexamined. In this work, we present the first study of Arabic KE. We evaluate four methods (ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact benchmarks, analyzing both multilingual and cross-lingual settings. Our experiments on Llama-2-7B-chat show show that parameter-based methods struggle with cross-lingual generalization, while instruction-tuned methods perform more robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show that joint Arabic-English training improves both editability and transfer. We release Arabic KE benchmarks and multilingual training for LTE data to support future research.
>
---
#### [new 030] MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决大型语言模型在跨文化理解中的文化偏见和公平性问题。作者提出了MCEval框架，通过动态构建文化问题和因果分析方法，评估13种语言和文化下的模型表现，揭示了语言与文化一致性对模型性能的影响，并强调了英文场景下方法的不公平性。**

- **链接: [http://arxiv.org/pdf/2507.09701v1](http://arxiv.org/pdf/2507.09701v1)**

> **作者:** Shulin Huang; Linyi Yang; Yue Zhang
>
> **摘要:** Large language models exhibit cultural biases and limited cross-cultural understanding capabilities, particularly when serving diverse global user populations. We propose MCEval, a novel multilingual evaluation framework that employs dynamic cultural question construction and enables causal analysis through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive evaluation spans 13 cultures and 13 languages, systematically assessing both cultural awareness and cultural bias across different linguistic scenarios. The framework provides 39,897 cultural awareness instances and 17,940 cultural bias instances. Experimental results reveal performance disparities across different linguistic scenarios, demonstrating that optimal cultural performance is not only linked to training data distribution, but also is related to language-culture alignment. The evaluation results also expose the fairness issue, where approaches appearing successful in the English scenario create substantial disadvantages. MCEval represents the first comprehensive multilingual cultural evaluation framework that provides deeper insights into LLMs' cultural understanding.
>
---
#### [new 031] Can You Detect the Difference?
- **分类: cs.CL; cs.AI; I.2.7; H.3.3**

- **简介: 该论文属于自然语言处理任务，旨在解决检测扩散模型生成文本的问题。论文比较了扩散模型（LLaDA）与自回归模型（LLaMA）生成文本的特征，发现LLaDA在多项指标上接近人类文本，导致现有检测方法效果下降，强调需开发针对扩散模型的检测技术。**

- **链接: [http://arxiv.org/pdf/2507.10475v1](http://arxiv.org/pdf/2507.10475v1)**

> **作者:** İsmail Tarım; Aytuğ Onan
>
> **备注:** 11 pages, 3 figures, 2 tables. Code and data: https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for AI-safety relevance
>
> **摘要:** The rapid advancement of large language models (LLMs) has raised concerns about reliably detecting AI-generated text. Stylometric metrics work well on autoregressive (AR) outputs, but their effectiveness on diffusion-based models is unknown. We present the first systematic comparison of diffusion-generated text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that LLaDA closely mimics human text in perplexity and burstiness, yielding high false-negative rates for AR-oriented detectors. LLaMA shows much lower perplexity but reduced lexical fidelity. Relying on any single metric fails to separate diffusion outputs from human writing. We highlight the need for diffusion-aware detectors and outline directions such as hybrid models, diffusion-specific stylometric signatures, and robust watermarking.
>
---
#### [new 032] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation
- **分类: cs.CL; cs.LG**

- **简介: 该论文属于自然语言处理任务，旨在解决大模型训练和部署中的高计算与内存成本问题。论文提出Mixture-of-Recursions（MoR）框架，通过参数共享与自适应递归深度机制，实现更高效的Token级计算，降低验证困惑度和预填充延迟，提升吞吐量，形成新的效率前沿。**

- **链接: [http://arxiv.org/pdf/2507.10524v1](http://arxiv.org/pdf/2507.10524v1)**

> **作者:** Sangmin Bae; Yujin Kim; Reza Bayat; Sungnyun Kim; Jiyoun Ha; Tal Schuster; Adam Fisch; Hrayr Harutyunyan; Ziwei Ji; Aaron Courville; Se-Young Yun
>
> **备注:** 36 pages, 9 figures, 14 tables, codes at https://github.com/raymin0223/mixture_of_recursions
>
> **摘要:** Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.
>
---
#### [new 033] Adapting Definition Modeling for New Languages: A Case Study on Belarusian
- **分类: cs.CL**

- **简介: 该论文属于定义建模任务，旨在帮助词典编纂者为更多语言创建定义。论文聚焦于将现有模型适配至白俄罗斯语，提出了一个包含43,150条定义的新数据集。研究表明，定义建模系统仅需少量数据即可适配新语言，但当前自动评估指标仍存在不足。**

- **链接: [http://arxiv.org/pdf/2507.09536v1](http://arxiv.org/pdf/2507.09536v1)**

> **作者:** Daniela Kazakouskaya; Timothee Mickus; Janine Siewert
>
> **备注:** To appear at SlavicNLP 2025
>
> **摘要:** Definition modeling, the task of generating new definitions for words in context, holds great prospect as a means to assist the work of lexicographers in documenting a broader variety of lects and languages, yet much remains to be done in order to assess how we can leverage pre-existing models for as-of-yet unsupported languages. In this work, we focus on adapting existing models to Belarusian, for which we propose a novel dataset of 43,150 definitions. Our experiments demonstrate that adapting a definition modeling systems requires minimal amounts of data, but that there currently are gaps in what automatic metrics do capture.
>
---
#### [new 034] RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking
- **分类: cs.CL**

- **简介: 该论文属于多媒体事实核查任务，旨在解决多模态虚假信息检测中因声明模糊或缺乏上下文带来的挑战。论文提出了RAMA框架，通过策略性查询生成、跨源证据聚合和多代理集成架构，提升自动化核查效果。实验表明其在处理模糊或似是而非的声明上表现优异。**

- **链接: [http://arxiv.org/pdf/2507.09174v1](http://arxiv.org/pdf/2507.09174v1)**

> **作者:** Shuo Yang; Zijian Yu; Zhenzhe Ying; Yuqin Dai; Guoqing Wang; Jun Lan; Jinfeng Xu; Jinze Li; Edith C. H. Ngai
>
> **摘要:** The rapid proliferation of multimodal misinformation presents significant challenges for automated fact-checking systems, especially when claims are ambiguous or lack sufficient context. We introduce RAMA, a novel retrieval-augmented multi-agent framework designed for verifying multimedia misinformation. RAMA incorporates three core innovations: (1) strategic query formulation that transforms multimodal claims into precise web search queries; (2) cross-verification evidence aggregation from diverse, authoritative sources; and (3) a multi-agent ensemble architecture that leverages the complementary strengths of multiple multimodal large language models and prompt variants. Extensive experiments demonstrate that RAMA achieves superior performance on benchmark datasets, particularly excelling in resolving ambiguous or improbable claims by grounding verification in retrieved factual evidence. Our findings underscore the necessity of integrating web-based evidence and multi-agent reasoning for trustworthy multimedia verification, paving the way for more reliable and scalable fact-checking solutions. RAMA will be publicly available at https://github.com/kalendsyang/RAMA.git.
>
---
#### [new 035] DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models
- **分类: cs.CL**

- **简介: 该论文属于数据归因评估任务，旨在解决大型语言模型（LLM）中训练数据对输出影响的量化问题。作者提出了DATE-LM基准，通过三个关键任务评估数据归因方法，并发布公开排行榜促进研究进展。**

- **链接: [http://arxiv.org/pdf/2507.09424v1](http://arxiv.org/pdf/2507.09424v1)**

> **作者:** Cathy Jiao; Yijun Pan; Emily Xiao; Daisy Sheng; Niket Jain; Hanzhang Zhao; Ishita Dasgupta; Jiaqi W. Ma; Chenyan Xiong
>
> **摘要:** Data attribution methods quantify the influence of training data on model outputs and are becoming increasingly relevant for a wide range of LLM research and applications, including dataset curation, model interpretability, data valuation. However, there remain critical gaps in systematic LLM-centric evaluation of data attribution methods. To this end, we introduce DATE-LM (Data Attribution Evaluation in Language Models), a unified benchmark for evaluating data attribution methods through real-world LLM applications. DATE-LM measures attribution quality through three key tasks -- training data selection, toxicity/bias filtering, and factual attribution. Our benchmark is designed for ease of use, enabling researchers to configure and run large-scale evaluations across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to conduct a large-scale evaluation of existing data attribution methods. Our findings show that no single method dominates across all tasks, data attribution methods have trade-offs with simpler baselines, and method performance is sensitive to task-specific evaluation design. Finally, we release a public leaderboard for quick comparison of methods and to facilitate community engagement. We hope DATE-LM serves as a foundation for future data attribution research in LLMs.
>
---
#### [new 036] Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决藏语因高质量语料稀缺导致的大模型表现不足问题。作者构建了最大藏语预训练语料库，并提出多语言大模型Banzhida，通过持续预训练提升其藏语生成能力，最终在多项任务上显著优于现有模型。**

- **链接: [http://arxiv.org/pdf/2507.09205v1](http://arxiv.org/pdf/2507.09205v1)**

> **作者:** Leiyu Pan; Bojian Xiong; Lei Yang; Renren Jin; Shaowei Zhang; Yue Chen; Ling Shi; Jiang Zhou; Junru Wu; Zhen Wang; Jianxiang Peng; Juesi Xiao; Tianyu Dong; Zhuowen Han; Zhuo Chen; Sangjee Dondrub; Caizang Tai; Haixing Zhao; Huaque Cairang; Suonan Cairang; Rou Te; Lengben Zhaxi; Gazang Zhaxi; Zhonglin Ye; Yuhui Zheng; Chunyan Peng; Secha Jia; Pema Tashi; Cizhen Jiacuo; Pema Dorjee; Hongkai Liu; Pema Yanggon; Tsehang Dorjee; Jiaxin Han; Qiongying Hu; Jilin Man; Huanke You; Yuqi Ren; Duo La; Deyi Xiong
>
> **摘要:** Large language models have achieved remarkable progress across many languages. However, Tibetan, as a representative low-resource language, is particularly underrepresented in existing models due to the scarcity of high-quality training corpora. To address this gap, we curate the largest Tibetan pre-training corpus to date, aggregating data from diverse sources and applying a dedicated data cleaning and processing pipeline tailored for Tibetan. With the curated data, we continue pre/post-training a multilingual base model into Banzhida, a multilingual large language model that advances generative AI for Tibetan. To evaluate the Tibetan capabilities of the model, we create new high-quality Tibetan benchmarks, and complement them with existing public benchmarks. Experimental results demonstrate that Banzhida consistently and significantly outperforms both open-source models of similar scale and Tibetan-tailored models across a wide range of tasks.
>
---
#### [new 037] Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis
- **分类: cs.CL**

- **简介: 该论文属于方面情感分析任务，旨在解决训练数据不足和不平衡导致模型性能下降的问题。通过使用大语言模型生成增强数据，并引入强化学习优化数据质量，提升了模型效果。**

- **链接: [http://arxiv.org/pdf/2507.09485v1](http://arxiv.org/pdf/2507.09485v1)**

> **作者:** Junjie Liu; Yuanhe Tian; Yan Song
>
> **摘要:** Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in social media scenarios to identify the sentiment polarity of specific aspect terms in a sentence. Although many existing studies leverage large language models (LLMs) to perform ABSA due to their strong context understanding capabilities, they still face challenges to learn the context information in the running text because of the short text, as well as the small and unbalanced labeled training data, where most data are labeled with positive sentiment. Data augmentation (DA) is a feasible strategy for providing richer contextual information, especially when using LLMs to create synthetic training data, but faces challenges in ensuring a high quality of the augmented data.In this paper, we propose an LLM-based ABSA approach with training data augmentation.Specifically, an LLM is prompted to generate augmented training data based on the original training data, so as to construct a new training data with larger size and balanced label distributions to better train an ABSA model. Meanwhile, in order to improve the quality of the augmented data, we propose a reinforcement learning approach to optimize the data augmentation. LLM.Experiment results and further analyses on English benchmark datasets for ABSA demonstrate the effectiveness of our approach, where superior performance is observed over strong baselines and most existing studies.
>
---
#### [new 038] Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation
- **分类: cs.CL**

- **简介: 该论文属于提示优化任务，旨在解决小型语言模型对提示设计敏感、难以高效优化复杂任务提示的问题。作者提出一种基于语法引导的进化搜索方法，结合遗传编程与局部搜索，自动生成并微调提示程序，在多个小模型和复杂任务上表现优于现有方法。**

- **链接: [http://arxiv.org/pdf/2507.10326v1](http://arxiv.org/pdf/2507.10326v1)**

> **作者:** Muzhaffar Hazman; Minh-Khoi Pham; Shweta Soundararajan; Goncalo Mordido; Leonardo Custode; David Lynch; Giorgio Cruciata; Yucheng Shi; Hongmeng Song; Wang Chao; Pan Yue; Aleksandar Milenovic; Alexandros Agapitos
>
> **备注:** Accepted for Publication at ECAI 2025
>
> **摘要:** Prompt engineering has proven to be a crucial step in leveraging pretrained large language models (LLMs) in solving various real-world tasks. Numerous solutions have been proposed that seek to automate prompt engineering by using the model itself to edit prompts. However, the majority of state-of-the-art approaches are evaluated on tasks that require minimal prompt templates and on very large and highly capable LLMs. In contrast, solving complex tasks that require detailed information to be included in the prompt increases the amount of text that needs to be optimised. Furthermore, smaller models have been shown to be more sensitive to prompt design. To address these challenges, we propose an evolutionary search approach to automated discrete prompt optimisation consisting of two phases. In the first phase, grammar-guided genetic programming is invoked to synthesise prompt-creating programmes by searching the space of programmes populated by function compositions of syntactic, dictionary-based and LLM-based prompt-editing functions. In the second phase, local search is applied to explore the neighbourhoods of best-performing programmes in an attempt to further fine-tune their performance. Our approach outperforms three state-of-the-art prompt optimisation approaches, PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose LLMs in four domain-specific challenging tasks. We also illustrate several examples where these benchmark methods suffer relatively severe performance degradation, while our approach improves performance in almost all task-model combinations, only incurring minimal degradation when it does not.
>
---
#### [new 039] Self-Improving Model Steering
- **分类: cs.CL**

- **简介: 该论文属于模型对齐任务，旨在解决传统模型转向方法依赖外部标注数据的问题。作者提出了SIMS框架，通过自主生成和优化对比样本实现无监督的自改进模型转向。**

- **链接: [http://arxiv.org/pdf/2507.08967v1](http://arxiv.org/pdf/2507.08967v1)**

> **作者:** Rongyi Zhu; Yuhui Wang; Tanqiu Jiang; Jiacheng Liang; Ting Wang
>
> **备注:** 16 pages, 9 figures
>
> **摘要:** Model steering represents a powerful technique that dynamically aligns large language models (LLMs) with human preferences during inference. However, conventional model-steering methods rely heavily on externally annotated data, not only limiting their adaptability to varying contexts but also tethering their effectiveness to annotation quality. In this paper, we present SIMS, the first self-improving model-steering framework that operates without relying on external supervision. At its core, SIMS autonomously generates and refines contrastive samples through iterative self-improvement cycles, enabling adaptive, context-specific steering. Additionally, SIMS employs novel strategies, including prompt ranking and contrast sampling, to further enhance steering efficacy. Extensive evaluation across diverse LLMs and benchmarks demonstrates that SIMS substantially outperforms existing methods in steering effectiveness and adaptability, highlighting self-improving model steering as a promising direction for future research on inference-time LLM alignment.
>
---
#### [new 040] Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces
- **分类: cs.CL; cs.LG**

- **简介: 该论文研究大语言模型（LLMs）的潜在空间几何结构，旨在理解其如何组织语义表示。作者通过分析11个解码器模型、6个科学主题和12层隐藏状态，发现高层语义信息集中在低维线性可分子空间中，尤其在深层和结构推理提示下更明显。基于此，他们提出几何感知工具，如基于传输的防御方法，并用MLP分类器作为轻量级潜空间防护措施，有效检测对抗和恶意提示。**

- **链接: [http://arxiv.org/pdf/2507.09709v1](http://arxiv.org/pdf/2507.09709v1)**

> **作者:** Baturay Saglam; Paul Kassianik; Blaine Nelson; Sajana Weerawardhena; Yaron Singer; Amin Karbasi
>
> **摘要:** Understanding the latent space geometry of large language models (LLMs) is key to interpreting their behavior and improving alignment. \baturay{However, it remains unclear to what extent LLMs internally organize representations related to semantic understanding. To investigate this, we conduct a large-scale empirical study of hidden states in transformer-based LLMs, analyzing 11 decoder-only models across 6 scientific topics and 12 layers each. We find that high-level semantic information consistently lies in low-dimensional subspaces that form linearly separable representations across distinct domains. This separability becomes more pronounced in deeper layers and under prompts that trigger structured reasoning or alignment behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry enables simple yet effective causal interventions in hidden space; for example, reasoning patterns like chain-of-thought can be captured by a single vector direction. Together, these findings support the development of geometry-aware tools that operate directly on latent representations to detect and mitigate harmful or adversarial content, using methods such as transport-based defenses that leverage this separability. As a proof of concept, we demonstrate this potential by training a simple MLP classifier as a lightweight latent-space guardrail, which detects adversarial and malicious prompts with high precision.
>
---
#### [new 041] Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于自然语言处理任务，旨在提升大模型的复杂推理能力。针对现有ReFT方法在固定位置修改表示效果不佳的问题，作者提出CRFT方法，通过分析信息流识别关键表示并进行优化。实验表明，CRFT在多个推理任务上表现优异，尤其在少样本设置下显著提升准确率。**

- **链接: [http://arxiv.org/pdf/2507.10085v1](http://arxiv.org/pdf/2507.10085v1)**

> **作者:** Chenxi Huang; Shaotian Yan; Liang Xie; Binbin Lin; Sinan Fan; Yue Xin; Deng Cai; Chen Shen; Jieping Ye
>
> **备注:** Accepted by ACL 2025
>
> **摘要:** Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient Fine-Tuning (PEFT) method, has attracted widespread attention for significantly improving parameter efficiency by editing representation space alone. In this work, we investigate applying ReFT to complex reasoning tasks. However, directly using the native ReFT method, which modifies fixed representations at the beginning and end of each layer, yields suboptimal performance, as these fixed-position representations have uncertain impact on the outputs. We observe that, in complex reasoning tasks, there often exist certain critical representations. These representations either integrate significant information from preceding layers or regulate subsequent layer representations. Through layer-by-layer propagation, they exert a substantial influence on the final output. Naturally, fine-tuning these critical representations has the potential to greatly enhance reasoning performance. Building upon these insights, we propose Critical Representation Fine-Tuning (CRFT), a novel method that identifies and optimizes these critical representations through information flow analysis. CRFT operates within a supervised learning framework, dynamically optimizing critical representations in a low-rank linear subspace while freezing the base model. The effectiveness and efficiency of our method are validated across eight benchmarks for arithmetic and commonsense reasoning, using LLaMA and Mistral model families. Furthermore, our method also adapts effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work highlights the untapped potential of representation-level optimization for CoT reasoning, offering a lightweight yet powerful alternative to traditional PEFT methods.
>
---
#### [new 042] SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks
- **分类: cs.CL**

- **简介: 该论文介绍了SpreadPy工具，用于模拟认知网络中的激活扩散与超扩散过程。任务是分析认知结构与功能关系，解决如何通过网络模型揭示个体差异与认知障碍问题。工作包括开发仿真框架，并通过三个案例研究验证其应用价值。**

- **链接: [http://arxiv.org/pdf/2507.09628v1](http://arxiv.org/pdf/2507.09628v1)**

> **作者:** Salvatore Citraro; Edith Haim; Alessandra Carini; Cynthia S. Q. Siew; Giulio Rossetti; Massimo Stella
>
> **摘要:** We introduce SpreadPy as a Python library for simulating spreading activation in cognitive single-layer and multiplex networks. Our tool is designed to perform numerical simulations testing structure-function relationships in cognitive processes. By comparing simulation results with grounded theories in knowledge modelling, SpreadPy enables systematic investigations of how activation dynamics reflect cognitive, psychological and clinical phenomena. We demonstrate the library's utility through three case studies: (1) Spreading activation on associative knowledge networks distinguishes students with high versus low math anxiety, revealing anxiety-related structural differences in conceptual organization; (2) Simulations of a creativity task show that activation trajectories vary with task difficulty, exposing how cognitive load modulates lexical access; (3) In individuals with aphasia, simulated activation patterns on lexical networks correlate with empirical error types (semantic vs. phonological) during picture-naming tasks, linking network structure to clinical impairments. SpreadPy's flexible framework allows researchers to model these processes using empirically derived or theoretical networks, providing mechanistic insights into individual differences and cognitive impairments. The library is openly available, supporting reproducible research in psychology, neuroscience, and education research.
>
---
#### [new 043] REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务中的模型评估领域，旨在解决当前大型推理模型评估方法受限于单一问题测试、无法有效评估多任务压力下的模型表现问题。论文提出了REST框架，通过同时测试多个问题，评估模型在真实场景下的推理能力，发现现有模型在多任务下的性能下降，并提供有效的评估手段。**

- **链接: [http://arxiv.org/pdf/2507.10541v1](http://arxiv.org/pdf/2507.10541v1)**

> **作者:** Zhuoshi Pan; Qizhi Pei; Yu Li; Qiyao Sun; Zinan Tang; H. Vicky Zhao; Conghui He; Lijun Wu
>
> **备注:** REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously
>
> **摘要:** Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST specifically evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key mechanistic insights emerge from our analysis: (1) the "overthinking trap" is a critical factor contributing to the performance degradation; (2) the models trained with "long2short" technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation.
>
---
#### [new 044] Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects
- **分类: cs.CL; cs.AI**

- **简介: 该论文提出了Absher基准，用于评估大语言模型对沙特方言的理解能力。任务是设计覆盖六类问题的测试集，解决LLM在区域方言和文化理解上的不足。工作包括构建18,000多道题目，并评估现有模型性能。**

- **链接: [http://arxiv.org/pdf/2507.10216v1](http://arxiv.org/pdf/2507.10216v1)**

> **作者:** Renad Al-Monef; Hassan Alhuzali; Nora Alturayeif; Ashwag Alasmari
>
> **摘要:** As large language models (LLMs) become increasingly central to Arabic NLP applications, evaluating their understanding of regional dialects and cultural nuances is essential, particularly in linguistically diverse settings like Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark specifically designed to assess LLMs performance across major Saudi dialects. \texttt{Absher} comprises over 18,000 multiple-choice questions spanning six distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage, Cultural Interpretation, and Location Recognition. These questions are derived from a curated dataset of dialectal words, phrases, and proverbs sourced from various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs, including multilingual and Arabic-specific models. We also provide detailed insights into their capabilities and limitations. Our results reveal notable performance gaps, particularly in tasks requiring cultural inference or contextual understanding. Our findings highlight the urgent need for dialect-aware training and culturally aligned evaluation methodologies to improve LLMs performance in real-world Arabic applications.
>
---
#### [new 045] From Sequence to Structure: Uncovering Substructure Reasoning in Transformers
- **分类: cs.CL; cs.AI**

- **简介: 该论文研究Transformer模型如何从文本序列中理解图结构，解决“Transformer如何捕捉图子结构”这一问题。作者提出“诱导子结构过滤”理论框架，分析多层Transformer内部机制，并验证其在不同图类型中的有效性，揭示了Transformer处理图数据的潜在能力。**

- **链接: [http://arxiv.org/pdf/2507.10435v1](http://arxiv.org/pdf/2507.10435v1)**

> **作者:** Xinnan Dai; Kai Yang; Jay Revolinsky; Kai Guo; Aoran Wang; Bohang Zhang; Jiliang Tang
>
> **摘要:** Recent studies suggest that large language models (LLMs) possess the capability to solve graph reasoning tasks. Notably, even when graph structures are embedded within textual descriptions, LLMs can still effectively answer related questions. This raises a fundamental question: How can a decoder-only Transformer architecture understand underlying graph structures? To address this, we start with the substructure extraction task, interpreting the inner mechanisms inside the transformers and analyzing the impact of the input queries. Specifically, through both empirical results and theoretical analysis, we present Induced Substructure Filtration (ISF), a perspective that captures the substructure identification in the multi-layer transformers. We further validate the ISF process in LLMs, revealing consistent internal dynamics across layers. Building on these insights, we explore the broader capabilities of Transformers in handling diverse graph types. Specifically, we introduce the concept of thinking in substructures to efficiently extract complex composite patterns, and demonstrate that decoder-only Transformers can successfully extract substructures from attributed graphs, such as molecular graphs. Together, our findings offer a new insight on how sequence-based Transformers perform the substructure extraction task over graph data.
>
---
#### [new 046] Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于社会科学研究任务，旨在评估大语言模型（LLMs）是否能代表多样化的文化道德观。研究通过道德基础问卷分析19种文化背景下的AI与人类道德直觉差异，发现LLMs未能准确反映文化多样性，模型规模增大也未显著改善表现。论文指出当前LLMs在道德对齐方面存在局限性，呼吁改进对齐方法和评估标准。**

- **链接: [http://arxiv.org/pdf/2507.10073v1](http://arxiv.org/pdf/2507.10073v1)**

> **作者:** Simon Münker
>
> **备注:** 15pages, 1 figure, 2 tables
>
> **摘要:** Are AI systems truly representing human values, or merely averaging across them? Our study suggests a concerning reality: Large Language Models (LLMs) fail to represent diverse cultural moral frameworks despite their linguistic capabilities. We expose significant gaps between AI-generated and human moral intuitions by applying the Moral Foundations Questionnaire across 19 cultural contexts. Comparing multiple state-of-the-art LLMs' origins against human baseline data, we find these models systematically homogenize moral diversity. Surprisingly, increased model size doesn't consistently improve cultural representation fidelity. Our findings challenge the growing use of LLMs as synthetic populations in social science research and highlight a fundamental limitation in current AI alignment approaches. Without data-driven alignment beyond prompting, these systems cannot capture the nuanced, culturally-specific moral intuitions. Our results call for more grounded alignment objectives and evaluation metrics to ensure AI systems represent diverse human values rather than flattening the moral landscape.
>
---
#### [new 047] MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking
- **分类: cs.CL**

- **简介: 论文提出MLAR，一种基于多层大语言模型的招聘流程自动化系统，旨在解决传统招聘中简历筛选效率低的问题。通过三层LLM结构提取岗位特征、解析简历信息并进行语义匹配，实现高效候选人推荐。实验表明MLAR在处理速度和匹配准确性上优于主流RPA平台。**

- **链接: [http://arxiv.org/pdf/2507.10472v1](http://arxiv.org/pdf/2507.10472v1)**

> **作者:** Mohamed T. Younes; Omar Walid; Mai Hassan; Ali Hamdi
>
> **摘要:** This paper introduces an innovative Applicant Tracking System (ATS) enhanced by a novel Robotic process automation (RPA) framework or as further referred to as MLAR. Traditional recruitment processes often encounter bottlenecks in resume screening and candidate shortlisting due to time and resource constraints. MLAR addresses these challenges employing Large Language Models (LLMs) in three distinct layers: extracting key characteristics from job postings in the first layer, parsing applicant resume to identify education, experience, skills in the second layer, and similarity matching in the third layer. These features are then matched through advanced semantic algorithms to identify the best candidates efficiently. Our approach integrates seamlessly into existing RPA pipelines, automating resume parsing, job matching, and candidate notifications. Extensive performance benchmarking shows that MLAR outperforms the leading RPA platforms, including UiPath and Automation Anywhere, in high-volume resume-processing tasks. When processing 2,400 resumes, MLAR achieved an average processing time of 5.4 seconds per resume, reducing processing time by approximately 16.9% compared to Automation Anywhere and 17.1% compared to UiPath. These results highlight the potential of MLAR to transform recruitment workflows by providing an efficient, accurate, and scalable solution tailored to modern hiring needs.
>
---
#### [new 048] Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale
- **分类: cs.CL**

- **简介: 该论文属于文档信息抽取任务，旨在解决从复杂财务文档中准确提取表格和键值对的问题。作者提出了Spatial ModernBERT模型，结合空间嵌入与Transformer架构，通过多分类头识别标签、行列结构，并设计后处理方法重建表格布局，实现高效精准的信息抽取。**

- **链接: [http://arxiv.org/pdf/2507.08865v1](http://arxiv.org/pdf/2507.08865v1)**

> **作者:** Javis AI Team; Amrendra Singh; Maulik Shah; Dharshan Sampath
>
> **摘要:** Extracting tables and key-value pairs from financial documents is essential for business workflows such as auditing, data analytics, and automated invoice processing. In this work, we introduce Spatial ModernBERT-a transformer-based model augmented with spatial embeddings-to accurately detect and extract tabular data and key-value fields from complex financial documents. We cast the extraction task as token classification across three heads: (1) Label Head, classifying each token as a label (e.g., PO Number, PO Date, Item Description, Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices; (3) Row Head, distinguishing the start of item rows and header rows. The model is pretrained on the PubTables-1M dataset, then fine-tuned on a financial document dataset, achieving robust performance through cross-entropy loss on each classification head. We propose a post-processing method to merge tokens using B-I-IB tagging, reconstruct the tabular layout, and extract key-value pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages both textual and spatial cues, facilitating highly accurate table and key-value extraction in real-world financial documents.
>
---
#### [new 049] Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于自然语言处理任务，旨在解决大型语言模型在多步推理与事实准确性上的不足。论文综述了将检索增强生成（RAG）与深度推理结合的方法，提出了优化RAG各阶段的推理增强策略、利用外部知识支持复杂推理的RAG增强方法，以及融合检索与推理的协同框架，目标是构建更高效、可信、以人为本的RAG-Reasoning系统。**

- **链接: [http://arxiv.org/pdf/2507.09477v1](http://arxiv.org/pdf/2507.09477v1)**

> **作者:** Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu
>
> **备注:** submitted to ARR May
>
> **摘要:** Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.
>
---
#### [new 050] From BERT to Qwen: Hate Detection across architectures
- **分类: cs.CL; cs.LG**

- **简介: 该论文属于自然语言处理任务，旨在解决在线平台识别仇恨言论的问题。通过对比传统双向Transformer编码器与新一代超大规模自回归语言模型（如BERT与Qwen），评估其在真实网络文本中进行仇恨言论检测的效果，验证模型规模提升是否带来实际性能改进。**

- **链接: [http://arxiv.org/pdf/2507.10468v1](http://arxiv.org/pdf/2507.10468v1)**

> **作者:** Ariadna Mon; Saúl Fenollosa; Jon Lecumberri
>
> **备注:** 4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)
>
> **摘要:** Online platforms struggle to curb hate speech without over-censoring legitimate discourse. Early bidirectional transformer encoders made big strides, but the arrival of ultra-large autoregressive LLMs promises deeper context-awareness. Whether this extra scale actually improves practical hate-speech detection on real-world text remains unverified. Our study puts this question to the test by benchmarking both model families, classic encoders and next-generation LLMs, on curated corpora of online interactions for hate-speech detection (Hate or No Hate).
>
---
#### [new 051] Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation
- **分类: cs.CL; cs.AI; 68T50; I.2.7; H.5.2**

- **简介: 该论文属于语音情感识别任务，旨在解决长序列语音处理中上下文信息丢失的问题。作者提出动态参数记忆机制（DPM），通过临时LoRA模块在推理过程中编码句子级情感与语义，使模型能有效处理超长语音序列。实验表明该方法在IEMOCAP数据集上取得最优性能。**

- **链接: [http://arxiv.org/pdf/2507.09076v1](http://arxiv.org/pdf/2507.09076v1)**

> **作者:** Jialong Mai; Xiaofen Xing; Yawei Li; Zhipeng Li; Jingyuan Xing; Xiangmin Xu
>
> **备注:** submitted to EMNLP 2025
>
> **摘要:** Recent research has focused on applying speech large language model (SLLM) to improve speech emotion recognition (SER). However, the inherently high frame rate in speech modality severely limits the signal processing and understanding capabilities of SLLM. For example, a SLLM with a 4K context window can only process 80 seconds of audio at 50Hz feature sampling rate before reaching its capacity limit. Input token compression methods used in SLLM overlook the continuity and inertia of emotions across multiple conversation turns. This paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding, enabling processing of unlimited-length audio with limited context windows in SLLM. Specifically, DPM progressively encodes sentence-level information and emotions into a temporary LoRA module during inference to effectively "memorize" the contextual information. We trained an emotion SLLM as a backbone and incorporated our DPM into inference for emotion recognition in conversation (ERC). Experimental results on the IEMOCAP dataset show that DPM significantly improves the emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance.
>
---
#### [new 052] Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition
- **分类: cs.CL; cs.AI; cs.LG**

- **简介: 该论文研究大语言模型在上下文学习中任务泛化的能力，聚焦于“off-by-one加法”这一抽象任务，通过可解释性技术分析模型内部机制，揭示了函数归纳机制及其在多任务中的复用性。**

- **链接: [http://arxiv.org/pdf/2507.09875v1](http://arxiv.org/pdf/2507.09875v1)**

> **作者:** Qinyuan Ye; Robin Jia; Xiang Ren
>
> **备注:** Code: https://github.com/INK-USC/function-induction
>
> **摘要:** Large language models demonstrate the intriguing ability to perform unseen tasks via in-context learning. However, it remains unclear what mechanisms inside the model drive such task-level generalization. In this work, we approach this question through the lens of off-by-one addition (i.e., 1+1=3, 2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function as a second step. Leveraging circuit-style interpretability techniques such as path patching, we analyze the models' internal computations behind their notable performance and present three key findings. First, we uncover a function induction mechanism that explains the model's generalization from standard addition to off-by-one addition. This mechanism resembles the structure of the induction head mechanism found in prior work and elevates it to a higher level of abstraction. Second, we show that the induction of the +1 function is governed by multiple attention heads in parallel, each of which emits a distinct piece of the +1 function. Finally, we find that this function induction mechanism is reused in a broader range of tasks, including synthetic tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8 addition. Overall, our findings offer deeper insights into how reusable and composable structures within language models enable task-level generalization.
>
---
#### [new 053] Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach
- **分类: cs.CL; cs.LG**

- **简介: 该论文属于自然语言处理任务，旨在提升模型对词替换攻击的鲁棒性和泛化能力。作者提出了一种基于增长界矩阵（GBM）的正则化方法，计算并优化LSTM、S4和CNN架构的GBM，以减轻输入扰动对输出的影响，从而增强对抗攻击的防御效果，并改善在干净文本上的表现。**

- **链接: [http://arxiv.org/pdf/2507.10330v1](http://arxiv.org/pdf/2507.10330v1)**

> **作者:** Mohammed Bouri; Adnane Saoud
>
> **备注:** Accepted to ACL Findings 2025
>
> **摘要:** Despite advancements in Natural Language Processing (NLP), models remain vulnerable to adversarial attacks, such as synonym substitutions. While prior work has focused on improving robustness for feed-forward and convolutional architectures, the robustness of recurrent networks and modern state space models (SSMs), such as S4, remains understudied. These architectures pose unique challenges due to their sequential processing and complex parameter dynamics. In this paper, we introduce a novel regularization technique based on Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the impact of input perturbations on model outputs. We focus on computing the GBM for three architectures: Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance resilience against word substitution attacks, (2) improve generalization on clean text, and (3) providing the first systematic analysis of SSM (S4) robustness. Extensive experiments across multiple architectures and benchmark datasets demonstrate that our method improves adversarial robustness by up to 8.8% over existing baselines. These results highlight the effectiveness of our approach, outperforming several state-of-the-art methods in adversarial defense. Codes are available at https://github.com/BouriMohammed/GBM
>
---
#### [new 054] Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在识别电子健康记录中的污名化和怀疑标记语言。通过扩展词典匹配和监督学习分类器，分析美国退伍军人事务部数据中的语言特征分布。研究发现，特定患者群体和医护人员类型更易使用污名化语言，揭示了医疗记录中系统性偏见问题。**

- **链接: [http://arxiv.org/pdf/2507.08969v1](http://arxiv.org/pdf/2507.08969v1)**

> **作者:** Drew Walker; Jennifer Love; Swati Rajwal; Isabel C Walker; Hannah LF Cooper; Abeed Sarker; Melvin Livingston III
>
> **备注:** 3 Tables
>
> **摘要:** Introduction: Electronic health records (EHR) are a critical medium through which patient stigmatization is perpetuated among healthcare teams. Methods: We identified linguistic features of doubt markers and stigmatizing labels in MIMIC-III EHR via expanded lexicon matching and supervised learning classifiers. Predictors of rates of linguistic features were assessed using Poisson regression models. Results: We found higher rates of stigmatizing labels per chart among patients who were Black or African American (RR: 1.16), patients with Medicare/Medicaid or government-run insurance (RR: 2.46), self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and mental health conditions. Patterns among doubt markers were similar, though male patients had higher rates of doubt markers (RR: 1.25). We found increased stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25), with similar patterns of doubt markers. Discussion: Stigmatizing language occurred at higher rates among historically stigmatized patients, perpetuated by multiple provider types.
>
---
#### [new 055] How Important is `Perfect' English for Machine Translation Prompts?
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在研究提示中的错误如何影响大语言模型在机器翻译和翻译评估中的表现。论文系统评估了不同类型的噪声对模型性能的影响，发现提示质量显著影响翻译效果，但某些噪声类型影响更大。研究揭示了模型对错误提示的鲁棒性及问题所在。**

- **链接: [http://arxiv.org/pdf/2507.09509v1](http://arxiv.org/pdf/2507.09509v1)**

> **作者:** Patrícia Schmidtová; Niyati Bafna; Seth Aycock; Gianluca Vico; Wiktor Kamzela; Katharina Hämmerl; Vilém Zouhar
>
> **摘要:** Large language models (LLMs) have achieved top results in recent machine translation evaluations, but they are also known to be sensitive to errors and perturbations in their prompts. We systematically evaluate how both humanly plausible and synthetic errors in user prompts affect LLMs' performance on two related tasks: Machine translation and machine translation evaluation. We provide both a quantitative analysis and qualitative insights into how the models respond to increasing noise in the user prompt. The prompt quality strongly affects the translation performance: With many errors, even a good prompt can underperform a minimal or poor prompt without errors. However, different noise types impact translation quality differently, with character-level and combined noisers degrading performance more than phrasal perturbations. Qualitative analysis reveals that lower prompt quality largely leads to poorer instruction following, rather than directly affecting translation quality itself. Further, LLMs can still translate in scenarios with overwhelming random noise that would make the prompt illegible to humans.
>
---
#### [new 056] ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning
- **分类: cs.CL; cs.AI; cs.HC**

- **简介: 该论文属于多模态讽刺生成任务，旨在解决现有数据集缺乏视觉线索及图文意图不匹配的问题。作者构建了包含4,970个多模态样本的M2SaG数据集，并提出ViSP框架，结合PPO强化学习与对比学习，提升讽刺文本生成质量。实验表明，ViSP在多项指标上优于基线模型，生成内容更具讽刺性和事实不协调性。**

- **链接: [http://arxiv.org/pdf/2507.09482v1](http://arxiv.org/pdf/2507.09482v1)**

> **作者:** Changli Wang; Rui Wu; Fang Yin
>
> **摘要:** Human emotions are complex, with sarcasm being a subtle and distinctive form. Despite progress in sarcasm research, sarcasm generation remains underexplored, primarily due to the overreliance on textual modalities and the neglect of visual cues, as well as the mismatch between image content and sarcastic intent in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm generation dataset with 4,970 samples, each containing an image, a sarcastic text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation framework that integrates Proximal Policy Optimization (PPO) and contrastive learning. PPO utilizes reward scores from DIP to steer the generation of sarcastic texts, while contrastive learning encourages the model to favor outputs with higher reward scores. These strategies improve overall generation quality and produce texts with more pronounced sarcastic intent. We evaluate ViSP across five metric sets and find it surpasses all baselines, including large language models, underscoring their limitations in sarcasm generation. Furthermore, we analyze the distributions of Sarcasm Scores and Factual Incongruity for both M2SaG and the texts generated by ViSP. The generated texts exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity (0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic content than the original dataset. % The dataset and code will be publicly available. Our dataset and code will be released at \textit{https://github.com/wclapply/ViSP}.
>
---
#### [new 057] ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making
- **分类: cs.CL; cs.AI**

- **简介: 该论文提出ALIGN系统，属于LLM对齐与个性化决策任务，旨在解决用户价值观多样导致的LLM决策偏差问题。通过基于提示的细粒度属性对齐方法，实现LLM动态个性化，支持可靠、负责任的决策。系统具备配置管理、结构化输出、多算法支持及可视化比较功能，并在公共意见调查和医疗分诊领域进行验证分析。**

- **链接: [http://arxiv.org/pdf/2507.09037v1](http://arxiv.org/pdf/2507.09037v1)**

> **作者:** Bharadwaj Ravichandran; David Joy; Paul Elliott; Brian Hu; Jadie Adams; Christopher Funk; Emily Veenhuis; Anthony Hoogs; Arslan Basharat
>
> **备注:** 10 pages total (including appendix), ICML 2025 Workshop on Reliable and Responsible Foundation Models
>
> **摘要:** Large language models (LLMs) are increasingly being used as decision aids. However, users have diverse values and preferences that can affect their decision-making, which requires novel methods for LLM alignment and personalization. Existing LLM comparison tools largely focus on benchmarking tasks, such as knowledge-based question answering. In contrast, our proposed ALIGN system focuses on dynamic personalization of LLM-based decision-makers through prompt-based alignment to a set of fine-grained attributes. Key features of our system include robust configuration management, structured output generation with reasoning, and several algorithm implementations with swappable LLM backbones, enabling different types of analyses. Our user interface enables a qualitative, side-by-side comparison of LLMs and their alignment to various attributes, with a modular backend for easy algorithm integration. Additionally, we perform a quantitative analysis comparing alignment approaches in two different domains: demographic alignment for public opinion surveys and value alignment for medical triage decision-making. The entire ALIGN framework is open source and will enable new research on reliable, responsible, and personalized LLM-based decision-makers.
>
---
#### [new 058] Abusive text transformation using LLMs
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于自然语言处理任务，旨在解决如何将含辱骂内容的文本转化为非辱骂文本的问题。研究使用多个大语言模型识别并转换含仇恨言论和脏话的文本，在保持原意的前提下清除不当内容，并通过情感与语义分析评估效果。**

- **链接: [http://arxiv.org/pdf/2507.10177v1](http://arxiv.org/pdf/2507.10177v1)**

> **作者:** Rohitash Chandra; Jiyong Choi
>
> **摘要:** Although Large Language Models (LLMs) have demonstrated significant advancements in natural language processing tasks, their effectiveness in the classification and transformation of abusive text into non-abusive versions remains an area for exploration. In this study, we aim to use LLMs to transform abusive text (tweets and reviews) featuring hate speech and swear words into non-abusive text, while retaining the intent of the text. We evaluate the performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and Groq, on their ability to identify abusive text. We them to transform and obtain a text that is clean from abusive and inappropriate content but maintains a similar level of sentiment and semantics, i.e. the transformed text needs to maintain its message. Afterwards, we evaluate the raw and transformed datasets with sentiment analysis and semantic analysis. Our results show Groq provides vastly different results when compared with other LLMs. We have identified similarities between GPT-4o and DeepSeek-V3.
>
---
#### [new 059] ClaritySpeech: Dementia Obfuscation in Speech
- **分类: cs.CL; cs.CR; cs.LG; cs.SD; eess.AS**

- **简介: 该论文属于语音处理任务，旨在解决失智症导致的语音障碍问题。通过结合自动语音识别、文本模糊化和零样本语音合成技术，提出ClaritySpeech框架，在低数据环境下修正受失智症影响的语音，同时保持说话人身份特征，以提升隐私保护与沟通效率。**

- **链接: [http://arxiv.org/pdf/2507.09282v1](http://arxiv.org/pdf/2507.09282v1)**

> **作者:** Dominika Woszczyk; Ranya Aloufi; Soteris Demetriou
>
> **备注:** Accepted at Interspeech 2025
>
> **摘要:** Dementia, a neurodegenerative disease, alters speech patterns, creating communication barriers and raising privacy concerns. Current speech technologies, such as automatic speech transcription (ASR), struggle with dementia and atypical speech, further challenging accessibility. This paper presents a novel dementia obfuscation in speech framework, ClaritySpeech, integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to correct dementia-affected speech while preserving speaker identity in low-data environments without fine-tuning. Results show a 16% and 10% drop in mean F1 score across various adversarial settings and modalities (audio, text, fusion) for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15 for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and accessibility.
>
---
#### [new 060] GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities
- **分类: cs.CL**

- **简介: 论文提出GoalfyMax，一种协议驱动的多智能体协作框架，旨在提升企业环境中复杂任务处理的自主性与适应性。它解决了传统单功能AI系统在协调、记忆复用和任务分解上的不足，通过标准化通信协议（MCP）、经验包架构（XP）及多种增强机制，实现高效协作与持续学习。**

- **链接: [http://arxiv.org/pdf/2507.09497v1](http://arxiv.org/pdf/2507.09497v1)**

> **作者:** Siyi Wu; Zeyu Wang; Xinyuan Song; Zhengpeng Zhou; Lifan Sun; Tianyu Shi
>
> **摘要:** Modern enterprise environments demand intelligent systems capable of handling complex, dynamic, and multi-faceted tasks with high levels of autonomy and adaptability. However, traditional single-purpose AI systems often lack sufficient coordination, memory reuse, and task decomposition capabilities, limiting their scalability in realistic settings. To address these challenges, we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent (A2A) communication layer built on the Model Context Protocol (MCP), allowing independent agents to coordinate through asynchronous, protocol-compliant interactions. It incorporates the Experience Pack (XP) architecture, a layered memory system that preserves both task rationales and execution traces, enabling structured knowledge retention and continual learning. Moreover, our system integrates advanced features including multi-turn contextual dialogue, long-short term memory modules, and dynamic safety validation, supporting robust, real-time strategy adaptation. Empirical results on complex task orchestration benchmarks and case study demonstrate that GoalfyMax achieves superior adaptability, coordination, and experience reuse compared to baseline frameworks. These findings highlight its potential as a scalable, future-ready foundation for multi-agent intelligent systems.
>
---
#### [new 061] The CoNLL-2013 Shared Task on Grammatical Error Correction
- **分类: cs.CL**

- **简介: 该论文属于自然语言处理中的语法错误纠正任务，旨在提升非母语者英文文本的语法正确性。论文定义了任务目标、提供了数据集，并设计了评估指标与工具。此外，还总结了参赛团队采用的方法及效果，推动了相关技术的发展。**

- **链接: [http://arxiv.org/pdf/2507.09474v1](http://arxiv.org/pdf/2507.09474v1)**

> **作者:** Hwee Tou Ng; Siew Mei Wu; Yuanbin Wu; Christian Hadiwinoto; Joel Tetreault
>
> **备注:** 12 pages
>
> **摘要:** The CoNLL-2013 shared task was devoted to grammatical error correction. In this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task. We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results.
>
---
#### [new 062] From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于自然语言处理任务，旨在解决现有韩语大模型评估基准不全面的问题。作者重构了KMMLU-Redux并构建了新的专业基准KMMLU-Pro，基于韩国国家技术资格和职业执照考试，更准确评估大模型在工业与专业领域的表现，并公开发布数据集。**

- **链接: [http://arxiv.org/pdf/2507.08924v1](http://arxiv.org/pdf/2507.08924v1)**

> **作者:** Seokhee Hong; Sunkyoung Kim; Guijin Son; Soyeon Kim; Yeonjung Hong; Jinsik Lee
>
> **摘要:** The development of Large Language Models (LLMs) requires robust benchmarks that encompass not only academic domains but also industrial fields to effectively evaluate their applicability in real-world scenarios. In this paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux, reconstructed from the existing KMMLU, consists of questions from the Korean National Technical Qualification exams, with critical errors removed to enhance reliability. KMMLU-Pro is based on Korean National Professional Licensure exams to reflect professional knowledge in Korea. Our experiments demonstrate that these benchmarks comprehensively represent industrial knowledge in Korea. We release our dataset publicly available.
>
---
#### [new 063] CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards
- **分类: cs.CL; cs.AI**

- **简介: 该论文属于大语言模型（LLM）评价任务，旨在解决现有评判模型专业性窄、鲁棒性差的问题。作者提出CompassJudger-2，通过任务驱动的多领域数据策略和可验证奖励监督提升判断能力，并引入新学习目标优化性能。成果包括提升跨领域判断准确率及推出评测基准JudgerBenchV2。**

- **链接: [http://arxiv.org/pdf/2507.09104v1](http://arxiv.org/pdf/2507.09104v1)**

> **作者:** Taolin Zhang; Maosong Cao; Alexander Lam; Songyang Zhang; Kai Chen
>
> **摘要:** Recently, the role of LLM-as-judge in evaluating large language models has gained prominence. However, current judge models suffer from narrow specialization and limited robustness, undermining their capacity for comprehensive evaluations. In this work, we present CompassJudger-2, a novel generalist judge model that overcomes these limitations via a task-driven, multi-domain data curation strategy. Central to our approach is supervising judgment tasks with verifiable rewards, guiding intrinsic critical reasoning through rejection sampling to foster robust, generalizable judgment capabilities. We introduce a refined learning objective with margin policy gradient loss to enhance performance. Empirically, CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, and our 7B model demonstrates competitive judgment accuracy with significantly larger models like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a comprehensive benchmark evaluating cross-domain judgment accuracy and rank consistency to standardize judge model evaluation. These contributions advance robust, scalable LLM judgment and establish new performance and evaluation standards.
>
---
#### [new 064] DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA
- **分类: cs.RO; cs.CL**

- **简介: 该论文属于多激光雷达（LiDAR）系统外参标定任务，旨在解决非重叠视场下多LiDAR系统的高精度、鲁棒标定问题。作者提出DLBAcalib方法，结合LBA优化与迭代精炼，实现无需目标和初始参数的标定，具有高精度和强鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.09176v1](http://arxiv.org/pdf/2507.09176v1)**

> **作者:** Han Ye; Yuqiang Jin; Jinyuan Liu; Tao Li; Wen-An Zhang; Minglei Fu
>
> **备注:** 9 pages,14 figures
>
> **摘要:** Accurate extrinsic calibration of multiple LiDARs is crucial for improving the foundational performance of three-dimensional (3D) map reconstruction systems. This paper presents a novel targetless extrinsic calibration framework for multi-LiDAR systems that does not rely on overlapping fields of view or precise initial parameter estimates. Unlike conventional calibration methods that require manual annotations or specific reference patterns, our approach introduces a unified optimization framework by integrating LiDAR bundle adjustment (LBA) optimization with robust iterative refinement. The proposed method constructs an accurate reference point cloud map via continuous scanning from the target LiDAR and sliding-window LiDAR bundle adjustment, while formulating extrinsic calibration as a joint LBA optimization problem. This method effectively mitigates cumulative mapping errors and achieves outlier-resistant parameter estimation through an adaptive weighting mechanism. Extensive evaluations in both the CARLA simulation environment and real-world scenarios demonstrate that our method outperforms state-of-the-art calibration techniques in both accuracy and robustness. Experimental results show that for non-overlapping sensor configurations, our framework achieves an average translational error of 5 mm and a rotational error of 0.2{\deg}, with an initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration process operates without specialized infrastructure or manual parameter tuning. The code is open source and available on GitHub (\underline{https://github.com/Silentbarber/DLBAcalib})
>
---
#### [new 065] FaceLLM: A Multimodal Large Language Model for Face Understanding
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文属于多模态语言模型任务，旨在解决现有模型在面部图像理解上的局限性。作者构建了专门的FaceLLM模型和FairFaceGPT数据集，利用ChatGPT生成高质量图文对，提升模型在表情、姿态等面部属性上的推理能力，推动领域专用多模态AI发展。**

- **链接: [http://arxiv.org/pdf/2507.10300v1](http://arxiv.org/pdf/2507.10300v1)**

> **作者:** Hatef Otroshi Shahreza; Sébastien Marcel
>
> **备注:** Accepted in ICCV 2025 workshops
>
> **摘要:** Multimodal large language models (MLLMs) have shown remarkable performance in vision-language tasks. However, existing MLLMs are primarily trained on generic datasets, limiting their ability to reason on domain-specific visual cues such as those in facial images. In particular, tasks that require detailed understanding of facial structure, expression, emotion, and demographic features remain underexplored by MLLMs due to the lack of large-scale annotated face image-text datasets. In this work, we introduce FaceLLM, a multimodal large language model trained specifically for facial image understanding. To construct the training data, we propose a novel weakly supervised pipeline that uses ChatGPT with attribute-aware prompts to generate high-quality question-answer pairs based on images from the FairFace dataset. The resulting corpus, called FairFaceGPT, covers a diverse set of attributes including expression, pose, skin texture, and forensic information. Our experiments demonstrate that FaceLLM improves the performance of MLLMs on various face-centric tasks and achieves state-of-the-art performance. This work highlights the potential of synthetic supervision via language models for building domain-specialized MLLMs, and sets a precedent for trustworthy, human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM models are publicly available in the project page.
>
---
#### [new 066] Automating SPARQL Query Translations between DBpedia and Wikidata
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于知识图谱互操作性任务，旨在解决不同知识图谱（如DBpedia与Wikidata）间SPARQL查询自动翻译的问题。研究使用多种大语言模型，通过零样本、少样本及思维链策略进行翻译实验，并评估性能差异和错误类型，验证其在两个基准数据集上的效果。**

- **链接: [http://arxiv.org/pdf/2507.10045v1](http://arxiv.org/pdf/2507.10045v1)**

> **作者:** Malte Christian Bartels; Debayan Banerjee; Ricardo Usbeck
>
> **备注:** 18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference happening on September 2025
>
> **摘要:** This paper investigates whether state-of-the-art Large Language Models (LLMs) can automatically translate SPARQL between popular Knowledge Graph (KG) schemas. We focus on translations between the DBpedia and Wikidata KG, and later on DBLP and OpenAlex KG. This study addresses a notable gap in KG interoperability research by rigorously evaluating LLM performance on SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100 DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and Mistral-Large-Instruct-2407 are selected based on their sizes and architectures and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs were compared with gold answers, and resulting errors were categorized. We find that the performance varies markedly across models and prompting strategies, and that translations for Wikidata to DBpedia work far better than translations for DBpedia to Wikidata.
>
---
#### [new 067] Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations
- **分类: cs.AI; cs.CL; cs.LO**

- **简介: 该论文属于神经符号推理任务，旨在解决大语言模型（LLM）在逻辑推理中输出不一致的问题。作者提出一种方法，将LLM直接集成到形式语义的解释函数中，基于矛盾容忍逻辑构建理论框架，并通过实验验证其可行性。**

- **链接: [http://arxiv.org/pdf/2507.09751v1](http://arxiv.org/pdf/2507.09751v1)**

> **作者:** Bradley P. Allen; Prateek Chhikara; Thomas Macaulay Ferguson; Filip Ilievski; Paul Groth
>
> **备注:** 29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)
>
> **摘要:** Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but they exhibit problems with logical consistency in the output they generate. How can we harness LLMs' broad-coverage parametric knowledge in formal reasoning despite their inconsistency? We present a method for directly integrating an LLM into the interpretation function of the formal semantics for a paraconsistent logic. We provide experimental evidence for the feasibility of the method by evaluating the function using datasets created from several short-form factuality benchmarks. Unlike prior work, our method offers a theoretical framework for neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the underlying logic's soundness and completeness properties.
>
---
#### [new 068] Semantic Source Code Segmentation using Small and Large Language Models
- **分类: cs.SE; cs.CL; cs.PL**

- **简介: 该论文属于代码分析任务，旨在解决低资源语言（如R）中源代码自动分段的问题。为实现这一目标，作者提出了两种基于大语言模型和小语言模型的新方法，并构建了一个人工标注的数据集StatCodeSeg。实验表明，上下文驱动的逐行分析优于范围划分方法，且小型模型表现更优。**

- **链接: [http://arxiv.org/pdf/2507.08992v1](http://arxiv.org/pdf/2507.08992v1)**

> **作者:** Abdelhalim Dahou; Ansgar Scherp; Sebastian Kurten; Brigitte Mathiak; Madhu Chauhan
>
> **备注:** 18 pages, 4 figures
>
> **摘要:** Source code segmentation, dividing code into functionally coherent segments, is crucial for knowledge retrieval and maintenance in software development. While enabling efficient navigation and comprehension of large codebases, manual and syntactic analysis approaches have become impractical as repositories grow, especially for low-resource languages like R and their research domains (e.g., social sciences, psychology).This paper introduces an automated, domain-specific approach for research R code segmentation using Large and Small Language Models (LLMs/SLMs). It presents two novel approaches and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches: line-by-line analysis with context and range-based segment determination. We experiment with LLMs and fine-tuned SLMs. To support the generalizability of our approaches, we also include experiments on Python code from the computer science domain.Our results show that context-based line-by-line analysis is superior over range-based segmentation.Using smaller language models like CodeBERT and an encoder-only version of CodeT5+ are better than their LLM counterparts. Most notably, these two best-performing models did not see R code during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of manually annotated code.
>
---
#### [new 069] LoRA Is Slower Than You Think
- **分类: cs.LG; cs.AI; cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决LoRA在不同模型架构和训练设置中速度提升不一致的问题。作者分析了LoRA的性能瓶颈，并提出了更高效的微调方法，验证其在速度和效果上的优势，为资源受限下的大模型微调提供实践指导。**

- **链接: [http://arxiv.org/pdf/2507.08833v1](http://arxiv.org/pdf/2507.08833v1)**

> **作者:** Seokmin Ko
>
> **摘要:** Low-Rank Adaptation (LoRA) is one of the most widely used techniques for fine-tuning large language models (LLMs). By introducing a small number of trainable low-rank weight matrices, LoRA substantially reduces the number of parameters that need to be updated, offering significant advantages in memory consumption and computational efficiency compared to full fine-tuning. However, we observed that LoRA does not consistently provide speed improvements across all model architectures and training setups. Motivated by this inconsistency, we conduct a comprehensive analysis of LoRA's performance and investigate the underlying factors limiting its speedup. Based on our findings, we propose several methods for more efficient fine-tuning of LLMs. We empirically evaluate these methods and compare them to LoRA, demonstrating that our approach achieves comparable or superior performance while delivering more consistent training speed improvements. Our work offers valuable insights and practical guidelines for practitioners seeking to optimize LLM fine-tuning under resource constraints.
>
---
#### [new 070] Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning
- **分类: cs.SD; cs.CL; eess.AS**

- **简介: 该论文属于语音转换任务，旨在解决缺乏目标说话人朗伯德语料的问题。通过比较隐式与显式声学特征条件模型，实现说话人身份转换同时保留朗伯德风格特征，提升语音可懂度并保持说话人相似性。**

- **链接: [http://arxiv.org/pdf/2507.09310v1](http://arxiv.org/pdf/2507.09310v1)**

> **作者:** Dominika Woszczyk; Manuel Sam Ribeiro; Thomas Merritt; Daniel Korzekwa
>
> **备注:** Presented at Clarity Challenge 2023
>
> **摘要:** Text-to-Speech (TTS) systems in Lombard speaking style can improve the overall intelligibility of speech, useful for hearing loss and noisy conditions. However, training those models requires a large amount of data and the Lombard effect is challenging to record due to speaker and noise variability and tiring recording conditions. Voice conversion (VC) has been shown to be a useful augmentation technique to train TTS systems in the absence of recorded data from the target speaker in the target speaking style. In this paper, we are concerned with Lombard speaking style transfer. Our goal is to convert speaker identity while preserving the acoustic attributes that define the Lombard speaking style. We compare voice conversion models with implicit and explicit acoustic feature conditioning. We observe that our proposed implicit conditioning strategy achieves an intelligibility gain comparable to the model conditioned on explicit acoustic features, while also preserving speaker similarity.
>
---
#### [new 071] On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model
- **分类: cs.AI; cs.CL; I.2.11; F.4.1; I.2.4; G.2.2**

- **简介: 该论文探讨如何通过“意图性”分析场景上下文，用于认知代理的知识表示。它属于自然语言处理与认知计算任务，旨在低成本识别文本中的潜在意图。工作包括：利用语义时空模型和尺度分离技术，区分内容与上下文，评估数据中的意图性特征。**

- **链接: [http://arxiv.org/pdf/2507.10000v1](http://arxiv.org/pdf/2507.10000v1)**

> **作者:** Mark Burgess
>
> **摘要:** Since Searle's work deconstructing intent and intentionality in the realm of philosophy, the practical meaning of intent has received little attention in science and technology. Intentionality and context are both central to the scope of Promise Theory's model of Semantic Spacetime, used as an effective Tiny Language Model. One can identify themes and concepts from a text, on a low level (without knowledge of the specific language) by using process coherence as a guide. Any agent process can assess superficially a degree of latent `intentionality' in data by looking for anomalous multi-scale anomalies and assessing the work done to form them. Scale separation can be used to sort parts into `intended' content and `ambient context', using the spacetime coherence as a measure. This offers an elementary but pragmatic interpretation of latent intentionality for very low computational cost, and without reference to extensive training or reasoning capabilities. The process is well within the reach of basic organisms as it does not require large scale artificial probabilistic batch processing. The level of concept formation depends, however, on the memory capacity of the agent.
>
---
#### [new 072] AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data
- **分类: cs.HC; cs.AI; cs.CL; H.5.0**

- **简介: 论文提出AInsight系统，属于决策支持任务，旨在解决专家在实时对话中难以利用历史数据的问题。系统通过监听对话、识别问题与方案，结合检索式大模型生成洞察，辅助医生决策。实验基于加拿大健康数据验证效果，展示潜力并指出挑战。**

- **链接: [http://arxiv.org/pdf/2507.09100v1](http://arxiv.org/pdf/2507.09100v1)**

> **作者:** Mohammad Abolnejadian; Shakiba Amirshahi; Matthew Brehmer; Anamaria Crisan
>
> **备注:** 7 pages and 4 figures. Proceedings of the 7th ACM Conference on Conversational User Interfaces (CUI '25)
>
> **摘要:** In decision-making conversations, experts must navigate complex choices and make on-the-spot decisions while engaged in conversation. Although extensive historical data often exists, the real-time nature of these scenarios makes it infeasible for decision-makers to review and leverage relevant information. This raises an interesting question: What if experts could utilize relevant past data in real-time decision-making through insights derived from past data? To explore this, we implemented a conversational user interface, taking doctor-patient interactions as an example use case. Our system continuously listens to the conversation, identifies patient problems and doctor-suggested solutions, and retrieves related data from an embedded dataset, generating concise insights using a pipeline built around a retrieval-based Large Language Model (LLM) agent. We evaluated the prototype by embedding Health Canada datasets into a vector database and conducting simulated studies using sample doctor-patient dialogues, showing effectiveness but also challenges, setting directions for the next steps of our work.
>
---
#### [new 073] ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching
- **分类: eess.AS; cs.CL**

- **简介: 该论文属于语音对话生成任务，旨在解决自回归模型推理慢且不稳定的问题。论文提出了非自回归模型ZipVoice-Dialog，采用流匹配技术，实现零样本语音对话生成，并设计了说话人转换嵌入、课程学习策略和立体对话生成方法。此外，构建了开源数据集OpenDialog并建立了评估基准。**

- **链接: [http://arxiv.org/pdf/2507.09318v1](http://arxiv.org/pdf/2507.09318v1)**

> **作者:** Han Zhu; Wei Kang; Liyong Guo; Zengwei Yao; Fangjun Kuang; Weiji Zhuang; Zhaoqing Li; Zhifeng Han; Dong Zhang; Xin Zhang; Xingchen Song; Long Lin; Daniel Povey
>
> **摘要:** Generating spoken dialogue is more challenging than monologue text-to-speech (TTS) due to the need for realistic turn-taking and distinct speaker timbres. Existing spoken dialogue generation models, being auto-regressive, suffer from slow and unstable inference. To overcome these limitations, we introduce ZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation model built upon flow matching. Key designs include: 1) speaker-turn embeddings for precise speaker turn-taking; 2) a curriculum learning strategy for stable speech-text alignment; 3) specialized strategies to enable stereo dialogue generation. Additionally, recognizing the lack of open-source large-scale spoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue dataset from in-the-wild speech data. Furthermore, we established a benchmark to comprehensively evaluate various models. Experimental results demonstrate that ZipVoice-Dialog achieves superior performance in intelligibility, speaker turn-taking accuracy, speaker similarity, and inference speed. Our codes, model checkpoints, demo samples, and the OpenDialog dataset are all publicly available at https://github.com/k2-fsa/ZipVoice.
>
---
#### [new 074] EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions
- **分类: cs.CR; cs.AI; cs.CL**

- **简介: 该论文属于网络安全威胁情报任务，旨在解决从黑客论坛中自动提取、聚类和优先级排序安全事件的问题。论文提出了一种无监督框架EventHunter，利用基于Transformer的嵌入和对比学习对讨论内容进行动态聚类，并通过量化指标对事件进行每日排名，以帮助安全分析师更早发现高优先级威胁。**

- **链接: [http://arxiv.org/pdf/2507.09762v1](http://arxiv.org/pdf/2507.09762v1)**

> **作者:** Yasir Ech-Chammakhy; Anas Motii; Anass Rabii; Jaafar Chbili
>
> **备注:** Accepted for publication at the 28th International Symposium on Research in Attacks, Intrusions, and Defenses (RAID 2025)
>
> **摘要:** Hacker forums provide critical early warning signals for emerging cybersecurity threats, but extracting actionable intelligence from their unstructured and noisy content remains a significant challenge. This paper presents an unsupervised framework that automatically detects, clusters, and prioritizes security events discussed across hacker forum posts. Our approach leverages Transformer-based embeddings fine-tuned with contrastive learning to group related discussions into distinct security event clusters, identifying incidents like zero-day disclosures or malware releases without relying on predefined keywords. The framework incorporates a daily ranking mechanism that prioritizes identified events using quantifiable metrics reflecting timeliness, source credibility, information completeness, and relevance. Experimental evaluation on real-world hacker forum data demonstrates that our method effectively reduces noise and surfaces high-priority threats, enabling security analysts to mount proactive responses. By transforming disparate hacker forum discussions into structured, actionable intelligence, our work addresses fundamental challenges in automated threat detection and analysis.
>
---
#### [new 075] Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources
- **分类: cs.CV; cs.CL; cs.IR; cs.MM**

- **简介: 该论文属于文本到遥感图像检索任务，旨在解决现有方法局限于RGB图像、难以利用多源传感器数据的问题。作者构建了包含超64万对SAR与光学图像及文本标注的数据集CrisisLandMark，并提出CLOSP框架实现跨模态检索，显著提升性能。**

- **链接: [http://arxiv.org/pdf/2507.10403v1](http://arxiv.org/pdf/2507.10403v1)**

> **作者:** Daniele Rege Cambrin; Lorenzo Vaiani; Giuseppe Gallipoli; Luca Cagliero; Paolo Garza
>
> **摘要:** Retrieving relevant imagery from vast satellite archives is crucial for applications like disaster response and long-term climate monitoring. However, most text-to-image retrieval systems are limited to RGB data, failing to exploit the unique physical information captured by other sensors, such as the all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the spectral signatures in optical multispectral data. To bridge this gap, we introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1 SAR and Sentinel-2 multispectral images paired with structured textual annotations for land cover, land use, and crisis events harmonized from authoritative land cover systems (CORINE and Dynamic World) and crisis-specific sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining), a novel framework that uses text as a bridge to align unpaired optical and SAR images into a unified embedding space. Our experiments show that CLOSP achieves a new state-of-the-art, improving retrieval nDGC by 54% over existing models. Additionally, we find that the unified training strategy overcomes the inherent difficulty of interpreting SAR imagery by transferring rich semantic knowledge from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which integrates geographic coordinates into our framework, creates a powerful trade-off between generality and specificity: while the CLOSP excels at general semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving location-dependent crisis events and rare geographic features. This work highlights that the integration of diverse sensor data and geographic context is essential for unlocking the full potential of remote sensing archives.
>
---
#### [new 076] MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 论文提出MENTOR，一种高效的多模态条件调优框架，用于自回归视觉生成模型。旨在解决文本到图像生成中视觉控制不精确、多模态输入平衡困难及训练成本高的问题。通过两阶段训练方法，实现细粒度的多模态对齐与指令调优，提升生成质量与可控性。**

- **链接: [http://arxiv.org/pdf/2507.09574v1](http://arxiv.org/pdf/2507.09574v1)**

> **作者:** Haozhe Zhao; Zefan Cai; Shuzheng Si; Liang Chen; Jiuxiang Gu; Wen Xiao; Junjie Hu
>
> **备注:** 24 pages,12 figures
>
> **摘要:** Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: https://github.com/HaozheZhao/MENTOR
>
---
#### [new 077] Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination
- **分类: cs.LG; cs.AI; cs.CL**

- **简介: 该论文属于自然语言处理与强化学习任务，旨在解决数据污染导致的强化学习结果不可靠问题。作者提出合成数据集RandomCalculation，验证只有准确奖励信号能提升模型推理能力，强调评估方法需基于无污染数据和多模型验证。**

- **链接: [http://arxiv.org/pdf/2507.10532v1](http://arxiv.org/pdf/2507.10532v1)**

> **作者:** Mingqi Wu; Zhihao Zhang; Qiaole Dong; Zhiheng Xi; Jun Zhao; Senjie Jin; Xiaoran Fan; Yuhao Zhou; Yanwei Fu; Qin Liu; Songyang Zhang; Qi Zhang
>
> **备注:** 26 pages
>
> **摘要:** The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions.
>
---
#### [new 078] DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology
- **分类: cs.AI; cs.CL; cs.MA**

- **简介: 该论文属于科学问答任务，旨在解决生态学领域复杂问题的文献检索与综合分析难题。作者提出了DeepResearch^Eco系统，通过递归代理工作流实现可控深度与广度的自动化科学综合，提升了信息检索的多样性和精细度，并具备参数化配置与透明推理优势。**

- **链接: [http://arxiv.org/pdf/2507.10522v1](http://arxiv.org/pdf/2507.10522v1)**

> **作者:** Jennifer D'Souza; Endres Keno Sander; Andrei Aioanei
>
> **备注:** 12 pages, 3 figures
>
> **摘要:** We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth- and breadth-controlled exploration of original research questions -- enhancing search diversity and nuance in the retrieval of relevant scientific literature. Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor. Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words. High-parameter settings yield expert-level analytical depth and contextual diversity. Source code available at: https://github.com/sciknoworg/deep-research.
>
---
#### [new 079] DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate
- **分类: cs.IR; cs.CL**

- **简介: 该论文属于检索增强辩论任务，旨在研究大语言模型在结构化辩论中的表现及评估能力。论文通过使用六个领先模型进行辩论与评价实验，并依据质量、数量、方式和关联四指标评估效果，揭示了模型在提供相关论点时表现良好但存在冗长问题。**

- **链接: [http://arxiv.org/pdf/2507.09090v1](http://arxiv.org/pdf/2507.09090v1)**

> **作者:** Anthony Miyaguchi; Conor Johnston; Aaryan Potdar
>
> **摘要:** Large Language Models (LLMs) demonstrate strong conversational abilities. In this Working Paper, we study them in the context of debating in two ways: their ability to perform in a structured debate along with a dataset of arguments to use and their ability to evaluate utterances throughout the debate. We deploy six leading publicly available models from three providers for the Retrieval-Augmented Debate and Evaluation. The evaluation is performed by measuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout this task, we found that although LLMs perform well in debates when given related arguments, they tend to be verbose in responses yet consistent in evaluation. The accompanying source code for this paper is located at https://github.com/dsgt-arc/touche-2025-rad.
>
---
#### [new 080] Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文属于医疗领域多模态大语言模型任务，旨在解决模型对提示敏感和置信度不准的问题。论文提出Prompt4Trust框架，通过强化学习生成辅助提示，提升模型在医疗视觉问答中的准确性和置信度一致性，并实现跨模型泛化。**

- **链接: [http://arxiv.org/pdf/2507.09279v1](http://arxiv.org/pdf/2507.09279v1)**

> **作者:** Anita Kriz; Elizabeth Laura Janes; Xing Shen; Tal Arbel
>
> **备注:** Preprint version. The peer-reviewed version of this paper has been accepted to ICCV 2025 Workshop CVAMD
>
> **摘要:** Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at https://github.com/xingbpshen/vccrl-llm.
>
---
#### [new 081] Evaluating LLMs on Sequential API Call Through Automated Test Generation
- **分类: cs.SE; cs.AI; cs.CL**

- **简介: 该论文属于自动化测试生成任务，旨在解决现有LLM评估中缺乏对连续API调用交互测试的问题。作者提出了StateGen框架，自动生成包含序列API交互的多样化测试用例，并构建了StateEval基准测试集，通过实验验证其有效性并指出当前LLMs在API集成方面的改进空间。**

- **链接: [http://arxiv.org/pdf/2507.09481v1](http://arxiv.org/pdf/2507.09481v1)**

> **作者:** Yuheng Huang; Da Song; Zhenlan Ji; Shuai Wang; Lei Ma
>
> **摘要:** By integrating tools from external APIs, Large Language Models (LLMs) have expanded their promising capabilities in a diverse spectrum of complex real-world tasks. However, testing, evaluation, and analysis of LLM tool use remain in their early stages. Most existing benchmarks rely on manually collected test cases, many of which cannot be automatically checked for semantic correctness and instead depend on static methods such as string matching. Additionally, these benchmarks often overlook the complex interactions that occur between sequential API calls, which are common in real-world applications. To fill the gap, in this paper, we introduce StateGen, an automated framework designed to generate diverse coding tasks involving sequential API interactions. StateGen combines state-machine-based API constraint solving and validation, energy-based sampling, and control-flow injection to generate executable programs. These programs are then translated into human-like natural language task descriptions through a collaboration of two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark encompassing 120 verified test cases spanning across three representative scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental results confirm that StateGen can effectively generate challenging and realistic API-oriented tasks, highlighting areas for improvement in current LLMs incorporating APIs.
>
---
#### [new 082] ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文属于视频理解任务，旨在解决现有大语言模型在视频推理中忽视视觉信息的问题。作者提出ViTCoT方法，通过图文交错的思维链提升模型对视频内容的理解能力，并构建了相关基准数据集ViTIB进行验证。实验表明该方法优于传统文本思维链。**

- **链接: [http://arxiv.org/pdf/2507.09876v1](http://arxiv.org/pdf/2507.09876v1)**

> **作者:** Yongheng Zhang; Xu Liu; Ruihan Tao; Qiguang Chen; Hao Fei; Wanxiang Che; Libo Qin
>
> **备注:** Accepted by ACM MM 2025
>
> **摘要:** Video understanding plays a vital role in bridging low-level visual signals with high-level cognitive reasoning, and is fundamental to applications such as autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid development of large language models (LLMs), particularly those utilizing Chain-of-Thought (CoT) technology, has significantly advanced video reasoning capabilities. However, current approaches primarily depend on textual information for reasoning, overlooking the visual modality in the actual video reasoning process. In contrast, humans naturally re-examine visual content while reasoning. Motivated by this, we introduce a novel video reasoning paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive and cognitively aligned reasoning. To the end, first, we construct the Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for key-video selection and manually verified. Furthermore, we extensively explore the potential of the ViTCoT paradigm in the video understanding field. Extensive experiments demonstrate that ViTCoT significantly enhances performance compared to the traditional text-only CoT paradigm and effectively activates more neuron values in MLLMs.
>
---
#### [new 083] Natural Language-based Assessment of L2 Oral Proficiency using LLMs
- **分类: eess.AS; cs.AI; cs.CL**

- **简介: 该论文属于二语口语能力评估任务，旨在解决能否用大型语言模型（LLM）替代人工评分的问题。作者使用Qwen 2.5 72B模型，在零样本设置下基于自然语言描述符对S&I语料库进行评分。结果显示，该方法表现优于BERT模型，具备良好可解释性和跨语言泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.10200v1](http://arxiv.org/pdf/2507.10200v1)**

> **作者:** Stefano Bannò; Rao Ma; Mengjie Qian; Siyuan Tang; Kate Knill; Mark Gales
>
> **备注:** Accepted for the 10th Workshop on Speech and Language Technology in Education (SLaTE 2025)
>
> **摘要:** Natural language-based assessment (NLA) is an approach to second language assessment that uses instructions - expressed in the form of can-do descriptors - originally intended for human examiners, aiming to determine whether large language models (LLMs) can interpret and apply them in ways comparable to human assessment. In this work, we explore the use of such descriptors with an open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available S&I Corpus in a zero-shot setting. Our results show that this approach - relying solely on textual information - achieves competitive performance: while it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it surpasses a BERT-based model trained specifically for this purpose. NLA proves particularly effective in mismatched task settings, is generalisable to other data types and languages, and offers greater interpretability, as it is grounded in clearly explainable, widely applicable language descriptors.
>
---
#### [new 084] TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit
- **分类: cs.MA; cs.AI; cs.CL; cs.HC; I.2.11; I.6.5; I.6.7**

- **简介: 论文提出TinyTroupe，一个基于大语言模型的多智能体角色模拟工具包。任务是解决现有MAS工具在细粒度角色设定、实验支持等方面的不足。工作包括设计可编程角色模拟系统，并通过实例展示其应用。**

- **链接: [http://arxiv.org/pdf/2507.09788v1](http://arxiv.org/pdf/2507.09788v1)**

> **作者:** Paulo Salem; Robert Sim; Christopher Olsen; Prerit Saxena; Rafael Barcelos; Yi Ding
>
> **备注:** 9 pages. Preprint to be submitted to peer-review
>
> **摘要:** Recent advances in Large Language Models (LLM) have led to a new class of autonomous agents, renewing and expanding interest in the area. LLM-powered Multiagent Systems (MAS) have thus emerged, both for assistive and simulation purposes, yet tools for realistic human behavior simulation -- with its distinctive challenges and opportunities -- remain underdeveloped. Existing MAS libraries and tools lack fine-grained persona specifications, population sampling facilities, experimentation support, and integrated validation, among other key capabilities, limiting their utility for behavioral studies, social simulation, and related applications. To address these deficiencies, in this work we introduce TinyTroupe, a simulation toolkit enabling detailed persona definitions (e.g., nationality, age, occupation, personality, beliefs, behaviors) and programmatic control via numerous LLM-driven mechanisms. This allows for the concise formulation of behavioral problems of practical interest, either at the individual or group level, and provides effective means for their solution. TinyTroupe's components are presented using representative working examples, such as brainstorming and market research sessions, thereby simultaneously clarifying their purpose and demonstrating their usefulness. Quantitative and qualitative evaluations of selected aspects are also provided, highlighting possibilities, limitations, and trade-offs. The approach, though realized as a specific Python implementation, is meant as a novel conceptual contribution, which can be partially or fully incorporated in other contexts. The library is available as open source at https://github.com/microsoft/tinytroupe.
>
---
#### [new 085] Principled Foundations for Preference Optimization
- **分类: cs.LG; cs.AI; cs.CL; I.2.6; I.2.7**

- **简介: 该论文属于机器学习任务，旨在理解偏好优化的理论基础。它揭示了直接偏好优化（DPO）与损失函数理论（Savage）及随机选择理论（Doignon-Falmagne、Machina）之间的联系。论文工作建立了DPO在更广泛理论框架下的位置，支持选择 abstention、非凸目标，并自然涵盖DPO的扩展形式，如引入边距和长度修正。**

- **链接: [http://arxiv.org/pdf/2507.07855v1](http://arxiv.org/pdf/2507.07855v1)**

> **作者:** Wenxuan Zhou; Shujian Zhang; Brice Magdalou; John Lambert; Ehsan Amid; Richard Nock; Andrew Hard
>
> **摘要:** In this paper, we show that direct preference optimization (DPO) is a very specific form of a connection between two major theories in the ML context of learning from preferences: loss functions (Savage) and stochastic choice (Doignon-Falmagne and Machina). The connection is established for all of Savage's losses and at this level of generality, (i) it includes support for abstention on the choice theory side, (ii) it includes support for non-convex objectives on the ML side, and (iii) it allows to frame for free some notable extensions of the DPO setting, including margins and corrections for length. Getting to understand how DPO operates from a general principled perspective is crucial because of the huge and diverse application landscape of models, because of the current momentum around DPO, but also -- and importantly -- because many state of the art variations on DPO definitely occupy a small region of the map that we cover. It also helps to understand the pitfalls of departing from this map, and figure out workarounds.
>
---
#### [new 086] RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation
- **分类: cs.CR; cs.CL**

- **简介: 该论文属于安全与知识图谱任务，旨在解决KG-RAG系统易受知识污染攻击的安全问题。作者提出一种隐蔽攻击方法，通过向知识图中注入误导三元组，破坏推理链，从而降低KG-RAG的生成准确性，并验证了攻击的有效性与系统脆弱性。**

- **链接: [http://arxiv.org/pdf/2507.08862v1](http://arxiv.org/pdf/2507.08862v1)**

> **作者:** Tianzhe Zhao; Jiaoyan Chen; Yanchi Ru; Haiping Zhu; Nan Hu; Jun Liu; Qika Lin
>
> **备注:** 13 pages, 6 figures
>
> **摘要:** Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving external data to mitigate hallucinations and outdated knowledge issues. Benefiting from the strong ability in facilitating diverse data sources and supporting faithful reasoning, knowledge graphs (KGs) have been increasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG) methods. Though RAG systems are widely applied in various applications, recent studies have also revealed its vulnerabilities to data poisoning attacks, where malicious information injected into external knowledge sources can mislead the system into producing incorrect or harmful responses. However, these studies focus exclusively on RAG systems using unstructured textual data sources, leaving the security risks of KG-RAG largely unexplored, despite the fact that KGs present unique vulnerabilities due to their structured and editable nature. In this work, we conduct the first systematic investigation of the security issue of KG-RAG methods through data poisoning attacks. To this end, we introduce a practical, stealthy attack setting that aligns with real-world implementation. We propose an attack strategy that first identifies adversarial target answers and then inserts perturbation triples to complete misleading inference chains in the KG, increasing the likelihood that KG-RAG methods retrieve and rely on these perturbations during generation. Through extensive experiments on two benchmarks and four recent KG-RAG methods, our attack strategy demonstrates strong effectiveness in degrading KG-RAG performance, even with minimal KG perturbations. In-depth analyses are also conducted to understand the safety threats within the internal stages of KG-RAG systems and to explore the robustness of LLMs against adversarial knowledge.
>
---
#### [new 087] Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey
- **分类: cs.AI; cs.CL**

- **简介: 该论文属于自然语言处理任务，旨在解决大型推理模型生成冗长推理链的问题。论文综述了实现简洁与自适应推理的方法、基准和挑战，以提升模型效率与实用性。**

- **链接: [http://arxiv.org/pdf/2507.09662v1](http://arxiv.org/pdf/2507.09662v1)**

> **作者:** Jason Zhu; Hongyu Li
>
> **摘要:** Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have demonstrated impressive performance on complex reasoning tasks like mathematics and programming with long Chain-of-Thought (CoT) reasoning sequences (slow-thinking), compared with traditional large language models (fast-thinking). However, these reasoning models also face a huge challenge that generating unnecessarily lengthy and redundant reasoning chains even for trivial questions. This phenomenon leads to a significant waste of inference resources, increases the response time for simple queries, and hinders the practical application of LRMs in real-world products. To this end, it is crucial to shorten lengthy reasoning chains and learn adaptive reasoning between fast and slow thinking based on input difficulty. In this survey, we provide a comprehensive overview of recent progress in concise and adaptive thinking for efficient reasoning of LRMs, including methodologies, benchmarks, and challenges for future exploration. We hope this survey can help researchers quickly understand the landscape of this field and inspire novel adaptive thinking ideas to facilitate better usage of LRMs.
>
---
#### [new 088] Overview of the TREC 2023 deep learning track
- **分类: cs.IR; cs.AI; cs.CL**

- **简介: 该论文属于信息检索任务，旨在评估不同深度学习模型在搜索排序中的表现。论文使用MS MARCO数据集和合成查询，比较了大型语言模型提示方法与传统“nnlm”方法的效果。结果表明，前者在多个指标上表现更优，且系统排序在合成查询与人工查询间具有一致性。**

- **链接: [http://arxiv.org/pdf/2507.08890v1](http://arxiv.org/pdf/2507.08890v1)**

> **作者:** Nick Craswell; Bhaskar Mitra; Emine Yilmaz; Hossein A. Rahmani; Daniel Campos; Jimmy Lin; Ellen M. Voorhees; Ian Soboroff
>
> **备注:** arXiv admin note: substantial text overlap with arXiv:2507.08191
>
> **摘要:** This is the fifth year of the TREC Deep Learning track. As in previous years, we leverage the MS MARCO datasets that made hundreds of thousands of human-annotated training labels available for both passage and document ranking tasks. We mostly repeated last year's design, to get another matching test set, based on the larger, cleaner, less-biased v2 passage and document set, with passage ranking as primary and document ranking as a secondary task (using labels inferred from passage). As we did last year, we sample from MS MARCO queries that were completely held out, unused in corpus construction, unlike the test queries in the first three years. This approach yields a more difficult test with more headroom for improvement. Alongside the usual MS MARCO (human) queries from MS MARCO, this year we generated synthetic queries using a fine-tuned T5 model and using a GPT-4 prompt. The new headline result this year is that runs using Large Language Model (LLM) prompting in some way outperformed runs that use the "nnlm" approach, which was the best approach in the previous four years. Since this is the last year of the track, future iterations of prompt-based ranking can happen in other tracks. Human relevance assessments were applied to all query types, not just human MS MARCO queries. Evaluation using synthetic queries gave similar results to human queries, with system ordering agreement of $\tau=0.8487$. However, human effort was needed to select a subset of the synthetic queries that were usable. We did not see clear evidence of bias, where runs using GPT-4 were favored when evaluated using synthetic GPT-4 queries, or where runs using T5 were favored when evaluated on synthetic T5 queries.
>
---
#### [new 089] Think Clearly: Improving Reasoning via Redundant Token Pruning
- **分类: cs.AI; cs.CL; cs.LG**

- **简介: 该论文属于自然语言处理任务，旨在提升大语言模型的推理能力。通过分析注意力机制，发现推理路径中存在冗余，尤其错误答案注意力更稀疏。作者提出一种结构感知剪枝方法，移除低贡献的冗余token，从而提升推理准确性，尤其在数学竞赛类难题上效果显著。**

- **链接: [http://arxiv.org/pdf/2507.08806v1](http://arxiv.org/pdf/2507.08806v1)**

> **作者:** Daewon Choi; Jimin Lee; Jihoon Tack; Woomin Song; Saket Dingliwal; Sai Muralidhar Jayanthi; Bhavana Ganesh; Jinwoo Shin; Aram Galstyan; Sravan Babu Bodapati
>
> **摘要:** Recent large language models have shown promising capabilities in long-form reasoning, following structured chains of thought before arriving at a final answer. However, we observe that these reasoning paths tend to include substantial redundancy; analyzing attention patterns reveals that attention scores are widely scattered, particularly incorrect answers exhibit greater attention sparsity. In this paper, we demonstrate that deliberately removing this redundancy in the reasoning process significantly improves performance through clear thinking, i.e., removing distraction. Specifically, we systematically identify reasoning redundancy by measuring token-level attention scores to a special end-of-thinking token, which is appended to an explicit instruction inserted to conclude each intermediate reasoning step. Furthermore, we propose structure-aware pruning that prioritizes removing tokens in low-contributing reasoning chunks over individual tokens. After evicting redundant tokens, we remove the injected end-of-thinking instruction, then resume the reasoning generation. We demonstrate that our method significantly improves overall accuracy across reasoning-intensive benchmarks without any training involved. In particular, our method shows strong performance on challenging mathematical competition benchmarks such as AIME and AMC, where reasoning redundancy is more prevalent.
>
---
#### [new 090] EmbRACE-3K: Embodied Reasoning and Action in Complex Environments
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 论文提出EmRACE-3K数据集，属于具身智能任务，旨在解决视觉-语言模型在动态交互环境中推理与规划能力不足的问题。数据集包含3000余个语言引导任务，涵盖导航、操作与多步骤目标执行，用于评估模型在探索、空间语义推理和多阶段任务中的表现，并通过微调提升模型能力。**

- **链接: [http://arxiv.org/pdf/2507.10548v1](http://arxiv.org/pdf/2507.10548v1)**

> **作者:** Mingxian Lin; Wei Huang; Yitang Li; Chengjie Jiang; Kui Wu; Fangwei Zhong; Shengju Qian; Xin Wang; Xiaojuan Qi
>
> **备注:** Project page: https://mxllc.github.io/EmbRACE-3K/
>
> **摘要:** Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.
>
---
#### [new 091] Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers
- **分类: cs.SD; cs.CL; eess.AS; I.2.7; I.5.5**

- **简介: 该论文属于语音情感分析任务，旨在解决在保护隐私的前提下，通过匿名化语音数据实现空中交通管制员（ATCO）压力检测的问题。作者评估了多种深度学习架构，在两个匿名化语音数据集上进行了实验，取得了较高准确率，表明隐私保护与模型性能可兼顾。**

- **链接: [http://arxiv.org/pdf/2507.08882v1](http://arxiv.org/pdf/2507.08882v1)**

> **作者:** Janaki Viswanathan; Alexander Blatt; Konrad Hagemann; Dietrich Klakow
>
> **备注:** 8 pages, 2 figures, 4 tables, publication identification number (URN)- urn:nbn:de:101:1-2022122008393409239462, see archived online publication- https://d-nb.info/127614606X/34 & Katalogeintrag: https://d-nb.info/127614606X/
>
> **摘要:** Air traffic control (ATC) demands multi-tasking under time pressure with high consequences of an error. This can induce stress. Detecting stress is a key point in maintaining the high safety standards of ATC. However, processing ATC voice data entails privacy restrictions, e.g. the General Data Protection Regulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with these restrictions. In this paper, different architectures for stress detection for anonymized ATCO speech are evaluated. Our best networks reach a stress detection accuracy of 93.6% on an anonymized version of the Speech Under Simulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our anonymized ATC simulation dataset. This shows that privacy does not have to be an impediment in building well-performing deep-learning-based models.
>
---
#### [new 092] PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization
- **分类: cs.IR; cs.AI; cs.CL; cs.LG**

- **简介: 该论文属于科学论文检索任务，旨在解决现有方法仅依赖摘要导致信息不足的问题。作者提出了PRISM，通过多细粒度表示查询与候选论文，并构建新基准SciFullBench，以提升文档到文档的检索性能。实验表明，该方法平均提升了4.3%的检索效果。**

- **链接: [http://arxiv.org/pdf/2507.10057v1](http://arxiv.org/pdf/2507.10057v1)**

> **作者:** Sangwoo Park; Jinheon Baek; Soyeong Jeong; Sung Ju Hwang
>
> **摘要:** Scientific paper retrieval, particularly framed as document-to-document retrieval, aims to identify relevant papers in response to a long-form query paper, rather than a short query string. Previous approaches to this task have focused on abstracts, embedding them into dense vectors as surrogates for full documents and calculating similarity across them, although abstracts provide only sparse and high-level summaries. To address this, we propose PRISM, a novel document-to-document retrieval method that introduces multiple, fine-grained representations for both the query and candidate papers. In particular, each query paper is decomposed into multiple aspect-specific views and individually embedded, which are then matched against candidate papers similarity segmented to consider their multifaceted dimensions. Moreover, we present SciFullBench, a novel benchmark in which the complete and segmented context of full papers for both queries and candidates is available. Then, experimental results show that PRISM improves performance by an average of 4.3% over existing retrieval baselines.
>
---
#### [new 093] Multiple Choice Learning of Low Rank Adapters for Language Modeling
- **分类: cs.LG; cs.AI; cs.CL; stat.ML**

- **简介: 该论文属于语言建模任务，旨在解决传统模型在生成句子时缺乏多样性和应对歧义的问题。作者提出了LoRA-MCL方法，结合了多选学习和低秩适应技术，以提高生成文本的多样性和相关性，并通过理论分析与实验验证其有效性。**

- **链接: [http://arxiv.org/pdf/2507.10419v1](http://arxiv.org/pdf/2507.10419v1)**

> **作者:** Victor Letzelter; Hugo Malard; Mathieu Fontaine; Gaël Richard; Slim Essid; Andrei Bursuc; Patrick Pérez
>
> **摘要:** We propose LoRA-MCL, a training scheme that extends next-token prediction in language models with a method designed to decode diverse, plausible sentence continuations at inference time. Traditional language modeling is an intrinsically ill-posed problem: given a context, multiple futures may be equally plausible. Our approach leverages Multiple Choice Learning (MCL) and the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying Multiple Choice Learning to Language Modeling, assuming the data is generated from a mixture of distributions. To illustrate the proposed approach, we use data sampled from mixtures of Markov chains. We then demonstrate with extensive experiments on real-world visual and audio captioning tasks that our method achieves high diversity and relevance in generated outputs.
>
---
#### [new 094] Devanagari Handwritten Character Recognition using Convolutional Neural Network
- **分类: cs.CV; cs.AI; cs.CL; 14J60; I.2.7; I.4; I.5; I.7.5**

- **简介: 该论文属于图像识别任务，旨在解决手写Devanagari字符的自动识别问题。作者使用卷积神经网络，基于DHCD数据集进行训练与测试，提升了识别准确率，达到96.36%的测试精度。**

- **链接: [http://arxiv.org/pdf/2507.10398v1](http://arxiv.org/pdf/2507.10398v1)**

> **作者:** Diksha Mehta; Prateek Mehta
>
> **备注:** 9 pages, 6 figures
>
> **摘要:** Handwritten character recognition is getting popular among researchers because of its possible applications in facilitating technological search engines, social media, recommender systems, etc. The Devanagari script is one of the oldest language scripts in India that does not have proper digitization tools. With the advancement of computing and technology, the task of this research is to extract handwritten Hindi characters from an image of Devanagari script with an automated approach to save time and obsolete data. In this paper, we present a technique to recognize handwritten Devanagari characters using two deep convolutional neural network layers. This work employs a methodology that is useful to enhance the recognition rate and configures a convolutional neural network for effective Devanagari handwritten text recognition (DHTR). This approach uses the Devanagari handwritten character dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each of these classes has 1700 images for training and testing purposes. This approach obtains promising results in terms of accuracy by achieving 96.36% accuracy in testing and 99.55% in training time.
>
---
#### [new 095] MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora
- **分类: cs.IR; cs.AI; cs.CL; cs.LG**

- **简介: 该论文属于生成检索任务，旨在解决动态语料库下模型索引持续更新的问题。现有方法需全量重训练，计算成本高。作者提出MixLoRA-DSI，结合低秩适应专家与分布外检测策略，实现参数亚线性增长，减少训练开销，提升更新效率。**

- **链接: [http://arxiv.org/pdf/2507.09924v1](http://arxiv.org/pdf/2507.09924v1)**

> **作者:** Tuan-Luc Huynh; Thuy-Trang Vu; Weiqing Wang; Trung Le; Dragan Gašević; Yuan-Fang Li; Thanh-Toan Do
>
> **摘要:** Continually updating model-based indexes in generative retrieval with new documents remains challenging, as full retraining is computationally expensive and impractical under resource constraints. We propose MixLoRA-DSI, a novel framework that combines an expandable mixture of Low-Rank Adaptation experts with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead of allocating new experts for each new corpus, our proposed expansion strategy enables sublinear parameter growth by selectively introducing new experts only when significant number of OOD documents are detected. Experiments on NQ320k and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update baselines, with minimal parameter overhead and substantially lower training costs.
>
---
#### [new 096] Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect
- **分类: cs.CV; cs.CL**

- **简介: 该论文研究视觉-语言模型（VLMs）是否具备类似人类的跨模态认知能力，聚焦于“bouba-kiki效应”这一经典测试案例。作者以CLIP的两种变体（ResNet和ViT）为对象，采用基于提示的概率评估与Grad-CAM视觉注意力解释方法，发现模型未能稳定表现出该效应，揭示其在跨模态理解上的局限性。**

- **链接: [http://arxiv.org/pdf/2507.10013v1](http://arxiv.org/pdf/2507.10013v1)**

> **作者:** Tom Kouwenhoven; Kiana Shahrasbi; Tessa Verhoef
>
> **摘要:** Recent advances in multimodal models have raised questions about whether vision-and-language models (VLMs) integrate cross-modal information in ways that reflect human cognition. One well-studied test case in this domain is the bouba-kiki effect, where humans reliably associate pseudowords like "bouba" with round shapes and "kiki" with jagged ones. Given the mixed evidence found in prior studies for this effect in VLMs, we present a comprehensive re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer (ViT), given their centrality in many state-of-the-art VLMs. We apply two complementary methods closely modelled after human experiments: a prompt-based evaluation that uses probabilities as model preference, and we use Grad-CAM as a novel way to interpret visual attention in shape-word matching tasks. Our findings show that these models do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both models lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models' responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition. These results contribute to the ongoing debate about the extent to which VLMs truly understand cross-modal concepts, highlighting limitations in their internal representations and alignment with human intuitions.
>
---
## 更新

#### [replaced 001] Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence
- **分类: cs.CV; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2507.01504v2](http://arxiv.org/pdf/2507.01504v2)**

> **作者:** Robert Aufschläger; Youssef Shoeb; Azarm Nowzad; Michael Heigl; Fabian Bally; Martin Schramm
>
> **备注:** accepted for publication at the 2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC 2025), taking place during November 18-21, 2025 in Gold Coast, Australia
>
> **摘要:** The collection and release of street-level recordings as Open Data play a vital role in advancing autonomous driving systems and AI research. However, these datasets pose significant privacy risks, particularly for pedestrians, due to the presence of Personally Identifiable Information (PII) that extends beyond biometric traits such as faces. In this paper, we present cRID, a novel cross-modal framework combining Large Vision-Language Models, Graph Attention Networks, and representation learning to detect textual describable clues of PII and enhance person re-identification (Re-ID). Our approach focuses on identifying and leveraging interpretable features, enabling the detection of semantically meaningful PII beyond low-level appearance cues. We conduct a systematic evaluation of PII presence in person image datasets. Our experiments show improved performance in practical cross-dataset Re-ID scenarios, notably from Market-1501 to CUHK03-np (detected), highlighting the framework's practical utility. Code is available at https://github.com/RAufschlaeger/cRID.
>
---
#### [replaced 002] IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable LLM-Generated Text Detector
- **分类: cs.LG; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2502.15902v2](http://arxiv.org/pdf/2502.15902v2)**

> **作者:** Zheng Chen; Yushi Feng; Changyang He; Yue Deng; Hongxi Pu; Bo Li
>
> **摘要:** Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinction between human-written and LLM-generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide interpretable evidence to support their decisions, thus undermining the reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and two Distinguishers that examine the probability that the input texts align with the predicted prompts. Empirical evaluations demonstrate that IPAD outperforms the strongest baselines by 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on out-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also performs robustly on structured datasets. Furthermore, an interpretability assessment is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.
>
---
#### [replaced 003] TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2506.18421v2](http://arxiv.org/pdf/2506.18421v2)**

> **作者:** Ce Li; Xiaofan Liu; Zhiyan Song; Ce Chi; Chen Zhao; Jingjing Yang; Zhendong Wang; Kexin Yang; Boshen Shi; Xing Wang; Chao Deng; Junlan Feng
>
> **备注:** Benmark report v1.1
>
> **摘要:** The majority of data in businesses and industries is stored in tables, databases, and data warehouses. Reasoning with table-structured data poses significant challenges for large language models (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective evaluation benchmark fairly reflecting the performances of LLMs on broad table reasoning abilities. In this paper, we fill in this gap, presenting a comprehensive table reasoning evolution benchmark, TReB, which measures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks. We construct a high quality dataset through an iterative data processing procedure. We create an evaluation framework to robustly measure table reasoning capabilities with three distinct inference modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs using this frame work and prove its effectiveness. Experimental results reveal that existing LLMs still have significant room for improvement in addressing the complex and real world Table related tasks. Both the dataset and evaluation framework are publicly available, with the dataset hosted on huggingface.co/datasets/JT-LM/JIUTIAN-TReB and the framework on github.com/JT-LM/jiutian-treb.
>
---
#### [replaced 004] MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in Indonesian
- **分类: cs.MM; cs.CL; cs.CV; cs.LG; eess.IV**

- **链接: [http://arxiv.org/pdf/2306.11341v2](http://arxiv.org/pdf/2306.11341v2)**

> **作者:** Willy Fitra Hendria
>
> **备注:** 10 pages, 5 figures, 5 tables
>
> **摘要:** Multimodal learning on video and text has seen significant progress, particularly in tasks like text-to-video retrieval, video-to-text retrieval, and video captioning. However, most existing methods and datasets focus exclusively on English. Despite Indonesian being one of the most widely spoken languages, multimodal research in Indonesian remains under-explored, largely due to the lack of benchmark datasets. To address this gap, we introduce the first public Indonesian video-text dataset by translating the English captions in the MSVD dataset into Indonesian. Using this dataset, we evaluate neural network models which were developed for the English video-text dataset on three tasks, i.e., text-to-video retrieval, video-to-text retrieval, and video captioning. Most existing models rely on feature extractors pretrained on English vision-language datasets, raising concerns about their applicability to Indonesian, given the scarcity of large-scale pretraining resources in the language. We apply a cross-lingual transfer learning approach by leveraging English-pretrained extractors and fine-tuning models on our Indonesian dataset. Experimental results demonstrate that this strategy improves performance across all tasks and metrics. We release our dataset publicly to support future research and hope it will inspire further progress in Indonesian multimodal learning.
>
---
#### [replaced 005] Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code
- **分类: cs.CL; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.07498v2](http://arxiv.org/pdf/2507.07498v2)**

> **作者:** Keqin Bao; Nuo Chen; Xiaoyuan Li; Binyuan Hui; Bowen Yu; Fuli Feng; Xiangnan He; Dayiheng Liu
>
> **摘要:** Enhancing reasoning capabilities remains a central focus in the LLM reasearch community. A promising direction involves requiring models to simulate code execution step-by-step to derive outputs for given inputs. However, as code is often designed for large-scale systems, direct application leads to over-reliance on complex data structures and algorithms, even for simple cases, resulting in overfitting to algorithmic patterns rather than core reasoning structures. To address this, we propose TeaR, which aims at teaching LLMs to reason better. TeaR leverages careful data curation and reinforcement learning to guide models in discovering optimal reasoning paths through code-related tasks, thereby improving general reasoning abilities. We conduct extensive experiments using two base models and three long-CoT distillation models, with model sizes ranging from 1.5 billion to 32 billion parameters, and across 17 benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results consistently show significant performance improvements. Notably, TeaR achieves a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.
>
---
#### [replaced 006] Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2405.11870v3](http://arxiv.org/pdf/2405.11870v3)**

> **作者:** Ermo Hua; Biqing Qi; Kaiyan Zhang; Kai Tian; Xingtai Lv; Ning Ding; Bowen Zhou
>
> **备注:** Accepted to ACL 2025, Oral & Panel Discussion
>
> **摘要:** Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are key processes for aligning Language Models (LMs) with human preferences post pre-training. While SFT excels in efficiency and PO in effectiveness, they are often combined sequentially without integrating their optimization objectives. This approach ignores the opportunities to bridge their paradigm gap and take the strengths from both. In this paper, we interpret SFT and PO with two sub-processes -- Preference Estimation and Transition Optimization -- defined at token level within the Markov Decision Process (MDP). This modeling shows that SFT is only a special case of PO with inferior estimation and optimization. PO estimates the model's preference by its entire generation, while SFT only scores model's subsequent predicted tokens based on prior tokens from ground truth answer. These priors deviates from model's distribution, hindering the preference estimation and transition optimization. Building on this view, we introduce Intuitive Fine-Tuning (IFT) to integrate SFT and PO into a single process. Through a temporal residual connection, IFT brings better estimation and optimization by capturing LMs' intuitive sense of its entire answers. But it solely relies on a single policy and the same volume of non-preference-labeled data as SFT. Our experiments show that IFT performs comparably or even superiorly to SFT and some typical PO methods across several tasks, particularly those require generation, reasoning, and fact-following abilities. An explainable Frozen Lake game further validates the effectiveness of IFT for getting competitive policy.
>
---
#### [replaced 007] Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2506.23146v3](http://arxiv.org/pdf/2506.23146v3)**

> **作者:** Dingzriui Wang; Xuanliang Zhang; Keyan Xu; Qingfu Zhu; Wanxiang Che; Yang Deng
>
> **摘要:** In-context learning (ICL) has emerged as an effective approach to enhance the performance of large language models (LLMs). However, its effectiveness varies significantly across models and tasks, posing challenges for practitioners to determine when ICL reliably improves performance. Current evaluation approaches, reliant on performance change after applying ICL, suffer from low reliability, poor attribution, and impracticality in data-insufficient scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that quantifies ICL effectiveness by modeling the slope between learning gain (loss decrease from demonstrations) and contextual relevance (demonstration-input relevance). LCS addresses key limitations of performance-based metrics: (1) it captures continuous loss changes even when outputs are incorrect, improving reliability; (2) its formulation attributes ICL failures to weak contextual alignment (inability to adapt inputs to demonstrations) or strong output calibration (self-verification of correctness); and (3) it minimizes reliance on labeled data via synthetic evaluation. Extensive experiments demonstrate that LCS strongly correlates with performance improvements in labeled settings and reliably reflects true effectiveness in biased or data-scarce scenarios. Further analysis reveals actionable thresholds for LCS and identifies model capabilities critical to ICL success.
>
---
#### [replaced 008] MedGemma Technical Report
- **分类: cs.AI; cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2507.05201v3](http://arxiv.org/pdf/2507.05201v3)**

> **作者:** Andrew Sellergren; Sahar Kazemzadeh; Tiam Jaroensri; Atilla Kiraly; Madeleine Traverse; Timo Kohlberger; Shawn Xu; Fayaz Jamil; Cían Hughes; Charles Lau; Justin Chen; Fereshteh Mahvar; Liron Yatziv; Tiffany Chen; Bram Sterling; Stefanie Anna Baby; Susanna Maria Baby; Jeremy Lai; Samuel Schmidgall; Lu Yang; Kejia Chen; Per Bjornsson; Shashir Reddy; Ryan Brush; Kenneth Philbrick; Mercy Asiedu; Ines Mezerreg; Howard Hu; Howard Yang; Richa Tiwari; Sunny Jansen; Preeti Singh; Yun Liu; Shekoofeh Azizi; Aishwarya Kamath; Johan Ferret; Shreya Pathak; Nino Vieillard; Ramona Merhej; Sarah Perrin; Tatiana Matejovicova; Alexandre Ramé; Morgane Riviere; Louis Rouillard; Thomas Mesnard; Geoffrey Cideron; Jean-bastien Grill; Sabela Ramos; Edouard Yvinec; Michelle Casbon; Elena Buchatskaya; Jean-Baptiste Alayrac; Dmitry Lepikhin; Vlad Feinberg; Sebastian Borgeaud; Alek Andreev; Cassidy Hardin; Robert Dadashi; Léonard Hussenot; Armand Joulin; Olivier Bachem; Yossi Matias; Katherine Chou; Avinatan Hassidim; Kavi Goel; Clement Farabet; Joelle Barral; Tris Warkentin; Jonathon Shlens; David Fleet; Victor Cotruta; Omar Sanseviero; Gus Martins; Phoebe Kirk; Anand Rao; Shravya Shetty; David F. Steiner; Can Kirmizibayrak; Rory Pilgrim; Daniel Golden; Lin Yang
>
> **摘要:** Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment faces challenges due to healthcare's diverse data, complex tasks, and the need to preserve privacy. Foundation models that perform well on medical tasks and require less task-specific tuning data are critical to accelerate the development of healthcare AI applications. We introduce MedGemma, a collection of medical vision-language foundation models based on Gemma 3 4B and 27B. MedGemma demonstrates advanced medical understanding and reasoning on images and text, significantly exceeding the performance of similar-sized generative models and approaching the performance of task-specific models, while maintaining the general capabilities of the Gemma 3 base models. For out-of-distribution tasks, MedGemma achieves 2.6-10% improvement on medical multimodal question answering, 15.5-18.1% improvement on chest X-ray finding classification, and 10.8% improvement on agentic evaluations compared to the base models. Fine-tuning MedGemma further improves performance in subdomains, reducing errors in electronic health record information retrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods for pneumothorax classification and histopathology patch classification. We additionally introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP. MedSigLIP powers the visual understanding capabilities of MedGemma and as an encoder achieves comparable or better performance than specialized medical image encoders. Taken together, the MedGemma collection provides a strong foundation of medical image and text capabilities, with potential to significantly accelerate medical research and development of downstream applications. The MedGemma collection, including tutorials and model weights, can be found at https://goo.gle/medgemma.
>
---
#### [replaced 009] Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights
- **分类: cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2507.08036v2](http://arxiv.org/pdf/2507.08036v2)**

> **作者:** Deepali Mishra; Chaklam Silpasuwanchai; Ashutosh Modi; Madhumita Sushil; Sorayouth Chumnanvej
>
> **备注:** 29 pages, 5 figures (1 in supplementary), 3 tables (1 in main text, 2 in supplementary). Scoping review and clinician survey
>
> **摘要:** Medical Visual Question Answering (MedVQA) is a promising tool to assist radiologists by automating medical image interpretation through question answering. Despite advances in models and datasets, MedVQA's integration into clinical workflows remains limited. This study systematically reviews 68 publications (2018-2024) and surveys 50 clinicians from India and Thailand to examine MedVQA's practical utility, challenges, and gaps. Following the Arksey and O'Malley scoping review framework, we used a two-pronged approach: (1) reviewing studies to identify key concepts, advancements, and research gaps in radiology workflows, and (2) surveying clinicians to capture their perspectives on MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs are non-diagnostic and lack clinical relevance. Most datasets and models do not support multi-view, multi-resolution imaging, EHR integration, or domain knowledge, features essential for clinical diagnosis. Furthermore, there is a clear mismatch between current evaluation metrics and clinical needs. The clinician survey confirms this disconnect: only 29.8% consider MedVQA systems highly useful. Key concerns include the absence of patient history or domain knowledge (87.2%), preference for manually curated datasets (51.1%), and the need for multi-view image support (78.7%). Additionally, 66% favor models focused on specific anatomical regions, and 89.4% prefer dialogue-based interactive systems. While MedVQA shows strong potential, challenges such as limited multimodal analysis, lack of patient context, and misaligned evaluation approaches must be addressed for effective clinical integration.
>
---
#### [replaced 010] Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2504.11183v2](http://arxiv.org/pdf/2504.11183v2)**

> **作者:** Ej Zhou; Weiming Lu
>
> **摘要:** Social bias in language models can potentially exacerbate social inequalities. Despite it having garnered wide attention, most research focuses on English data. In a low-resource scenario, the models often perform worse due to insufficient training data. This study aims to leverage high-resource language corpora to evaluate bias and experiment with debiasing methods in low-resource languages. We evaluated the performance of recent multilingual models in five languages: English, Chinese, Russian, Indonesian and Thai, and analyzed four bias dimensions: gender, religion, nationality, and race-color. By constructing multilingual bias evaluation datasets, this study allows fair comparisons between models across languages. We have further investigated three debiasing methods-CDA, Dropout, SenDeb-and demonstrated that debiasing methods from high-resource languages can be effectively transferred to low-resource ones, providing actionable insights for fairness research in multilingual NLP.
>
---
#### [replaced 011] LLM Agents Are the Antidote to Walled Gardens
- **分类: cs.LG; cs.CL; cs.CY; cs.SI; 68T50, 68M10, 91B26; I.2.11; I.2.7; H.4.5**

- **链接: [http://arxiv.org/pdf/2506.23978v2](http://arxiv.org/pdf/2506.23978v2)**

> **作者:** Samuele Marro; Philip Torr
>
> **摘要:** While the Internet's core infrastructure was designed to be open and universal, today's application layer is dominated by closed, proprietary platforms. Open and interoperable APIs require significant investment, and market leaders have little incentive to enable data exchange that could erode their user lock-in. We argue that LLM-based agents fundamentally disrupt this status quo. Agents can automatically translate between data formats and interact with interfaces designed for humans: this makes interoperability dramatically cheaper and effectively unavoidable. We name this shift universal interoperability: the ability for any two digital services to exchange data seamlessly using AI-mediated adapters. Universal interoperability undermines monopolistic behaviours and promotes data portability. However, it can also lead to new security risks and technical debt. Our position is that the ML community should embrace this development while building the appropriate frameworks to mitigate the downsides. By acting now, we can harness AI to restore user freedom and competitive markets without sacrificing security.
>
---
#### [replaced 012] LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop
- **分类: cs.CY; cs.AI; cs.CL; cs.HC**

- **链接: [http://arxiv.org/pdf/2507.04295v2](http://arxiv.org/pdf/2507.04295v2)**

> **作者:** Runcong Zhao; Artem Bobrov; Jiazheng Li; Yulan He
>
> **摘要:** Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.
>
---
#### [replaced 013] Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media
- **分类: cs.CL; cs.AI; cs.SI**

- **链接: [http://arxiv.org/pdf/2504.12355v2](http://arxiv.org/pdf/2504.12355v2)**

> **作者:** Muhammad Ahmad; Fida Ullah; Ummhy Habiba; ldar Batyrshin; Grigori Sidorov
>
> **摘要:** Drug overdose remains a critical global health issue, often driven by misuse of opioids, painkillers, and psychiatric medications. Traditional research methods face limitations, whereas social media offers real-time insights into self-reported substance use and overdose symptoms. This study proposes an AI-driven NLP framework trained on annotated social media data to detect commonly used drugs and associated overdose symptoms. Using a hybrid annotation strategy with LLMs and human annotators, we applied traditional ML models, neural networks, and advanced transformer-based models. Our framework achieved 98% accuracy in multi-class and 97% in multi-label classification, outperforming baseline models by up to 8%. These findings highlight the potential of AI for supporting public health surveillance and personalized intervention strategies.
>
---
#### [replaced 014] Structuring Radiology Reports: Challenging LLMs with Lightweight Models
- **分类: cs.CL; cs.LG**

- **链接: [http://arxiv.org/pdf/2506.00200v2](http://arxiv.org/pdf/2506.00200v2)**

> **作者:** Johannes Moll; Louisa Fay; Asfandyar Azhar; Sophie Ostmeier; Tim Lueth; Sergios Gatidis; Curtis Langlotz; Jean-Benoit Delbrouck
>
> **摘要:** Radiology reports are critical for clinical decision-making but often lack a standardized format, limiting both human interpretability and machine learning (ML) applications. While large language models (LLMs) have shown strong capabilities in reformatting clinical text, their high computational requirements, lack of transparency, and data privacy concerns hinder practical deployment. To address these challenges, we explore lightweight encoder-decoder models (<300M parameters)-specifically T5 and BERT2BERT-for structuring radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark these models against eight open-source LLMs (1B-70B), adapted using prefix prompting, in-context learning (ICL), and low-rank adaptation (LoRA) finetuning. Our best-performing lightweight model outperforms all LLMs adapted using prompt-based techniques on a human-annotated test set. While some LoRA-finetuned LLMs achieve modest gains over the lightweight model on the Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%, GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of substantially greater computational resources. For example, LLaMA-3-70B incurred more than 400 times the inference time, cost, and carbon emissions compared to the lightweight model. These results underscore the potential of lightweight, task-specific models as sustainable and privacy-preserving solutions for structuring clinical text in resource-constrained healthcare settings.
>
---
#### [replaced 015] TheraGen: Therapy for Every Generation
- **分类: cs.CL; cs.AI; cs.HC**

- **链接: [http://arxiv.org/pdf/2409.13748v2](http://arxiv.org/pdf/2409.13748v2)**

> **作者:** Kartikey Doshi; Jimit Shah; Narendra Shekokar
>
> **备注:** This paper contains major errors in methodology and results. It should not be cited
>
> **摘要:** We present TheraGen, an advanced AI-powered mental health chatbot utilizing the LLaMA 2 7B model. This approach builds upon recent advancements in language models and transformer architectures. TheraGen provides all-day personalized, compassionate mental health care by leveraging a large dataset of 1 million conversational entries, combining anonymized therapy transcripts, online mental health discussions, and psychological literature, including APA resources. Our implementation employs transfer learning, fine-tuning, and advanced training techniques to optimize performance. TheraGen offers a user-friendly interface for seamless interaction, providing empathetic responses and evidence-based coping strategies. Evaluation results demonstrate high user satisfaction rates, with 94% of users reporting improved mental well-being. The system achieved a BLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response accuracy. With an average response time of 1395 milliseconds, TheraGen ensures real-time, efficient support. While not a replacement for professional therapy, TheraGen serves as a valuable complementary tool, significantly improving user well-being and addressing the accessibility gap in mental health treatments. This paper details TheraGen's architecture, training methodology, ethical considerations, and future directions, contributing to the growing field of AI-assisted mental healthcare and offering a scalable solution to the pressing need for mental health support.
>
---
#### [replaced 016] LEXam: Benchmarking Legal Reasoning on 340 Law Exams
- **分类: cs.CL; cs.AI; cs.LG; 68T50; I.2**

- **链接: [http://arxiv.org/pdf/2505.12864v3](http://arxiv.org/pdf/2505.12864v3)**

> **作者:** Yu Fan; Jingwei Ni; Jakob Merane; Etienne Salimbeni; Yang Tian; Yoan Hermstrüwer; Yinya Huang; Mubashara Akhtar; Florian Geering; Oliver Dreyer; Daniel Brunner; Markus Leippold; Mrinmaya Sachan; Alexander Stremitzer; Christoph Engel; Elliott Ash; Joel Niklaus
>
> **摘要:** Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: https://lexam-benchmark.github.io/
>
---
#### [replaced 017] Beyond classical and contemporary models: a transformative AI framework for student dropout prediction in distance learning using RAG, Prompt engineering, and Cross-modal fusion
- **分类: cs.CL; cs.AI; cs.CY; cs.IR; I.2.7; I.2.1; K.3.1**

- **链接: [http://arxiv.org/pdf/2507.05285v2](http://arxiv.org/pdf/2507.05285v2)**

> **作者:** Miloud Mihoubi; Meriem Zerkouk; Belkacem Chikhaoui
>
> **备注:** 13 pages, 8 figures, 1 Algorithms, 17th International Conference on Education and New Learning Technologies,: 30 June-2 July, 2025 Location: Palma, Spain
>
> **摘要:** Student dropout in distance learning remains a critical challenge, with profound societal and economic consequences. While classical machine learning models leverage structured socio-demographic and behavioral data, they often fail to capture the nuanced emotional and contextual factors embedded in unstructured student interactions. This paper introduces a transformative AI framework that redefines dropout prediction through three synergistic innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment analysis, prompt engineering to decode academic stressors,and cross-modal attention fusion to dynamically align textual, behavioral, and socio-demographic insights. By grounding sentiment analysis in a curated knowledge base of pedagogical content, our RAG-enhanced BERT model interprets student comments with unprecedented contextual relevance, while optimized prompts isolate indicators of academic distress (e.g., "isolation," "workload anxiety"). A cross-modal attention layer then fuses these insights with temporal engagement patterns, creating holistic risk pro-files. Evaluated on a longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and an F1-score of 0.88, outperforming conventional models by 7% and reducing false negatives by 21%. Beyond prediction, the system generates interpretable interventions by retrieving contextually aligned strategies (e.g., mentorship programs for isolated learners). This work bridges the gap between predictive analytics and actionable pedagogy, offering a scalable solution to mitigate dropout risks in global education systems
>
---
#### [replaced 018] SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding
- **分类: cs.CL; cs.AI; cs.HC**

- **链接: [http://arxiv.org/pdf/2507.04189v2](http://arxiv.org/pdf/2507.04189v2)**

> **作者:** Runcong Zhao; Qinglin Zhu; Hainiu Xu; Bin Liang; Lin Gui; Yulan He
>
> **摘要:** Understanding character relationships is essential for interpreting complex narratives and conducting socially grounded AI research. However, manual annotation is time-consuming and low in coverage, while large language models (LLMs) often produce hallucinated or logically inconsistent outputs. We present SymbolicThought, a human-in-the-loop framework that combines LLM-based extraction with symbolic reasoning. The system constructs editable character relationship graphs, refines them using seven types of logical constraints, and enables real-time validation and conflict resolution through an interactive interface. To support logical supervision and explainable social analysis, we release a dataset of 160 interpersonal relationships with corresponding logical structures. Experiments show that SymbolicThought improves annotation accuracy and consistency while significantly reducing time cost, offering a practical tool for narrative understanding, explainable AI, and LLM evaluation.
>
---
#### [replaced 019] DTECT: Dynamic Topic Explorer & Context Tracker
- **分类: cs.CL; cs.AI; cs.IR**

- **链接: [http://arxiv.org/pdf/2507.07910v2](http://arxiv.org/pdf/2507.07910v2)**

> **作者:** Suman Adhya; Debarshi Kumar Sanyal
>
> **备注:** Code: https://github.com/AdhyaSuman/DTECT | Demo: https://huggingface.co/spaces/AdhyaSuman/DTECT | Video: https://youtu.be/B8nNfxFoJAU
>
> **摘要:** The explosive growth of textual data over time presents a significant challenge in uncovering evolving themes and trends. Existing dynamic topic modeling techniques, while powerful, often exist in fragmented pipelines that lack robust support for interpretation and user-friendly exploration. We introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end system that bridges the gap between raw textual data and meaningful temporal insights. DTECT provides a unified workflow that supports data preprocessing, multiple model architectures, and dedicated evaluation metrics to analyze the topic quality of temporal topic models. It significantly enhances interpretability by introducing LLM-driven automatic topic labeling, trend analysis via temporally salient words, interactive visualizations with document-level summarization, and a natural language chat interface for intuitive data querying. By integrating these features into a single, cohesive platform, DTECT empowers users to more effectively track and understand thematic dynamics. DTECT is open-source and available at https://github.com/AdhyaSuman/DTECT.
>
---
#### [replaced 020] IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2310.10873v3](http://arxiv.org/pdf/2310.10873v3)**

> **作者:** Shaokun Zhang; Xiaobo Xia; Zhaoqing Wang; Ling-Hao Chen; Jiale Liu; Qingyun Wu; Tongliang Liu
>
> **备注:** Accepted by ICLR 2024
>
> **摘要:** In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influence-driven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.
>
---
#### [replaced 021] DeepGesture: A conversational gesture synthesis system based on emotions and semantics
- **分类: cs.HC; cs.CL; cs.LG; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2507.03147v2](http://arxiv.org/pdf/2507.03147v2)**

> **作者:** Thanh Hoang-Minh
>
> **备注:** Project page: https://deepgesture.github.io
>
> **摘要:** Along with the explosion of large language models, improvements in speech synthesis, advancements in hardware, and the evolution of computer graphics, the current bottleneck in creating digital humans lies in generating character movements that correspond naturally to text or speech inputs. In this work, we present DeepGesture, a diffusion-based gesture synthesis framework for generating expressive co-speech gestures conditioned on multimodal signals - text, speech, emotion, and seed motion. Built upon the DiffuseStyleGesture model, DeepGesture introduces novel architectural enhancements that improve semantic alignment and emotional expressiveness in generated gestures. Specifically, we integrate fast text transcriptions as semantic conditioning and implement emotion-guided classifier-free diffusion to support controllable gesture generation across affective states. To visualize results, we implement a full rendering pipeline in Unity based on BVH output from the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces gestures with improved human-likeness and contextual appropriateness. Our system supports interpolation between emotional states and demonstrates generalization to out-of-distribution speech, including synthetic voices - marking a step forward toward fully multimodal, emotionally aware digital humans. Project page: https://deepgesture.github.io
>
---
#### [replaced 022] Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2507.01853v3](http://arxiv.org/pdf/2507.01853v3)**

> **作者:** Samridhi Raj Sinha; Rajvee Sheth; Abhishek Upperwal; Mayank Singh
>
> **摘要:** The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that address the requirements of linguistically diverse regions, such as India, and go beyond English-centric benchmarks. We introduce EKA-EVAL, a unified evaluation framework that integrates over 35+ benchmarks (including 10 Indic benchmarks) across nine major evaluation categories. The framework provides broader coverage than existing Indian language evaluation tools, offering 11 core capabilities through a modular architecture, seamless integration with Hugging Face and proprietary models, and plug-and-play usability. As the first end-to-end suite for scalable, multilingual LLM benchmarking, the framework combines extensive benchmarks, modular workflows, and dedicated support for low-resource Indian languages to enable inclusive assessment of LLM capabilities across diverse domains. We conducted extensive comparisons against five existing baselines, demonstrating that EKA-EVAL achieves the highest participant ratings in four out of five categories. The framework is open-source and publicly available at: https://github.com/lingo-iitgn/eka-eval.
>
---
#### [replaced 023] SEE: Strategic Exploration and Exploitation for Cohesive In-Context Prompt Optimization
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2402.11347v2](http://arxiv.org/pdf/2402.11347v2)**

> **作者:** Wendi Cui; Zhuohang Li; Hao Sun; Damien Lopez; Kamalika Das; Bradley Malin; Sricharan Kumar; Jiaxin Zhang
>
> **备注:** Accepted to ACL 2025 (Main Conference)
>
> **摘要:** Designing optimal prompts for Large Language Models (LLMs) is a complicated and resource-intensive task, often requiring substantial human expertise and effort. Existing approaches typically separate the optimization of prompt instructions and in-context learning examples, leading to incohesive prompts that are defined and represented by suboptimal task performance. To overcome these challenges, we propose a novel Cohesive In-Context Prompt Optimization framework that refines both prompt instructions and examples. However, formulating such an optimization in the discrete and high-dimensional space of natural language poses significant challenges in both convergence and computational efficiency. To address these issues, we introduce SEE, a scalable and efficient prompt optimization framework that adopts metaheuristic optimization principles and strategically balances exploration and exploitation to enhance optimization performance and achieve efficient convergence. SEE features a quad-phased design that alternates between global traversal (exploration) and local optimization (exploitation) and adaptively chooses LLM operators during the optimization process. We have conducted a comprehensive evaluation across 35 benchmark tasks, and SEE significantly outperforms state-of-the-art baseline methods by a large margin, achieving an average performance gain of 13.94 while reducing computational costs by 58.67.
>
---
#### [replaced 024] HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization
- **分类: cs.CL; cs.LG; I.2.7**

- **链接: [http://arxiv.org/pdf/2504.21018v2](http://arxiv.org/pdf/2504.21018v2)**

> **作者:** Enes Özeren; Yihong Liu; Hinrich Schütze
>
> **备注:** 18 pages, 3 figures, 15 tables. After ACL reviews: Corrected typos, Table 4 caption updated and the order of the results changed, numbers are unchanged. This paper will appear in ACL SRW 2025
>
> **摘要:** Many pre-trained language models (PLMs) exhibit suboptimal performance on mid- and low-resource languages, largely due to limited exposure to these languages during pre-training. A common strategy to address this is to introduce new tokens specific to the target languages, initialize their embeddings, and apply continual pre-training on target-language data. Among such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword embedding initialization heuristic that is both effective and efficient. However, OFA restricts target-language token embeddings to be convex combinations of a fixed number of source-language embeddings, which may limit expressiveness. To overcome this limitation, we propose HYPEROFA, a hypernetwork-based approach for more adaptive token embedding initialization. The hypernetwork is trained to map from an external multilingual word vector space to the PLMs token embedding space using source-language tokens. Once trained, it can generate flexible embeddings for target-language tokens, serving as a good starting point for continual pretraining. Experiments demonstrate that HYPEROFA consistently outperforms random initialization baseline and matches or exceeds the performance of OFA in both continual pre-training convergence and downstream task performance. We make the code publicly available.
>
---
#### [replaced 025] Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization
- **分类: cs.LG; cs.CL**

- **链接: [http://arxiv.org/pdf/2505.24859v2](http://arxiv.org/pdf/2505.24859v2)**

> **作者:** Joschka Braun; Carsten Eickhoff; Seyed Ali Bahrainian
>
> **备注:** 29 pages, 21 figures, published at ICML 2025 Workshop on Reliable and Responsible Foundation Models
>
> **摘要:** Steering vectors are a lightweight method for controlling text properties by adding a learned bias to language model activations at inference time. So far, steering vectors have predominantly been evaluated in multiple-choice settings, while their effectiveness in free-form generation tasks remains understudied. Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of steering vectors in adaptively controlling topical focus, sentiment, toxicity, and readability in abstractive summaries of the NEWTS dataset. We find that steering effectively controls the targeted summary properties, but high steering strengths consistently degrade both intrinsic and extrinsic text quality. Compared to steering, prompting offers weaker control, while preserving text quality. Combining steering and prompting yields the strongest control over text properties and offers the most favorable efficacy-quality trade-off at moderate steering strengths. Our results underscore the practical trade-off between control strength and text quality preservation when applying steering vectors to free-form generation tasks.
>
---
#### [replaced 026] A Survey of Automatic Prompt Optimization with Instruction-focused Heuristic-based Search Algorithm
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2502.18746v2](http://arxiv.org/pdf/2502.18746v2)**

> **作者:** Wendi Cui; Zhuohang Li; Hao Sun; Damien Lopez; Kamalika Das; Bradley A. Malin; Sricharan Kumar; Jiaxin Zhang
>
> **备注:** Accepted to ACL 2025
>
> **摘要:** Recent advances in Large Language Models have led to remarkable achievements across a variety of Natural Language Processing tasks, making prompt engineering increasingly central to guiding model outputs. While manual methods can be effective, they typically rely on intuition and do not automatically refine prompts over time. In contrast, automatic prompt optimization employing heuristic-based search algorithms can systematically explore and improve prompts with minimal human oversight. This survey proposes a comprehensive taxonomy of these methods, categorizing them by where optimization occurs, what is optimized, what criteria drive the optimization, which operators generate new prompts, and which iterative search algorithms are applied. We further highlight specialized datasets and tools that support and accelerate automated prompt refinement. We conclude by discussing key open challenges pointing toward future opportunities for more robust and versatile LLM applications.
>
---
#### [replaced 027] Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models
- **分类: cs.LG; cs.CL; cs.DC**

- **链接: [http://arxiv.org/pdf/2505.17826v2](http://arxiv.org/pdf/2505.17826v2)**

> **作者:** Xuchen Pan; Yanxi Chen; Yushuo Chen; Yuchang Sun; Daoyuan Chen; Wenhao Zhang; Yuexiang Xie; Yilun Huang; Yilei Zhang; Dawei Gao; Weijie Shi; Yaliang Li; Bolin Ding; Jingren Zhou
>
> **备注:** This technical report will be continuously updated as the codebase evolves. GitHub: https://github.com/modelscope/Trinity-RFT
>
> **摘要:** Trinity-RFT is a general-purpose, unified and easy-to-use framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a modular and decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT; (2) seamless integration for agent-environment interaction with high efficiency and robustness; and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for development and research of advanced reinforcement learning paradigms at both macroscopic and microscopic levels. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples, applications and experiments that demonstrate its functionalities and user-friendliness.
>
---
#### [replaced 028] Cascade Speculative Drafting for Even Faster LLM Inference
- **分类: cs.LG; cs.CL**

- **链接: [http://arxiv.org/pdf/2312.11462v5](http://arxiv.org/pdf/2312.11462v5)**

> **作者:** Ziyi Chen; Xiaocong Yang; Jiacheng Lin; Chenkai Sun; Kevin Chen-Chuan Chang; Jie Huang
>
> **备注:** NeurIPS 2024
>
> **摘要:** Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in drafting for improved efficiency. Combining both cascades, CS Drafting achieves greater speedup compared to the baselines in our experiments, while preserving the same output distribution as the target model.
>
---
#### [replaced 029] OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale
- **分类: cs.CL; cs.DB**

- **链接: [http://arxiv.org/pdf/2503.02240v2](http://arxiv.org/pdf/2503.02240v2)**

> **作者:** Haoyang Li; Shang Wu; Xiaokang Zhang; Xinmei Huang; Jing Zhang; Fuxin Jiang; Shuai Wang; Tieying Zhang; Jianjun Chen; Rui Shi; Hong Chen; Cuiping Li
>
> **摘要:** Text-to-SQL, the task of translating natural language questions into SQL queries, plays a crucial role in enabling non-experts to interact with databases. While recent advancements in large language models (LLMs) have significantly enhanced text-to-SQL performance, existing approaches face notable limitations in real-world text-to-SQL applications. Prompting-based methods often depend on closed-source LLMs, which are expensive, raise privacy concerns, and lack customization. Fine-tuning-based methods, on the other hand, suffer from poor generalizability due to the limited coverage of publicly available training data. To overcome these challenges, we propose a novel and scalable text-to-SQL data synthesis framework for automatically synthesizing large-scale, high-quality, and diverse datasets without extensive human intervention. Using this framework, we introduce SynSQL-2.5M, the first million-scale text-to-SQL dataset, containing 2.5 million samples spanning over 16,000 synthetic databases. Each sample includes a database, SQL query, natural language question, and chain-of-thought (CoT) solution. Leveraging SynSQL-2.5M, we develop OmniSQL, a powerful open-source text-to-SQL model available in three sizes: 7B, 14B, and 32B. Extensive evaluations across nine datasets demonstrate that OmniSQL achieves state-of-the-art performance, matching or surpassing leading closed-source and open-source LLMs, including GPT-4o and DeepSeek-V3, despite its smaller size. We release all code, datasets, and models to support further research.
>
---
#### [replaced 030] Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2503.22362v2](http://arxiv.org/pdf/2503.22362v2)**

> **作者:** Yuan He; Bailan He; Zifeng Ding; Alisia Lupidi; Yuqicheng Zhu; Shuo Chen; Caiqi Zhang; Jiaoyan Chen; Yunpu Ma; Volker Tresp; Ian Horrocks
>
> **备注:** Accepted at COLM 2025
>
> **摘要:** Understanding and mitigating hallucinations in Large Language Models (LLMs) is crucial for ensuring reliable content generation. While previous research has primarily focused on "when" LLMs hallucinate, our work explains "why" and directly links model behaviour to the pre-training data that forms their prior knowledge. Specifically, we demonstrate that an asymmetry exists in the recognition of logically equivalent facts, which can be attributed to frequency discrepancies of entities appearing as subjects versus objects. Given that most pre-training datasets are inaccessible, we leverage the fully open-source OLMo series by indexing its Dolma dataset to estimate entity frequencies. Using relational facts (represented as triples) from Wikidata5M, we construct probing datasets to isolate this effect. Our experiments reveal that facts with a high-frequency subject and a low-frequency object are better recognised than their inverse, despite their logical equivalence. The pattern reverses in low-to-high frequency settings, and no statistically significant asymmetry emerges when both entities are high-frequency. These findings highlight the influential role of pre-training data in shaping model predictions and provide insights for inferring the characteristics of pre-training data in closed or partially closed LLMs.
>
---
#### [replaced 031] An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2403.04963v4](http://arxiv.org/pdf/2403.04963v4)**

> **作者:** Xuanxin Wu; Yuki Arase
>
> **备注:** Accepted by ACM Transactions on Intelligent Systems and Technology. Our human evaluation corpus is available at: https://github.com/WuXuanxin/human-eval-llm-simplification
>
> **摘要:** Recent studies have used both automatic metrics and human evaluations to assess the simplification abilities of LLMs. However, the suitability of existing evaluation methodologies for LLMs remains in question. First, the suitability of current automatic metrics on LLMs' simplification evaluation is still uncertain. Second, current human evaluation approaches in sentence simplification often fall into two extremes: they are either too superficial, failing to offer a clear understanding of the models' performance, or overly detailed, making the annotation process complex and prone to inconsistency, which in turn affects the evaluation's reliability. To address these problems, this study provides in-depth insights into LLMs' performance while ensuring the reliability of the evaluation. We design an error-based human annotation framework to assess the LLMs' simplification capabilities. We select both closed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and Llama-3.2-3B. We believe that these models offer a representative selection across large, medium, and small sizes of LLMs. Results show that LLMs generally generate fewer erroneous simplification outputs compared to the previous state-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and Qwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct meta-evaluations on widely used automatic metrics using our human annotations. We find that these metrics lack sufficient sensitivity to assess the overall high-quality simplifications, particularly those generated by high-performance LLMs.
>
---
#### [replaced 032] EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration
- **分类: cs.LG; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2410.06238v2](http://arxiv.org/pdf/2410.06238v2)**

> **作者:** Allen Nie; Yi Su; Bo Chang; Jonathan N. Lee; Ed H. Chi; Quoc V. Le; Minmin Chen
>
> **备注:** 28 pages. Published at ICML 2025
>
> **摘要:** Despite their success in many domains, large language models (LLMs) remain under-studied in scenarios requiring optimal decision-making under uncertainty. This is crucial as many real-world applications, ranging from personalized recommendations to healthcare interventions, demand that LLMs not only predict but also actively learn to make optimal decisions through exploration. In this work, we measure LLMs' (in)ability to make optimal decisions in bandits, a state-less reinforcement learning setting relevant to many applications. We develop a comprehensive suite of environments, including both context-free and contextual bandits with varying task difficulties, to benchmark LLMs' performance. Motivated by the existence of optimal exploration algorithms, we propose efficient ways to integrate this algorithmic knowledge into LLMs: by providing explicit algorithm-guided support during inference; and through algorithm distillation via in-context demonstrations and fine-tuning, using synthetic data generated from these algorithms. Impressively, these techniques allow us to achieve superior exploration performance with smaller models, surpassing larger models on various tasks. We conducted an extensive ablation study to shed light on various factors, such as task difficulty and data representation, that influence the efficiency of LLM exploration. Additionally, we conduct a rigorous analysis of the LLM's exploration efficiency using the concept of regret, linking its ability to explore to the model size and underlying algorithm.
>
---
#### [replaced 033] Watermarking Degrades Alignment in Language Models: Analysis and Mitigation
- **分类: cs.CL; cs.CR; cs.LG; I.2.7**

- **链接: [http://arxiv.org/pdf/2506.04462v3](http://arxiv.org/pdf/2506.04462v3)**

> **作者:** Apurv Verma; NhatHai Phan; Shubhendu Trivedi
>
> **备注:** Published at the 1st Workshop on GenAI Watermarking (ICLR 2025). Code: https://github.com/dapurv5/alignmark
>
> **摘要:** Watermarking techniques for large language models (LLMs) can significantly impact output quality, yet their effects on truthfulness, safety, and helpfulness remain critically underexamined. This paper presents a systematic analysis of how two popular watermarking approaches-Gumbel and KGW-affect these core alignment properties across four aligned LLMs. Our experiments reveal two distinct degradation patterns: guard attenuation, where enhanced helpfulness undermines model safety, and guard amplification, where excessive caution reduces model helpfulness. These patterns emerge from watermark-induced shifts in token distribution, surfacing the fundamental tension that exists between alignment objectives. To mitigate these degradations, we propose Alignment Resampling (AR), an inference-time sampling method that uses an external reward model to restore alignment. We establish a theoretical lower bound on the improvement in expected reward score as the sample size is increased and empirically demonstrate that sampling just 2-4 watermarked generations effectively recovers or surpasses baseline (unwatermarked) alignment scores. To overcome the limited response diversity of standard Gumbel watermarking, our modified implementation sacrifices strict distortion-freeness while maintaining robust detectability, ensuring compatibility with AR. Experimental results confirm that AR successfully recovers baseline alignment in both watermarking approaches, while maintaining strong watermark detectability. This work reveals the critical balance between watermark strength and model alignment, providing a simple inference-time solution to responsibly deploy watermarked LLMs in practice.
>
---
#### [replaced 034] Exploring Gender Bias Beyond Occupational Titles
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2507.02679v2](http://arxiv.org/pdf/2507.02679v2)**

> **作者:** Ahmed Sabir; Rajesh Sharma
>
> **备注:** Work in progress
>
> **摘要:** In this work, we investigate the correlation between gender and contextual biases, focusing on elements such as action verbs, object nouns, and particularly on occupations. We introduce a novel dataset, GenderLexicon, and a framework that can estimate contextual bias and its related gender bias. Our model can interpret the bias with a score and thus improve the explainability of gender bias. Also, our findings confirm the existence of gender biases beyond occupational stereotypes. To validate our approach and demonstrate its effectiveness, we conduct evaluations on five diverse datasets, including a Japanese dataset.
>
---
#### [replaced 035] Mechanistic Indicators of Understanding in Large Language Models
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2507.08017v2](http://arxiv.org/pdf/2507.08017v2)**

> **作者:** Pierre Beckmann; Matthieu Queloz
>
> **备注:** 32 pages
>
> **摘要:** Recent findings in mechanistic interpretability (MI), the field probing the inner workings of Large Language Models (LLMs), challenge the view that these models rely solely on superficial statistics. We offer an accessible synthesis of these findings that doubles as an introduction to MI while integrating these findings within a novel theoretical framework for thinking about machine understanding. We argue that LLMs develop internal structures that are functionally analogous to the kind of understanding that consists in seeing connections. To sharpen this idea, we propose a three-tiered conception of understanding. First, conceptual understanding emerges when a model forms "features" as directions in latent space, learning the connections between diverse manifestations of something. Second, state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world. Third, principled understanding emerges when a model ceases to rely on a collection of memorized facts and discovers a "circuit" connecting these facts. However, these forms of understanding remain radically different from human understanding, as the phenomenon of "parallel mechanisms" shows. We conclude that the debate should move beyond the yes-or-no question of whether LLMs understand to investigate how their strange minds work and forge conceptions that fit them.
>
---
#### [replaced 036] Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2505.17086v2](http://arxiv.org/pdf/2505.17086v2)**

> **作者:** Yihong Wu; Liheng Ma; Muzhi Li; Jiaming Zhou; Jianye Hao; Ho-fung Leung; Irwin King; Yingxue Zhang; Jian-Yun Nie
>
> **摘要:** Large Language Models (LLMs) have demonstrated remarkable versatility, due to the lack of factual knowledge, their application to Question Answering (QA) tasks remains hindered by hallucination. While Retrieval-Augmented Generation mitigates these issues by integrating external knowledge, existing approaches rely heavily on in-context learning, whose performance is constrained by the fundamental reasoning capabilities of LLMs. In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex Question Answering, comprising a planner that decomposes questions into a directed acyclic graph of subquestions and a worker that resolves questions via retrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy Gradient Optimization), a novel reinforcement learning method that replaces traditional policy gradient updates with Maximum Likelihood Estimation (MLE) by sampling trajectories from an asymptotically optimal policy. MyGO eliminates the need for gradient rescaling and reference models, ensuring stable and efficient training. Empirical results across multiple datasets demonstrate the effectiveness of Mujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a scalable and resource-efficient solution for complex QA tasks.
>
---
#### [replaced 037] FlexOlmo: Open Language Models for Flexible Data Use
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2507.07024v2](http://arxiv.org/pdf/2507.07024v2)**

> **作者:** Weijia Shi; Akshita Bhagia; Kevin Farhat; Niklas Muennighoff; Pete Walsh; Jacob Morrison; Dustin Schwenk; Shayne Longpre; Jake Poznanski; Allyson Ettinger; Daogao Liu; Margaret Li; Dirk Groeneveld; Mike Lewis; Wen-tau Yih; Luca Soldaini; Kyle Lo; Noah A. Smith; Luke Zettlemoyer; Pang Wei Koh; Hannaneh Hajishirzi; Ali Farhadi; Sewon Min
>
> **摘要:** We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.
>
---
#### [replaced 038] VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information
- **分类: cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2412.00947v3](http://arxiv.org/pdf/2412.00947v3)**

> **作者:** Ryo Kamoi; Yusen Zhang; Sarkar Snigdha Sarathi Das; Ranran Haoran Zhang; Rui Zhang
>
> **备注:** COLM 2025. VisOnlyQA dataset, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA. Please also refer to our project website at https://visonlyqa.github.io/
>
> **摘要:** Large Vision Language Models (LVLMs) have achieved remarkable performance in various vision-language tasks. However, it is still unclear how accurately LVLMs can perceive visual information in images. In particular, the capability of LVLMs to perceive geometric information, such as shape, angle, and size, remains insufficiently analyzed, although the perception of these properties is crucial for tasks that require a detailed visual understanding. In this work, we introduce VisOnlyQA, a dataset for evaluating the geometric perception of LVLMs, and reveal that LVLMs often cannot accurately perceive basic geometric information in images, while human performance is nearly perfect. VisOnlyQA consists of 12 tasks that directly ask about geometric information in geometric shapes, charts, chemical structures, and 3D shapes. Our experiments highlight the following findings: (i) State-of-the-art LVLMs struggle with basic geometric perception. 23 LVLMs we evaluate, including GPT-4o and Gemini 2.5 Pro, work poorly on VisOnlyQA. (ii) Additional training data does not resolve this issue. Fine-tuning on the training set of VisOnlyQA is not always effective, even for in-distribution tasks. (iii) LLM may be the bottleneck. LVLMs using stronger LLMs exhibit better geometric perception on VisOnlyQA, while it does not require complex reasoning, suggesting that the way LVLMs process information from visual encoders is a bottleneck. The datasets, code, and model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.
>
---
#### [replaced 039] READoc: A Unified Benchmark for Realistic Document Structured Extraction
- **分类: cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2409.05137v3](http://arxiv.org/pdf/2409.05137v3)**

> **作者:** Zichao Li; Aizier Abulaiti; Yaojie Lu; Xuanang Chen; Jia Zheng; Hongyu Lin; Xianpei Han; Le Sun
>
> **备注:** ACL 2025 Findings
>
> **摘要:** Document Structured Extraction (DSE) aims to extract structured content from raw documents. Despite the emergence of numerous DSE systems, their unified evaluation remains inadequate, significantly hindering the field's advancement. This problem is largely attributed to existing benchmark paradigms, which exhibit fragmented and localized characteristics. To address these limitations and offer a thorough evaluation of DSE systems, we introduce a novel benchmark named READoc, which defines DSE as a realistic task of converting unstructured PDFs into semantically rich Markdown. The READoc dataset is derived from 3,576 diverse and real-world documents from arXiv, GitHub, and Zenodo. In addition, we develop a DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring modules, to conduct a unified evaluation of state-of-the-art DSE approaches. By evaluating a range of pipeline tools, expert visual models, and general VLMs, we identify the gap between current work and the unified, realistic DSE objective for the first time. We aspire that READoc will catalyze future research in DSE, fostering more comprehensive and practical solutions.
>
---
#### [replaced 040] Topic Modeling as Multi-Objective Contrastive Optimization
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2402.07577v3](http://arxiv.org/pdf/2402.07577v3)**

> **作者:** Thong Nguyen; Xiaobao Wu; Xinshuai Dong; Cong-Duy T Nguyen; See-Kiong Ng; Anh Tuan Luu
>
> **备注:** Accepted at ICLR 2024 (poster). Official version available at: https://openreview.net/forum?id=HdAoLSBYXj
>
> **摘要:** Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents. However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling. Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents. To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents. Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that balances the trade-off between the ELBO and the contrastive objective. Extensive experiments demonstrate that our framework consistently produces higher-performing neural topic models in terms of topic coherence, topic diversity, and downstream performance.
>
---
#### [replaced 041] Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction
- **分类: cs.LG; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2504.15266v3](http://arxiv.org/pdf/2504.15266v3)**

> **作者:** Vaishnavh Nagarajan; Chen Henry Wu; Charles Ding; Aditi Raghunathan
>
> **备注:** ICML 2025 (oral)
>
> **摘要:** We design a suite of minimal algorithmic tasks that are a loose abstraction of open-ended real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic; multi-token approaches, namely teacherless training and diffusion models, comparatively excel in producing diverse and original output. Secondly, to elicit randomness without hurting coherence, we find that injecting noise at the input layer (dubbed seed-conditioning) works surprisingly as well as (and in some conditions, better than) temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and temperature sampling. We make part of the code available under https://github.com/chenwu98/algorithmic-creativity
>
---
#### [replaced 042] Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs
- **分类: cs.CL; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.07186v2](http://arxiv.org/pdf/2507.07186v2)**

> **作者:** Itay Itzhak; Yonatan Belinkov; Gabriel Stanovsky
>
> **备注:** CoLM 2025
>
> **摘要:** Large language models (LLMs) exhibit cognitive biases -- systematic tendencies of irrational decision-making, similar to those seen in humans. Prior work has found that these biases vary across models and can be amplified by instruction tuning. However, it remains unclear if these differences in biases stem from pretraining, finetuning, or even random noise due to training stochasticity. We propose a two-step causal experimental approach to disentangle these factors. First, we finetune models multiple times using different random seeds to study how training randomness affects over $30$ cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping instruction datasets between models to isolate bias sources. This swap uses datasets that led to different bias patterns, directly testing whether biases are dataset-dependent. Our findings reveal that while training randomness introduces some variability, biases are mainly shaped by pretraining: models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data. These insights suggest that understanding biases in finetuned models requires considering their pretraining origins beyond finetuning effects. This perspective can guide future efforts to develop principled strategies for evaluating and mitigating bias in LLMs.
>
---
#### [replaced 043] Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2408.11415v2](http://arxiv.org/pdf/2408.11415v2)**

> **作者:** Simon Münker
>
> **备注:** 14 pages, 2 tables
>
> **摘要:** Contemporary research in social sciences increasingly utilizes state-of-the-art generative language models to annotate or generate content. While these models achieve benchmark-leading performance on common language tasks, their application to novel out-of-domain tasks remains insufficiently explored. To address this gap, we investigate how personalized language models align with human responses on the Moral Foundation Theory Questionnaire. We adapt open-source generative language models to different political personas and repeatedly survey these models to generate synthetic data sets where model-persona combinations define our sub-populations. Our analysis reveals that models produce inconsistent results across multiple repetitions, yielding high response variance. Furthermore, the alignment between synthetic data and corresponding human data from psychological studies shows a weak correlation, with conservative persona-prompted models particularly failing to align with actual conservative populations. These results suggest that language models struggle to coherently represent ideologies through in-context prompting due to their alignment process. Thus, using language models to simulate social interactions requires measurable improvements in in-context optimization or parameter manipulation to align with psychological and sociological stereotypes properly.
>
---
#### [replaced 044] PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2507.04607v2](http://arxiv.org/pdf/2507.04607v2)**

> **作者:** Xinliang Frederick Zhang; Nick Beauchamp; Lu Wang
>
> **摘要:** Large language model (LLM) personalization aims to align model outputs with individuals' unique preferences and opinions. While recent efforts have implemented various personalization methods, a unified theoretical framework that can systematically understand the drivers of effective personalization is still lacking. In this work, we integrate the well-established cognitive dual-memory model into LLM personalization, by mirroring episodic memory to historical user engagements and semantic memory to long-term, evolving user beliefs. Specifically, we systematically investigate memory instantiations and introduce a unified framework, PRIME, using episodic and semantic memory mechanisms. We further augment PRIME with a novel personalized thinking capability inspired by the slow thinking strategy. Moreover, recognizing the absence of suitable benchmarks, we introduce a dataset using Change My View (CMV) from Reddit, specifically designed to evaluate long-context personalization. Extensive experiments validate PRIME's effectiveness across both long- and short-context scenarios. Further analysis confirms that PRIME effectively captures dynamic personalization beyond mere popularity biases.
>
---
#### [replaced 045] StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model
- **分类: cs.CL; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2507.07803v2](http://arxiv.org/pdf/2507.07803v2)**

> **作者:** Shoutao Guo; Xiang Li; Mengge Liu; Wei Chen; Yang Feng
>
> **备注:** The code is at https://github.com/ictnlp/StreamUni; The model is at https://huggingface.co/ICTNLP/StreamUni-Phi4
>
> **摘要:** Streaming speech translation (StreamST) requires determining appropriate timing, known as policy, to generate translations while continuously receiving source speech inputs, balancing low latency with high translation quality. However, existing StreamST methods typically operate on sentence-level speech segments, referred to as simultaneous speech translation (SimulST). In practice, they require collaboration with segmentation models to accomplish StreamST, where the truncated speech segments constrain SimulST models to make policy decisions and generate translations based on limited contextual information. Moreover, SimulST models struggle to learn effective policies due to the complexity of speech inputs and cross-lingual generation. To address these challenges, we propose StreamUni, which achieves StreamST through a unified Large Speech-Language Model (LSLM). Specifically, StreamUni incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate multi-stage outputs. Leveraging these multi-stage outputs, StreamUni simultaneously accomplishes speech segmentation, policy decision, and translation generation, completing StreamST without requiring massive policy-specific training. Additionally, we propose a streaming CoT training method that enhances low-latency policy decisions and generation capabilities using limited CoT data. Experiments demonstrate that our approach achieves state-of-the-art performance on StreamST tasks.
>
---
#### [replaced 046] CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2409.01389v2](http://arxiv.org/pdf/2409.01389v2)**

> **作者:** Ivana Beňová; Michal Gregor; Albert Gatt
>
> **备注:** 9 pages, 2 figure, 6 tables, CogSci conference 2025
>
> **摘要:** How do vision-language (VL) transformer models ground verb phrases and do they integrate contextual and world knowledge in this process? We introduce the CV-Probes dataset, containing image-caption pairs involving verb phrases that require both social knowledge and visual context to interpret (e.g., "beg"), as well as pairs involving verb phrases that can be grounded based on information directly available in the image (e.g., "sit"). We show that VL models struggle to ground VPs that are strongly context-dependent. Further analysis using explainable AI techniques shows that such models may not pay sufficient attention to the verb token in the captions. Our results suggest a need for improved methodologies in VL model training and evaluation. The code and dataset will be available https://github.com/ivana-13/CV-Probes.
>
---
#### [replaced 047] LASER: Attention with Exponential Transformation
- **分类: cs.LG; cs.CL**

- **链接: [http://arxiv.org/pdf/2411.03493v2](http://arxiv.org/pdf/2411.03493v2)**

> **作者:** Sai Surya Duvvuri; Inderjit S. Dhillon
>
> **备注:** ICML 2025
>
> **摘要:** Transformers have had tremendous impact for several sequence related tasks, largely due to their ability to retrieve from any part of the sequence via softmax based dot-product attention. This mechanism plays a crucial role in Transformer's performance. We analyze the gradients backpropagated through the softmax operation in the attention mechanism and observe that these gradients can often be small. This poor gradient signal backpropagation can lead to inefficient learning of parameters preceeding the attention operations. To this end, we introduce a new attention mechanism called LASER, which we analytically show to admit a larger gradient signal. We show that LASER attention can be implemented by making small modifications to existing attention implementations. We conduct experiments on autoregressive large language models (LLMs) with upto 7.7 billion parameters with an average improvement of upto 1.44% over standard attention on downstream evaluations and 1.65% finetuning improvements. Additionally, LASER demonstrates generalization performance improvement across a variety of tasks (vision, text and speech):Vision Transformer (ViT) on Imagenet, Conformer on the Librispeech speech-to-text and BERT with 2.2 billion parameters.
>
---
#### [replaced 048] Expert-level validation of AI-generated medical text with scalable language models
- **分类: cs.CL; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.03152v2](http://arxiv.org/pdf/2507.03152v2)**

> **作者:** Asad Aali; Vasiliki Bikia; Maya Varma; Nicole Chiou; Sophie Ostmeier; Arnav Singhvi; Magdalini Paschali; Ashwin Kumar; Andrew Johnston; Karimar Amador-Martinez; Eduardo Juan Perez Guerrero; Paola Naovi Cruz Rivera; Sergios Gatidis; Christian Bluethgen; Eduardo Pontes Reis; Eddy D. Zandee van Rilland; Poonam Laxmappa Hosamani; Kevin R Keet; Minjoung Go; Evelyn Ling; David B. Larson; Curtis Langlotz; Roxana Daneshjou; Jason Hom; Sanmi Koyejo; Emily Alsentzer; Akshay S. Chaudhari
>
> **摘要:** With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challenging because 1) manual review is costly and 2) expert-composed reference outputs are often unavailable in real-world settings. While the "LM-as-judge" paradigm (a LM evaluating another LM) offers scalable evaluation, even frontier LMs can miss subtle but clinically significant errors. To address these challenges, we propose MedVAL, a self-supervised framework that leverages synthetic data to train evaluator LMs to assess whether LM-generated medical outputs are factually consistent with inputs, without requiring physician labels or reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a dataset containing 840 outputs annotated by physicians, following a physician-defined taxonomy of risk levels and error categories. Across 6 diverse medical tasks and 10 state-of-the-art LMs spanning open-source, proprietary, and medically adapted models, MedVAL fine-tuning significantly improves (p < 0.001) alignment with physicians on both seen and unseen tasks, increasing average F1 scores from 66% to 83%, with per-sample safety classification scores up to 86%. MedVAL improves the performance of even the best-performing proprietary LM (GPT-4o) by 8%. To support a scalable, risk-aware pathway towards clinical integration, we open-source the 1) codebase (https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench (https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), and 3) MedVAL-4B (https://huggingface.co/stanfordmimi/MedVAL-4B), the best-performing open-source LM. Our research provides the first evidence of LMs approaching expert-level validation ability for medical text.
>
---
#### [replaced 049] A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications
- **分类: cs.AI; cs.CL; cs.LG**

- **链接: [http://arxiv.org/pdf/2410.15595v3](http://arxiv.org/pdf/2410.15595v3)**

> **作者:** Wenyi Xiao; Zechuan Wang; Leilei Gan; Shuai Zhao; Zongrui Li; Ruirui Lei; Wanggui He; Luu Anh Tuan; Long Chen; Hao Jiang; Zhou Zhao; Fei Wu
>
> **备注:** 45 pages, 12 Figures. Project page: https://github.com/Mr-Loevan/DPO-Survey
>
> **摘要:** With the rapid advancement of large language models (LLMs), aligning policy models with human preferences has become increasingly critical. Direct Preference Optimization (DPO) has emerged as a promising approach for alignment, acting as an RL-free alternative to Reinforcement Learning from Human Feedback (RLHF). Despite DPO's various advancements and inherent limitations, an in-depth review of these aspects is currently lacking in the literature. In this work, we present a comprehensive review of the challenges and opportunities in DPO, covering theoretical analyses, variants, relevant preference datasets, and applications. Specifically, we categorize recent studies on DPO based on key research questions to provide a thorough understanding of DPO's current landscape. Additionally, we propose several future research directions to offer insights on model alignment for the research community. An updated collection of relevant papers can be found on https://github.com/Mr-Loevan/DPO-Survey.
>
---
#### [replaced 050] B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2502.12992v2](http://arxiv.org/pdf/2502.12992v2)**

> **作者:** Yifan Wang; Sukrut Rao; Ji-Ung Lee; Mayank Jobanputra; Vera Demberg
>
> **摘要:** Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural architectures. Meanwhile, B-cos networks have been introduced to improve model explainability by proposing an architecture that removes bias terms and promotes input-weight alignment. Although B-cos networks have shown success in building explainable systems, their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos language models (LMs) empowered for natural language processing (NLP) tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous methods. Our automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post-hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we are also the first to explore the transformation of decoder-only models to B-cos LMs for generation tasks.
>
---
#### [replaced 051] Evaluation of Attribution Bias in Generator-Aware Retrieval-Augmented Large Language Models
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2410.12380v2](http://arxiv.org/pdf/2410.12380v2)**

> **作者:** Amin Abolghasemi; Leif Azzopardi; Seyyed Hadi Hashemi; Maarten de Rijke; Suzan Verberne
>
> **备注:** Accepted at ACL 2025 (Findings)
>
> **摘要:** Attributing answers to source documents is an approach used to enhance the verifiability of a model's output in retrieval augmented generation (RAG). Prior work has mainly focused on improving and evaluating the attribution quality of large language models (LLMs) in RAG, but this may come at the expense of inducing biases in the attribution of answers. We define and examine two aspects in the evaluation of LLMs in RAG pipelines, namely attribution sensitivity and bias with respect to authorship information. We explicitly inform an LLM about the authors of source documents, instruct it to attribute its answers, and analyze (i) how sensitive the LLM's output is to the author of source documents, and (ii) whether the LLM exhibits a bias towards human-written or AI-generated source documents. We design an experimental setup in which we use counterfactual evaluation to study three LLMs in terms of their attribution sensitivity and bias in RAG pipelines. Our results show that adding authorship information to source documents can significantly change the attribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have an attribution bias towards explicit human authorship, which can serve as a competing hypothesis for findings of prior work that shows that LLM-generated content may be preferred over human-written contents. Our findings indicate that metadata of source documents can influence LLMs' trust, and how they attribute their answers. Furthermore, our research highlights attribution bias and sensitivity as a novel aspect of brittleness in LLMs.
>
---
#### [replaced 052] Consistency in Language Models: Current Landscape, Challenges, and Future Directions
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.00268v2](http://arxiv.org/pdf/2505.00268v2)**

> **作者:** Jekaterina Novikova; Carol Anderson; Borhane Blili-Hamelin; Domenic Rosati; Subhabrata Majumdar
>
> **备注:** Accepted in ICML 2025 Workshop on Reliable and Responsible Foundation Models
>
> **摘要:** The hallmark of effective language use lies in consistency: expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language models (LMs) struggle to maintain reliable consistency across task- and domain-specific applications. Here we examine the landscape of consistency research in LMs, analyze current approaches to measure aspects of consistency, and identify critical research gaps. Our findings point to an urgent need for quality benchmarks to measure and interdisciplinary approaches to ensure consistency while preserving utility.
>
---
#### [replaced 053] Auditing Prompt Caching in Language Model APIs
- **分类: cs.CL; cs.CR; cs.LG**

- **链接: [http://arxiv.org/pdf/2502.07776v2](http://arxiv.org/pdf/2502.07776v2)**

> **作者:** Chenchen Gu; Xiang Lisa Li; Rohith Kuditipudi; Percy Liang; Tatsunori Hashimoto
>
> **备注:** Accepted at ICML 2025
>
> **摘要:** Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.
>
---
#### [replaced 054] Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy
- **分类: cs.MA; cs.AI; cs.CL; cs.CY; cs.HC**

- **链接: [http://arxiv.org/pdf/2503.09639v4](http://arxiv.org/pdf/2503.09639v4)**

> **作者:** Abe Bohan Hou; Hongru Du; Yichen Wang; Jingyu Zhang; Zixiao Wang; Paul Pu Liang; Daniel Khashabi; Lauren Gardner; Tianxing He
>
> **备注:** Accepted to COLM 2025
>
> **摘要:** Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents' attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.
>
---
#### [replaced 055] SymRAG: Efficient Neuro-Symbolic Retrieval Through Adaptive Query Routing
- **分类: cs.AI; cs.CL; cs.IR**

- **链接: [http://arxiv.org/pdf/2506.12981v2](http://arxiv.org/pdf/2506.12981v2)**

> **作者:** Safayat Bin Hakim; Muhammad Adil; Alvaro Velasquez; Houbing Herbert Song
>
> **备注:** Accepted at 19th International Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)
>
> **摘要:** Current Retrieval-Augmented Generation systems use uniform processing, causing inefficiency as simple queries consume resources similar to complex multi-hop tasks. We present SymRAG, a framework that introduces adaptive query routing via real-time complexity and load assessment to select symbolic, neural, or hybrid pathways. SymRAG's neuro-symbolic approach adjusts computational pathways based on both query characteristics and system load, enabling efficient resource allocation across diverse query types. By combining linguistic and structural query properties with system load metrics, SymRAG allocates resources proportional to reasoning requirements. Evaluated on 2,000 queries across HotpotQA (multi-hop reasoning) and DROP (discrete reasoning) using Llama-3.2-3B and Mistral-7B models, SymRAG achieves competitive accuracy (97.6--100.0% exact match) with efficient resource utilization (3.6--6.2% CPU utilization, 0.985--3.165s processing). Disabling adaptive routing increases processing time by 169--1151%, showing its significance for complex models. These results suggest adaptive computation strategies are more sustainable and scalable for hybrid AI systems that use dynamic routing and neuro-symbolic frameworks.
>
---
#### [replaced 056] Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement
- **分类: cs.CL; cs.AI; cs.HC**

- **链接: [http://arxiv.org/pdf/2505.08245v2](http://arxiv.org/pdf/2505.08245v2)**

> **作者:** Haoran Ye; Jing Jin; Yuhang Xie; Xin Zhang; Guojie Song
>
> **备注:** 474 references
>
> **摘要:** The advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. This progress presents novel challenges, such as measuring human-like psychological constructs, moving beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This review paper introduces and synthesizes the emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. The reviewed literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances LLM capabilities. Diverse perspectives are integrated to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, the review provides actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.
>
---
#### [replaced 057] Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization
- **分类: cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2412.17739v4](http://arxiv.org/pdf/2412.17739v4)**

> **作者:** Ermo Hua; Che Jiang; Xingtai Lv; Kaiyan Zhang; Youbang Sun; Yuchen Fan; Xuekai Zhu; Biqing Qi; Ning Ding; Bowen Zhou
>
> **备注:** Accepted to ICML 2025
>
> **摘要:** Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become a trend. While prior works mainly address RoPE's limitations within attention, this paper uncovers the adverse effects on length generalization from nearly all parts of LMs. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectrum damage caused by: 1) linear layers and activation functions; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs \textit{Fourier Series} and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales and benchmarks show that, within varying context windows, FoPE maintains a more stable performance compared to other baselines. Several analyses and ablations bring further support to our method and theoretical modeling.
>
---
#### [replaced 058] Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2502.13640v2](http://arxiv.org/pdf/2502.13640v2)**

> **作者:** Maiya Goloburda; Nurkhan Laiyk; Diana Turmakhan; Yuxia Wang; Mukhammed Togmanov; Jonibek Mansurov; Askhat Sametov; Nurdaulet Mukhituly; Minghan Wang; Daniil Orel; Zain Muhammad Mujahid; Fajri Koto; Timothy Baldwin; Preslav Nakov
>
> **摘要:** Large language models (LLMs) are known to have the potential to generate harmful content, posing risks to users. While significant progress has been made in developing taxonomies for LLM risks and safety evaluation prompts, most studies have focused on monolingual contexts, primarily in English. However, language- and region-specific risks in bilingual contexts are often overlooked, and core findings can diverge from those in monolingual settings. In this paper, we introduce Qorgau, a novel dataset specifically designed for safety evaluation in Kazakh and Russian, reflecting the unique bilingual context in Kazakhstan, where both Kazakh (a low-resource language) and Russian (a high-resource language) are spoken. Experiments with both multilingual and language-specific LLMs reveal notable differences in safety performance, emphasizing the need for tailored, region-specific datasets to ensure the responsible and safe deployment of LLMs in countries like Kazakhstan. Warning: this paper contains example data that may be offensive, harmful, or biased.
>
---
#### [replaced 059] Towards Pareto Optimal Throughput in Small Language Model Serving
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2404.03353v2](http://arxiv.org/pdf/2404.03353v2)**

> **作者:** Pol G. Recasens; Yue Zhu; Chen Wang; Eun Kyung Lee; Olivier Tardieu; Alaa Youssef; Jordi Torres; Josep Ll. Berral
>
> **备注:** Revised version of the paper published at EuroMLSys'24
>
> **摘要:** Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks. Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance. In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels. Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator. In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs.
>
---
#### [replaced 060] PyVision: Agentic Vision with Dynamic Tooling
- **分类: cs.CL; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2507.07998v2](http://arxiv.org/pdf/2507.07998v2)**

> **作者:** Shitian Zhao; Haoquan Zhang; Shaoheng Lin; Ming Li; Qilong Wu; Kaipeng Zhang; Chen Wei
>
> **备注:** 26 Pages, 10 Figures, Technical report
>
> **摘要:** LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.
>
---
#### [replaced 061] BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2506.06955v4](http://arxiv.org/pdf/2506.06955v4)**

> **作者:** Ha-Thanh Nguyen; Chaoran Liu; Qianying Liu; Hideyuki Tachibana; Su Myat Noe; Yusuke Miyao; Koichi Takeda; Sadao Kurohashi
>
> **备注:** This version includes minor typo corrections in the example image
>
> **摘要:** We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety.
>
---
#### [replaced 062] Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2507.08031v2](http://arxiv.org/pdf/2507.08031v2)**

> **作者:** Hong Jia; Shiya Fu; Feng Xia; Vassilis Kostakos; Ting Dang
>
> **摘要:** The emergence of Small Language Models (SLMs) as privacy-preserving alternatives for sensitive applications raises a fundamental question about their inherent understanding capabilities compared to Large Language Models (LLMs). This paper investigates the mental health understanding capabilities of current SLMs through systematic evaluation across diverse classification tasks. Employing zero-shot and few-shot learning paradigms, we benchmark their performance against established LLM baselines to elucidate their relative strengths and limitations in this critical domain. We assess five state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding tasks. Our findings reveal that SLMs achieve mean performance within 2\% of LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot settings), demonstrating notable competence despite orders of magnitude fewer parameters. Both model categories experience similar degradation on multi-class severity tasks (a drop of over 30\%), suggesting that nuanced clinical understanding challenges transcend model scale. Few-shot prompting provides substantial improvements for SLMs (up to 14.6\%), while LLM gains are more variable. Our work highlights the potential of SLMs in mental health understanding, showing they can be effective privacy-preserving tools for analyzing sensitive online text data. In particular, their ability to quickly adapt and specialize with minimal data through few-shot learning positions them as promising candidates for scalable mental health screening tools.
>
---
#### [replaced 063] Your Absorbing Discrete Diffusion Secretly Models the Bayesian Posterior
- **分类: cs.CL; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.07586v2](http://arxiv.org/pdf/2507.07586v2)**

> **作者:** Cooper Doyle
>
> **备注:** 12 pages, 2 figures, 2 tables
>
> **摘要:** Discrete diffusion language models learn to reconstruct text from randomly masked inputs, yet under mild assumptions their denoiser already implements the exact Bayesian posterior over the original tokens. We prove that the expected denoiser output under the forward corruption distribution recovers the true posterior, and that a simple Monte Carlo estimator converges to this posterior at rate O(1/sqrt(K)) with finite-sample concentration bounds. Building on this insight, we introduce an inference-time ensemble that runs K independent denoising passes and aggregates both posterior means and variances without any extra training. On WikiText-2, our MC-marginal sampler recovers the analytic lambda-DCE zero-shot perplexity (approximately 39) to within a few points at K=128, and its per-token variance shows a strong rank correlation with reconstruction error (Spearman rho = 0.996). This cost-proportional procedure yields calibrated uncertainty estimates and a direct trade-off between compute and posterior fidelity in discrete diffusion LMs.
>
---
#### [replaced 064] Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2506.22777v2](http://arxiv.org/pdf/2506.22777v2)**

> **作者:** Miles Turpin; Andy Arditi; Marvin Li; Joe Benton; Julian Michael
>
> **备注:** Published at ICML 2025 Workshop on Reliable and Responsible Foundation Models
>
> **摘要:** Language models trained with reinforcement learning (RL) can engage in reward hacking--the exploitation of unintended strategies for high reward--without revealing this behavior in their chain-of-thought reasoning. This makes the detection of reward hacking difficult, posing risks for high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL fine-tuning intervention that trains models to explicitly acknowledge when they are influenced by prompt cues--hints which point to incorrect answers (e.g., "a Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently train models with RL on environments where held-out prompt cues signal which incorrect answers will receive high reward, incentivizing models to exploit these cues instead of reasoning correctly. We measure how often models exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained model's responses consist of undetected reward hacks. In comparison, when we perform RL without VFT, the rate of undetected reward hacks goes up to 88%; with a debiasing baseline intervention, this increases further to 99%. VFT achieves this by substantially increasing how often models verbalize the influence of cues, from 8% to 43% after VFT, and up to 94% after RL. Baselines remain low even after RL (11% and 1%). Our results show that teaching models to explicitly verbalize reward hacking behavior before RL significantly improves their detection, offering a practical path toward more transparent and safe AI systems.
>
---
#### [replaced 065] Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2506.23377v2](http://arxiv.org/pdf/2506.23377v2)**

> **作者:** Taejin Kim; Siun-Chuon Mau; Konrad Vesey
>
> **备注:** 7 pages, 5 main pages of text, 5 figures, 2 tables. Research work performed at CACI INTL INC
>
> **摘要:** Large language models (LLMs) are used in a variety of mission-critical roles. Due to the rapidly developing nature of LLMs, there is a lack of quantifiable understanding of the bias and perspective associated with LLM output. Inspired by this need, this paper considers the broader issue of perspective or viewpoint of general text and perspective control of large-language model (LLM) output. Perspective-Dial consists of two main components: a (1) metric space, dubbed Perspective Space, that enables quantitative measurements of different perspectives regarding a topic, and the use of (2) Systematic Prompt Engineering that utilizes greedy-coordinate descent to control LLM output perspective based on measurement feedback from the Perspective Space. The empirical nature of the approach allows progress to side step a principled understanding of perspective or bias -- effectively quantifying and adjusting outputs for a variety of topics. Potential applications include detection, tracking and mitigation of LLM bias, narrative detection, sense making and tracking in public discourse, and debate bot advocating given perspective.
>
---
#### [replaced 066] Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models
- **分类: cs.CL; cs.AI; I.2.7**

- **链接: [http://arxiv.org/pdf/2411.07611v5](http://arxiv.org/pdf/2411.07611v5)**

> **作者:** Shuai Niu; Jing Ma; Hongzhan Lin; Liang Bai; Zhihua Wang; Yida Xu; Yunya Song; Xian Yang
>
> **备注:** 13 pages. 7 figures
>
> **摘要:** Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable multimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in the same encoding space, enabling it to be naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.
>
---
#### [replaced 067] KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding
- **分类: cs.LG; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2503.02951v2](http://arxiv.org/pdf/2503.02951v2)**

> **作者:** Zhangchen Xu; Yang Liu; Yueqin Yin; Mingyuan Zhou; Radha Poovendran
>
> **备注:** Accepted by ACL 2025. Codes and Data: https://kodcode-ai.github.io/
>
> **摘要:** We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.
>
---
#### [replaced 068] Logits are All We Need to Adapt Closed Models
- **分类: cs.LG; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2502.06806v4](http://arxiv.org/pdf/2502.06806v4)**

> **作者:** Gaurush Hiranandani; Haolun Wu; Subhojyoti Mukherjee; Sanmi Koyejo
>
> **备注:** 29 pages, 8 figures
>
> **摘要:** Many commercial Large Language Models (LLMs) are often closed-source, limiting developers to prompt tuning for aligning content generation with specific applications. While these models currently do not provide access to token logits, we argue that if such access were available, it would enable more powerful adaptation techniques beyond prompt engineering. In this paper, we propose a token-level probability reweighting framework that, given access to logits and a small amount of task-specific data, can effectively steer black-box LLMs toward application-specific content generation. Our approach views next-token prediction through the lens of supervised classification. We show that aligning black-box LLMs with task-specific data can be formulated as a label noise correction problem, leading to Plugin model -- an autoregressive probability reweighting model that operates solely on logits. We provide theoretical justification for why reweighting logits alone is sufficient for task adaptation. Extensive experiments with multiple datasets, LLMs, and reweighting models demonstrate the effectiveness of our method, advocating for broader access to token logits in closed-source models.
>
---
#### [replaced 069] Single Word Change is All You Need: Using LLMs to Create Synthetic Training Examples for Text Classifiers
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2401.17196v3](http://arxiv.org/pdf/2401.17196v3)**

> **作者:** Lei Xu; Sarah Alnegheimish; Laure Berti-Equille; Alfredo Cuesta-Infante; Kalyan Veeramachaneni
>
> **摘要:** In text classification, creating an adversarial example means subtly perturbing a few words in a sentence without changing its meaning, causing it to be misclassified by a classifier. A concerning observation is that a significant portion of adversarial examples generated by existing methods change only one word. This single-word perturbation vulnerability represents a significant weakness in classifiers, which malicious users can exploit to efficiently create a multitude of adversarial examples. This paper studies this problem and makes the following key contributions: (1) We introduce a novel metric $\rho$ to quantitatively assess a classifier's robustness against single-word perturbation. (2) We present the SP-Attack, designed to exploit the single-word perturbation vulnerability, achieving a higher attack success rate, better preserving sentence meaning, while reducing computation costs compared to state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims to improve \r{ho} by applying data augmentation in learning. Experimental results on 4 datasets and BERT and distilBERT classifiers show that SP-Defense improves $\rho$ by 14.6% and 13.9% and decreases the attack success rate of SP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the attack success rate of existing attack methods that involve multiple-word perturbations.
>
---
#### [replaced 070] SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs
- **分类: cs.CV; cs.CL; cs.HC**

- **链接: [http://arxiv.org/pdf/2507.07610v2](http://arxiv.org/pdf/2507.07610v2)**

> **作者:** Siting Wang; Luoyang Sun; Cheng Deng; Kun Shao; Minnan Pei; Zheng Tian; Haifeng Zhang; Jun Wang
>
> **摘要:** Humans can directly imagine and manipulate visual images in their minds, a capability known as spatial visualization. While multi-modal Large Language Models (MLLMs) support imagination-based reasoning, spatial visualization remains insufficiently evaluated, typically embedded within broader mathematical and logical assessments. Existing evaluations often rely on IQ tests or math competitions that may overlap with training data, compromising assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 automatically generated problems. Our evaluation of 33 state-of-the-art MLLMs not only reveals wide performance variations and demonstrates the benchmark's strong discriminative power, but also uncovers counter-intuitive findings: models exhibit unexpected behaviors by showing difficulty perception that misaligns with human intuition, displaying dramatic 2D-to-3D performance cliffs, and defaulting to formula derivation despite spatial tasks requiring visualization alone. SpatialVizBench empirically demonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark is publicly available.
>
---
#### [replaced 071] Disambiguate First, Parse Later: Generating Interpretations for Ambiguity Resolution in Semantic Parsing
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2502.18448v2](http://arxiv.org/pdf/2502.18448v2)**

> **作者:** Irina Saparina; Mirella Lapata
>
> **备注:** Findings of ACL 2025
>
> **摘要:** Handling ambiguity and underspecification is an important challenge in natural language interfaces, particularly for tasks like text-to-SQL semantic parsing. We propose a modular approach that resolves ambiguity using natural language interpretations before mapping these to logical forms (e.g., SQL queries). Although LLMs excel at parsing unambiguous utterances, they show strong biases for ambiguous ones, typically predicting only preferred interpretations. We constructively exploit this bias to generate an initial set of preferred disambiguations and then apply a specialized infilling model to identify and generate missing interpretations. To train the infilling model, we introduce an annotation method that uses SQL execution to validate different meanings. Our approach improves interpretation coverage and generalizes across datasets with different annotation styles, database structures, and ambiguity types.
>
---
#### [replaced 072] DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models
- **分类: cs.CL; cs.LG**

- **链接: [http://arxiv.org/pdf/2504.02882v2](http://arxiv.org/pdf/2504.02882v2)**

> **作者:** Sunghee Jung; Donghun Lee; Shinbok Lee; Gaeun Seo; Daniel Lee; Byeongil Ko; Junrae Cho; Kihyun Kim; Eunggyun Kim; Myeongcheol Shin
>
> **备注:** Accepted to SIGDIAL 2025
>
> **摘要:** Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling.
>
---
#### [replaced 073] EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective
- **分类: cs.SE; cs.CL; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.12185v3](http://arxiv.org/pdf/2505.12185v3)**

> **作者:** Sen Fang; Weiyuan Ding; Bowen Xu
>
> **备注:** 20 pages, 11 figures
>
> **摘要:** Assessing the programming capabilities of Large Language Models (LLMs) is crucial for their effective use in software engineering. Current evaluations, however, predominantly measure the accuracy of generated code on static benchmarks, neglecting the critical aspect of model robustness during programming tasks. While adversarial attacks offer insights on model robustness, their effectiveness is limited and evaluation could be constrained. Current adversarial attack methods for robustness evaluation yield inconsistent results, struggling to provide a unified evaluation across different LLMs. We introduce EVALOOP, a novel assessment framework that evaluate the robustness from a self-consistency perspective, i.e., leveraging the natural duality inherent in popular software engineering tasks, e.g., code generation and code summarization. EVALOOP initiates a self-contained feedback loop: an LLM generates output (e.g., code) from an input (e.g., natural language specification), and then use the generated output as the input to produce a new output (e.g., summarizes that code into a new specification). EVALOOP repeats the process to assess the effectiveness of EVALOOP in each loop. This cyclical strategy intrinsically evaluates robustness without rely on any external attack setups, providing a unified metric to evaluate LLMs' robustness in programming. We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1 performance within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo, despite superior initial code generation compared to DeepSeek-V2, demonstrated lower robustness over repeated evaluation loop.
>
---
#### [replaced 074] InstCache: A Predictive Cache for LLM Serving
- **分类: cs.CL; cs.DC**

- **链接: [http://arxiv.org/pdf/2411.13820v2](http://arxiv.org/pdf/2411.13820v2)**

> **作者:** Longwei Zou; Yan Liu; Jiamu Kang; Tingfeng Liu; Jiangang Kong; Yangdong Deng
>
> **摘要:** The revolutionary capabilities of Large Language Models (LLMs) are attracting rapidly growing popularity and leading to soaring user requests to inference serving systems. Caching techniques, which leverage data reuse to reduce computation, offer opportunities to optimize the performance of LLM inference engines. On the one hand, the low-level key-value (KV) cache working at the token level is widely adopted, albeit it incurs significant overhead as request volume grows. On the other hand, instruction-level caching, which stores full instruction-response pairs, is expected to play an increasingly crucial role. However, the high variability in the content and length of instructions make it rare for identical instructions to recur within a short time window, presenting challenges for effective caching instruction-response pairs. To address this challenge, we propose InstCache, a predictive caching mechanism for LLM serving systems. Leveraging the capability of LLMs, we can effectively reorder the representation space of instruction texts and develop a sufficient level of spatial locality. Such spatial locality enables us to predict potential instructions located in a compact region in the space, resulting in an effective caching system at runtime. Experimental results demonstrate that InstCache achieves a 2.3x higher hit rate compared to the upper bound of traditional caching mechanisms on WildChat dataset and reduces the time per output token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets, respectively.
>
---
#### [replaced 075] KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts in K-12 Education?
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2412.08985v3](http://arxiv.org/pdf/2412.08985v3)**

> **作者:** Tianshi Zheng; Weihan Li; Jiaxin Bai; Weiqi Wang; Yangqiu Song
>
> **备注:** ACL 2025 Main
>
> **摘要:** Retrieval-Augmented Generation (RAG) systems show remarkable potential as question answering tools in the K-12 Education domain, where knowledge is typically queried within the restricted scope of authoritative textbooks. However, discrepancies between these textbooks and the parametric knowledge inherent in Large Language Models (LLMs) can undermine the effectiveness of RAG systems. To systematically investigate RAG system robustness against such knowledge discrepancies, we introduce KnowShiftQA. This novel question answering dataset simulates these discrepancies by applying deliberate hypothetical knowledge updates to both answers and source documents, reflecting how textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across five subjects, designed with a comprehensive question typology focusing on context utilization and knowledge integration. Our extensive experiments on retrieval and question answering performance reveal that most RAG systems suffer a substantial performance drop when faced with these knowledge discrepancies. Furthermore, questions requiring the integration of contextual (textbook) knowledge with parametric (LLM) knowledge pose a significant challenge to current LLMs.
>
---
#### [replaced 076] From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating Hindi News Veracity Explanations
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2507.05179v2](http://arxiv.org/pdf/2507.05179v2)**

> **作者:** Pulkit Bansal; Raghvendra Kumar; Shakti Singh; Sriparna Saha; Adam Jatowt
>
> **摘要:** In an era of rampant misinformation, generating reliable news explanations is vital, especially for under-represented languages like Hindi. Lacking robust automated tools, Hindi faces challenges in scaling misinformation detection. To bridge this gap, we propose a novel framework integrating Direct Preference Optimization (DPO) with curriculum learning to align machine-generated explanations with human reasoning. Fact-checked explanations from credible sources serve as preferred responses, while LLM outputs highlight system limitations and serve as non-preferred responses. To refine task-specific alignment, we introduce two key parameters -- Actuality and Finesse -- into the DPO loss function, enhancing explanation quality and consistency. Experiments with LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's effectiveness in generating coherent, contextually relevant explanations. This scalable approach combats misinformation and extends automated explanation generation to low-resource languages.
>
---
#### [replaced 077] The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2505.20564v3](http://arxiv.org/pdf/2505.20564v3)**

> **作者:** Chris Emezue; NaijaVoices Community; Busayo Awobade; Abraham Owodunni; Handel Emezue; Gloria Monica Tobechukwu Emezue; Nefertiti Nneoma Emezue; Sewade Ogun; Bunmi Akinremi; David Ifeoluwa Adelani; Chris Pal
>
> **备注:** Accepted for publication at Interspeech 2025
>
> **摘要:** The development of high-performing, robust, and reliable speech technologies depends on large, high-quality datasets. However, African languages -- including our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to insufficient data. Popular voice-enabled technologies do not support any of the 2000+ African languages, limiting accessibility for circa one billion people. While previous dataset efforts exist for the target languages, they lack the scale and diversity needed for robust speech models. To bridge this gap, we introduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+ speakers. We outline our unique data collection approach, analyze its acoustic diversity, and demonstrate its impact through finetuning experiments on automatic speech recognition, averagely achieving 75.86% (Whisper), 52.06% (MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices' potential to advance multilingual speech processing for African languages.
>
---
#### [replaced 078] Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models
- **分类: cs.CL; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.15634v4](http://arxiv.org/pdf/2505.15634v4)**

> **作者:** Zihao Li; Xu Wang; Yuzhe Yang; Ziyu Yao; Haoyi Xiong; Mengnan Du
>
> **摘要:** Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this reasoning for complex problems, but requires costly and high-quality long CoT data and fine-tuning. This work, inspired by the deep thinking paradigm of DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of an LLM without external datasets. Our method first employs Sparse Autoencoders (SAEs) to extract interpretable features from vanilla CoT. These features are then used to steer the LLM's internal states during generation. Recognizing that many LLMs do not have corresponding pre-trained SAEs, we further introduce a novel SAE-free steering algorithm, which directly computes steering directions from the residual activations of an LLM, obviating the need for an explicit SAE. Experimental results demonstrate that both our SAE-based and subsequent SAE-free steering algorithms significantly enhance the reasoning capabilities of LLMs.
>
---
#### [replaced 079] ACEBench: Who Wins the Match Point in Tool Usage?
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2501.12851v5](http://arxiv.org/pdf/2501.12851v5)**

> **作者:** Chen Chen; Xinlong Hao; Weiwen Liu; Xu Huang; Xingshan Zeng; Shuai Yu; Dexun Li; Shuai Wang; Weinan Gan; Yuefeng Huang; Wulong Liu; Xinzhi Wang; Defu Lian; Baoqun Yin; Yasheng Wang; Wu Liu
>
> **摘要:** Large Language Models (LLMs) have demonstrated significant potential in decision-making and reasoning, particularly when integrated with various tools to effectively solve complex problems. However, existing benchmarks for evaluating LLMs' tool usage face several limitations: (1) limited evaluation scenarios, often lacking assessments in real multi-turn dialogue contexts; (2) narrow evaluation dimensions, with insufficient detailed assessments of how LLMs use tools; and (3) reliance on LLMs or real API executions for evaluation, which introduces significant overhead. To address these challenges, we introduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs. ACEBench categorizes data into three primary types based on evaluation methodology: Normal, Special, and Agent. "Normal" evaluates tool usage in basic scenarios; "Special" evaluates tool usage in situations with ambiguous or incomplete instructions; "Agent" evaluates tool usage through multi-agent interactions to simulate real-world, multi-turn dialogues. We conducted extensive experiments using ACEBench, analyzing various LLMs in-depth and providing a more granular examination of error causes across different data types.
>
---
#### [replaced 080] Large Language Models as Neurolinguistic Subjects: Discrepancy between Performance and Competence
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2411.07533v3](http://arxiv.org/pdf/2411.07533v3)**

> **作者:** Linyang He; Ercong Nie; Helmut Schmid; Hinrich Schütze; Nima Mesgarani; Jonathan Brennan
>
> **摘要:** This study investigates the linguistic understanding of Large Language Models (LLMs) regarding signifier (form) and signified (meaning) by distinguishing two LLM assessment paradigms: psycholinguistic and neurolinguistic. Traditional psycholinguistic evaluations often reflect statistical rules that may not accurately represent LLMs' true linguistic competence. We introduce a neurolinguistic approach, utilizing a novel method that combines minimal pair and diagnostic probing to analyze activation patterns across model layers. This method allows for a detailed examination of how LLMs represent form and meaning, and whether these representations are consistent across languages. We found: (1) Psycholinguistic and neurolinguistic methods reveal that language performance and competence are distinct; (2) Direct probability measurement may not accurately assess linguistic competence; (3) Instruction tuning won't change much competence but improve performance; (4) LLMs exhibit higher competence and performance in form compared to meaning. Additionally, we introduce new conceptual minimal pair datasets for Chinese (COMPS-ZH) and German (COMPS-DE), complementing existing English datasets.
>
---
#### [replaced 081] Personalization of Large Language Models: A Survey
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2411.00027v3](http://arxiv.org/pdf/2411.00027v3)**

> **作者:** Zhehao Zhang; Ryan A. Rossi; Branislav Kveton; Yijia Shao; Diyi Yang; Hamed Zamani; Franck Dernoncourt; Joe Barrow; Tong Yu; Sungchul Kim; Ruiyi Zhang; Jiuxiang Gu; Tyler Derr; Hongjie Chen; Junda Wu; Xiang Chen; Zichao Wang; Subrata Mitra; Nedim Lipka; Nesreen Ahmed; Yu Wang
>
> **备注:** Accepted at the Transactions on Machine Learning Research (TMLR) journal
>
> **摘要:** Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.
>
---
#### [replaced 082] ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models
- **分类: cs.CL; cs.DB**

- **链接: [http://arxiv.org/pdf/2506.22791v2](http://arxiv.org/pdf/2506.22791v2)**

> **作者:** Jianxin Yan; Wangze Ni; Lei Chen; Xuemin Lin; Peng Cheng; Zhan Qin; Kui Ren
>
> **摘要:** Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of real-world conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.
>
---
#### [replaced 083] Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning
- **分类: cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2506.10521v4](http://arxiv.org/pdf/2506.10521v4)**

> **作者:** Yuhao Zhou; Yiheng Wang; Xuming He; Ruoyao Xiao; Zhiwei Li; Qiantai Feng; Zijie Guo; Yuejin Yang; Hao Wu; Wenxuan Huang; Jiaqi Wei; Dan Si; Xiuqi Yao; Jia Bu; Haiwen Huang; Tianfan Fu; Shixiang Tang; Ben Fei; Dongzhan Zhou; Fenghua Ling; Yan Lu; Siqi Sun; Chenhui Li; Guanjie Zheng; Jiancheng Lv; Wenlong Zhang; Lei Bai
>
> **备注:** 82 pages
>
> **摘要:** Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientists' First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries.
>
---
#### [replaced 084] CASCADE Your Datasets for Cross-Mode Knowledge Retrieval of Language Models
- **分类: cs.LG; cs.CL**

- **链接: [http://arxiv.org/pdf/2504.01450v2](http://arxiv.org/pdf/2504.01450v2)**

> **作者:** Runlong Zhou; Yi Zhang
>
> **备注:** COLM 2025
>
> **摘要:** Language models often struggle with cross-mode knowledge retrieval -- the ability to access knowledge learned in one format (mode) when queried in another. We demonstrate that models trained on multiple data sources (e.g., Wikipedia and TinyStories) exhibit significantly reduced accuracy when retrieving knowledge in a format different from its original training mode. This paper quantitatively investigates this phenomenon through a controlled study of random token sequence memorization across different modes. We first explore dataset rewriting as a solution, revealing that effective cross-mode retrieval requires prohibitively extensive rewriting efforts that follow a sigmoid-like relationship. As an alternative, we propose CASCADE, a novel pretraining algorithm that uses cascading datasets with varying sequence lengths and computing losses on only the second half of each training sequence to capture knowledge at different scales. Our experiments demonstrate that CASCADE outperforms dataset rewriting approaches, even when compressed into a single model with a unified loss function. This work provides both qualitative evidence of cross-mode retrieval limitations and a practical solution to enhance language models' ability to access knowledge independently of its presentational format.
>
---
#### [replaced 085] A General Framework for Inference-time Scaling and Steering of Diffusion Models
- **分类: cs.LG; cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2501.06848v4](http://arxiv.org/pdf/2501.06848v4)**

> **作者:** Raghav Singhal; Zachary Horvitz; Ryan Teehan; Mengye Ren; Zhou Yu; Kathleen McKeown; Rajesh Ranganath
>
> **摘要:** Diffusion models produce impressive results in modalities ranging from images and video to protein design and text. However, generating samples with user-specified properties remains a challenge. Recent research proposes fine-tuning models to maximize rewards that capture desired properties, but these methods require expensive training and are prone to mode collapse. In this work, we present Feynman-Kac (FK) steering, an inference-time framework for steering diffusion models with reward functions. FK steering works by sampling a system of multiple interacting diffusion processes, called particles, and resampling particles at intermediate steps based on scores computed using functions called potentials. Potentials are defined using rewards for intermediate states and are selected such that a high value indicates that the particle will yield a high-reward sample. We explore various choices of potentials, intermediate rewards, and samplers. We evaluate FK steering on text-to-image and text diffusion models. For steering text-to-image models with a human preference reward, we find that FK steering a 0.8B parameter model outperforms a 2.6B parameter fine-tuned model on prompt fidelity, with faster sampling and no training. For steering text diffusion models with rewards for text quality and specific text attributes, we find that FK steering generates lower perplexity, more linguistically acceptable outputs and enables gradient-free control of attributes like toxicity. Our results demonstrate that inference-time scaling and steering of diffusion models - even with off-the-shelf rewards - can provide significant sample quality gains and controllability benefits. Code is available at https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .
>
---
#### [replaced 086] MoRE: A Mixture of Reflectors Framework for Large Language Model-Based Sequential Recommendation
- **分类: cs.IR; cs.CL**

- **链接: [http://arxiv.org/pdf/2409.06377v2](http://arxiv.org/pdf/2409.06377v2)**

> **作者:** Weicong Qin; Yi Xu; Weijie Yu; Chenglei Shen; Xiao Zhang; Ming He; Jianping Fan; Jun Xu
>
> **备注:** First 2 authors contributes equally to this work, accepted by RecSys'25 spotlight oral. Corresponding author is Weijie Yu(yu@uibe.edu.cn)
>
> **摘要:** Large language models (LLMs) have emerged as a cutting-edge approach in sequential recommendation, leveraging historical interactions to model dynamic user preferences. Current methods mainly focus on learning processed recommendation data in the form of sequence-to-sequence text. While effective, they exhibit three key limitations: 1) failing to decouple intra-user explicit features (e.g., product titles) from implicit behavioral patterns (e.g., brand loyalty) within interaction histories; 2) underutilizing cross-user collaborative filtering (CF) signals; and 3) relying on inefficient reflection update strategies. To address this, We propose MoRE (Mixture of REflectors), which introduces three perspective-aware offline reflection processes to address these gaps. This decomposition directly resolves Challenges 1 (explicit/implicit ambiguity) and 2 (CF underutilization). Furthermore, MoRE's meta-reflector employs a self-improving strategy and a dynamic selection mechanism (Challenge 3) to adapt to evolving user preferences. First, two intra-user reflectors decouple explicit and implicit patterns from a user's interaction sequence, mimicking traditional recommender systems' ability to distinguish surface-level and latent preferences. A third cross-user reflector captures CF signals by analyzing user similarity patterns from multiple users' interactions. To optimize reflection quality, MoRE's meta-reflector employs a offline self-improving strategy that evaluates reflection impacts through comparisons of presence/absence and iterative refinement of old/new versions, with a online contextual bandit mechanism dynamically selecting the optimal perspective for recommendation for each user. Code: https://github.com/E-qin/MoRE-Rec.
>
---
#### [replaced 087] BEExformer: A Fast Inferencing Binarized Transformer with Early Exits
- **分类: cs.CL; cs.AI; cs.NE**

- **链接: [http://arxiv.org/pdf/2412.05225v2](http://arxiv.org/pdf/2412.05225v2)**

> **作者:** Wazib Ansar; Saptarsi Goswami; Amlan Chakrabarti
>
> **备注:** This revised manuscript includes 18 pages, 17 figures, and 6 tables. Methodology and results sections have been improved for clarity and depth, incorporating additional comparisons, ablations, and a new evaluation dataset. A few relevant references were added, and overall organization refined for better readability
>
> **摘要:** Large Language Models (LLMs) based on transformers achieve cutting-edge results on a variety of applications. However, their enormous size and processing requirements hinder deployment on constrained resources. To enhance efficiency, binarization and Early Exit (EE) have proved to be effective solutions. However, binarization may lead to performance loss as reduced precision affects gradient estimation and parameter updates. Besides, research on EE mechanisms is still in its early stages. To address these challenges, we introduce Binarized Early Exit Transformer (BEExformer), the first-ever selective learning-based transformer integrating Binarization-Aware Training (BAT) with EE for efficient and fast textual inference. Each transformer block has an integrated Selective-Learn Forget Network (SLFN) to enhance contextual retention while eliminating irrelevant information. The BAT employs a differentiable second-order approximation to the sign function, enabling gradient computation that captures both the sign and magnitude of the weights. This aids in 21.30 times reduction in model size. The EE mechanism hinges on fractional reduction in entropy among intermediate transformer blocks with soft-routing loss estimation. This accelerates inference by reducing FLOPs by 52.08% and even improves accuracy by 2.89% by resolving the "overthinking" problem inherent in deep networks. Extensive evaluation through comparison with the SOTA methods and various ablations across six datasets covering multiple NLP tasks demonstrates its Pareto-optimal performance-efficiency trade-off.
>
---
#### [replaced 088] The distribution of syntactic dependency distances
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2211.14620v3](http://arxiv.org/pdf/2211.14620v3)**

> **作者:** Sonia Petrini; Ramon Ferrer-i-Cancho
>
> **备注:** minor corrections; in press in Glottometrics
>
> **摘要:** The syntactic structure of a sentence can be represented as a graph, where vertices are words and edges indicate syntactic dependencies between them. In this setting, the distance between two linked words is defined as the difference between their positions. Here we wish to contribute to the characterization of the actual distribution of syntactic dependency distances, which has previously been argued to follow a power-law distribution. Here we propose a new model with two exponential regimes in which the probability decay is allowed to change after a break-point. This transition could mirror the transition from the processing of word chunks to higher-level structures. We find that a two-regime model - where the first regime follows either an exponential or a power-law decay - is the most likely one in all 20 languages we considered, independently of sentence length and annotation style. Moreover, the break-point exhibits low variation across languages and averages values of 4-5 words, suggesting that the amount of words that can be simultaneously processed abstracts from the specific language to a high degree. The probability decay slows down after the breakpoint, consistently with a universal chunk-and-pass mechanism. Finally, we give an account of the relation between the best estimated model and the closeness of syntactic dependencies as function of sentence length, according to a recently introduced optimality score.
>
---
#### [replaced 089] A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A Field Experiment
- **分类: cs.RO; cs.CL; cs.SD**

- **链接: [http://arxiv.org/pdf/2503.06241v2](http://arxiv.org/pdf/2503.06241v2)**

> **作者:** Koji Inoue; Yuki Okafuji; Jun Baba; Yoshiki Ohira; Katsuya Hyodo; Tatsuya Kawahara
>
> **备注:** This paper has been accepted for presentation at IEEE/RSJ International Conference on Intelligent Robots and Systems 2025 (IROS 2025) and represents the author's version of the work
>
> **摘要:** Turn-taking is a crucial aspect of human-robot interaction, directly influencing conversational fluidity and user engagement. While previous research has explored turn-taking models in controlled environments, their robustness in real-world settings remains underexplored. In this study, we propose a noise-robust voice activity projection (VAP) model, based on a Transformer architecture, to enhance real-time turn-taking in dialogue robots. To evaluate the effectiveness of the proposed system, we conducted a field experiment in a shopping mall, comparing the VAP system with a conventional cloud-based speech recognition system. Our analysis covered both subjective user evaluations and objective behavioral analysis. The results showed that the proposed system significantly reduced response latency, leading to a more natural conversation where both the robot and users responded faster. The subjective evaluations suggested that faster responses contribute to a better interaction experience.
>
---
#### [replaced 090] Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection
- **分类: cs.CL; cs.AI**

- **链接: [http://arxiv.org/pdf/2501.03940v3](http://arxiv.org/pdf/2501.03940v3)**

> **作者:** Pablo Miralles-González; Javier Huertas-Tato; Alejandro Martín; David Camacho
>
> **摘要:** The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.
>
---
#### [replaced 091] DataDecide: How to Predict Best Pretraining Data with Small Experiments
- **分类: cs.LG; cs.CL**

- **链接: [http://arxiv.org/pdf/2504.11393v2](http://arxiv.org/pdf/2504.11393v2)**

> **作者:** Ian Magnusson; Nguyen Tai; Ben Bogin; David Heineman; Jena D. Hwang; Luca Soldaini; Akshita Bhagia; Jiacheng Liu; Dirk Groeneveld; Oyvind Tafjord; Noah A. Smith; Pang Wei Koh; Jesse Dodge
>
> **备注:** ICML 2025
>
> **摘要:** Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.
>
---
#### [replaced 092] Perception-Aware Policy Optimization for Multimodal Reasoning
- **分类: cs.CL**

- **链接: [http://arxiv.org/pdf/2507.06448v2](http://arxiv.org/pdf/2507.06448v2)**

> **作者:** Zhenhailong Wang; Xuehang Guo; Sofia Stoica; Haiyang Xu; Hongru Wang; Hyeonjeong Ha; Xiusi Chen; Yangyi Chen; Ming Yan; Fei Huang; Heng Ji
>
> **摘要:** Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose Perception-Aware Policy Optimization (PAPO), a simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term to the GRPO objective, which, despite its simplicity, yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. We also observe a substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO. We conduct comprehensive analysis of PAPO and identify a unique loss hacking issue, which we rigorously analyze and mitigate through a Double Entropy Loss. Overall, our work introduces a deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.
>
---
