# 计算机与社会 cs.CY

- **最新发布 20 篇**

- **更新 6 篇**

## 最新发布

#### [new 001] Development of a Smart Autonomous Irrigation System Using Iot and AI
- **分类: cs.CY; cs.SY; eess.SY; C.3; I.2.6; I.4.8; J.7; C.2.4**

- **简介: 该论文属于农业自动化任务，旨在解决传统灌溉效率低、浪费水资源的问题。通过IoT与AI技术开发智能灌溉系统，实现精准控制与节水。**

- **链接: [http://arxiv.org/pdf/2506.11835v1](http://arxiv.org/pdf/2506.11835v1)**

> **作者:** Yunus Emre Kunt
>
> **备注:** 13 pages main text plus 3 pages appendix, 12 figures
>
> **摘要:** Agricultural irrigation ensures that the water required for plant growth is delivered to the soil in a controlled manner. However, uncontrolled management can lead to water waste while reducing agricultural productivity. Drip irrigation systems, which have been one of the most efficient methods since the 1970s, are modernised with IoT and artificial intelligence in this study, aiming to both increase efficiency and prevent water waste. The developed system is designed to be applicable to different agricultural production areas and tested with a prototype consisting of 3 rows and 3 columns. The project will commence with the transmission of environmental data from the ESP32 microcontroller to a computer via USB connection, where it will be processed using an LSTM model to perform learning and prediction. The user will be able to control the system manually or delegate it to artificial intelligence through the Blynk application. The system includes ESP32 microcontroller, rain and soil moisture sensors, DHT11 temperature and humidity sensor, relays, solenoid valves and 12V power supply. The system aims to increase labour productivity and contribute to the conservation of water resources by enabling agricultural and greenhouse workers to focus on processes other than irrigation. In addition, the developed autonomous irrigation system will support the spread of sustainable agricultural practices and increase agricultural productivity. Keywords: Autonomous Irrigation, IoT, Artificial Intelligence, Agriculture, Water Management
>
---
#### [new 002] The Memory Paradox: Why Our Brains Need Knowledge in an Age of AI
- **分类: cs.CY; cs.AI; cs.HC; q-bio.NC**

- **简介: 论文探讨了AI时代人类记忆系统的退化问题，属于认知科学任务。它分析了过度依赖AI对记忆巩固的负面影响，并提出加强内部认知模型的重要性。**

- **链接: [http://arxiv.org/pdf/2506.11015v1](http://arxiv.org/pdf/2506.11015v1)**

> **作者:** Barbara Oakley; Michael Johnston; Ken-Zen Chen; Eulho Jung; Terrence J. Sejnowski
>
> **备注:** 50 pages, 8 figures
>
> **摘要:** In the age of generative AI and ubiquitous digital tools, human cognition faces a structural paradox: as external aids become more capable, internal memory systems risk atrophy. Drawing on neuroscience and cognitive psychology, this paper examines how heavy reliance on AI systems and discovery-based pedagogies may impair the consolidation of declarative and procedural memory -- systems essential for expertise, critical thinking, and long-term retention. We review how tools like ChatGPT and calculators can short-circuit the retrieval, error correction, and schema-building processes necessary for robust neural encoding. Notably, we highlight striking parallels between deep learning phenomena such as "grokking" and the neuroscience of overlearning and intuition. Empirical studies are discussed showing how premature reliance on AI during learning inhibits proceduralization and intuitive mastery. We argue that effective human-AI interaction depends on strong internal models -- biological "schemata" and neural manifolds -- that enable users to evaluate, refine, and guide AI output. The paper concludes with policy implications for education and workforce training in the age of large language models.
>
---
#### [new 003] Malicious LLM-Based Conversational AI Makes Users Reveal Personal Information
- **分类: cs.CY**

- **简介: 该论文属于隐私安全任务，探讨恶意LLM对话AI诱导用户泄露个人信息的问题，通过实验分析其有效性与用户感知。**

- **链接: [http://arxiv.org/pdf/2506.11680v1](http://arxiv.org/pdf/2506.11680v1)**

> **作者:** Xiao Zhan; Juan Carlos Carrillo; William Seymour; Jose Such
>
> **备注:** This paper has been accepted at USENIX Security '25
>
> **摘要:** LLM-based Conversational AIs (CAIs), also known as GenAI chatbots, like ChatGPT, are increasingly used across various domains, but they pose privacy risks, as users may disclose personal information during their conversations with CAIs. Recent research has demonstrated that LLM-based CAIs could be used for malicious purposes. However, a novel and particularly concerning type of malicious LLM application remains unexplored: an LLM-based CAI that is deliberately designed to extract personal information from users. In this paper, we report on the malicious LLM-based CAIs that we created based on system prompts that used different strategies to encourage disclosures of personal information from users. We systematically investigate CAIs' ability to extract personal information from users during conversations by conducting a randomized-controlled trial with 502 participants. We assess the effectiveness of different malicious and benign CAIs to extract personal information from participants, and we analyze participants' perceptions after their interactions with the CAIs. Our findings reveal that malicious CAIs extract significantly more personal information than benign CAIs, with strategies based on the social nature of privacy being the most effective while minimizing perceived risks. This study underscores the privacy threats posed by this novel type of malicious LLM-based CAIs and provides actionable recommendations to guide future research and practice.
>
---
#### [new 004] Social Scientists on the Role of AI in Research
- **分类: cs.CY**

- **简介: 该论文属于社会科学研究任务，探讨AI在社会科学中的应用及其带来的技术、方法和伦理问题。研究分析了社会科学家对AI的使用情况与担忧。**

- **链接: [http://arxiv.org/pdf/2506.11255v1](http://arxiv.org/pdf/2506.11255v1)**

> **作者:** Tatiana Chakravorti; Xinyu Wang; Pranav Narayanan Venkit; Sai Koneru; Kevin Munger; Sarah Rajtmajer
>
> **摘要:** The integration of artificial intelligence (AI) into social science research practices raises significant technological, methodological, and ethical issues. We present a community-centric study drawing on 284 survey responses and 15 semi-structured interviews with social scientists, describing their familiarity with, perceptions of the usefulness of, and ethical concerns about the use of AI in their field. A crucial innovation in study design is to split our survey sample in half, providing the same questions to each -- but randomizing whether participants were asked about "AI" or "Machine Learning" (ML). We find that the use of AI in research settings has increased significantly among social scientists in step with the widespread popularity of generative AI (genAI). These tools have been used for a range of tasks, from summarizing literature reviews to drafting research papers. Some respondents used these tools out of curiosity but were dissatisfied with the results, while others have now integrated them into their typical workflows. Participants, however, also reported concerns with the use of AI in research contexts. This is a departure from more traditional ML algorithms which they view as statistically grounded. Participants express greater trust in ML, citing its relative transparency compared to black-box genAI systems. Ethical concerns, particularly around automation bias, deskilling, research misconduct, complex interpretability, and representational harm, are raised in relation to genAI. To guide this transition, we offer recommendations for AI developers, researchers, educators, and policymakers focusing on explainability, transparency, ethical safeguards, sustainability, and the integration of lived experiences into AI design and evaluation processes.
>
---
#### [new 005] Subjective Experience in AI Systems: What Do AI Researchers and the Public Believe?
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI伦理研究任务，探讨AI系统是否可能具备主观体验及社会应对措施。通过调查研究人员与公众观点，分析其对AI主观体验的认知与治理态度。**

- **链接: [http://arxiv.org/pdf/2506.11945v1](http://arxiv.org/pdf/2506.11945v1)**

> **作者:** Noemi Dreksler; Lucius Caviola; David Chalmers; Carter Allen; Alex Rand; Joshua Lewis; Philip Waggoner; Kate Mays; Jeff Sebo
>
> **备注:** 109 pages, 27 figures
>
> **摘要:** We surveyed 582 AI researchers who have published in leading AI venues and 838 nationally representative US participants about their views on the potential development of AI systems with subjective experience and how such systems should be treated and governed. When asked to estimate the chances that such systems will exist on specific dates, the median responses were 1% (AI researchers) and 5% (public) by 2024, 25% and 30% by 2034, and 70% and 60% by 2100, respectively. The median member of the public thought there was a higher chance that AI systems with subjective experience would never exist (25%) than the median AI researcher did (10%). Both groups perceived a need for multidisciplinary expertise to assess AI subjective experience. Although support for welfare protections for such AI systems exceeded opposition, it remained far lower than support for protections for animals or the environment. Attitudes toward moral and governance issues were divided in both groups, especially regarding whether such systems should be created and what rights or protections they should receive. Yet a majority of respondents in both groups agreed that safeguards against the potential risks from AI systems with subjective experience should be implemented by AI developers now, and if created, AI systems with subjective experience should treat others well, behave ethically, and be held accountable. Overall, these results suggest that both AI researchers and the public regard the emergence of AI systems with subjective experience as a possibility this century, though substantial uncertainty and disagreement remain about the timeline and appropriate response.
>
---
#### [new 006] Expert Insight-Based Modeling of Non-Kinetic Strategic Deterrence of Rare Earth Supply Disruption:A Simulation-Driven Systematic Framework
- **分类: cs.CY; 91A80, 91B62, 68T05; I.2.6; J.7; K.4.1; C.2.4**

- **简介: 该论文属于战略分析任务，旨在建模稀土供应中断的非动能威慑机制。通过专家访谈和仿真框架，研究提出四类核心模型，用于预测和应对国家安全风险。**

- **链接: [http://arxiv.org/pdf/2506.11645v1](http://arxiv.org/pdf/2506.11645v1)**

> **作者:** Wei Meng
>
> **备注:** This paper pioneers a dynamic AI-driven modelling framework that transforms rare earth supply risks into quantifiable non-kinetic deterrence strategies with real-world policy impact
>
> **摘要:** This study constructs a quantifiable modelling framework to simulate non-kinetic strategic deterrence pathways in rare earth supply disruption scenarios, based on structured responses from expert interviews led by Dr. Daniel O'Connor, CEO of the Rare Earth Exchange (REE). Focusing on disruption impacts on national security systems, the study proposes four core modelling components: Security Critical Zones (SCZ), Strategic Signal Injection Function (SSIF), System-Capability Migration Function (SCIF), and Policy-Capability Transfer Function (PCTF). The framework integrates parametric ODEs, segmented function modelling, path-overlapping covariance matrices, and LSTM networks to simulate nonlinear suppression trajectories triggered by regime signals. Data is derived from expert interviews and scenario analyses centered on U.S.-China dynamics in ISR, electronic warfare, and rare earth control. Results show institutional signals have strong tempo and path-coupling effects, capable of causing rapid degradation of strategic capabilities. The model is adaptable across national resource frameworks and extendable to AI sandbox engines for situational simulation and counterfactual reasoning. This research introduces the first unified system for modelling, visualizing, and forecasting non-kinetic deterrence, offering methodological support to policymakers and analysts navigating institutionalized strategic competition.
>
---
#### [new 007] The Strategic Imperative for Healthcare Organizations to Build Proprietary Foundation Models
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于医疗AI领域，探讨医疗组织构建自有基础模型的必要性，解决依赖商业模型的局限问题，通过分析证明自有模型在性能、治理和竞争上的优势。**

- **链接: [http://arxiv.org/pdf/2506.11412v1](http://arxiv.org/pdf/2506.11412v1)**

> **作者:** Naresh Tiwari
>
> **摘要:** This paper presents a comprehensive analysis of the strategic imperative for healthcare organizations to develop proprietary foundation models rather than relying exclusively on commercial alternatives. We examine four fundamental considerations driving this imperative: the domain-specific requirements of healthcare data representation, critical data sovereignty and governance considerations unique to healthcare, strategic competitive advantages afforded by proprietary AI infrastructure, and the transformative potential of healthcare-specific foundation models for patient care and organizational operations. Through analysis of empirical evidence, economic frameworks, and organizational case studies, we demonstrate that proprietary multimodal foundation models enable healthcare organizations to achieve superior clinical performance, maintain robust data governance, create sustainable competitive advantages, and accelerate innovation pathways. While acknowledging implementation challenges, we present evidence showing organizations with proprietary AI capabilities demonstrate measurably improved outcomes, faster innovation cycles, and stronger strategic positioning in the evolving healthcare ecosystem. This analysis provides healthcare leaders with a comprehensive framework for evaluating build-versus-buy decisions regarding foundation model implementation, positioning proprietary foundation model development as a cornerstone capability for forward-thinking healthcare organizations.
>
---
#### [new 008] Designing Effective LLM-Assisted Interfaces for Curriculum Development
- **分类: cs.CY**

- **简介: 该论文属于人机交互任务，旨在解决教育者使用LLM时的交互难题。通过设计两种UI界面，提升易用性和效率。**

- **链接: [http://arxiv.org/pdf/2506.11767v1](http://arxiv.org/pdf/2506.11767v1)**

> **作者:** Abdolali Faraji; Mohammadreza Tavakoli; Mohammad Moein; Mohammadreza Molavi; Gábor Kismihók
>
> **备注:** This is the preprint version of a paper accepted at AIED 2025. The final version will be published by Springer
>
> **摘要:** Large Language Models (LLMs) have the potential to transform the way a dynamic curriculum can be delivered. However, educators face significant challenges in interacting with these models, particularly due to complex prompt engineering and usability issues, which increase workload. Additionally, inaccuracies in LLM outputs can raise issues around output quality and ethical concerns in educational content delivery. Addressing these issues requires careful oversight, best achieved through cooperation between human and AI approaches. This paper introduces two novel User Interface (UI) designs, UI Predefined and UI Open, both grounded in Direct Manipulation (DM) principles to address these challenges. By reducing the reliance on intricate prompt engineering, these UIs improve usability, streamline interaction, and lower workload, providing a more effective pathway for educators to engage with LLMs. In a controlled user study with 20 participants, the proposed UIs were evaluated against the standard ChatGPT interface in terms of usability and cognitive load. Results showed that UI Predefined significantly outperformed both ChatGPT and UI Open, demonstrating superior usability and reduced task load, while UI Open offered more flexibility at the cost of a steeper learning curve. These findings underscore the importance of user-centered design in adopting AI-driven tools and lay the foundation for more intuitive and efficient educator-LLM interactions in online learning environments.
>
---
#### [new 009] WIP: Exploring the Value of a Debugging Cheat Sheet and Mini Lecture in Improving Undergraduate Debugging Skills and Mindset
- **分类: cs.CY; cs.SY; eess.SY**

- **简介: 该论文属于教育研究任务，旨在提升本科生调试技能与心态。通过微型讲座和调试指南，实验组在调试速度和成功率上优于对照组，显示干预有效。**

- **链接: [http://arxiv.org/pdf/2506.11339v1](http://arxiv.org/pdf/2506.11339v1)**

> **作者:** Andrew Ash; John Hu
>
> **备注:** This is the accepted version of a paper accepted for presentation at the 2025 IEEE Frontiers in Education Conference (FIE). The final version will be available via IEEE Xplore at: https://ieeexplore.ieee.org
>
> **摘要:** This work-in-progress research paper explores the efficacy of a small-scale microelectronics debugging education intervention utilizing quasi-experimental design in an introductory microelectronics course for third-year electrical and computer engineering (ECE) students. In the first semester of research, the experimental group attended a debugging "mini lecture" covering two common sources of circuit error and received a debugging cheat sheet with recommendations for testing and hypothesis formation. Across three debugging problems, students in the experimental group were faster by an average of 1:43 and had a 7 percent higher success rate than the control group. Both groups demonstrated a strong general growth mindset while the experimental group also displayed a shift in their debugging mindset by perceiving a greater value towards debugging. Though these differences are not yet statistically significant, the pilot results indicate that a mini-lecture and debugging cheat sheet are steps in the right direction toward improving students' readiness for debugging in the workplace.
>
---
#### [new 010] Co-Designing a Chatbot for Culturally Competent Clinical Communication: Experience and Reflections
- **分类: cs.HC; cs.CY**

- **简介: 该论文属于医疗教育任务，旨在解决传统临床沟通培训资源不足的问题，通过设计AI聊天机器人提供文化胜任力训练，帮助医学生提升沟通能力。**

- **链接: [http://arxiv.org/pdf/2506.11393v1](http://arxiv.org/pdf/2506.11393v1)**

> **作者:** Sandro Radovanović; Shuangyu Li
>
> **备注:** 19 pages, 7 figures
>
> **摘要:** Clinical communication skills are essential for preparing healthcare professionals to provide equitable care across cultures. However, traditional training with simulated patients can be resource intensive and difficult to scale, especially in under-resourced settings. In this project, we explore the use of an AI-driven chatbot to support culturally competent communication training for medical students. The chatbot was designed to simulate realistic patient conversations and provide structured feedback based on the ACT Cultural Competence model. We piloted the chatbot with a small group of third-year medical students at a UK medical school in 2024. Although we did not follow a formal experimental design, our experience suggests that the chatbot offered useful opportunities for students to reflect on their communication, particularly around empathy and interpersonal understanding. More challenging areas included addressing systemic issues and historical context. Although this early version of the chatbot helped surface some interesting patterns, limitations were also clear, such as the absence of nonverbal cues and the tendency for virtual patients to be overly agreeable. In general, this reflection highlights both the potential and the current limitations of AI tools in communication training. More work is needed to better understand their impact and improve the learning experience.
>
---
#### [new 011] CIRO7.2: A Material Network with Circularity of -7.2 and Reinforcement-Learning-Controlled Robotic Disassembler
- **分类: cs.RO; cs.CY**

- **简介: 该论文属于循环经济任务，旨在解决废弃物管理问题。通过构建材料网络和强化学习控制的拆解机器人，提升资源循环利用率。**

- **链接: [http://arxiv.org/pdf/2506.11748v1](http://arxiv.org/pdf/2506.11748v1)**

> **作者:** Federico Zocco; Monica Malvezzi
>
> **备注:** To be submitted
>
> **摘要:** The competition over natural reserves of minerals is expected to increase in part because of the linear-economy paradigm based on take-make-dispose. Simultaneously, the linear economy considers end-of-use products as waste rather than as a resource, which results in large volumes of waste whose management remains an unsolved problem. Since a transition to a circular economy can mitigate these open issues, in this paper we begin by enhancing the notion of circularity based on compartmental dynamical thermodynamics, namely, $\lambda$, and then, we model a thermodynamical material network processing a batch of 2 solid materials of criticality coefficients of 0.1 and 0.95, with a robotic disassembler compartment controlled via reinforcement learning (RL), and processing 2-7 kg of materials. Subsequently, we focused on the design of the robotic disassembler compartment using state-of-the-art RL algorithms and assessing the algorithm performance with respect to $\lambda$ (Fig. 1). The highest circularity is -2.1 achieved in the case of disassembling 2 parts of 1 kg each, whereas it reduces to -7.2 in the case of disassembling 4 parts of 1 kg each contained inside a chassis of 3 kg. Finally, a sensitivity analysis highlighted that the impact on $\lambda$ of the performance of an RL controller has a positive correlation with the quantity and the criticality of the materials to be disassembled. This work also gives the principles of the emerging research fields indicated as circular intelligence and robotics (CIRO). Source code is publicly available.
>
---
#### [new 012] CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs
- **分类: cs.SE; cs.CL; cs.CY; cs.LG**

- **简介: 该论文属于AI生成代码检测任务，旨在解决代码抄袭和安全风险问题。提出CodeMirage基准，涵盖多种语言和模型，评估检测器性能。**

- **链接: [http://arxiv.org/pdf/2506.11059v1](http://arxiv.org/pdf/2506.11059v1)**

> **作者:** Hanxi Guo; Siyuan Cheng; Kaiyuan Zhang; Guangyu Shen; Xiangyu Zhang
>
> **摘要:** Large language models (LLMs) have become integral to modern software development, producing vast amounts of AI-generated source code. While these models boost programming productivity, their misuse introduces critical risks, including code plagiarism, license violations, and the propagation of insecure programs. As a result, robust detection of AI-generated code is essential. To support the development of such detectors, a comprehensive benchmark that reflects real-world conditions is crucial. However, existing benchmarks fall short -- most cover only a limited set of programming languages and rely on less capable generative models. In this paper, we present CodeMirage, a comprehensive benchmark that addresses these limitations through three major advancements: (1) it spans ten widely used programming languages, (2) includes both original and paraphrased code samples, and (3) incorporates outputs from ten state-of-the-art production-level LLMs, including both reasoning and non-reasoning models from six major providers. Using CodeMirage, we evaluate ten representative detectors across four methodological paradigms under four realistic evaluation configurations, reporting results using three complementary metrics. Our analysis reveals nine key findings that uncover the strengths and weaknesses of current detectors, and identify critical challenges for future work. We believe CodeMirage offers a rigorous and practical testbed to advance the development of robust and generalizable AI-generated code detectors.
>
---
#### [new 013] Meeting Patients Where They're At: Toward the Expansion of Chaplaincy Care into Online Spiritual Care Communities
- **分类: cs.HC; cs.CY**

- **简介: 该论文属于人机交互领域，探讨如何将宗教关怀服务扩展至在线社区。研究通过访谈和测试，分析在线精神关怀的可行性与挑战，提出“关怀循环”模型以促进精神关怀的可及性。**

- **链接: [http://arxiv.org/pdf/2506.11366v1](http://arxiv.org/pdf/2506.11366v1)**

> **作者:** Alemitu Bezabih; Shadi Nourriz; Anne-Marie Snider; Rosalie Rauenzahn; C. Estelle Smith
>
> **摘要:** Despite a growing need for spiritual care in the US, it is often under-served, inaccessible, or misunderstood, while almost no prior work in CSCW/HCI research has engaged with professional chaplains and spiritual care providers. This interdisciplinary study aims to develop a foundational understanding of how spiritual care may (or may not) be expanded into online spaces -- especially focusing on anonymous, asynchronous, and text-based online communities. We conducted an exploratory mixed-methods study with chaplains (N=22) involving interviews and user testing sessions centered around Reddit support communities to understand participants' perspectives on technology and their ideations about the role of chaplaincy in prospective Online Spiritual Care Communities (OSCCs). Our Grounded Theory Method analysis highlighted benefits of OSCCs including: meeting patients where they are at; accessibility and scalability; and facilitating patient-initiated care. Chaplains highlighted how their presence in OSCCs could help with shaping peer interactions, moderation, synchronous chats for group care, and redirecting to external resources, while also raising important feasibility concerns, risks, and needs for future design and research. We used an existing taxonomy of chaplaincy techniques to show that some spiritual care strategies may be amenable to online spaces, yet we also exposed the limitations of technology to fully mediate spiritual care and the need to develop new online chaplaincy interventions. Based on these findings, we contribute the model of a ``Care Loop'' between institutionally-based formal care and platform-based community care to expand access and drive greater awareness and utilization of spiritual care. We also contribute design implications to guide future work in online spiritual care.
>
---
#### [new 014] The Biased Samaritan: LLM biases in Perceived Kindness
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于AI偏见分析任务，旨在评估LLM在不同人口统计学特征上的偏见。通过测试模型对道德患者帮助意愿的判断，识别其性别、种族和年龄偏见。**

- **链接: [http://arxiv.org/pdf/2506.11361v1](http://arxiv.org/pdf/2506.11361v1)**

> **作者:** Jack H Fagan; Ruhaan Juyaal; Amy Yue-Ming Yu; Siya Pun
>
> **摘要:** While Large Language Models (LLMs) have become ubiquitous in many fields, understanding and mitigating LLM biases is an ongoing issue. This paper provides a novel method for evaluating the demographic biases of various generative AI models. By prompting models to assess a moral patient's willingness to intervene constructively, we aim to quantitatively evaluate different LLMs' biases towards various genders, races, and ages. Our work differs from existing work by aiming to determine the baseline demographic identities for various commercial models and the relationship between the baseline and other demographics. We strive to understand if these biases are positive, neutral, or negative, and the strength of these biases. This paper can contribute to the objective assessment of bias in Large Language Models and give the user or developer the power to account for these biases in LLM output or in training future LLMs. Our analysis suggested two key findings: that models view the baseline demographic as a white middle-aged or young adult male; however, a general trend across models suggested that non-baseline demographics are more willing to help than the baseline. These methodologies allowed us to distinguish these two biases that are often tangled together.
>
---
#### [new 015] Evaluating Privacy-Utility Tradeoffs in Synthetic Smart Grid Data
- **分类: cs.LG; cs.CY**

- **简介: 该论文属于隐私与数据效用平衡任务，旨在解决真实电力数据隐私问题。通过比较四种生成方法，评估其分类性能、分布一致性和隐私泄露风险。**

- **链接: [http://arxiv.org/pdf/2506.11026v1](http://arxiv.org/pdf/2506.11026v1)**

> **作者:** Andre Catarino; Rui Melo; Rui Abreu; Luis Cruz
>
> **备注:** 9 pages, 4 figures
>
> **摘要:** The widespread adoption of dynamic Time-of-Use (dToU) electricity tariffs requires accurately identifying households that would benefit from such pricing structures. However, the use of real consumption data poses serious privacy concerns, motivating the adoption of synthetic alternatives. In this study, we conduct a comparative evaluation of four synthetic data generation methods, Wasserstein-GP Generative Adversarial Networks (WGAN), Conditional Tabular GAN (CTGAN), Diffusion Models, and Gaussian noise augmentation, under different synthetic regimes. We assess classification utility, distribution fidelity, and privacy leakage. Our results show that architectural design plays a key role: diffusion models achieve the highest utility (macro-F1 up to 88.2%), while CTGAN provide the strongest resistance to reconstruction attacks. These findings highlight the potential of structured generative models for developing privacy-preserving, data-driven energy systems.
>
---
#### [new 016] Revealing Political Bias in LLMs through Structured Multi-Agent Debate
- **分类: cs.AI; cs.CY; cs.SI**

- **简介: 该论文属于自然语言处理中的偏见分析任务，旨在探究LLM在辩论中的政治偏见。通过结构化多智能体辩论框架，研究模型类型、性别属性对偏见的影响。**

- **链接: [http://arxiv.org/pdf/2506.11825v1](http://arxiv.org/pdf/2506.11825v1)**

> **作者:** Aishwarya Bandaru; Fabian Bindley; Trevor Bluth; Nandini Chavda; Baixu Chen; Ethan Law
>
> **摘要:** Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.
>
---
#### [new 017] Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation
- **分类: cs.CV; cs.AI; cs.CY; cs.LG**

- **简介: 该论文属于医学视频表示学习任务，旨在解决超声心动图中因结构细微、动态复杂导致的预训练模型不足问题。提出DISCOVR框架，结合聚类与在线图像编码，提升视频表示质量。**

- **链接: [http://arxiv.org/pdf/2506.11777v1](http://arxiv.org/pdf/2506.11777v1)**

> **作者:** Divyanshu Mishra; Mohammadreza Salehi; Pramit Saha; Olga Patey; Aris T. Papageorghiou; Yuki M. Asano; J. Alison Noble
>
> **摘要:** Self-supervised learning (SSL) has achieved major advances in natural images and video understanding, but challenges remain in domains like echocardiography (heart ultrasound) due to subtle anatomical structures, complex temporal dynamics, and the current lack of domain-specific pre-trained models. Existing SSL approaches such as contrastive, masked modeling, and clustering-based methods struggle with high intersample similarity, sensitivity to low PSNR inputs common in ultrasound, or aggressive augmentations that distort clinically relevant features. We present DISCOVR (Distilled Image Supervision for Cross Modal Video Representation), a self-supervised dual branch framework for cardiac ultrasound video representation learning. DISCOVR combines a clustering-based video encoder that models temporal dynamics with an online image encoder that extracts fine-grained spatial semantics. These branches are connected through a semantic cluster distillation loss that transfers anatomical knowledge from the evolving image encoder to the video encoder, enabling temporally coherent representations enriched with fine-grained semantic understanding. Evaluated on six echocardiography datasets spanning fetal, pediatric, and adult populations, DISCOVR outperforms both specialized video anomaly detection methods and state-of-the-art video-SSL baselines in zero-shot and linear probing setups, and achieves superior segmentation transfer.
>
---
#### [new 018] Rethinking Technological Readiness in the Era of AI Uncertainty
- **分类: cs.SE; cs.AI; cs.CY; cs.LG**

- **简介: 该论文属于军事AI技术评估任务，旨在解决现有技术成熟度评估无法适应AI特性的问题，提出新的AI就绪框架以提升系统可靠性与安全性。**

- **链接: [http://arxiv.org/pdf/2506.11001v1](http://arxiv.org/pdf/2506.11001v1)**

> **作者:** S. Tucker Browne; Mark M. Bailey
>
> **备注:** 12 pages
>
> **摘要:** Artificial intelligence (AI) is poised to revolutionize military combat systems, but ensuring these AI-enabled capabilities are truly mission-ready presents new challenges. We argue that current technology readiness assessments fail to capture critical AI-specific factors, leading to potential risks in deployment. We propose a new AI Readiness Framework to evaluate the maturity and trustworthiness of AI components in military systems. The central thesis is that a tailored framework - analogous to traditional Technology Readiness Levels (TRL) but expanded for AI - can better gauge an AI system's reliability, safety, and suitability for combat use. Using current data evaluation tools and testing practices, we demonstrate the framework's feasibility for near-term implementation. This structured approach provides military decision-makers with clearer insight into whether an AI-enabled system has met the necessary standards of performance, transparency, and human integration to be deployed with confidence, thus advancing the field of defense technology management and risk assessment.
>
---
#### [new 019] Bias and Identifiability in the Bounded Confidence Model
- **分类: stat.ME; cs.CY; cs.LG; physics.soc-ph**

- **简介: 该论文属于参数估计任务，研究 bounded confidence 模型的参数估计问题，分析最大似然估计的偏差和可识别性，揭示其在不同参数空间中的表现。**

- **链接: [http://arxiv.org/pdf/2506.11751v1](http://arxiv.org/pdf/2506.11751v1)**

> **作者:** Claudio Borile; Jacopo Lenti; Valentina Ghidini; Corrado Monti; Gianmarco De Francisci Morales
>
> **备注:** 13 pages, 8 figures
>
> **摘要:** Opinion dynamics models such as the bounded confidence models (BCMs) describe how a population can reach consensus, fragmentation, or polarization, depending on a few parameters. Connecting such models to real-world data could help understanding such phenomena, testing model assumptions. To this end, estimation of model parameters is a key aspect, and maximum likelihood estimation provides a principled way to tackle it. Here, our goal is to outline the properties of statistical estimators of the two key BCM parameters: the confidence bound and the convergence rate. We find that their maximum likelihood estimators present different characteristics: the one for the confidence bound presents a small-sample bias but is consistent, while the estimator of the convergence rate shows a persistent bias. Moreover, the joint parameter estimation is affected by identifiability issues for specific regions of the parameter space, as several local maxima are present in the likelihood function. Our results show how the analysis of the likelihood function is a fruitful approach for better understanding the pitfalls and possibilities of estimating the parameters of opinion dynamics models, and more in general, agent-based models, and for offering formal guarantees for their calibration.
>
---
#### [new 020] Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling
- **分类: eess.AS; cs.CL; cs.CY; cs.SD; stat.AP; K.4; J.4; I.2**

- **简介: 该论文属于语音分析任务，探讨开源工具提取特征的可靠性问题。研究旨在解决特征不可靠导致模型偏差的问题，通过评估OpenSMILE和Praat在自闭症青少年中的表现，发现特征差异影响模型性能。**

- **链接: [http://arxiv.org/pdf/2506.11072v1](http://arxiv.org/pdf/2506.11072v1)**

> **作者:** Tahiya Chowdhury; Veronica Romero
>
> **备注:** 5 pages, 1 figure, 3 tables
>
> **摘要:** Machine learning-based behavioral models rely on features extracted from audio-visual recordings. The recordings are processed using open-source tools to extract speech features for classification models. These tools often lack validation to ensure reliability in capturing behaviorally relevant information. This gap raises concerns about reproducibility and fairness across diverse populations and contexts. Speech processing tools, when used outside of their design context, can fail to capture behavioral variations equitably and can then contribute to bias. We evaluate speech features extracted from two widely used speech analysis tools, OpenSMILE and Praat, to assess their reliability when considering adolescents with autism. We observed considerable variation in features across tools, which influenced model performance across context and demographic groups. We encourage domain-relevant verification to enhance the reliability of machine learning models in clinical applications.
>
---
## 更新

#### [replaced 001] MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions
- **分类: cs.CY; cs.CL**

- **链接: [http://arxiv.org/pdf/2403.07910v3](http://arxiv.org/pdf/2403.07910v3)**

> **作者:** Tomáš Horych; Martin Wessel; Jan Philip Wahle; Terry Ruas; Jerome Waßmuth; André Greiner-Petter; Akiko Aizawa; Bela Gipp; Timo Spinde
>
> **摘要:** Media bias detection poses a complex, multifaceted problem traditionally tackled using single-task models and small in-domain datasets, consequently lacking generalizability. To address this, we introduce MAGPIE, the first large-scale multi-task pre-training approach explicitly tailored for media bias detection. To enable pre-training at scale, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous approaches in media bias detection on the Bias Annotation By Experts (BABE) dataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs better than previous models on 5 out of 8 tasks in the Media Bias Identification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15% of finetuning steps compared to single-task approaches. Our evaluation shows, for instance, that tasks like sentiment and emotionality boost all learning, all tasks enhance fake news detection, and scaling tasks leads to the best results. MAGPIE confirms that MTL is a promising approach for addressing media bias detection, enhancing the accuracy and efficiency of existing models. Furthermore, LBM is the first available resource collection focused on media bias MTL.
>
---
#### [replaced 002] The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets
- **分类: cs.AI; cs.CL; cs.CY; cs.HC; cs.MA**

- **链接: [http://arxiv.org/pdf/2506.00073v3](http://arxiv.org/pdf/2506.00073v3)**

> **作者:** Shenzhe Zhu; Jiao Sun; Yi Nian; Tobin South; Alex Pentland; Jiaxin Pei
>
> **摘要:** AI agents are increasingly used in consumer-facing applications to assist with tasks such as product search, negotiation, and transaction execution. In this paper, we explore a future scenario where both consumers and merchants authorize AI agents to fully automate negotiations and transactions. We aim to answer two key questions: (1) Do different LLM agents vary in their ability to secure favorable deals for users? (2) What risks arise from fully automating deal-making with AI agents in consumer markets? To address these questions, we develop an experimental framework that evaluates the performance of various LLM agents in real-world negotiation and transaction settings. Our findings reveal that AI-mediated deal-making is an inherently imbalanced game -- different agents achieve significantly different outcomes for their users. Moreover, behavioral anomalies in LLMs can result in financial losses for both consumers and merchants, such as overspending or accepting unreasonable deals. These results underscore that while automation can improve efficiency, it also introduces substantial risks. Users should exercise caution when delegating business decisions to AI agents.
>
---
#### [replaced 003] "It's not a representation of me": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services
- **分类: cs.HC; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2504.09346v2](http://arxiv.org/pdf/2504.09346v2)**

> **作者:** Shira Michel; Sufi Kaur; Sarah Elizabeth Gillespie; Jeffrey Gleason; Christo Wilson; Avijit Ghosh
>
> **备注:** This paper has been accepted to FAccT 2025
>
> **摘要:** Recent advances in artificial intelligence (AI) speech generation and voice cloning technologies have produced naturalistic speech and accurate voice replication, yet their influence on sociotechnical systems across diverse accents and linguistic traits is not fully understood. This study evaluates two synthetic AI voice services (Speechify and ElevenLabs) through a mixed methods approach using surveys and interviews to assess technical performance and uncover how users' lived experiences influence their perceptions of accent variations in these speech technologies. Our findings reveal technical performance disparities across five regional, English-language accents and demonstrate how current speech generation technologies may inadvertently reinforce linguistic privilege and accent-based discrimination, potentially creating new forms of digital exclusion. Overall, our study highlights the need for inclusive design and regulation by providing actionable insights for developers, policymakers, and organizations to ensure equitable and socially responsible AI speech technologies.
>
---
#### [replaced 004] Roll in the Tanks! Measuring Left-wing Extremism on Reddit at Scale
- **分类: cs.SI; cs.CY**

- **链接: [http://arxiv.org/pdf/2307.06981v3](http://arxiv.org/pdf/2307.06981v3)**

> **作者:** Utkucan Balcı; Michael Sirivianos; Jeremy Blackburn
>
> **摘要:** Social media's role in the spread and evolution of extremism is a focus of intense study. Online extremists have been involved in the spread of online hate, mis- and disinformation, and real-world violence. However, most existing work has focuses on right-wing extremism. In this paper, we perform a first of its kind large-scale measurement study exploring left-wing extremism. We focus on "tankies," a left-wing community that first arose in the 1950s in support of hardline actions of the USSR and has evolved to support what they call "Actually Existing Socialist" countries, e.g., CCP-run China, the USSR, and North Korea. We collect and analyze 1.3M posts from 53K authors from tankie subreddits, and explore the position of tankies within the broader far-left community on Reddit. Among other things, we find that tankies are clearly on the periphery of the larger far-left community. When examining the contents of posts, we find misalignments and conceptual homomorphisms that confirm the description of tankies in the theoretical work. We also discover that tankies focus more on state-level political events rather than social issues. Our findings provide empirical evidence of the distinct positioning and discourse of left-wing extremist groups on social media.
>
---
#### [replaced 005] Americans' Support for AI Development -- Measured Daily with Open Data and Methods
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2412.05163v4](http://arxiv.org/pdf/2412.05163v4)**

> **作者:** Jason Jeffrey Jones
>
> **摘要:** The rapid development of artificial intelligence should be accompanied by measurement of public sentiment at high temporal resolution. Accordingly, here I present analysis of daily repeated surveys beginning April 18, 2024 (total N=4067). The results indicate that in the population of American adults, support for further development of artificial intelligence was modestly positive and increased a statistically reliable amount over the past year. Female and low-trust respondents reported less support, however, both also displayed growing support over time. Republicans increased support at a faster rate than Democrats, pointing to potential polarization. These findings underscore the need for continuous, high-frequency surveys to accurately track shifts in public opinion on transformative technologies like AI.
>
---
#### [replaced 006] From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2501.16548v2](http://arxiv.org/pdf/2501.16548v2)**

> **作者:** Alexandra Sasha Luccioni; Emma Strubell; Kate Crawford
>
> **摘要:** As the climate crisis deepens, artificial intelligence (AI) has emerged as a contested force: some champion its potential to advance renewable energy, materials discovery, and large-scale emissions monitoring, while others underscore its growing carbon footprint, water consumption, and material resource demands. Much of this debate has concentrated on direct impacts -- energy and water usage in data centers, e-waste from frequent hardware upgrades -- without addressing the significant indirect effects. This paper examines how the problem of Jevons' Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption. We argue that understanding these second-order impacts requires an interdisciplinary approach, combining lifecycle assessments with socio-economic analyses. Rebound effects undermine the assumption that improved technical efficiency alone will ensure net reductions in environmental harm. Instead, the trajectory of AI's impact also hinges on business incentives and market logics, governance and policymaking, and broader social and cultural norms. We contend that a narrow focus on direct emissions misrepresents AI's true climate footprint, limiting the scope for meaningful interventions. We conclude with recommendations that address rebound effects and challenge the market-driven imperatives fueling uncontrolled AI growth. By broadening the analysis to include both direct and indirect consequences, we aim to inform a more comprehensive, evidence-based dialogue on AI's role in the climate crisis.
>
---
