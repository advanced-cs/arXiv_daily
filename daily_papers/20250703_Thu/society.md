# 计算机与社会 cs.CY

- **最新发布 11 篇**

- **更新 5 篇**

## 最新发布

#### [new 001] A Practical SAFE-AI Framework for Small and Medium-Sized Enterprises Developing Medical Artificial Intelligence Ethics Policies
- **分类: cs.CY**

- **简介: 该论文属于AI伦理框架研究，旨在解决中小企业在开发医疗AI时面临的伦理合规难题。提出SAFE-AI框架，整合伦理审查与敏捷开发，确保AI系统公平、透明且可操作。**

- **链接: [http://arxiv.org/pdf/2507.01304v1](http://arxiv.org/pdf/2507.01304v1)**

> **作者:** Ion Nemteanu; Adir Mancebo Jr.; Leslie Joe; Ryan Lopez; Patricia Lopez; Warren Woodrich Pettine
>
> **备注:** 31 pages, two figures
>
> **摘要:** Artificial intelligence (AI) offers incredible possibilities for patient care, but raises significant ethical issues, such as the potential for bias. Powerful ethical frameworks exist to minimize these issues, but are often developed for academic or regulatory environments and tend to be comprehensive but overly prescriptive, making them difficult to operationalize within fast-paced, resource-constrained environments. We introduce the Scalable Agile Framework for Execution in AI (SAFE-AI) designed to balance ethical rigor with business priorities by embedding ethical oversight into standard Agile-based product development workflows. The framework emphasizes the early establishment of testable acceptance criteria, fairness metrics, and transparency metrics to manage model uncertainty, while also promoting continuous monitoring and re-evaluation of these metrics across the AI lifecycle. A core component of this framework are responsibility metrics using scenario-based probability analogy mapping designed to enhance transparency and stakeholder trust. This ensures that retraining or tuning activities are subject to lightweight but meaningful ethical review. By focusing on the minimum necessary requirements for responsible development, our framework offers a scalable, business-aligned approach to ethical AI suitable for organizations without dedicated ethics teams.
>
---
#### [new 002] Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review
- **分类: cs.CY; cs.AI; 62P25; K.3.1; H.5.2**

- **简介: 该论文属于教育技术研究，旨在探讨生成式AI对学生学习成果的影响。通过文献综述与蒙特卡洛模拟，分析学生对GenAI的感知及其与学习成效的关系。**

- **链接: [http://arxiv.org/pdf/2507.01062v1](http://arxiv.org/pdf/2507.01062v1)**

> **作者:** Seyma Yaman Kayadibi
>
> **备注:** 35 pages, 4 figures. All figures are image-based: one Python code screenshot, one regression model output, one success score distribution chart, and one PRISMA diagram. This article presents a standalone segment from the author's master's thesis at Victoria University
>
> **摘要:** The exponential development of generative artificial intelligence (GenAI) technologies like ChatGPT has raised increasing curiosity about their use in higher education, specifically with respect to how students view them, make use of them, and the implications for learning outcomes. This paper employs a hybrid methodological approach involving a systematic literature review and simulation-based modeling to explore student perceptions of GenAI use in the context of higher education. A total of nineteen empirical articles from 2023 through 2025 were selected from the PRISMA-based search targeting the Scopus database. Synthesis of emerging patterns from the literature was achieved by thematic categorization. Six of these had enough quantitative information, i.e., item-level means and standard deviations, to permit probabilistic modeling. One dataset, from the resulting subset, was itself selected as a representative case with which to illustrate inverse-variance weighting by Monte Carlo simulation, by virtue of its well-designed Likert scale format and thematic alignment with the use of computing systems by the researcher. The simulation provided a composite "Success Score" forecasting the strength of the relationship between student perceptions and learning achievements. Findings reveal that attitude factors concerned with usability and real-world usefulness are significantly better predictors of positive learning achievement than affective or trust-based factors. Such an interdisciplinary perspective provides a unique means of linking thematic results with predictive modelling, resonating with longstanding controversies about the proper use of GenAI tools within the university.
>
---
#### [new 003] AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions
- **分类: cs.CY; cs.AI; cs.LG**

- **简介: 该论文属于基础设施监测任务，旨在解决如何利用AI和遥感技术提升交通设施的韧性与可持续性。通过综述现有方法和数据，识别研究空白并提出未来方向。**

- **链接: [http://arxiv.org/pdf/2507.01547v1](http://arxiv.org/pdf/2507.01547v1)**

> **作者:** Ubada El Joulani; Tatiana Kalganova; Stergios-Aristoteles Mitoulis; Sotirios Argyroudis
>
> **摘要:** Critical infrastructure, such as transport networks, underpins economic growth by enabling mobility and trade. However, ageing assets, climate change impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging from natural disasters to cyber attacks and conflicts pose growing risks to their resilience and functionality. This review paper explores how emerging digital technologies, specifically Artificial Intelligence (AI), can enhance damage assessment and monitoring of transport infrastructure. A systematic literature review examines existing AI models and datasets for assessing damage in roads, bridges, and other critical infrastructure impacted by natural disasters. Special focus is given to the unique challenges and opportunities associated with bridge damage detection due to their structural complexity and critical role in connectivity. The integration of SAR (Synthetic Aperture Radar) data with AI models is also discussed, with the review revealing a critical research gap: a scarcity of studies applying AI models to SAR data for comprehensive bridge damage assessment. Therefore, this review aims to identify the research gaps and provide foundations for AI-driven solutions for assessing and monitoring critical transport infrastructures.
>
---
#### [new 004] From Reports to Reality: Testing Consistency in Instagram's Digital Services Act Compliance Data
- **分类: cs.CY**

- **简介: 该论文属于合规性研究任务，旨在评估Instagram在《数字服务法》（DSA）下的数据一致性。通过构建多层级框架，分析平台合规情况，提出提升报告与监管质量的建议。**

- **链接: [http://arxiv.org/pdf/2507.01787v1](http://arxiv.org/pdf/2507.01787v1)**

> **作者:** Marie-Therese Sekwenz; Ben Wagner; Hans De Bruijn
>
> **摘要:** The Digital Services Act (DSA) introduces harmonized rules for content moderation and platform governance in the European Union, mandating robust compliance mechanisms, particularly for very large online platforms and search engines. This study examined compliance with DSA requirements, focusing on Instagram as a case study. We develop and apply a multi-level consistency framework to evaluate DSA compliance. Our findings contribute to the broader discussion on empirically-based regulation, providing insight into how researchers, regulators, auditors and platforms can better utilize DSA mechanisms to improve reporting and enforcement quality and accountability. This work underscores that consistency can help detect potential compliance failures. It also demonstrates that platforms should be evaluated as part of an interconnected ecosystem rather than through isolated processes, which is crucial for effective compliance evaluation under the DSA.
>
---
#### [new 005] Can AI be Consentful?
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于伦理与法律研究任务，探讨AI系统中“同意”机制的不足，分析其在数据使用和输出控制方面的局限性，提出需要更新法律与伦理框架以应对AI带来的新挑战。**

- **链接: [http://arxiv.org/pdf/2507.01051v1](http://arxiv.org/pdf/2507.01051v1)**

> **作者:** Giada Pistilli; Bruna Trevelin
>
> **摘要:** The evolution of generative AI systems exposes the challenges of traditional legal and ethical frameworks built around consent. This chapter examines how the conventional notion of consent, while fundamental to data protection and privacy rights, proves insufficient in addressing the implications of AI-generated content derived from personal data. Through legal and ethical analysis, we show that while individuals can consent to the initial use of their data for AI training, they cannot meaningfully consent to the numerous potential outputs their data might enable or the extent to which the output is used or distributed. We identify three fundamental challenges: the scope problem, the temporality problem, and the autonomy trap, which collectively create what we term a ''consent gap'' in AI systems and their surrounding ecosystem. We argue that current legal frameworks inadequately address these emerging challenges, particularly regarding individual autonomy, identity rights, and social responsibility, especially in cases where AI-generated content creates new forms of personal representation beyond the scope of the original consent. By examining how these consent limitations intersect with broader principles of responsible AI (including fairness, transparency, accountability, and autonomy) we demonstrate the need to evolve ethical and legal approaches to consent.
>
---
#### [new 006] Epitome: Pioneering an Experimental Platform for AI-Social Science Integration
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 该论文属于AI与社会科学交叉研究任务，旨在解决人机交互和社会影响分析问题。论文提出Epitome平台，整合LLMs与社会科学研究，支持复杂实验设计与分析。**

- **链接: [http://arxiv.org/pdf/2507.01061v1](http://arxiv.org/pdf/2507.01061v1)**

> **作者:** Jingjing Qu; Kejia Hu; Jun Zhu; Wenhao Li; Teng Wang; Zhiyun Chen; Yulei Ye; Chaochao Lu; Aimin Zhou; Xiangfeng Wang; James Evan
>
> **备注:** 18 pages, 5figures
>
> **摘要:** The integration of Large Language Models (LLMs) into social science experiments represents a transformative approach to understanding human-AI interactions and their societal impacts. We introduce Epitome, the world's first open experimental platform dedicated to the deep integration of artificial intelligence and social science. Rooted in theoretical foundations from management, communication studies, sociology, psychology, and ethics, Epitome focuses on the interactive impacts of AI on individuals, organizations, and society during its real-world deployment. It constructs a theoretical support system through cross-disciplinary experiments. The platform offers a one-stop comprehensive experimental solution spanning "foundation models-complex application development-user feedback" through seven core modules, while embedding the classical "control-comparison-comparative causal logic" of social science experiments into multilevel human-computer interaction environments, including dialogues, group chats, and multi-agent virtual scenarios. With its canvas-style, user-friendly interface, Epitome enables researchers to easily design and run complex experimental scenarios, facilitating systematic investigations into the social impacts of AI and exploration of integrated solutions.To demonstrate its capabilities, we replicated three seminal social science experiments involving LLMs, showcasing Epitome's potential to streamline complex experimental designs and produce robust results, suitable for publishing in the top selective journals. Our findings highlight the platform's utility in enhancing the efficiency and quality of human-AI interactions, providing valuable insights into the societal implications of AI technologies. Epitome thus offers a powerful tool for advancing interdisciplinary research at the intersection of AI and social science, with potential applications in policy-making, ...
>
---
#### [new 007] Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing
- **分类: cs.CY; cs.AI; H.5.2; I.2**

- **简介: 该论文属于AI伦理研究，探讨AI披露对写作评价的影响及作者身份差异。通过实验分析人类与AI评分者的判断差异，揭示AI透明度带来的不公平现象。**

- **链接: [http://arxiv.org/pdf/2507.01418v1](http://arxiv.org/pdf/2507.01418v1)**

> **作者:** Inyoung Cheong; Alicia Guo; Mina Lee; Zhehui Liao; Kowe Kadoma; Dongyoung Go; Joseph Chee Chang; Peter Henderson; Mor Naaman; Amy X. Zhang
>
> **备注:** Presented at CHIWORK 2025 Workshop on Generative AI Disclosure, Ownership, and Accountability in Co-Creative Domains
>
> **摘要:** As AI integrates in various types of human writing, calls for transparency around AI assistance are growing. However, if transparency operates on uneven ground and certain identity groups bear a heavier cost for being honest, then the burden of openness becomes asymmetrical. This study investigates how AI disclosure statement affects perceptions of writing quality, and whether these effects vary by the author's race and gender. Through a large-scale controlled experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated a single human-written news article while disclosure statements and author demographics were systematically varied. This approach reflects how both human and algorithmic decisions now influence access to opportunities (e.g., hiring, promotion) and social recognition (e.g., content recommendation algorithms). We find that both human and LLM raters consistently penalize disclosed AI use. However, only LLM raters exhibit demographic interaction effects: they favor articles attributed to women or Black authors when no disclosure is present. But these advantages disappear when AI assistance is revealed. These findings illuminate the complex relationships between AI disclosure and author identity, highlighting disparities between machine and human evaluation patterns.
>
---
#### [new 008] From Literature to ReWA: Discussing Reproductive Well-being in HCI
- **分类: cs.HC; cs.CY**

- **简介: 该论文属于文献综述任务，旨在探讨生殖健康领域技术设计的不足，分析现有研究中的问题，并提出ReWA框架以提升包容性。**

- **链接: [http://arxiv.org/pdf/2507.01121v1](http://arxiv.org/pdf/2507.01121v1)**

> **作者:** Hafsah Mahzabin Chowdhury; Sharifa Sultana
>
> **备注:** 23 pages
>
> **摘要:** Reproductive well-being is shaped by intersecting cultural, religious, gendered, and political contexts, yet current technologies often reflect narrow, Western-centric assumptions. In this literature review, we synthesize findings from 147 peer-reviewed papers published between 2015 and 2025 across HCI, CSCW and social computing, ICTD, digital and public health, and AI for well-being scholarship to map the evolving reproductive well-being landscape. We identify three thematic waves that focused on early access and education, cultural sensitivity and privacy, and AI integration with policy-aware design, and highlight how technologies support or constrain diverse reproductive experiences. Our analysis reveals critical gaps in inclusivity, with persistent exclusions of men and non-binary users, migrants, and users in the Global South. Additionally, we surfaced the significant absence of literature on the role of stakeholders (e.g., husband and family members, household maids and cleaning helping hands, midwife, etc.) in the reproductive well-being space. Drawing on the findings from the literature, we propose the ReWA framework to support reproductive well-being for all agendas through six design orientations associated with: location, culture, and history; polyvocality and agency; rationality, temporality, distributive roles, and methodology.
>
---
#### [new 009] The Thin Line Between Comprehension and Persuasion in LLMs
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于自然语言处理任务，探讨LLMs在辩论中的表现与理解能力。研究旨在分析LLMs是否真正理解对话内容，发现其能有效说服但缺乏深层理解。**

- **链接: [http://arxiv.org/pdf/2507.01936v1](http://arxiv.org/pdf/2507.01936v1)**

> **作者:** Adrian de Wynter; Tangming Yuan
>
> **摘要:** Large language models (LLMs) are excellent at maintaining high-level, convincing dialogues. They are being fast deployed as chatbots and evaluators in sensitive areas, such as peer review and mental health applications. This, along with the disparate accounts on their reasoning capabilities, calls for a closer examination of LLMs and their comprehension of dialogue. In this work we begin by evaluating LLMs' ability to maintain a debate--one of the purest yet most complex forms of human communication. Then we measure how this capability relates to their understanding of what is being talked about, namely, their comprehension of dialogical structures and the pragmatic context. We find that LLMs are capable of maintaining coherent, persuasive debates, often swaying the beliefs of participants and audiences alike. We also note that awareness or suspicion of AI involvement encourage people to be more critical of the arguments made. When polling LLMs on their comprehension of deeper structures of dialogue, however, they cannot demonstrate said understanding. Our findings tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand the context. More broadly, for the field of argumentation theory we posit that, if an agent can convincingly maintain a dialogue, it is not necessary for it to know what it is talking about. Hence, the modelling of pragmatic context and coherence are secondary to effectiveness.
>
---
#### [new 010] MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于AI公平性研究任务，旨在解决多智能体系统中隐性偏见问题。提出MALIBU基准，评估LLM在多智能体交互中的偏见表现。**

- **链接: [http://arxiv.org/pdf/2507.01019v1](http://arxiv.org/pdf/2507.01019v1)**

> **作者:** Imran Mirza; Cole Huang; Ishwara Vasista; Rohan Patil; Asli Akalin; Sean O'Brien; Kevin Zhu
>
> **备注:** Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025
>
> **摘要:** Multi-agent systems, which consist of multiple AI models interacting within a shared environment, are increasingly used for persona-based interactions. However, if not carefully designed, these systems can reinforce implicit biases in large language models (LLMs), raising concerns about fairness and equitable representation. We present MALIBU, a novel benchmark developed to assess the degree to which LLM-based multi-agent systems implicitly reinforce social biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems through scenario-based assessments. AI models complete tasks within predefined contexts, and their responses undergo evaluation by an LLM-based multi-agent judging system in two phases. In the first phase, judges score responses labeled with specific demographic personas (e.g., gender, race, religion) across four metrics. In the second phase, judges compare paired responses assigned to different personas, scoring them and selecting the superior response. Our study quantifies biases in LLM-generated outputs, revealing that bias mitigation may favor marginalized personas over true neutrality, emphasizing the need for nuanced detection, balanced fairness strategies, and transparent evaluation benchmarks in multi-agent systems.
>
---
#### [new 011] Epistemic Scarcity: The Economics of Unresolvable Unknowns
- **分类: econ.GN; cs.AI; cs.CY; physics.hist-ph; q-fin.EC; 91B42, 91B40, 68T01; J.4; I.2.1; K.4.1; K.4.2**

- **简介: 该论文属于经济学与AI伦理研究，探讨AI在经济协调中的局限性，批判现有AI伦理框架，提出奥地利学派视角下的替代方案。**

- **链接: [http://arxiv.org/pdf/2507.01483v1](http://arxiv.org/pdf/2507.01483v1)**

> **作者:** Craig S Wright
>
> **备注:** 47 pages - submission to QJAE
>
> **摘要:** This paper presents a praxeological analysis of artificial intelligence and algorithmic governance, challenging assumptions about the capacity of machine systems to sustain economic and epistemic order. Drawing on Misesian a priori reasoning and Austrian theories of entrepreneurship, we argue that AI systems are incapable of performing the core functions of economic coordination: interpreting ends, discovering means, and communicating subjective value through prices. Where neoclassical and behavioural models treat decisions as optimisation under constraint, we frame them as purposive actions under uncertainty. We critique dominant ethical AI frameworks such as Fairness, Accountability, and Transparency (FAT) as extensions of constructivist rationalism, which conflict with a liberal order grounded in voluntary action and property rights. Attempts to encode moral reasoning in algorithms reflect a misunderstanding of ethics and economics. However complex, AI systems cannot originate norms, interpret institutions, or bear responsibility. They remain opaque, misaligned, and inert. Using the concept of epistemic scarcity, we explore how information abundance degrades truth discernment, enabling both entrepreneurial insight and soft totalitarianism. Our analysis ends with a civilisational claim: the debate over AI concerns the future of human autonomy, institutional evolution, and reasoned choice. The Austrian tradition, focused on action, subjectivity, and spontaneous order, offers the only coherent alternative to rising computational social control.
>
---
## 更新

#### [replaced 001] Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions
- **分类: cs.CR; cs.CY; cs.SE**

- **链接: [http://arxiv.org/pdf/2506.23866v2](http://arxiv.org/pdf/2506.23866v2)**

> **作者:** Jason Kayembe; Iness Ben Guirat; Jan Tobias Mühlberg
>
> **备注:** Post-proceedings paper presented at LOCO '24: 1st International Workshop on Low Carbon Computing, 2024-12-03, in Glasgow, UK/Online. This paper is based on https://doi.org/10.48550/arXiv.2411.16340 but presents novel insights and additional technical content; this paper has been reviewed again by the LOCO PC
>
> **摘要:** In this paper, we explore the intersection of privacy, security, and environmental sustainability in cloud-based office solutions, focusing on quantifying user- and network-side energy use and associated carbon emissions. We hypothesise that privacy-focused services are typically more energy-efficient than those funded through data collection and advertising. To evaluate this, we propose a framework that systematically measures environmental costs based on energy usage and network data traffic during well-defined, automated usage scenarios. To test our hypothesis, we first analyse how underlying architectures and business models, such as monetisation through personalised advertising, contribute to the environmental footprint of these services. We then explore existing methodologies and tools for software environmental impact assessment. We apply our framework to three mainstream email services selected to reflect different privacy policies, from ad-supported tracking-intensive models to privacy-focused designs: Microsoft Outlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a self-hosted email solution, evaluated with and without end-to-end encryption. We show that the self-hosted solution, even with 14% of device energy and 15% of emissions overheads from PGP encryption, remains the most energy-efficient, saving up to 33% of emissions per session compared to Gmail. Among commercial providers, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per session compared to Outlook, whose emissions can be further reduced by 2% through ad-blocking.
>
---
#### [replaced 002] Vehicle-group-based Crash Risk Prediction and Interpretation on Highways
- **分类: cs.LG; cs.CY**

- **链接: [http://arxiv.org/pdf/2402.12415v3](http://arxiv.org/pdf/2402.12415v3)**

> **作者:** Tianheng Zhu; Ling Wang; Yiheng Feng; Wanjing Ma; Mohamed Abdel-Aty
>
> **备注:** Accepted and published in IEEE Transactions on Intelligent Transportation Systems, vol. 26, no. 6, pp. 7807-7818, June 2025. DOI: 10.1109/TITS.2025.3556543
>
> **摘要:** Previous studies in predicting crash risks primarily associated the number or likelihood of crashes on a road segment with traffic parameters or geometric characteristics, usually neglecting the impact of vehicles' continuous movement and interactions with nearby vehicles. Recent technology advances, such as Connected and Automated Vehicles (CAVs) and Unmanned Aerial Vehicles (UAVs) are able to collect high-resolution trajectory data, which enables trajectory-based risk analysis. This study investigates a new vehicle group (VG) based risk analysis method and explores risk evolution mechanisms considering VG features. An impact-based vehicle grouping method is proposed to cluster vehicles into VGs by evaluating their responses to the erratic behaviors of nearby vehicles. The risk of a VG is aggregated based on the risk between each vehicle pair in the VG, measured by inverse Time-to-Collision (iTTC). A Logistic Regression and a Graph Neural Network (GNN) are then employed to predict VG risks using aggregated and disaggregated VG information. Both methods achieve excellent performance with AUC values exceeding 0.93. For the GNN model, GNNExplainer with feature perturbation is applied to identify critical individual vehicle features and their directional impact on VG risks. Overall, this research contributes a new perspective for identifying, predicting, and interpreting traffic risks.
>
---
#### [replaced 003] Regulating Algorithmic Management: A Multi-Stakeholder Study of Challenges in Aligning Software and the Law for Workplace Scheduling
- **分类: cs.CY; cs.HC; cs.SE**

- **链接: [http://arxiv.org/pdf/2505.02329v3](http://arxiv.org/pdf/2505.02329v3)**

> **作者:** Jonathan Lynn; Rachel Y. Kim; Sicun Gao; Daniel Schneider; Sachin S. Pandya; Min Kyung Lee
>
> **备注:** FAccT'25
>
> **摘要:** Algorithmic management (AM)'s impact on worker well-being has led to calls for regulation. However, little is known about the effectiveness and challenges in real-world AM regulation across the regulatory process -- rule operationalization, software use, and enforcement. Our multi-stakeholder study addresses this gap within workplace scheduling, one of the few AM domains with implemented regulations. We interviewed 38 stakeholders across the regulatory process: regulators, defense attorneys, worker advocates, managers, and workers. Our findings suggest that the efficacy of AM regulation is influenced by: (i) institutional constraints that challenge efforts to encode law into AM software, (ii) on-the-ground use of AM software that shapes its ability to facilitate compliance, (iii) mismatches between software and regulatory contexts that hinder enforcement, and (iv) unique concerns that software introduces when used to regulate AM. These findings underscore the importance of a sociotechnical approach to AM regulation, which considers organizational and collaborative contexts alongside the inherent attributes of software. We offer future research directions and implications for technology policy and design.
>
---
#### [replaced 004] Large Language Models, and LLM-Based Agents, Should Be Used to Enhance the Digital Public Sphere
- **分类: cs.CY; cs.IR; K.4**

- **链接: [http://arxiv.org/pdf/2410.12123v3](http://arxiv.org/pdf/2410.12123v3)**

> **作者:** Seth Lazar; Luke Thorburn; Tian Jin; Luca Belli
>
> **摘要:** This paper argues that large language model-based recommenders can displace today's attention-allocation machinery. LLM-based recommenders would ingest open-web content, infer a user's natural-language goals, and present information that matches their reflective preferences. Properly designed, they could deliver personalization without industrial-scale data hoarding, return control to individuals, optimize for genuine ends rather than click-through proxies, and support autonomous attention management. Synthesizing evidence of current systems' harms with recent work on LLM-driven pipelines, we identify four key research hurdles: generating candidates without centralized data, maintaining computational efficiency, modeling preferences robustly, and defending against prompt-injection. None looks prohibitive; surmounting them would steer the digital public sphere toward democratic, human-centered values.
>
---
#### [replaced 005] Adapting Probabilistic Risk Assessment for AI
- **分类: cs.AI; cs.CY; cs.LG; cs.SY; eess.SY; stat.AP**

- **链接: [http://arxiv.org/pdf/2504.18536v3](http://arxiv.org/pdf/2504.18536v3)**

> **作者:** Anna Katariina Wisakanto; Joe Rogero; Avyay M. Casheekar; Richard Mallah
>
> **备注:** Project website with workbook tool available at: https://pra-for-ai.github.io/pra/
>
> **摘要:** Modern general-purpose artificial intelligence (AI) systems present an urgent risk management challenge, as their rapidly evolving capabilities and potential for catastrophic harm outpace our ability to reliably assess their risks. Current methods often rely on selective testing and undocumented assumptions about risk priorities, frequently failing to make a serious attempt at assessing the set of pathways through which AI systems pose direct or indirect risks to society and the biosphere. This paper introduces the probabilistic risk assessment (PRA) for AI framework, adapting established PRA techniques from high-reliability industries (e.g., nuclear power, aerospace) for the new challenges of advanced AI. The framework guides assessors in identifying potential risks, estimating likelihood and severity bands, and explicitly documenting evidence, underlying assumptions, and analyses at appropriate granularities. The framework's implementation tool synthesizes the results into a risk report card with aggregated risk estimates from all assessed risks. It introduces three methodological advances: (1) Aspect-oriented hazard analysis provides systematic hazard coverage guided by a first-principles taxonomy of AI system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk pathway modeling analyzes causal chains from system aspects to societal impacts using bidirectional analysis and incorporating prospective techniques; and (3) Uncertainty management employs scenario decomposition, reference scales, and explicit tracing protocols to structure credible projections with novelty or limited data. Additionally, the framework harmonizes diverse assessment methods by integrating evidence into comparable, quantified absolute risk estimates for lifecycle decisions. We have implemented this as a workbook tool for AI developers, evaluators, and regulators.
>
---
