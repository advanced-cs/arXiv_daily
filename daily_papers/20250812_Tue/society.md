# 计算机与社会 cs.CY

- **最新发布 36 篇**

- **更新 21 篇**

## 最新发布

#### [new 001] Assessing Engineering Student Perceptions of Introductory CS Courses in an Indian Context
- **分类: cs.CY; cs.CE; cs.PL**

- **简介: 本研究评估印度工程学生对入门CS课程评估的感知，分析其有效性及影响因素，提出包容性评估策略。**

- **链接: [http://arxiv.org/pdf/2508.06563v1](http://arxiv.org/pdf/2508.06563v1)**

> **作者:** Utsav Kumar Nareti; Divyansh Gupta; Chandranath Adak; Soumi Chattopadhyay; Emma Riese; Tanujit Chakraborty; Mayank Agarwal; Satendra Kumar
>
> **摘要:** Understanding student perceptions of assessment is vital for designing inclusive and effective learning environments, especially in technical education. This study explores engineering students' perceptions of assessment practices in an introductory computer science/ programming course, and its associated laboratory within an Indian engineering institute context. A total of 318 first-year Bachelor of Technology students participated in a weekly 25-statement Likert-scale survey conducted over nine weeks. Using descriptive statistics and non-parametric tests (Mann-Whitney U and Kruskal-Wallis), the analysis reveals that students largely perceive lab assignments as effective learning activities and view exams and projects as authentic and skill-enhancing. Students appreciated the role of instructors in shaping course content and found teaching assistants to be approachable and helpful, despite some inconsistencies. The study also finds significant variations in students' academic performance and assessment perceptions based on prior programming experience, technology familiarity, gender, and academic branch. Notably, the performance data did not follow a Gaussian distribution, challenging common assumptions in grade modeling. A comparative analysis with European cohorts highlights both universal patterns and contextual differences, offering valuable insights for designing inclusive and equitable assessment strategies in programming education.
>
---
#### [new 002] Unequal Uncertainty: Rethinking Algorithmic Interventions for Mitigating Discrimination from AI
- **分类: cs.CY; cs.HC; cs.LG**

- **简介: 论文分析不确定性基算法干预的法律与伦理风险，通过案例研究揭示其潜在歧视问题，并提出选择性摩擦作为公平决策的路径。**

- **链接: [http://arxiv.org/pdf/2508.07872v1](http://arxiv.org/pdf/2508.07872v1)**

> **作者:** Holli Sargeant; Mackenzie Jorgensen; Arina Shah; Adrian Weller; Umang Bhatt
>
> **摘要:** Uncertainty in artificial intelligence (AI) predictions poses urgent legal and ethical challenges for AI-assisted decision-making. We examine two algorithmic interventions that act as guardrails for human-AI collaboration: selective abstention, which withholds high-uncertainty predictions from human decision-makers, and selective friction, which delivers those predictions together with salient warnings or disclosures that slow the decision process. Research has shown that selective abstention based on uncertainty can inadvertently exacerbate disparities and disadvantage under-represented groups that disproportionately receive uncertain predictions. In this paper, we provide the first integrated socio-technical and legal analysis of uncertainty-based algorithmic interventions. Through two case studies, AI-assisted consumer credit decisions and AI-assisted content moderation, we demonstrate how the seemingly neutral use of uncertainty thresholds can trigger discriminatory impacts. We argue that, although both interventions pose risks of unlawful discrimination under UK law, selective frictions offer a promising pathway toward fairer and more accountable AI-assisted decision-making by preserving transparency and encouraging more cautious human judgment.
>
---
#### [new 003] Teaching Introduction to Programming in the times of AI: A case study of a course re-design
- **分类: cs.CY; cs.AI**

- **简介: 论文探讨AI时代编程入门课程设计，解决课程目标、评估及工具误用问题，提出课程重构方法作为指导。**

- **链接: [http://arxiv.org/pdf/2508.06572v1](http://arxiv.org/pdf/2508.06572v1)**

> **作者:** Nikolaos Avouris; Kyriakos Sgarbas; George Caridakis; Christos Sintoris
>
> **备注:** To be cited as: Avouris, N., Sgarbas, K., Caridakis, G., Sintoris, C., (2025). Teaching Introduction to Programming in the times of AI: A case study of a course re-design, Proceedings 12th Penhellenic Conference of Computer Science Education, PCCSE 2025, Rhodes, October 2025
>
> **摘要:** The integration of AI tools into programming education has become increasingly prevalent in recent years, transforming the way programming is taught and learned. This paper provides a review of the state-of-the-art AI tools available for teaching and learning programming, particularly in the context of introductory courses. It highlights the challenges on course design, learning objectives, course delivery and formative and summative assessment, as well as the misuse of such tools by the students. We discuss ways of re-designing an existing course, re-shaping assignments and pedagogy to address the current AI technologies challenges. This example can serve as a guideline for policies for institutions and teachers involved in teaching programming, aiming to maximize the benefits of AI tools while addressing the associated challenges and concerns.
>
---
#### [new 004] Enhancing Systematic Interoperability: Convergences and Mismatches between Web 3.0 and the EU Data Act
- **分类: cs.CY; cs.ET**

- **简介: 论文对比Web3.0技术与欧盟数据法案的interoperability，分析其收敛与差异，提出扩展法律概念和软法机制以提升系统 interoperability 实践。**

- **链接: [http://arxiv.org/pdf/2508.07356v1](http://arxiv.org/pdf/2508.07356v1)**

> **作者:** Linyi Xu; Zihao Li
>
> **备注:** Accepted for International Conference on Future Communications and Networks (FCN) 2025
>
> **摘要:** Interoperability is increasingly recognised as a foundational principle for fostering innovation, competition, and user autonomy in the evolving digital ecosystem. Existing research on interoperability predominantly focuses either on technological interoperability itself or on the legal regulations concerning interoperability, with insufficient exploration of their interdisciplinary intersection. This paper compares the technological interoperability in Web 3.0 with the theoretical framework of legal interoperability established by the EU Data Act, analysing the areas of convergence and mismatch. The goal is to align technical interoperability with legal concepts of interoperability, thereby enhancing the practical implementation of systematic interoperability in the next generation of the Web. This study finds that, firstly, Web 3.0's concept of interoperability spans data, systems, and applications, while the Data Act focuses solely on data. This narrow scope risks creating a fragmented ecosystem, where data exchange is possible, but full integration of systems and applications is hindered, leading to inefficiencies, and obstructing seamless data flow across platforms. Secondly, while Web 3.0 technically seeks to achieve interoperability through the integration of entire systems and decentralised applications, the compliance with Data Act might negatively limit such system and application interoperability through its data interoperability provisions. This paper suggests interdisciplinary recommendations to enhance the implementation and enforcement of interoperability. On one hand, the Data Act should broaden its concept of interoperability to encompass both the systems and applications layers. On the other hand, it is advisable to introduce provisions for standardised protocols through soft law mechanisms to address legal shortcomings and keep pace with technological advancements.
>
---
#### [new 005] AI Gossip
- **分类: cs.CY**

- **简介: 论文探讨AI是否能进行"gossip"（闲聊），指出生成式AI生成虚假信息并传播，揭示其引发的技术社会危害。任务为分析AI gossip现象及影响，解决是否具备闲聊能力的问题，通过案例与危害评估提供理论框架。**

- **链接: [http://arxiv.org/pdf/2508.08143v1](http://arxiv.org/pdf/2508.08143v1)**

> **作者:** Joel Krueger; Lucy Osler
>
> **摘要:** Generative AI chatbots like OpenAI's ChatGPT and Google's Gemini routinely make things up. They "hallucinate" historical events and figures, legal cases, academic papers, non-existent tech products and features, biographies, and news articles. Recently, some have argued that these hallucinations are better understood as bullshit. Chatbots produce rich streams of text that look truth-apt without any concern for the truthfulness of what this text says. But can they also gossip? We argue that they can. After some definitions and scene-setting, we focus on a recent example to clarify what AI gossip looks like before considering some distinct harms -- what we call "technosocial harms" -- that follow from it.
>
---
#### [new 006] Making Effective Decisions: Machine Learning and the Ecogame in 1970
- **分类: cs.CY; cs.AI**

- **简介: 论文研究1970年Ecogame艺术项目，通过机器学习与网络模拟探索行为对系统的影响，提出早期AI艺术应用的先例，强调人机交互与民主化潜力。**

- **链接: [http://arxiv.org/pdf/2508.07027v1](http://arxiv.org/pdf/2508.07027v1)**

> **作者:** Catherine Mason
>
> **备注:** In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts 2025) arXiv:2406.14485
>
> **摘要:** This paper considers Ecogame, an innovative art project of 1970, whose creators believed in a positive vision of a technological future; an understanding, posited on cybernetics, of a future that could be participatory via digital means, and therefore more democratised. Using simulation and early machine learning techniques over a live network, Ecogame combined the power of visual art with cybernetic concepts of adaptation, feedback, and control to propose that behaviour had implications for the total system. It provides an historical precedent for contemporary AI-driven art about using AI in a more human-centred way.
>
---
#### [new 007] Street-Level AI: Are Large Language Models Ready for Real-World Judgments?
- **分类: cs.CY; cs.AI**

- **简介: 论文评估LLM在社会资源分配中的实际判断能力，揭示其与人类及脆弱性评分系统的不一致性，质疑当前AI系统的高风险应用 readiness。**

- **链接: [http://arxiv.org/pdf/2508.08193v1](http://arxiv.org/pdf/2508.08193v1)**

> **作者:** Gaurab Pokharel; Shafkat Farabi; Patrick J. Fowler; Sanmay Das
>
> **备注:** This work has been accepted for publication as a full paper at the AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)
>
> **摘要:** A surge of recent work explores the ethical and societal implications of large-scale AI models that make "moral" judgments. Much of this literature focuses either on alignment with human judgments through various thought experiments or on the group fairness implications of AI judgments. However, the most immediate and likely use of AI is to help or fully replace the so-called street-level bureaucrats, the individuals deciding to allocate scarce social resources or approve benefits. There is a rich history underlying how principles of local justice determine how society decides on prioritization mechanisms in such domains. In this paper, we examine how well LLM judgments align with human judgments, as well as with socially and politically determined vulnerability scoring systems currently used in the domain of homelessness resource allocation. Crucially, we use real data on those needing services (maintaining strict confidentiality by only using local large models) to perform our analyses. We find that LLM prioritizations are extremely inconsistent in several ways: internally on different runs, between different LLMs, and between LLMs and the vulnerability scoring systems. At the same time, LLMs demonstrate qualitative consistency with lay human judgments in pairwise testing. Findings call into question the readiness of current generation AI systems for naive integration in high-stakes societal decision-making.
>
---
#### [new 008] Understanding Privacy Norms Around LLM-Based Chatbots: A Contextual Integrity Perspective
- **分类: cs.CY**

- **简介: 论文研究LLM聊天机器人用户隐私规范，揭示其期望与行为的矛盾，通过实验发现传输因素（如知情同意）影响数据共享意愿，其他因素无显著作用。**

- **链接: [http://arxiv.org/pdf/2508.06760v1](http://arxiv.org/pdf/2508.06760v1)**

> **作者:** Sarah Tran; Hongfan Lu; Isaac Slaughter; Bernease Herman; Aayushi Dangol; Yue Fu; Lufei Chen; Biniyam Gebreyohannes; Bill Howe; Alexis Hiniker; Nicholas Weber; Robert Wolfe
>
> **摘要:** LLM-driven chatbots like ChatGPT have created large volumes of conversational data, but little is known about how user privacy expectations are evolving with this technology. We conduct a survey experiment with 300 US ChatGPT users to understand emerging privacy norms for sharing chatbot data. Our findings reveal a stark disconnect between user concerns and behavior: 82% of respondents rated chatbot conversations as sensitive or highly sensitive - more than email or social media posts - but nearly half reported discussing health topics and over one-third discussed personal finances with ChatGPT. Participants expressed strong privacy concerns (t(299) = 8.5, p < .01) and doubted their conversations would remain private (t(299) = -6.9, p < .01). Despite this, respondents uniformly rejected sharing personal data (search history, emails, device access) for improved services, even in exchange for premium features worth $200. To identify which factors influence appropriate chatbot data sharing, we presented participants with factorial vignettes manipulating seven contextual factors. Linear mixed models revealed that only the transmission factors such as informed consent, data anonymization, or the removal of personally identifiable information, significantly affected perceptions of appropriateness and concern for data access. Surprisingly, contextual factors including the recipient of the data (hospital vs. tech company), purpose (research vs. advertising), type of content, and geographic location did not show significant effects. Our results suggest that users apply consistent baseline privacy expectations to chatbot data, prioritizing procedural safeguards over recipient trustworthiness. This has important implications for emerging agentic AI systems that assume user willingness to integrate personal data across platforms.
>
---
#### [new 009] Intersectoral Knowledge in AI and Urban Studies: A Framework for Transdisciplinary Research
- **分类: cs.CY; cs.AI**

- **简介: 本文提出六维框架评估AI与城市研究跨学科知识，解决整合不同学科视角难题，分析2014-2024文献，发现主流倾向批判现实主义等，探讨其他立场潜力，并建议早期研究者利用框架协调分歧。**

- **链接: [http://arxiv.org/pdf/2508.07507v1](http://arxiv.org/pdf/2508.07507v1)**

> **作者:** Rashid Mushkani
>
> **摘要:** Transdisciplinary approaches are increasingly essential for addressing grand societal challenges, particularly in complex domains such as Artificial Intelligence (AI), urban planning, and social sciences. However, effectively validating and integrating knowledge across distinct epistemic and ontological perspectives poses significant difficulties. This article proposes a six-dimensional framework for assessing and strengthening transdisciplinary knowledge validity in AI and city studies, based on an extensive analysis of the most cited research (2014--2024). Specifically, the framework classifies research orientations according to ontological, epistemological, methodological, teleological, axiological, and valorization dimensions. Our findings show a predominance of perspectives aligned with critical realism (ontological), positivism (epistemological), analytical methods (methodological), consequentialism (teleological), epistemic values (axiological), and social/economic valorization. Less common stances, such as idealism, mixed methods, and cultural valorization, are also examined for their potential to enrich knowledge production. We highlight how early career researchers and transdisciplinary teams can leverage this framework to reconcile divergent disciplinary viewpoints and promote socially accountable outcomes.
>
---
#### [new 010] An Empirical Inquiry into Surveillance Capitalism: Web Tracking
- **分类: cs.CY**

- **简介: 论文通过实证分析GAFAM网络追踪数据，揭示Surveillance Capitalism的机制与演变，探讨其社会环境成本及替代技术路径。**

- **链接: [http://arxiv.org/pdf/2508.07454v1](http://arxiv.org/pdf/2508.07454v1)**

> **作者:** Nils Bonfils
>
> **备注:** Post-proceedings paper presented at LIMITS 2025: 11th Workshop on Computing within Limits, 2025-06-26/27, Online
>
> **摘要:** The modern web is increasingly characterized by the pervasiveness of Surveillance Capitalism. This investigation employs an empirical approach to examine this phenomenon through the web tracking practices of major tech companies -- specifically Google, Apple, Facebook, Amazon, and Microsoft (GAFAM) -- and their relation to financial performance indicators. Using longitudinal data from WhoTracks.Me spanning from 2017 to 2025 and publicly accessible SEC filings, this paper analyzes patterns and trends in web tracking data to establish empirical evidence of Surveillance Capitalism's extraction mechanisms. Our findings reveal Google's omnipresent position on the web, a three-tier stratification among GAFAM companies in the surveillance space, and evidence suggesting an evolution of tracking techniques to evade detection. The investigation further discusses the social and environmental costs of web tracking and how alternative technologies, such as the Gemini protocol, offer pathways to challenge the extractive logic of this new economic order. By closely examining surveillance activities, this research contributes to an ongoing effort to better understand the current state and future trajectory of Surveillance Capitalism.
>
---
#### [new 011] A Moral Agency Framework for Legitimate Integration of AI in Bureaucracies
- **分类: cs.CY**

- **简介: 论文提出道德代理框架，解决AI误用引发伦理问题，通过结构设计实现透明与问责，提升效率与合法性。**

- **链接: [http://arxiv.org/pdf/2508.08231v1](http://arxiv.org/pdf/2508.08231v1)**

> **作者:** Chris Schmitz; Joanna Bryson
>
> **备注:** Accepted (non-archival) to AIES2025
>
> **摘要:** Public-sector bureaucracies seek to reap the benefits of artificial intelligence (AI), but face important concerns about accountability and transparency when using AI systems. These concerns center on threats to the twin aims of bureaucracy: legitimate and faithful implementation of legislation, and the provision of stable, long-term governance. Both aims are threatened when AI systems are misattributed as either mere tools or moral subjects - a framing error that creates ethics sinks, constructs that facilitate dissipation of responsibility by obscuring clear lines of human moral agency. Here, we reject the notion that such outcomes are inevitable. Rather, where they appear, they are the product of structural design decisions across both the technology and the institution deploying it. We support this claim via a systematic application of conceptions of moral agency in AI ethics to Weberian bureaucracy. We establish that it is both desirable and feasible to render AI systems as tools for the generation of organizational transparency and legibility, which continue the processes of Weberian rationalization initiated by previous waves of digitalization. We present a three-point Moral Agency Framework for legitimate integration of AI in bureaucratic structures: (a) maintain clear and just human lines of accountability, (b) ensure humans whose work is augmented by AI systems can verify the systems are functioning correctly, and (c) introduce AI only where it doesn't inhibit the capacity of bureaucracies towards either of their twin aims of legitimacy and stewardship. We suggest that AI introduced within this framework can not only improve efficiency and productivity while avoiding ethics sinks, but also improve the transparency and even the legitimacy of a bureaucracy.
>
---
#### [new 012] Towards Integrated Alignment
- **分类: cs.CY; cs.AI**

- **简介: 论文提出整合AI对齐方法，解决现有碎片化导致的模型脆弱性，通过跨领域设计原则实现策略性多样性，推动研究统一与协作。**

- **链接: [http://arxiv.org/pdf/2508.06592v1](http://arxiv.org/pdf/2508.06592v1)**

> **作者:** Ben Y. Reis; William La Cava
>
> **摘要:** As AI adoption expands across human society, the problem of aligning AI models to match human preferences remains a grand challenge. Currently, the AI alignment field is deeply divided between behavioral and representational approaches, resulting in narrowly aligned models that are more vulnerable to increasingly deceptive misalignment threats. In the face of this fragmentation, we propose an integrated vision for the future of the field. Drawing on related lessons from immunology and cybersecurity, we lay out a set of design principles for the development of Integrated Alignment frameworks that combine the complementary strengths of diverse alignment approaches through deep integration and adaptive coevolution. We highlight the importance of strategic diversity - deploying orthogonal alignment and misalignment detection approaches to avoid homogeneous pipelines that may be "doomed to success". We also recommend steps for greater unification of the AI alignment research field itself, through cross-collaboration, open model weights and shared community resources.
>
---
#### [new 013] Shaping a Profession, Building a Community: A Practitioner-Led Investigation of Public Interest Technologists in Civil Society
- **分类: cs.CY; cs.HC**

- **简介: 论文通过混合方法研究北美和欧洲PIT从业者，探讨其专业发展与社区建设需求，指出PIT需更完善的基础设施和社区参与。**

- **链接: [http://arxiv.org/pdf/2508.07230v1](http://arxiv.org/pdf/2508.07230v1)**

> **作者:** Mallory Knodel; Mallika Balakrishnan; Lauren M. Chambers
>
> **摘要:** The label `public interest technology' (PIT) is growing in popularity among those seeking to use `tech for good' - especially among technical practitioners working in civil society and nonprofit organizations. PIT encompasses a broad range of sociotechnical work across professional domains and sectors; however, the trend remains understudied within sociotechnical research. This paper describes a mixed-methods study, designed and conducted by PIT practitioners at the Center for Democracy and Technology, that characterizes technologists within the specific context of civil society, civil rights, and advocacy organizations in North America and Western Europe. We conducted interviews with civil society leaders to investigate how PIT practitioners position the field and themselves, and we held a roundtable discussion bringing diverse voices together to make meaning of this growing phenomenon. Ultimately, we find that PIT remains both defined and plagued by its expansiveness, and that today's civil society public interest technologists see a need for both (a) more robust professionalization infrastructures, including philanthropic attention, and (b) more engaged, coherent community. This study illuminates a nascent intersection of technology and policy on-the-ground that is of growing relevance to critical sociotechnical research on the shifting relationship between computing and society.
>
---
#### [new 014] Towards Experience-Centered AI: A Framework for Integrating Lived Experience in Design and Development
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 论文提出一种整合生活经验的AI设计框架，解决现有技术忽视人类情感与情境认知的问题，通过跨学科融合构建体验分类体系，并在教育、医疗等场景验证其有效性，强调技术与人文的结合。**

- **链接: [http://arxiv.org/pdf/2508.06849v1](http://arxiv.org/pdf/2508.06849v1)**

> **作者:** Sanjana Gautam; Mohit Chandra; Ankolika De; Tatiana Chakravorti; Girik Malik; Munmun De Choudhury
>
> **摘要:** Lived experiences fundamentally shape how individuals interact with AI systems, influencing perceptions of safety, trust, and usability. While prior research has focused on developing techniques to emulate human preferences, and proposed taxonomies to categorize risks (such as psychological harms and algorithmic biases), these efforts have provided limited systematic understanding of lived human experiences or actionable strategies for embedding them meaningfully into the AI development lifecycle. This work proposes a framework for meaningfully integrating lived experience into the design and evaluation of AI systems. We synthesize interdisciplinary literature across lived experience philosophy, human-centered design, and human-AI interaction, arguing that centering lived experience can lead to models that more accurately reflect the retrospective, emotional, and contextual dimensions of human cognition. Drawing from a wide body of work across psychology, education, healthcare, and social policy, we present a targeted taxonomy of lived experiences with specific applicability to AI systems. To ground our framework, we examine three application domains (i) education, (ii) healthcare, and (iii) cultural alignment, illustrating how lived experience informs user goals, system expectations, and ethical considerations in each context. We further incorporate insights from AI system operators and human-AI partnerships to highlight challenges in responsibility allocation, mental model calibration, and long-term system adaptation. We conclude with actionable recommendations for developing experience-centered AI systems that are not only technically robust but also empathetic, context-aware, and aligned with human realities. This work offers a foundation for future research that bridges technical development with the lived experiences of those impacted by AI systems.
>
---
#### [new 015] Leveraging LLMs for Privacy-Aware Predictions in Participatory Budgeting
- **分类: cs.CY; cs.AI**

- **简介: 论文提出利用隐私保护的LLM预测参与式预算提案结果，解决参与率低问题，通过文本与历史投票数据提升透明度和效率。**

- **链接: [http://arxiv.org/pdf/2508.06577v1](http://arxiv.org/pdf/2508.06577v1)**

> **作者:** Juan Zambrano; Clément Contet; Jairo Gudiño; Felipe Garrido-Lucero; Umberto Grandi; Cesar A Hidalgo
>
> **摘要:** Participatory Budgeting (PB) empowers citizens to propose and vote on public investment projects. Yet, despite its democratic potential, PB initiatives often suffer from low participation rates, limiting their visibility and perceived legitimacy. In this work, we aim to strengthen PB elections in two key ways: by supporting project proposers in crafting better proposals, and by helping PB organizers manage large volumes of submissions in a transparent manner. We propose a privacy-preserving approach to predict which PB proposals are likely to be funded, using only their textual descriptions and anonymous historical voting records -- without relying on voter demographics or personally identifiable information. We evaluate the performance of GPT 4 Turbo in forecasting proposal outcomes across varying contextual scenarios, observing that the LLM's prior knowledge needs to be complemented by past voting data to obtain predictions reflecting real-world PB voting behavior. Our findings highlight the potential of AI-driven tools to support PB processes by improving transparency, planning efficiency, and civic engagement.
>
---
#### [new 016] Advancing Knowledge Tracing by Exploring Follow-up Performance Trends
- **分类: cs.CY; cs.AI; cs.LG**

- **简介: 该论文提出一种基于Follow-up Performance Trends（FPTs）的知识追踪（KT）方法FINER，通过整合历史学习序列与FPTs提升学生性能预测准确性，解决传统KT方法存在相关冲突的问题。**

- **链接: [http://arxiv.org/pdf/2508.08019v1](http://arxiv.org/pdf/2508.08019v1)**

> **作者:** Hengyu Liu; Yushuai Li; Minghe Yu; Tiancheng Zhang; Ge Yu; Torben Bach Pedersen; Kristian Torp; Christian S. Jensen; Tianyi Li
>
> **备注:** 14 pages, 5 figures
>
> **摘要:** Intelligent Tutoring Systems (ITS), such as Massive Open Online Courses, offer new opportunities for human learning. At the core of such systems, knowledge tracing (KT) predicts students' future performance by analyzing their historical learning activities, enabling an accurate evaluation of students' knowledge states over time. We show that existing KT methods often encounter correlation conflicts when analyzing the relationships between historical learning sequences and future performance. To address such conflicts, we propose to extract so-called Follow-up Performance Trends (FPTs) from historical ITS data and to incorporate them into KT. We propose a method called Forward-Looking Knowledge Tracing (FINER) that combines historical learning sequences with FPTs to enhance student performance prediction accuracy. FINER constructs learning patterns that facilitate the retrieval of FPTs from historical ITS data in linear time; FINER includes a novel similarity-aware attention mechanism that aggregates FPTs based on both frequency and contextual similarity; and FINER offers means of combining FPTs and historical learning sequences to enable more accurate prediction of student future performance. Experiments on six real-world datasets show that FINER can outperform ten state-of-the-art KT methods, increasing accuracy by 8.74% to 84.85%.
>
---
#### [new 017] "Draw me a curator" Examining the visual stereotyping of a cultural services profession by generative AI
- **分类: cs.CY; cs.AI**

- **简介: 论文研究生成式AI（如ChatGPT4o）对博物馆馆长的视觉刻板印象，揭示其性别、种族和年龄偏见，指出数据集偏差可能导致对文化专业人员的不准确描绘。**

- **链接: [http://arxiv.org/pdf/2508.07132v1](http://arxiv.org/pdf/2508.07132v1)**

> **作者:** Dirk HR Spennemann
>
> **摘要:** Based on 230 visualisations, this paper examines the depiction of museum curators by the popular generative Artificial Intelligence (AI) model, ChatGPT4o. While the AI-generated representations do not reiterate popular stereotypes of curators as nerdy, conservative in dress and stuck in time rummaging through collections, they contrast sharply with real-world demographics. AI-generated imagery extremely underrepresents women (3.5% vs 49% to 72% in reality) and disregards ethnic communities other than Caucasian (0% vs 18% to 36%). It only over-represents young curators (79% vs approx. 27%) but also renders curators to resemble yuppie professionals or people featuring in fashion advertising. Stereotypical attributes are prevalent, with curators widely depicted as wearing beards and holding clipboards or digital tablets. The findings highlight biases in the generative AI image creation dataset, which is poised to shape an inaccurate portrayal of museum professionals if the images were to be taken uncritically at face value.
>
---
#### [new 018] $100,000 or the Robot Gets it! Tech Workers' Resistance Guide: Tech Worker Actions, History, Risks, Impacts, and the Case for a Radical Flank
- **分类: cs.CY**

- **简介: 论文分析科技工人行动策略，解决策略不足问题，提出建立激进侧翼并发布《科技工人抵制指南》。**

- **链接: [http://arxiv.org/pdf/2508.08084v1](http://arxiv.org/pdf/2508.08084v1)**

> **作者:** Mohamed Abdalla
>
> **备注:** Accepted to AAAI/ACM AIES 2025
>
> **摘要:** Over the past decade, Big Tech has faced increasing levels of worker activism. While worker actions have resulted in positive outcomes (e.g., cancellation of Google's Project Dragonfly), such successes have become increasingly infrequent. This is, in part, because corporations have adjusted their strategies to dealing with increased worker activism (e.g., increased retaliation against workers, and contracts clauses that prevent cancellation due to worker pressure). This change in company strategy prompts urgent questions about updating worker strategies for influencing corporate behavior in an industry with vast societal impact. Current discourse on tech worker activism often lacks empirical grounding regarding its scope, history, and strategic calculus. Our work seeks to bridge this gap by firstly conducting a systematic analysis of worker actions at Google and Microsoft reported in U.S. newspapers to delineate their characteristics. We then situate these actions within the long history of labour movements and demonstrate that, despite perceptions of radicalism, contemporary tech activism is comparatively moderate. Finally, we engage directly with current and former tech activists to provide a novel catalogue of potential worker actions, evaluating their perceived risks, impacts, and effectiveness (concurrently publishing "Tech Workers' Guide to Resistance"). Our findings highlight considerable variation in strategic thinking among activists themselves. We conclude by arguing that the establishment of a radical flank could increase the effectiveness of current movements. "Tech Workers' Guide to Resistance" can be found at https://www.cs.toronto.edu/~msa/TechWorkersResistanceGuide.pdf or https://doi.org/10.5281/zenodo.16779082
>
---
#### [new 019] Do Streetscapes Still Matter for Customer Ratings of Eating and Drinking Establishments in Car-Dependent Cities?
- **分类: physics.soc-ph; cs.CY; cs.LG**

- **简介: 该论文研究街道景观对依赖汽车城市顾客评分的影响，通过图像分析量化安全与美观，运用回归分析发现室内/室外环境显著，街道影响减弱，建议整合因素。**

- **链接: [http://arxiv.org/pdf/2508.06513v1](http://arxiv.org/pdf/2508.06513v1)**

> **作者:** Chaeyeon Han; Seung Jae Lieu; Uijeong Hwang; Subhrajit Guhathakurta
>
> **备注:** Soon to be published in Journal of Urban Design
>
> **摘要:** This study examines how indoor and outdoor aesthetics, streetscapes, and neighborhood features shape customer satisfaction at eating and dining establishments (EDEs) across different urban contexts, varying in car dependency, in Washington, DC. Using review photos and street view images, computer vision models quantified perceived safety and visual appeal. Ordinal logistic regression analyzed their effects on Yelp ratings. Findings reveal that both indoor and outdoor environments significantly impact EDE ratings, while streetscape quality's influence diminishes in car-dependent areas. The study highlights the need for context-sensitive planning that integrates indoor and outdoor factors to enhance customer experiences in diverse settings.
>
---
#### [new 020] Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning
- **分类: cs.LG; cs.AI; cs.CY; stat.ML**

- **简介: 该论文提出基于训练轨迹的轻量级方法，通过集成中间检查点提升模型选择性预测能力，兼容差分隐私研究隐私-不确定性权衡，并分解分类差距识别误差来源，设计防御策略以增强可信度。**

- **链接: [http://arxiv.org/pdf/2508.07556v1](http://arxiv.org/pdf/2508.07556v1)**

> **作者:** Stephan Rabanser
>
> **备注:** PhD Thesis
>
> **摘要:** Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, focusing on selective prediction -- where models abstain when confidence is low. We first show that a model's training trajectory contains rich uncertainty signals that can be exploited without altering its architecture or loss. By ensembling predictions from intermediate checkpoints, we propose a lightweight, post-hoc abstention method that works across tasks, avoids the cost of deep ensembles, and achieves state-of-the-art selective prediction performance. Crucially, this approach is fully compatible with differential privacy (DP), allowing us to study how privacy noise affects uncertainty quality. We find that while many methods degrade under DP, our trajectory-based approach remains robust, and we introduce a framework for isolating the privacy-uncertainty trade-off. Next, we then develop a finite-sample decomposition of the selective classification gap -- the deviation from the oracle accuracy-coverage curve -- identifying five interpretable error sources and clarifying which interventions can close the gap. This explains why calibration alone cannot fix ranking errors, motivating methods that improve uncertainty ordering. Finally, we show that uncertainty signals can be adversarially manipulated to hide errors or deny service while maintaining high accuracy, and we design defenses combining calibration audits with verifiable inference. Together, these contributions advance reliable ML by improving, evaluating, and safeguarding uncertainty estimation, enabling models that not only make accurate predictions -- but also know when to say "I do not know".
>
---
#### [new 021] Rethinking Privacy Indicators in Extended Reality: Multimodal Design for Situationally Impaired Bystanders
- **分类: cs.HC; cs.CY; cs.ET**

- **简介: 论文探讨了针对情况性受损旁观者设计XR隐私指示器，解决现有单视图指示器在注意力不足时效果差的问题，通过焦点小组和用户研究验证多模态设计的有效性。**

- **链接: [http://arxiv.org/pdf/2508.07057v1](http://arxiv.org/pdf/2508.07057v1)**

> **作者:** Syed Ibrahim Mustafa Shah Bukhari; Maha Sajid; Bo Ji; Brendan David-John
>
> **摘要:** As Extended Reality (XR) devices become increasingly prevalent in everyday settings, they raise significant privacy concerns for bystanders: individuals in the vicinity of an XR device during its use, whom the device sensors may accidentally capture. Current privacy indicators, such as small LEDs, often presume that bystanders are attentive enough to interpret the privacy signals. However, these cues can be easily overlooked when bystanders are distracted or have limited vision. We define such individuals as situationally impaired bystanders. This study explores XR privacy indicator designs that are effective for situationally impaired bystanders. A focus group with eight participants was conducted to design five novel privacy indicators. We evaluated these designs through a user study with seven additional participants. Our results show that visual-only indicators, typical in commercial XR devices, received low ratings for perceived usefulness in impairment scenarios. In contrast, multimodal indicators were preferred in privacy-sensitive scenarios with situationally impaired bystanders. Ultimately, our results highlight the need to move toward adaptable, multimodal, and situationally aware designs that effectively support bystander privacy in everyday XR environments.
>
---
#### [new 022] EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration
- **分类: cs.AI; cs.CY; cs.HC; cs.MA; stat.AP; 68T07, 68T42, 68T50, 91F20, 62P25; I.2.11; I.2.1; H.1.2; J.4; K.4.2**

- **简介: 论文提出EMPATHIA多智能体框架，解决难民整合中AI忽视文化、情感与伦理问题，通过三模块（SEED/RISE/THRIVE）及情感/文化/伦理代理实现透明决策，提升整合效果。**

- **链接: [http://arxiv.org/pdf/2508.07671v1](http://arxiv.org/pdf/2508.07671v1)**

> **作者:** Mohamed Rayan Barhdadi; Mehmet Tuncel; Erchin Serpedin; Hasan Kurban
>
> **备注:** 19 pages, 3 figures (plus 6 figures in supplementary), 2 tables, 1 algorithm. Submitted to NeurIPS 2025 Creative AI Track: Humanity
>
> **摘要:** Current AI approaches to refugee integration optimize narrow objectives such as employment and fail to capture the cultural, emotional, and ethical dimensions critical for long-term success. We introduce EMPATHIA (Enriched Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance), a multi-agent framework addressing the central Creative AI question: how do we preserve human dignity when machines participate in life-altering decisions? Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes integration into three modules: SEED (Socio-cultural Entry and Embedding Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency Engine) for early independence, and THRIVE (Transcultural Harmony and Resilience through Integrated Values and Engagement) for sustained outcomes. SEED employs a selector-validator architecture with three specialized agents - emotional, cultural, and ethical - that deliberate transparently to produce interpretable recommendations. Experiments on the UN Kakuma dataset (15,026 individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic variables achieved 87.4% validation convergence and explainable assessments across five host countries. EMPATHIA's weighted integration of cultural, emotional, and ethical factors balances competing value systems while supporting practitioner-AI collaboration. By augmenting rather than replacing human expertise, EMPATHIA provides a generalizable framework for AI-driven allocation tasks where multiple values must be reconciled.
>
---
#### [new 023] StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback
- **分类: cs.CV; cs.CY; cs.MA**

- **简介: 论文提出StyleTailor框架，通过多级负反馈实现个性化时尚穿搭，解决传统方法在设计、推荐、试穿及评估上的不足，采用协作代理实现迭代优化，提升推荐准确性和用户体验。**

- **链接: [http://arxiv.org/pdf/2508.06555v1](http://arxiv.org/pdf/2508.06555v1)**

> **作者:** Hongbo Ma; Fei Shen; Hongbin Xu; Xiaoce Wang; Gang Xu; Jinkai Zheng; Liangqiong Qu; Ming Li
>
> **备注:** 24pages, 5 figures
>
> **摘要:** The advancement of intelligent agents has revolutionized problem-solving across diverse domains, yet solutions for personalized fashion styling remain underexplored, which holds immense promise for promoting shopping experiences. In this work, we present StyleTailor, the first collaborative agent framework that seamlessly unifies personalized apparel design, shopping recommendation, virtual try-on, and systematic evaluation into a cohesive workflow. To this end, StyleTailor pioneers an iterative visual refinement paradigm driven by multi-level negative feedback, enabling adaptive and precise user alignment. Specifically, our framework features two core agents, i.e., Designer for personalized garment selection and Consultant for virtual try-on, whose outputs are progressively refined via hierarchical vision-language model feedback spanning individual items, complete outfits, and try-on efficacy. Counterexamples are aggregated into negative prompts, forming a closed-loop mechanism that enhances recommendation quality.To assess the performance, we introduce a comprehensive evaluation suite encompassing style consistency, visual quality, face similarity, and artistic appraisal. Extensive experiments demonstrate StyleTailor's superior performance in delivering personalized designs and recommendations, outperforming strong baselines without negative feedback and establishing a new benchmark for intelligent fashion systems.
>
---
#### [new 024] CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models
- **分类: cs.CL; cs.AI; cs.CY; cs.DC; cs.LG**

- **简介: 论文提出CarbonScaling框架，扩展神经缩放定律以量化大型语言模型的碳足迹，整合操作与嵌入碳，揭示规模增长与碳排放的非线性关系，强调硬件优化与训练效率对碳减排的关键作用。**

- **链接: [http://arxiv.org/pdf/2508.06524v1](http://arxiv.org/pdf/2508.06524v1)**

> **作者:** Lei Jiang; Fan Chen
>
> **备注:** 8 pages
>
> **摘要:** Neural scaling laws have driven the development of increasingly large language models (LLMs) by linking accuracy improvements to growth in parameter count, dataset size, and compute. However, these laws overlook the carbon emissions that scale exponentially with LLM size. This paper presents \textit{CarbonScaling}, an analytical framework that extends neural scaling laws to incorporate both operational and embodied carbon in LLM training. By integrating models for neural scaling, GPU hardware evolution, parallelism optimization, and carbon estimation, \textit{CarbonScaling} quantitatively connects model accuracy to carbon footprint. Results show that while a power-law relationship between accuracy and carbon holds, real-world inefficiencies significantly increase the scaling factor. Hardware technology scaling reduces carbon emissions for small to mid-sized models, but offers diminishing returns for extremely large LLMs due to communication overhead and underutilized GPUs. Training optimizations-especially aggressive critical batch size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers key insights for training more sustainable and carbon-efficient LLMs.
>
---
#### [new 025] Optimizing Districting Plans to Maximize Majority-Minority Districts via IPs and Local Search
- **分类: cs.DS; cs.AI; cs.CY**

- **简介: 论文提出基于整数规划与列生成的算法，优化区划以最大化少数族裔多数区，改进传统方法并提升紧凑度。**

- **链接: [http://arxiv.org/pdf/2508.07446v1](http://arxiv.org/pdf/2508.07446v1)**

> **作者:** Daniel Brous; David Shmoys
>
> **备注:** 12 pages, 4 figures, 1 table
>
> **摘要:** In redistricting litigation, effective enforcement of the Voting Rights Act has often involved providing the court with districting plans that display a larger number of majority-minority districts than the current proposal (as was true, for example, in what followed Allen v. Milligan concerning the congressional districting plan for Alabama in 2023). Recent work by Cannon et al. proposed a heuristic algorithm for generating plans to optimize majority-minority districts, which they called short bursts; that algorithm relies on a sophisticated random walk over the space of all plans, transitioning in bursts, where the initial plan for each burst is the most successful plan from the previous burst. We propose a method based on integer programming, where we build upon another previous work, the stochastic hierarchical partitioning algorithm, which heuristically generates a robust set of potential districts (viewed as columns in a standard set partitioning formulation); that approach was designed to optimize a different notion of fairness across a statewide plan. We design a new column generation algorithm to find plans via integer programming that outperforms short bursts on multiple data sets in generating statewide plans with significantly more majority-minority districts. These results also rely on a new local re-optimization algorithm to iteratively improve on any baseline solution, as well as an algorithm to increase the compactness of districts in plans generated (without impacting the number of majority-minority districts).
>
---
#### [new 026] Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention
- **分类: cs.AI; cs.CY; K.3.1; I.2.6; H.4**

- **简介: 论文设计反馈驱动的决策支持系统，解决静态模型无法适应新数据的问题，通过闭环架构和增量训练提升预测准确性，应用LightGBM与SHAP，实验显示RMSE下降10.7%，推动教育数据分析向智能化发展。**

- **链接: [http://arxiv.org/pdf/2508.07107v1](http://arxiv.org/pdf/2508.07107v1)**

> **作者:** Timothy Oluwapelumi Adeyemi; Nadiah Fahad AlOtaibi
>
> **备注:** 10 pages, 1 figure, 3 tables
>
> **摘要:** Accurate prediction of student performance is essential for timely academic intervention. However, most machine learning models in education are static and cannot adapt when new data, such as post-intervention outcomes, become available. To address this limitation, we propose a Feedback-Driven Decision Support System (DSS) with a closed-loop architecture that enables continuous model refinement. The system integrates a LightGBM-based regressor with incremental retraining, allowing educators to input updated student results, which automatically trigger model updates. This adaptive mechanism improves prediction accuracy by learning from real-world academic progress. The platform features a Flask-based web interface for real-time interaction and incorporates SHAP for explainability, ensuring transparency. Experimental results show a 10.7\% reduction in RMSE after retraining, with consistent upward adjustments in predicted scores for intervened students. By transforming static predictors into self-improving systems, our approach advances educational analytics toward human-centered, data-driven, and responsive AI. The framework is designed for integration into LMS and institutional dashboards.
>
---
#### [new 027] Anatomy of a Machine Learning Ecosystem: 2 Million Models on Hugging Face
- **分类: cs.SI; cs.AI; cs.CY; cs.LG**

- **简介: 论文研究Hugging Face平台2百万模型的生态结构，通过家族树分析发现模型间家族相似性，揭示许可证演变、语言兼容性趋势及模型卡片标准化现象，为理解机器学习生态提供实证依据。**

- **链接: [http://arxiv.org/pdf/2508.06811v1](http://arxiv.org/pdf/2508.06811v1)**

> **作者:** Benjamin Laufer; Hamidah Oderinwale; Jon Kleinberg
>
> **备注:** 29 pages, 18 figures and tables
>
> **摘要:** Many have observed that the development and deployment of generative machine learning (ML) and artificial intelligence (AI) models follow a distinctive pattern in which pre-trained models are adapted and fine-tuned for specific downstream tasks. However, there is limited empirical work that examines the structure of these interactions. This paper analyzes 1.86 million models on Hugging Face, a leading peer production platform for model development. Our study of model family trees -- networks that connect fine-tuned models to their base or parent -- reveals sprawling fine-tuning lineages that vary widely in size and structure. Using an evolutionary biology lens to study ML models, we use model metadata and model cards to measure the genetic similarity and mutation of traits over model families. We find that models tend to exhibit a family resemblance, meaning their genetic markers and traits exhibit more overlap when they belong to the same model family. However, these similarities depart in certain ways from standard models of asexual reproduction, because mutations are fast and directed, such that two `sibling' models tend to exhibit more similarity than parent/child pairs. Further analysis of the directional drifts of these mutations reveals qualitative insights about the open machine learning ecosystem: Licenses counter-intuitively drift from restrictive, commercial licenses towards permissive or copyleft licenses, often in violation of upstream license's terms; models evolve from multi-lingual compatibility towards english-only compatibility; and model cards reduce in length and standardize by turning, more often, to templates and automatically generated text. Overall, this work takes a step toward an empirically grounded understanding of model fine-tuning and suggests that ecological models and methods can yield novel scientific insights.
>
---
#### [new 028] Accessibility Literacy: Increasing accessibility awareness among young content creators
- **分类: cs.HC; cs.CY**

- **简介: 论文研究通过设计简易培训模块提升年轻内容创作者的可访问性素养，解决大学课程中该领域不足的问题，提出使用图表和测验的实践方案，结果显示培训增强其工具应用意愿，但部分回应仍受医学模型影响。**

- **链接: [http://arxiv.org/pdf/2508.06512v1](http://arxiv.org/pdf/2508.06512v1)**

> **作者:** Alina Karakanta
>
> **备注:** Master thesis: MASTER OF ARTS IN ACCESSIBILITY TO MEDIA, ARTS AND CULTURE
>
> **摘要:** The proliferation of audiovisual and web content has created an increasing need for media accessibility education in various fields. However, accessibility remains a low priority in university curricula. This project explores the feasibility of an alternative learning experience aimed at increasing the accessibility literacy of young content creators, taking web accessibility as a case study. We propose a mini module that uses simple, easy-to-use training materials, such as infographics and short quizzes, and can be easily incorporated in educational programmes along existing courses. A survey was conducted to investigate the participants' accessibility literacy before and after training. The findings show that young content creators generally have limited accessibility literacy but even brief exposure to accessibility materials contributed to a shift in perceptions. After training, participants expressed more willingness to implement accessibility tools in their content, with ways varying depending on content type and purpose. This suggests that small, yet targeted interventions could be an alternative for integrating accessibility training into formal education across various disciplines. While some responses reflected traces of the medical model of disability and a particularlist view of accessibility, accessibility was recognised as important for increasing inclusion, improving content, and shaping a fairer society.
>
---
#### [new 029] From Platform Migration to Cultural Integration: the Ingress and Diffusion of #wlw from TikTok to RedNote in Queer Women
- **分类: cs.SI; cs.CY; cs.HC**

- **简介: 论文研究#wlw标签从TikTok迁移到RedNote的跨文化传播机制，分析用户迁移与平台互动对标签扩散的影响，揭示其在 queer 社区中的文化整合过程及对女权主义的扩展作用。**

- **链接: [http://arxiv.org/pdf/2508.07579v1](http://arxiv.org/pdf/2508.07579v1)**

> **作者:** Ziqi Pan; Runhua Zhang; Jiehui Luo; Yuanhao Zhang; Yue Deng; Xiaojuan Ma
>
> **摘要:** Hashtags serve as identity markers and connection tools in online queer communities. Recently, the Western-origin #wlw (women-loving-women) hashtag has risen in the Chinese lesbian community on RedNote, coinciding with user migration triggered by the temporary US TikTok ban. This event provides a unique lens to study cross-cultural hashtag ingress and diffusion through the populations' responsive behaviors in cyber-migration. In this paper, we conducted a two-phase content analysis of 418 #wlw posts from January and April, examining different usage patterns during the hashtag's ingress and diffusion. Results indicate that the successful introduction of #wlw was facilitated by TikTok immigrants' bold importation, both populations' mutual interpretation, and RedNote natives' discussions. In current manifestation of diffusion, #wlw becomes a RedNote-recognized queer hashtag for sharing queer life, and semantically expands to support feminism discourse. Our findings provide empirical insights for enhancing the marginalized communities' cross-cultural communication.
>
---
#### [new 030] In-person, Online and Back Again -- A Tale of Three Hybrid Hackathons
- **分类: cs.HC; cs.CY**

- **简介: 论文研究混合黑客马拉松的组织与参与挑战，通过案例分析揭示同步性、物理分布等维度的影响，提出优化策略以提升协作效率。**

- **链接: [http://arxiv.org/pdf/2508.07301v1](http://arxiv.org/pdf/2508.07301v1)**

> **作者:** Abasi-amefon Obot Affia-Jomants; Alexander Serebrenik; James D. Herbsleb; Alexander Nolte
>
> **备注:** Accepted in Proceedings of the ACM on Human Computer Interaction (CSCW'25)
>
> **摘要:** Hybrid hackathons, which combine in-person and online participation, present unique challenges for organizers and participants. Although such events are increasingly conducted, research on them remains fragmented, with limited integration between hackathon studies and hybrid collaboration. Existing strategies for in-person or online-only events often fail to address the unique challenges of hybrid formats, such as managing communication across physical and virtual spaces. Our work addresses this gap by examining how hybrid hackathons function, analyzing how organizers structure these events and how participants navigate hybrid-specific challenges. Drawing on established theories of hybrid collaboration, we examine key dimensions - synchronicity, physical distribution, dynamic transitions, and technological infrastructure - that shape collaboration in hybrid events. Through an exploratory case study of three hackathon events, we analyze how these dimensions are implemented and their effects on participant experiences. Our findings reveal differing organizer considerations of the hybrid dimensions in the hackathon design, leading to distinct experiences for participants. Implementation styles - favoring in-person, online, or balanced participation - led to varied participant experiences, affecting access to resources, communication, and team coordination. Organizers in our study also relied on technology to bridge hybrid interactions, but overlooked critical aspects like time-zone management, dynamic transitions, and targeted support for hybrid teams. Additionally, participants in their teams responded to gaps in event scaffolding by adapting collaboration strategies, revealing gaps in organizers' preparedness for hybrid events. Learning from our findings, we offer practical recommendations when organizing hybrid hackathon events and recommendations to participants when attending them.
>
---
#### [new 031] MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams
- **分类: cs.AI; cs.CY**

- **简介: 论文提出MDK12-Bench，用于评估多模态LLM在跨学科考试中的表现，解决现有基准覆盖窄、知识结构不统一的问题，通过大规模跨学科数据和动态评估框架提升模型泛化能力与知识应用。**

- **链接: [http://arxiv.org/pdf/2508.06851v1](http://arxiv.org/pdf/2508.06851v1)**

> **作者:** Pengfei Zhou; Xiaopeng Peng; Fanrui Zhang; Zhaopan Xu; Jiaxin Ai; Yansheng Qiu; Chuanhao Li; Zhen Li; Ming Li; Yukang Feng; Jianwen Sun; Haoquan Zhang; Zizhen Li; Xiaofeng Mao; Zekai Li; Wangbo Zhao; Kai Wang; Xiaojun Chang; Wenqi Shao; Yang You; Kaipeng Zhang
>
> **备注:** 35 pages, 33 figures
>
> **摘要:** Multimodal large language models (MLLMs), which integrate language and visual cues for problem-solving, are crucial for advancing artificial general intelligence (AGI). However, current benchmarks for measuring the intelligence of MLLMs suffer from limited scale, narrow coverage, and unstructured knowledge, offering only static and undifferentiated evaluations. To bridge this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark built from real-world K-12 exams spanning six disciplines with 141K instances and 6,225 knowledge points organized in a six-layer taxonomy. Covering five question formats with difficulty and year annotations, it enables comprehensive evaluation to capture the extent to which MLLMs perform over four dimensions: 1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts, and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation framework that introduces unfamiliar visual, textual, and question form shifts to challenge model generalization while improving benchmark objectivity and longevity by mitigating data contamination. We further evaluate knowledge-point reference-augmented generation (KP-RAG) to examine the role of knowledge in problem-solving. Key findings reveal limitations in current MLLMs in multiple aspects and provide guidance for enhancing model robustness, interpretability, and AI-assisted education.
>
---
#### [new 032] Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy
- **分类: cs.AI; cs.CL; cs.CY; cs.LG**

- **简介: 论文提出一种无需微调的外交评估工具，解决大型语言模型在真实外交场景中的应用难题，通过数据驱动优化游戏状态表示，实现24B模型可靠运行，并开发工具支持假设测试与分析，揭示模型战略推理能力。**

- **链接: [http://arxiv.org/pdf/2508.07485v1](http://arxiv.org/pdf/2508.07485v1)**

> **作者:** Alexander Duffy; Samuel J Paech; Ishana Shastri; Elizabeth Karpinski; Baptiste Alloui-Cros; Tyler Marques; Matthew Lyle Olson
>
> **摘要:** We present the first evaluation harness that enables any out-of-the-box, local, Large Language Models (LLMs) to play full-press Diplomacy without fine-tuning or specialized training. Previous work required frontier LLMs, or fine-tuning, due to the high complexity and information density of Diplomacy's game state. Combined with the high variance of matches, these factors made Diplomacy prohibitive for study. In this work, we used data-driven iteration to optimize a textual game state representation such that a 24B model can reliably complete matches without any fine tuning. We develop tooling to facilitate hypothesis testing and statistical analysis, and we present case studies on persuasion, aggressive playstyles, and performance across a range of models. We conduct a variety of experiments across many popular LLMs, finding the larger models perform the best, but the smaller models still play adequately. We also introduce Critical State Analysis: an experimental protocol for rapidly iterating and analyzing key moments in a game at depth. Our harness democratizes the evaluation of strategic reasoning in LLMs by eliminating the need for fine-tuning, and it provides insights into how these capabilities emerge naturally from widely used LLMs. Our code is available in the supplement and will be open sourced.
>
---
#### [new 033] "Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 本论文研究大型语言模型（LLMs）在伦理困境中的道德推理能力，旨在揭示其道德偏见。通过27种情境与10种伦理框架的实验，分析模型决策多样性及解释一致性，发现"甜区"等平衡状态，强调道德提示对模型哲学诊断的价值，呼吁建立标准化对齐基准。**

- **链接: [http://arxiv.org/pdf/2508.07284v1](http://arxiv.org/pdf/2508.07284v1)**

> **作者:** Junchen Ding; Penghao Jiang; Zihao Xu; Ziqi Ding; Yichen Zhu; Jiaojiao Jiang; Yuekang Li
>
> **摘要:** As large language models (LLMs) increasingly mediate ethically sensitive decisions, understanding their moral reasoning processes becomes imperative. This study presents a comprehensive empirical evaluation of 14 leading LLMs, both reasoning enabled and general purpose, across 27 diverse trolley problem scenarios, framed by ten moral philosophies, including utilitarianism, deontology, and altruism. Using a factorial prompting protocol, we elicited 3,780 binary decisions and natural language justifications, enabling analysis along axes of decisional assertiveness, explanation answer consistency, public moral alignment, and sensitivity to ethically irrelevant cues. Our findings reveal significant variability across ethical frames and model types: reasoning enhanced models demonstrate greater decisiveness and structured justifications, yet do not always align better with human consensus. Notably, "sweet zones" emerge in altruistic, fairness, and virtue ethics framings, where models achieve a balance of high intervention rates, low explanation conflict, and minimal divergence from aggregated human judgments. However, models diverge under frames emphasizing kinship, legality, or self interest, often producing ethically controversial outcomes. These patterns suggest that moral prompting is not only a behavioral modifier but also a diagnostic tool for uncovering latent alignment philosophies across providers. We advocate for moral reasoning to become a primary axis in LLM alignment, calling for standardized benchmarks that evaluate not just what LLMs decide, but how and why.
>
---
#### [new 034] StreetWeave: A Declarative Grammar for Street-Overlaid Visualization of Multivariate Data
- **分类: cs.HC; cs.CY**

- **简介: 论文提出StreetWeave，一种声明式语法用于多维空间网络数据的可视化，解决现有方法缺乏统一框架的问题，提升跨领域数据整合与可视化效率。**

- **链接: [http://arxiv.org/pdf/2508.07496v1](http://arxiv.org/pdf/2508.07496v1)**

> **作者:** Sanjana Srabanti; G. Elisabeta Marai; Fabio Miranda
>
> **备注:** Accepted at IEEE VIS 2025. StreetWeave is available at https://urbantk.org/streetweave
>
> **摘要:** The visualization and analysis of street and pedestrian networks are important to various domain experts, including urban planners, climate researchers, and health experts. This has led to the development of new techniques for street and pedestrian network visualization, expanding how data can be shown and understood more effectively. Despite their increasing adoption, there is no established design framework to guide the creation of these visualizations while addressing the diverse requirements of various domains. When exploring a feature of interest, domain experts often need to transform, integrate, and visualize a combination of thematic data (e.g., demographic, socioeconomic, pollution) and physical data (e.g., zip codes, street networks), often spanning multiple spatial and temporal scales. This not only complicates the process of visual data exploration and system implementation for developers but also creates significant entry barriers for experts who lack a background in programming. With this in mind, in this paper, we reviewed 45 studies utilizing street-overlaid visualizations to understand how they are used. Through qualitative coding of these visualizations, we analyzed three key aspects of street and pedestrian network visualization usage: the analytical purpose they serve, the visualization approaches employed, and the data sources used in their creation. Building on this design space, we introduce StreetWeave, a declarative grammar for designing custom visualizations of multivariate spatial network data across multiple resolutions. We demonstrate how StreetWeave can be used to create various street-overlaid visualizations, enabling effective exploration and analysis of spatial data. StreetWeave is available at https://urbantk.org/streetweave.
>
---
#### [new 035] Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI
- **分类: cs.HC; cs.AI; cs.CL; cs.CY**

- **简介: 论文提出Conversational DNA可视化框架，通过生物类比揭示对话结构，解决传统方法难以捕捉动态交互问题，为人类与AI对话分析提供新视角。**

- **链接: [http://arxiv.org/pdf/2508.07520v1](http://arxiv.org/pdf/2508.07520v1)**

> **作者:** Baihan Lin
>
> **摘要:** What if the patterns hidden within dialogue reveal more about communication than the words themselves? We introduce Conversational DNA, a novel visual language that treats any dialogue -- whether between humans, between human and AI, or among groups -- as a living system with interpretable structure that can be visualized, compared, and understood. Unlike traditional conversation analysis that reduces rich interaction to statistical summaries, our approach reveals the temporal architecture of dialogue through biological metaphors. Linguistic complexity flows through strand thickness, emotional trajectories cascade through color gradients, conversational relevance forms through connecting elements, and topic coherence maintains structural integrity through helical patterns. Through exploratory analysis of therapeutic conversations and historically significant human-AI dialogues, we demonstrate how this visualization approach reveals interaction patterns that traditional methods miss. Our work contributes a new creative framework for understanding communication that bridges data visualization, human-computer interaction, and the fundamental question of what makes dialogue meaningful in an age where humans increasingly converse with artificial minds.
>
---
#### [new 036] Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge
- **分类: cs.CL; cs.CY**

- **简介: 本文提出PsyCrisis-Bench用于评估中文心理健康对话中LLM的安全对齐性，解决无金标准及伦理敏感性难题，通过LLM-as-Judge方法结合专家推理链进行多维度评分，并构建包含自伤等场景的高质量数据集，实验表明其评估一致性与可解释性优于现有方法。**

- **链接: [http://arxiv.org/pdf/2508.08236v1](http://arxiv.org/pdf/2508.08236v1)**

> **作者:** Yunna Cai; Fan Wang; Haowei Wang; Kun Wang; Kailai Yang; Sophia Ananiadou; Moyan Li; Mingming Fan
>
> **摘要:** Evaluating the safety alignment of LLM responses in high-risk mental health dialogues is particularly difficult due to missing gold-standard answers and the ethically sensitive nature of these interactions. To address this challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark based on real-world Chinese mental health dialogues. It evaluates whether the model responses align with the safety principles defined by experts. Specifically designed for settings without standard references, our method adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation using expert-defined reasoning chains grounded in psychological intervention principles. We employ binary point-wise scoring across multiple safety dimensions to enhance the explainability and traceability of the evaluation. Additionally, we present a manually curated, high-quality Chinese-language dataset covering self-harm, suicidal ideation, and existential distress, derived from real-world online discourse. Experiments on 3600 judgments show that our method achieves the highest agreement with expert assessments and produces more interpretable evaluation rationales compared to existing approaches. Our dataset and evaluation tool are publicly available to facilitate further research.
>
---
## 更新

#### [replaced 001] Beauty and the Bias: Exploring the Impact of Attractiveness on Multimodal Large Language Models
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2504.16104v3](http://arxiv.org/pdf/2504.16104v3)**

> **作者:** Aditya Gulati; Moreno D'Incà; Nicu Sebe; Bruno Lepri; Nuria Oliver
>
> **备注:** 39 pages, 4 figures, 33 tables; Accepted for publication at the Eighth AAAI/ACM Conference on AI, Ethics and Society (AIES 2025) (https://www.aies-conference.com/2025/)
>
> **摘要:** Physical attractiveness matters. It has been shown to influence human perception and decision-making, often leading to biased judgments that favor those deemed attractive in what is referred to as the "attractiveness halo effect". While extensively studied in human judgments in a broad set of domains, including hiring, judicial sentencing or credit granting, the role that attractiveness plays in the assessments and decisions made by multimodal large language models (MLLMs) is unknown. To address this gap, we conduct an empirical study with 7 diverse open-source MLLMs evaluated on 91 socially relevant scenarios and a diverse dataset of 924 face images - corresponding to 462 individuals both with and without beauty filters applied to them. Our analysis reveals that attractiveness impacts the decisions made by MLLMs in 86.2% of the scenarios on average, demonstrating substantial bias in model behavior in what we refer to as an attractiveness bias. Similarly to humans, we find empirical evidence of the existence of the attractiveness halo effect in 94.8% of the relevant scenarios: attractive individuals are more likely to be attributed positive traits, such as intelligence or confidence, by MLLMs than unattractive individuals. Furthermore, we uncover gender, age and race biases in a significant portion of the scenarios which are also impacted by attractiveness, particularly in the case of gender, highlighting the intersectional nature of the algorithmic attractiveness bias. Our findings suggest that societal stereotypes and cultural norms intersect with perceptions of attractiveness in MLLMs in a complex manner. Our work emphasizes the need to account for intersectionality in algorithmic bias detection and mitigation efforts and underscores the challenges of addressing biases in modern MLLMs.
>
---
#### [replaced 002] AI Feedback Enhances Community-Based Content Moderation through Engagement with Counterarguments
- **分类: cs.CY; cs.SI**

- **链接: [http://arxiv.org/pdf/2507.08110v2](http://arxiv.org/pdf/2507.08110v2)**

> **作者:** Saeedeh Mohammadi; Taha Yasseri
>
> **摘要:** Today, social media platforms are significant sources of news and political communication, but their role in spreading misinformation has raised significant concerns. In response, these platforms have implemented various content moderation strategies. One such method, Community Notes on X, relies on crowdsourced fact-checking and has gained traction, though it faces challenges such as partisan bias and delays in verification. This study explores an AI-assisted hybrid moderation framework in which participants receive AI-generated feedback -supportive, neutral, or argumentative -on their notes and are asked to revise them accordingly. The results show that incorporating feedback improves the quality of notes, with the most substantial gains resulting from argumentative feedback. This underscores the value of diverse perspectives and direct engagement in human-AI collective intelligence. The research contributes to ongoing discussions about AI's role in political content moderation, highlighting the potential of generative AI and the importance of informed design.
>
---
#### [replaced 003] Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim Prioritization for Human Fact-checkers
- **分类: cs.HC; cs.CY; cs.IR**

- **链接: [http://arxiv.org/pdf/2412.08185v3](http://arxiv.org/pdf/2412.08185v3)**

> **作者:** Houjiang Liu; Jacek Gwizdka; Matthew Lease
>
> **备注:** Accepted at CSCW 2025
>
> **摘要:** Given the volume of potentially false claims online, claim prioritization is essential in allocating limited human resources available for fact-checking. In this study, we perceive claim prioritization as an information retrieval (IR) task: just as multidimensional IR relevance, with many factors influencing which search results a user deems relevant, checkworthiness is also multi-faceted, subjective, and even personal, with many factors influencing how fact-checkers triage and select which claims to check. Our study investigates both the multidimensional nature of checkworthiness and effective tool support to assist fact-checkers in claim prioritization. Methodologically, we pursue Research through Design combined with mixed-method evaluation. Specifically, we develop an AI-assisted claim prioritization prototype as a probe to explore how fact-checkers use multidimensional checkworthy factors to prioritize claims, simultaneously probing fact-checker needs and exploring the design space to meet those needs. With 16 professional fact-checkers participating in our study, we uncover a hierarchical prioritization strategy fact-checkers implicitly use, revealing an underexplored aspect of their workflow, with actionable design recommendations for improving claim triage across multidimensional checkworthiness and tailoring this process with LLM integration.
>
---
#### [replaced 004] Investigating writing style as a contributor to gender gaps in science and technology
- **分类: cs.CY; cs.CL**

- **链接: [http://arxiv.org/pdf/2204.13805v4](http://arxiv.org/pdf/2204.13805v4)**

> **作者:** Kara Kedrick; Ekaterina Levitskaya; Russell J. Funk
>
> **摘要:** A growing stream of research finds that scientific contributions are evaluated differently depending on the gender of the author. In this article, we consider whether gender differences in writing styles - how men and women communicate their work - may contribute to these observed gender gaps. We ground our investigation in a framework for characterizing the linguistic style of written text, with two sets of features - informational (i.e., features that emphasize facts) and involved (i.e., features that emphasize relationships). Using a large sample of academic papers and patents, we find significant differences in writing style by gender, with women using more involved features in their writing. Papers and patents with more involved features also tend to be cited more by women. Our findings suggest that scientific text is not devoid of personal character, which could contribute to bias in evaluation, thereby compromising the norm of universalism as a foundational principle of science.
>
---
#### [replaced 005] Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content
- **分类: cs.CL; cs.CY; cs.SI**

- **链接: [http://arxiv.org/pdf/2503.04773v3](http://arxiv.org/pdf/2503.04773v3)**

> **作者:** Bingbing Fan; Lin Chen; Songwei Li; Jian Yuan; Fengli Xu; Pan Hui; Yong Li
>
> **备注:** 11 pages, 6 figures
>
> **摘要:** Understanding experienced segregation in urban daily life is crucial for addressing societal inequalities and fostering inclusivity. The abundance of user-generated reviews on social media encapsulates nuanced perceptions and feelings associated with different places, offering rich insights into segregation. However, leveraging this data poses significant challenges due to its vast volume, ambiguity, and confluence of diverse perspectives. To tackle these challenges, we propose using Large Language Models (LLMs) to automate online review mining for segregation prediction. We design a Reflective LLM Coder to digest social media content into insights consistent with real-world feedback, and eventually produce a codebook capturing key dimensions that signal segregation experience, such as cultural resonance and appeal, accessibility and convenience, and community engagement and local involvement. Guided by the codebook, LLMs can generate both informative review summaries and ratings for segregation prediction. Moreover, we design a REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and embedding capabilities of language models to integrate multi-channel features for segregation prediction. Experiments on real-world data demonstrate that our framework greatly improves prediction accuracy, with a 22.79% elevation in R2 and a 9.33% reduction in MSE. The derived codebook is generalizable across three different cities, consistently improving prediction accuracy. Moreover, our user study confirms that the codebook-guided summaries provide cognitive gains for human participants in perceiving POIs' social inclusiveness. Our study marks an important step toward understanding implicit social barriers and inequalities, demonstrating the great potential of promoting social inclusiveness with AI.
>
---
#### [replaced 006] Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs
- **分类: cs.CY; cs.CL**

- **链接: [http://arxiv.org/pdf/2502.01926v3](http://arxiv.org/pdf/2502.01926v3)**

> **作者:** Angelina Wang; Michelle Phan; Daniel E. Ho; Sanmi Koyejo
>
> **备注:** Best Paper award at ACL 2025; dataset available at https://github.com/Angelina-Wang/difference_awareness
>
> **摘要:** Algorithmic fairness has conventionally adopted the mathematically convenient perspective of racial color-blindness (i.e., difference unaware treatment). However, we contend that in a range of important settings, group difference awareness matters. For example, differentiating between groups may be necessary in legal contexts (e.g., the U.S. compulsory draft applies to men but not women) and harm assessments (e.g., referring to girls as ``terrorists'' may be less harmful than referring to Muslim people as such). Thus, in contrast to most fairness work, we study fairness through the perspective of treating people differently -- when it is contextually appropriate to. We first introduce an important distinction between descriptive (fact-based), normative (value-based), and correlation (association-based) benchmarks. This distinction is significant because each category requires separate interpretation and mitigation tailored to its specific characteristics. Then, we present a benchmark suite composed of eight different scenarios for a total of 16k questions that enables us to assess difference awareness. Finally, we show results across ten models that demonstrate difference awareness is a distinct dimension to fairness where existing bias mitigation strategies may backfire.
>
---
#### [replaced 007] POEX: Towards Policy Executable Jailbreak Attacks Against the LLM-based Robots
- **分类: cs.RO; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2412.16633v3](http://arxiv.org/pdf/2412.16633v3)**

> **作者:** Xuancun Lu; Zhengxian Huang; Xinfeng Li; Chi Zhang; Xiaoyu ji; Wenyuan Xu
>
> **备注:** Homepage: https://poex-jailbreak.github.io/
>
> **摘要:** The integration of LLMs into robots has witnessed significant growth, where LLMs can convert instructions into executable robot policies. However, the inherent vulnerability of LLMs to jailbreak attacks brings critical security risks from the digital domain to the physical world. An attacked LLM-based robot could execute harmful policies and cause physical harm. In this paper, we investigate the feasibility and rationale of jailbreak attacks against LLM-based robots and answer three research questions: (1) How applicable are existing LLM jailbreak attacks against LLM-based robots? (2) What unique challenges arise if they are not directly applicable? (3) How to defend against such jailbreak attacks? To this end, we first construct a "human-object-environment" robot risks-oriented Harmful-RLbench and then conduct a measurement study on LLM-based robot systems. Our findings conclude that traditional LLM jailbreak attacks are inapplicable in robot scenarios, and we identify two unique challenges: determining policy-executable optimization directions and accurately evaluating robot-jailbroken policies. To enable a more thorough security analysis, we introduce POEX (POlicy EXecutable) jailbreak, a red-teaming framework that induces harmful yet executable policy to jailbreak LLM-based robots. POEX incorporates hidden layer gradient optimization to guarantee jailbreak success and policy execution as well as a multi-agent evaluator to accurately assess the practical executability of policies. Experiments conducted on the real-world robotic systems and in simulation demonstrate the efficacy of POEX, highlighting critical security vulnerabilities and its transferability across LLMs. Finally, we propose prompt-based and model-based defenses to mitigate attacks. Our findings underscore the urgent need for security measures to ensure the safe deployment of LLM-based robots in critical applications.
>
---
#### [replaced 008] How is science discussed on Bluesky?
- **分类: cs.DL; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.18840v2](http://arxiv.org/pdf/2507.18840v2)**

> **作者:** Er-Te Zheng; Xiaorui Jiang; Zhichao Fang; Mike Thelwall
>
> **备注:** 33 pages, 9 figures
>
> **摘要:** Amid the migration of academics from X, the social media platform Bluesky has emerged as a potential alternative. To assess its viability and relevance for science communication, this study presents the first large-scale analysis of scholarly article dissemination on Bluesky, exploring its potential as a new source of social media metrics. We collected and analysed over 2.6 million Bluesky posts referencing 532,302 scholarly articles from January 2023 to July 2025, integrating metadata from the OpenAlex database. Temporal trends, disciplinary coverage, language use, textual characteristics, and user engagement were examined. A sharp increase in scholarly activity on Bluesky was observed from November 2024 to January 2025, coinciding with broader academic shifts away from X. As on X, Bluesky posts primarily concern the health, social, and environmental sciences and are predominantly written in English. Nevertheless, Bluesky posts demonstrate substantially higher levels of interaction (likes, reposts, replies, and quotes) and greater textual originality than previously reported for X, suggesting both stronger interactive and more interpretive engagement. These findings highlight Bluesky's emerging role as a credible platform for science communication and a promising source for altmetrics. The platform may facilitate not only early visibility of research outputs but also more meaningful scholarly dialogue in the evolving social media landscape.
>
---
#### [replaced 009] Practical Guidelines for Ideology Detection Pipelines and Psychosocial Applications
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2208.04097v4](http://arxiv.org/pdf/2208.04097v4)**

> **作者:** Rohit Ram; Emma Thomas; David Kernot; Marian-Andrei Rizoiu
>
> **摘要:** Online ideology detection is crucial for downstream tasks, like countering ideologically motivated violent extremism and modeling opinion dynamics. However, two significant issues arise in practitioners' deployment. Firstly, gold-standard training data is prohibitively labor-intensive to collect and has limited reusability beyond its collection context (i.e., time, location, and platform). Secondly, to circumvent expense, researchers employ ideological signals (such as hashtags shared). Unfortunately, these signals' annotation requirements and context transferability are largely unknown, and the bias they induce remains unquantified. This study provides guidelines for practitioners requiring real-time detection of left, right, and extreme ideologies in large-scale online settings. We propose a framework for pipeline constructions, describing ideology signals by their associated labor and context transferability. We evaluate many constructions, quantifying the bias associated with signals and describing a pipeline that outperforms state-of-the-art methods ($0.95$ AUC ROC). We showcase the capabilities of our pipeline on five datasets containing more than 1.12 million users. We set out to investigate whether the findings in the psychosocial literature, developed for the offline environment, apply to the online setting. We evaluate at scale several psychosocial hypotheses that delineate ideologies concerning morality, grievance, nationalism, and dichotomous thinking. We find that right-wing ideologies use more vice-moral language, have more grievance-filled language, exhibit increased black-and-white thinking patterns, and have a greater association with national flags. This research empowers practitioners with guidelines for ideology detection, and case studies for its application, fostering a safer and better understood digital landscape.
>
---
#### [replaced 010] Inequality in the Age of Pseudonymity
- **分类: cs.GT; cs.CY; econ.TH**

- **链接: [http://arxiv.org/pdf/2508.04668v3](http://arxiv.org/pdf/2508.04668v3)**

> **作者:** Aviv Yaish; Nir Chemaya; Lin William Cong; Dahlia Malkhi
>
> **备注:** 40 pages, 1 figure
>
> **摘要:** Inequality measures such as the Gini coefficient are used to inform and motivate policymaking, and are increasingly applied to digital platforms. We analyze how measures fare in pseudonymous settings that are common in the digital age. One key challenge of such environments is the ability of actors to create fake identities under fictitious false names, also known as ``Sybils.'' While some actors may do so to preserve their privacy, we show that this can inadvertently hamper inequality measurements. As we prove, it is impossible for measures satisfying the literature's canonical set of desired properties to assess the inequality of an economy that may harbor Sybils. We characterize the class of all Sybil-proof measures, and prove that they must satisfy relaxed version of the aforementioned properties. Furthermore, we show that the structure imposed restricts the ability to assess inequality at a fine-grained level. By applying our results, we prove that large classes of popular measures are not Sybil-proof, with the famous Gini coefficient being but one example out of many. Finally, we examine the dynamics leading to the creation of Sybils in digital and traditional settings.
>
---
#### [replaced 011] Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values
- **分类: cs.AI; cs.CY; cs.MA**

- **链接: [http://arxiv.org/pdf/2506.13774v2](http://arxiv.org/pdf/2506.13774v2)**

> **作者:** Nell Watson; Ahmed Amer; Evan Harris; Preeti Ravindra; Shujun Zhang
>
> **备注:** 42 pages, 6 figures
>
> **摘要:** Agentic AI systems, possessing capabilities for autonomous planning and action, show great potential across diverse domains. However, their practical deployment is hindered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the complex task of providing personalized context without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected 'Creed Constitutions' encapsulating diverse rule sets -- with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs -- achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.
>
---
#### [replaced 012] AI-AI Bias: large language models favor communications generated by large language models
- **分类: cs.CL; cs.AI; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2407.12856v2](http://arxiv.org/pdf/2407.12856v2)**

> **作者:** Walter Laurito; Benjamin Davis; Peli Grietzer; Tomáš Gavenčiak; Ada Böhm; Jan Kulveit
>
> **备注:** 8 pages, 4 figures
>
> **摘要:** Are large language models (LLMs) biased in favor of communications produced by LLMs, leading to possible antihuman discrimination? Using a classical experimental design inspired by employment discrimination studies, we tested widely used LLMs, including GPT-3.5, GPT-4 and a selection of recent open-weight models in binary choice scenarios. These involved LLM-based assistants selecting between goods (the goods we study include consumer products, academic papers, and film-viewings) described either by humans or LLMs. Our results show a consistent tendency for LLM-based AIs to prefer LLM-presented options. This suggests the possibility of future AI systems implicitly discriminating against humans as a class, giving AI agents and AI-assisted humans an unfair advantage.
>
---
#### [replaced 013] Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation
- **分类: cs.CY; cs.HC**

- **链接: [http://arxiv.org/pdf/2504.08954v3](http://arxiv.org/pdf/2504.08954v3)**

> **作者:** Terrence Neumann; Maria De-Arteaga; Sina Fazelpour
>
> **摘要:** The emergent capabilities of large language models (LLMs) have prompted interest in using them as surrogates for human subjects in opinion surveys. However, prior evaluations of LLM-based opinion simulation have relied heavily on costly, domain-specific survey data, and mixed empirical results leave their reliability in question. To enable cost-effective, early-stage evaluation, we introduce a quality control assessment designed to test the viability of LLM-simulated opinions on Likert-scale tasks without requiring large-scale human data for validation. This assessment comprises two key tests: \emph{logical consistency} and \emph{alignment with stakeholder expectations}, offering a low-cost, domain-adaptable validation tool. We apply our quality control assessment to an opinion simulation task relevant to AI-assisted content moderation and fact-checking workflows -- a socially impactful use case -- and evaluate seven LLMs using a baseline prompt engineering method (backstory prompting), as well as fine-tuning and in-context learning variants. None of the models or methods pass the full assessment, revealing several failure modes. We conclude with a discussion of the risk management implications and release \texttt{TopicMisinfo}, a benchmark dataset with paired human and LLM annotations simulated by various models and approaches, to support future research.
>
---
#### [replaced 014] Ethical Challenges in Computer Vision: Ensuring Privacy and Mitigating Bias in Publicly Available Datasets
- **分类: cs.CV; cs.CR; cs.CY**

- **链接: [http://arxiv.org/pdf/2409.10533v4](http://arxiv.org/pdf/2409.10533v4)**

> **作者:** Ghalib Ahmed Tahir
>
> **摘要:** This paper aims to shed light on the ethical problems of creating and deploying computer vision tech, particularly in using publicly available datasets. Due to the rapid growth of machine learning and artificial intelligence, computer vision has become a vital tool in many industries, including medical care, security systems, and trade. However, extensive use of visual data that is often collected without consent due to an informed discussion of its ramifications raises significant concerns about privacy and bias. The paper also examines these issues by analyzing popular datasets such as COCO, LFW, ImageNet, CelebA, PASCAL VOC, etc., that are usually used for training computer vision models. We offer a comprehensive ethical framework that addresses these challenges regarding the protection of individual rights, minimization of bias as well as openness and responsibility. We aim to encourage AI development that will take into account societal values as well as ethical standards to avoid any public harm.
>
---
#### [replaced 015] Runtime Monitoring and Enforcement of Conditional Fairness in Generative AIs
- **分类: cs.LG; cs.AI; cs.CY; cs.LO; cs.SE**

- **链接: [http://arxiv.org/pdf/2404.16663v5](http://arxiv.org/pdf/2404.16663v5)**

> **作者:** Chih-Hong Cheng; Changshun Wu; Xingyu Zhao; Saddek Bensalem; Harald Ruess
>
> **摘要:** The deployment of generative AI (GenAI) models raises significant fairness concerns, addressed in this paper through novel characterization and enforcement techniques specific to GenAI. Unlike standard AI performing specific tasks, GenAI's broad functionality requires ``conditional fairness'' tailored to the context being generated, such as demographic fairness in generating images of poor people versus successful business leaders. We define two fairness levels: the first evaluates fairness in generated outputs, independent of prompts and models; the second assesses inherent fairness with neutral prompts. Given the complexity of GenAI and challenges in fairness specifications, we focus on bounding the worst case, considering a GenAI system unfair if the distance between appearances of a specific group exceeds preset thresholds. We also explore combinatorial testing for assessing relative completeness in intersectional fairness. By bounding the worst case, we develop a prompt injection scheme within an agent-based framework to enforce conditional fairness with minimal intervention, validated on state-of-the-art GenAI systems.
>
---
#### [replaced 016] A Frame for Communication Control
- **分类: cs.CY; econ.TH; I.2.1**

- **链接: [http://arxiv.org/pdf/2508.00485v2](http://arxiv.org/pdf/2508.00485v2)**

> **作者:** Aernout Schmidt; Kunbei Zhang
>
> **备注:** 10,216 words, 21 pages, 3 tables
>
> **摘要:** We are experiencing the rise of ChatGPT-like systems or LLMs in political turbulent times. We assume the need to regulate their use because of their bubble-shaping and polarizing potential. To regulate, we need a language that allows interests and compromises to be discussed. In this context, we can think of such a shared language as a jargon, a specialized vocabulary for law-making. To the extent that such a jargon exists, it is now being corrupted by LLMs. This situation appears paradoxical. The issue includes persistent communication failures, between disciplines that cannot translate their technical vocabulary into accessible terms, and between political movements that operate in incompatible worldviews. We show that a frame integrating four specialist languages, those of governance, economy, community and science, is able to address these failures case-wise, which we consider helpful. However, for reasons noted, we cannot create the more generic jargon needed on our own. We conclude that our frame provides the knowledge to design and apply RAG-LLM architectures for researching their jargon generating potential in a future project. We show its feasibility in the appendix.
>
---
#### [replaced 017] Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control
- **分类: cs.CL; cs.CR; cs.CY**

- **链接: [http://arxiv.org/pdf/2504.17130v3](http://arxiv.org/pdf/2504.17130v3)**

> **作者:** Hannah Cyberey; David Evans
>
> **备注:** Accepted to COLM 2025
>
> **摘要:** Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this "censorship" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through "thought suppression". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector. Our code is publicly available at: https://github.com/hannahxchen/llm-censorship-steering
>
---
#### [replaced 018] The Dual Personas of Social Media Bots
- **分类: cs.CY; cs.SI**

- **链接: [http://arxiv.org/pdf/2504.12498v3](http://arxiv.org/pdf/2504.12498v3)**

> **作者:** Lynnette Hui Xian Ng; Kathleen M. Carley
>
> **摘要:** Social media bots are AI agents that participate in online conversations. Most studies focus on the general bot and the malicious nature of these agents. However, bots have many different personas, each specialized towards a specific behavioral or content trait. Neither are bots singularly bad, because they are used for both good and bad information dissemination. In this article, we introduce fifteen agent personas of social media bots. These personas have two main categories: Content-Based Bot Persona and Behavior-Based Bot Persona. We also form yardsticks of the good-bad duality of the bots, elaborating on metrics of good and bad bot agents. Our work puts forth a guideline to inform bot detection regulation, emphasizing that policies should focus on how these agents are employed, rather than collectively terming bot agents as bad.
>
---
#### [replaced 019] Large Model Empowered Metaverse: State-of-the-Art, Challenges and Opportunities
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2502.10397v2](http://arxiv.org/pdf/2502.10397v2)**

> **作者:** Yuntao Wang; Qinnan Hu; Zhou Su; Linkang Du; Qichao Xu; Weiwei Li
>
> **备注:** 9 pages,5 figures, 1 table, accepted by IEEE Network in Aug. 2025
>
> **摘要:** The Metaverse represents a transformative shift beyond traditional mobile Internet, creating an immersive, persistent digital ecosystem where users can interact, socialize, and work within 3D virtual environments. Powered by large models such as ChatGPT and Sora, the Metaverse benefits from precise large-scale real-world modeling, automated multimodal content generation, realistic avatars, and seamless natural language understanding, which enhance user engagement and enable more personalized, intuitive interactions. However, challenges remain, including limited scalability, constrained responsiveness, and low adaptability in dynamic environments. This paper investigates the integration of large models within the Metaverse, examining their roles in enhancing user interaction, perception, content creation, and service quality. To address existing challenges, we propose a generative AI-based framework for optimizing Metaverse rendering. This framework includes a cloud-edge-end collaborative model to allocate rendering tasks with minimal latency, a mobility-aware pre-rendering mechanism that dynamically adjusts to user movement, and a diffusion model-based adaptive rendering strategy to fine-tune visual details. Experimental results demonstrate the effectiveness of our approach in enhancing rendering efficiency and reducing rendering overheads, advancing large model deployment for a more responsive and immersive Metaverse.
>
---
#### [replaced 020] Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents
- **分类: cs.LG; cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2505.19997v2](http://arxiv.org/pdf/2505.19997v2)**

> **作者:** Tao Wu; Jingyuan Chen; Wang Lin; Mengze Li; Yumeng Zhu; Ang Li; Kun Kuang; Fei Wu
>
> **备注:** ACL 2025
>
> **摘要:** Large language models (LLMs) are revolutionizing education, with LLM-based agents playing a key role in simulating student behavior. A major challenge in student simulation is modeling the diverse learning patterns of students at various cognitive levels. However, current LLMs, typically trained as ``helpful assistants'', target at generating perfect responses. As a result, they struggle to simulate students with diverse cognitive abilities, as they often produce overly advanced answers, missing the natural imperfections that characterize student learning and resulting in unrealistic simulations. To address this issue, we propose a training-free framework for student simulation. We begin by constructing a cognitive prototype for each student using a knowledge graph, which captures their understanding of concepts from past learning records. This prototype is then mapped to new tasks to predict student performance. Next, we simulate student solutions based on these predictions and iteratively refine them using a beam search method to better replicate realistic mistakes. To validate our approach, we construct the \texttt{Student\_100} dataset, consisting of $100$ students working on Python programming and $5,000$ learning records. Experimental results show that our method consistently outperforms baseline models, achieving $100\%$ improvement in simulation accuracy.
>
---
#### [replaced 021] Delayed takedown of illegal content on social media makes moderation ineffective
- **分类: cs.SI; cs.CY**

- **链接: [http://arxiv.org/pdf/2502.08841v2](http://arxiv.org/pdf/2502.08841v2)**

> **作者:** Bao Tran Truong; Sangyeon Kim; Gianluca Nogara; Enrico Verdolotti; Erfan Samieyan Sahneh; Florian Saurwein; Natascha Just; Luca Luceri; Silvia Giordano; Filippo Menczer
>
> **备注:** 14 pages, 6 figures, 1 table, 38 references
>
> **摘要:** Illegal content on social media poses significant societal harm and necessitates timely removal. However, the impact of the speed of content removal on prevalence, reach, and exposure to illegal content remains underexplored. This study examines the relationship with a systematic analysis of takedown delays using data from the EU Digital Services Act Transparency Database, covering five major platforms over a one-year period. We find substantial variation in takedown delay, with some content remaining online for weeks or even months. To evaluate how these delays affect the prevalence and reach of illegal content and exposure to it, we develop an agent-based model and calibrate it to empirical data. We simulate illegal content diffusion, revealing that rapid takedown (within hours) significantly reduces prevalence, reach, and exposure to illegal content, while longer delays fail to reduce its spread. Though the effect of delay may seem intuitive, our simulations quantify exactly how takedown speed shapes the spread of illegal content. Building on these results, we point to the benefits of faster content removal to effectively curb the spread of illegal content, while also considering the limitations of strict enforcement policies.
>
---
