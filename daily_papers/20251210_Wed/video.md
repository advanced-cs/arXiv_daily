# 计算机视觉 cs.CV

- **最新发布 131 篇**

- **更新 69 篇**

## 最新发布

#### [new 001] Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery
- **分类: cs.CV; cs.AI; cs.LG; cs.RO**

- **简介: 该论文致力于自动化生成无遮挡的开放手术视频。针对多摄像头无影灯系统中因调整灯光导致的图像错位问题，提出自动校正帧对齐并选择最优无遮挡视角的方法，生成稳定、舒适、高质量的手术视频，并通过用户研究验证其优越性。**

- **链接: [https://arxiv.org/pdf/2512.08577v1](https://arxiv.org/pdf/2512.08577v1)**

> **作者:** Yuna Kato; Shohei Mori; Hideo Saito; Yoshifumi Takatsume; Hiroki Kajita; Mariko Isogawa
>
> **摘要:** Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.
>
---
#### [new 002] CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning
- **分类: cs.CV**

- **简介: 该论文针对3D空间推理中缺乏显式结构理解的问题，提出一种受中央-外周视觉启发的多模态模型CVP。通过引入目标关联令牌和以自我为中心的网格，增强局部关注与全局场景感知，提升复杂环境下的空间理解能力，在多个3D基准上达到最优性能。**

- **链接: [https://arxiv.org/pdf/2512.08135v1](https://arxiv.org/pdf/2512.08135v1)**

> **作者:** Zeyuan Chen; Xiang Zhang; Haiyang Xu; Jianwen Xie; Zhuowen Tu
>
> **备注:** Accepted to WACV 2026
>
> **摘要:** We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.
>
---
#### [new 003] VisKnow: Constructing Visual Knowledge Base for Object Understanding
- **分类: cs.CV**

- **简介: 该论文聚焦视觉理解任务，旨在解决现有对象知识分散、缺乏系统组织的问题。提出VisKnow框架，构建结构化多模态视觉知识库，以支持细粒度识别与视觉问答等任务。**

- **链接: [https://arxiv.org/pdf/2512.08221v1](https://arxiv.org/pdf/2512.08221v1)**

> **作者:** Ziwei Yao; Qiyang Wan; Ruiping Wang; Xilin Chen
>
> **备注:** 16 pages, 12 figures, 7 tables. Under review
>
> **摘要:** Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.
>
---
#### [new 004] ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention
- **分类: cs.CV; cs.AI**

- **简介: 该论文研究拖拽式图像编辑任务，旨在解决现有方法对上下文信息利用不足导致编辑结果不连贯的问题。提出ContextDrag，通过上下文保持的特征注入与位置一致注意力机制，实现高保真、精准的图像编辑，无需微调或反演。**

- **链接: [https://arxiv.org/pdf/2512.08477v1](https://arxiv.org/pdf/2512.08477v1)**

> **作者:** Huiguo He; Pengyu Yan; Ziqi Yi; Weizhi Zhong; Zheng Liu; Yejun Tang; Huan Yang; Kun Gai; Guanbin Li; Lianwen Jin
>
> **摘要:** Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.
>
---
#### [new 005] SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds
- **分类: cs.CV**

- **简介: 该论文针对LiDAR点云实时3D目标检测任务，解决计算冗余问题。提出SSCATeR方法，利用时序数据复用和稀疏散射卷积，仅处理动态变化区域，在保持精度的同时显著提升效率，实现最高6.61倍加速。**

- **链接: [https://arxiv.org/pdf/2512.08557v1](https://arxiv.org/pdf/2512.08557v1)**

> **作者:** Alexander Dow; Manduhu Manduhu; Matheus Santos; Ben Bartlett; Gerard Dooly; James Riordan
>
> **备注:** 22 Pages, 26 Figures, This work has been submitted to the IEEE Sensors Journal for possible publication
>
> **摘要:** This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.
>
---
#### [new 006] Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视觉-语言模型中的少样本分类任务，旨在解决CLIP因文本模板与图像相似性引入的偏差问题。作者提出使用“空提示”捕获无偏特征，并通过两阶段框架校正模板偏差，提升分类准确率与模型鲁棒性。**

- **链接: [https://arxiv.org/pdf/2512.08606v1](https://arxiv.org/pdf/2512.08606v1)**

> **作者:** Zhenyu Zhang; Guangyao Chen; Yixiong Zou; Zhimeng Huang; Yuhua Li
>
> **备注:** 14 pages, 8 figures, Association for the Advancement of Artificial Intelligence (AAAI2026, poster)
>
> **摘要:** The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.
>
---
#### [new 007] Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank
- **分类: cs.CV**

- **简介: 该论文聚焦生成建模任务，旨在降低训练成本与提升表示学习效率。提出Repulsor框架，通过对比记忆库引入大量负样本，解耦批大小与负样本数，加速收敛并提升生成质量，无需外部编码器或增加推理开销。**

- **链接: [https://arxiv.org/pdf/2512.08648v1](https://arxiv.org/pdf/2512.08648v1)**

> **作者:** Shaofeng Zhang; Xuanqi Chen; Ning Liao; Haoxiang Zhao; Xiaoxing Wang; Haoru Tan; Sitong Wu; Xiaosong Jia; Qi Fan; Junchi Yan
>
> **备注:** 19 pages, 19 figures
>
> **摘要:** The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.
>
---
#### [new 008] PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation
- **分类: cs.CV**

- **简介: 该论文提出PaintFlow框架，解决油画生成与编辑中风格一致性与细粒度控制难题。通过融合参考图、手绘草图和文本提示，结合自监督风格迁移与空间语义对齐技术，实现交互式油画创作与编辑。**

- **链接: [https://arxiv.org/pdf/2512.08534v1](https://arxiv.org/pdf/2512.08534v1)**

> **作者:** Zhangli Hu; Ye Chen; Jiajun Yao; Bingbing Ni
>
> **备注:** 14 pages
>
> **摘要:** Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.
>
---
#### [new 009] DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation
- **分类: cs.CV**

- **简介: 该论文研究从T1加权图像生成BOLD功能图像的任务，解决BOLD数据缺失问题。提出DINO-BOLDNet，利用冻结的DINOv3编码器提取结构特征，结合切片间注意力与多尺度解码器生成高质量BOLD图像，并引入DINO感知损失提升一致性。**

- **链接: [https://arxiv.org/pdf/2512.08337v1](https://arxiv.org/pdf/2512.08337v1)**

> **作者:** Jianwei Wang; Qing Wang; Menglan Ruan; Rongjun Ge; Chunfeng Yang; Yang Chen; Chunming Xie
>
> **摘要:** Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.
>
---
#### [new 010] Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文研究皮肤病变分类中的个体肤色偏见问题，提出基于分布感知的重加权方法。将肤色视为连续属性，利用核密度估计和统计距离度量，设计距离-based重加权损失函数，缓解少数肤色样本的表征不足，提升模型在个体层面的公平性。**

- **链接: [https://arxiv.org/pdf/2512.08733v1](https://arxiv.org/pdf/2512.08733v1)**

> **作者:** Kuniko Paxton; Zeinab Dehghani; Koorosh Aslansefat; Dhavalkumar Thakker; Yiannis Papadopoulos
>
> **摘要:** Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.
>
---
#### [new 011] Animal Re-Identification on Microcontrollers
- **分类: cs.CV**

- **简介: 该论文研究动物重识别任务，旨在解决现有模型过大、难以部署于微控制器的问题。提出轻量框架，通过改进MobileNetV2结构和数据高效微调，在低分辨率输入下实现高精度、小体积的端侧动物重识别。**

- **链接: [https://arxiv.org/pdf/2512.08198v1](https://arxiv.org/pdf/2512.08198v1)**

> **作者:** Yubo Chen; Di Zhao; Yun Sing Koh; Talia Xu
>
> **摘要:** Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.
>
---
#### [new 012] GeoLoom: High-quality Geometric Diagram Generation from Textual Input
- **分类: cs.CV**

- **简介: 该论文研究文本到几何图形的生成任务，旨在解决生成结果空间准确性不足的问题。提出GeoLoom框架，通过自然语言转形式语言GeoLingua并求解坐标，提升几何图生成的精确性与可解释性。**

- **链接: [https://arxiv.org/pdf/2512.08180v1](https://arxiv.org/pdf/2512.08180v1)**

> **作者:** Xiaojing Wei; Ting Zhang; Wei He; Jingdong Wang; Hua Huang
>
> **摘要:** High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.
>
---
#### [new 013] Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery
- **分类: cs.CV**

- **简介: 该论文研究足球运动员跟踪任务，旨在解决遮挡、外观相似和密集场景下的跟踪难题。提出一种结合SAM初始化、CSRT跟踪与基于球衣颜色重识别的轻量方法，提升遮挡恢复能力，并在多维度指标下验证其有效性。**

- **链接: [https://arxiv.org/pdf/2512.08467v1](https://arxiv.org/pdf/2512.08467v1)**

> **作者:** Chamath Ranasinghe; Uthayasanker Thayasivam
>
> **备注:** 8 pages, 5 figures
>
> **摘要:** Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.
>
---
#### [new 014] Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries
- **分类: cs.CV**

- **简介: 该论文研究鱼类细粒度重识别任务，旨在解决电子监控中视频数据过多难以人工审核的问题。作者构建了AutoFish数据集，提出基于视觉Transformer的深度学习 pipeline，结合难样本挖掘与定制化图像变换，显著提升重识别精度。**

- **链接: [https://arxiv.org/pdf/2512.08400v1](https://arxiv.org/pdf/2512.08400v1)**

> **作者:** Samitha Nuwan Thilakarathna; Ercan Avsar; Martin Mathias Nielsen; Malte Pedersen
>
> **备注:** The paper has been accepted for publication at Northern Lights Deep Learning (NLDL) Conference 2025
>
> **摘要:** Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git
>
---
#### [new 015] InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models
- **分类: cs.CV; cs.AI**

- **简介: 该论文研究视觉-语言模型的高效长序列处理，旨在解决传统方法在扩展序列长度时性能下降或计算复杂度高的问题。提出InfiniteVL架构，结合滑动窗口与线性注意力，实现低资源下高性能、无限输入的多模态理解。**

- **链接: [https://arxiv.org/pdf/2512.08829v1](https://arxiv.org/pdf/2512.08829v1)**

> **作者:** Hongyuan Tao; Bencheng Liao; Shaoyu Chen; Haoran Yin; Qian Zhang; Wenyu Liu; Xinggang Wang
>
> **备注:** 16 pages, 8 figures, conference or other essential info
>
> **摘要:** Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.
>
---
#### [new 016] SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos
- **分类: cs.CV**

- **简介: 该论文属于4D人体网格重建任务，旨在解决视频中人体姿态恢复的时序不一致与遮挡问题。提出无需训练的SAM-Body4D框架，利用视频分割与遮挡感知模块生成时序一致、鲁棒的多人3D人体轨迹。**

- **链接: [https://arxiv.org/pdf/2512.08406v1](https://arxiv.org/pdf/2512.08406v1)**

> **作者:** Mingqi Gao; Yunqi Miao; Jungong Han
>
> **摘要:** Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.
>
---
#### [new 017] Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于图像生成任务，旨在解决扩散模型中视觉伪影和不真实区域的问题。作者提出“自 refining 扩散”框架，利用可解释AI生成缺陷激活图，定位并优化生成过程中的瑕疵区域，提升图像质量，在多种模型和任务上取得显著改进。**

- **链接: [https://arxiv.org/pdf/2512.08774v1](https://arxiv.org/pdf/2512.08774v1)**

> **作者:** Seoyeon Lee; Gwangyeol Yu; Chaewon Kim; Jonghyuk Park
>
> **备注:** 10 pages, 9 figures, 7 tables
>
> **摘要:** Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.
>
---
#### [new 018] Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge
- **分类: cs.CV**

- **简介: 该论文聚焦于从口腔内3D扫描中检测牙齿解剖标志点的任务，旨在解决因牙齿形态复杂和个体差异带来的精确定位难题。通过组织3DTeethLand挑战赛，发布了首个公开3D牙齿标志点数据集，推动深度学习方法在临床正畸中的应用。**

- **链接: [https://arxiv.org/pdf/2512.08323v1](https://arxiv.org/pdf/2512.08323v1)**

> **作者:** Achraf Ben-Hamadou; Nour Neifar; Ahmed Rekik; Oussama Smaoui; Firas Bouzguenda; Sergi Pujades; Niels van Nistelrooij; Shankeeth Vinayahalingam; Kaibo Shi; Hairong Jin; Youyi Zheng; Tibor Kubík; Oldřich Kodym; Petr Šilling; Kateřina Trávníčková; Tomáš Mojžiš; Jan Matula; Jeffry Hartanto; Xiaoying Zhu; Kim-Ngan Nguyen; Tudor Dascalu; Huikai Wu; and Weijie Liu; Shaojie Zhuang; Guangshun Wei; Yuanfeng Zhou
>
> **备注:** MICCAI 2024, 3DTeethLand, Challenge report, under review
>
> **摘要:** Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.
>
---
#### [new 019] What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance
- **分类: cs.CV**

- **简介: 该论文研究行人重识别（ReID）中语义属性的重要性，提出MoSAIC-ReID框架，通过LoRA专家和路由机制量化各属性贡献。旨在揭示模型依赖的关键特征，提升模型可解释性，并指导语义知识的实践应用。**

- **链接: [https://arxiv.org/pdf/2512.08697v1](https://arxiv.org/pdf/2512.08697v1)**

> **作者:** Athena Psalta; Vasileios Tsironis; Konstantinos Karantzalos
>
> **摘要:** State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID
>
---
#### [new 020] FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models
- **分类: cs.CV; cs.AI**

- **简介: 该论文聚焦地图视觉问答任务，旨在解决现有模型在多步、跨图地理推理上的不足。作者提出FRIEDA基准，包含真实地图与需多步推理的复杂问题，评估模型对拓扑、度量和方向关系的理解，揭示当前模型与人类性能间存在显著差距。**

- **链接: [https://arxiv.org/pdf/2512.08016v1](https://arxiv.org/pdf/2512.08016v1)**

> **作者:** Jiyoon Pyo; Yuankun Jiao; Dongwon Jung; Zekun Li; Leeje Jang; Sofia Kirsanova; Jina Kim; Yijun Lin; Qin Liu; Junyi Xie; Hadi Askari; Nan Xu; Muhao Chen; Yao-Yi Chiang
>
> **摘要:** Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.
>
---
#### [new 021] Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models
- **分类: cs.CV; cs.AI**

- **简介: 该论文针对多模态大推理模型（MLRM）通过层级推理推断图像地理隐私的问题，提出名为ReasonBreak的对抗框架，通过概念感知扰动破坏推理链。并构建GeoPrivacy-6K数据集验证，显著提升地理隐私保护效果。**

- **链接: [https://arxiv.org/pdf/2512.08503v1](https://arxiv.org/pdf/2512.08503v1)**

> **作者:** Jiaming Zhang; Che Wang; Yang Cao; Longtao Huang; Wei Yang Bryan Lim
>
> **摘要:** Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.
>
---
#### [new 022] LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training
- **分类: cs.CV**

- **简介: 该论文针对腹腔镜分割中标注稀缺与语义不一致问题，提出LapFM模型。通过构建层次化概念体系和置信度驱动的伪标签迭代机制，利用海量无标签数据预训练，实现跨粒度手术目标的通用分割。**

- **链接: [https://arxiv.org/pdf/2512.08439v1](https://arxiv.org/pdf/2512.08439v1)**

> **作者:** Qing Xu; Kun Yuan; Yuxiang Luo; Yuhao Zhai; Wenting Duan; Nassir Navab; Zhen Chen
>
> **摘要:** Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.
>
---
#### [new 023] Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation
- **分类: cs.CV; cs.AI; cs.GR; cs.LG**

- **简介: 该论文提出Terrain Diffusion，旨在解决传统Perlin噪声在地形生成中缺乏真实感与大尺度连贯性的问题。其结合扩散模型与无限生成算法InfiniteDiffusion，实现可随机访问、种子一致、内存恒定的高质量无限地形生成。**

- **链接: [https://arxiv.org/pdf/2512.08309v1](https://arxiv.org/pdf/2512.08309v1)**

> **作者:** Alexander Goslin
>
> **备注:** Project website: https://xandergos.github.io/terrain-diffusion/ Code: https://github.com/xandergos/terrain-diffusion/
>
> **摘要:** For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.
>
---
#### [new 024] UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation
- **分类: cs.CV**

- **简介: 该论文研究内容感知布局生成任务，旨在统一多种条件下的布局生成。现有方法无法兼顾多样性与统一性，本文提出UniLayDiff，首次用单一扩散Transformer模型融合多模态输入与关系约束，实现端到端统一生成，显著提升布局质量与任务覆盖范围。**

- **链接: [https://arxiv.org/pdf/2512.08897v1](https://arxiv.org/pdf/2512.08897v1)**

> **作者:** Zeyang Liu; Le Wang; Sanping Zhou; Yuxuan Wu; Xiaolong Sun; Gang Hua; Haoxiang Li
>
> **摘要:** Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.
>
---
#### [new 025] On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs
- **分类: cs.CV**

- **简介: 该论文研究基于多相机系统的实时大规模3D重建。针对单目方案因视场受限导致覆盖不全的问题，提出首个面向多相机设备的在线3DGS框架，通过无标定初始化、轻量优化与高斯采样策略，实现高效、低漂移、高保真的大场景在线重建。**

- **链接: [https://arxiv.org/pdf/2512.08498v1](https://arxiv.org/pdf/2512.08498v1)**

> **作者:** Yijia Guo; Tong Hu; Zhiwei Li; Liwen Hu; Keming Qian; Xitong Lin; Shengbo Chen; Tiejun Huang; Lei Ma
>
> **摘要:** Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.
>
---
#### [new 026] SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection
- **分类: cs.CV**

- **简介: 该论文研究3D目标检测中的迁移学习，探索基于大规模Waymo数据集的提示调优方法。提出场景导向的提示池（SOP²），验证其在不同场景下的适应性，旨在挖掘提示机制在3D视觉中的潜力。**

- **链接: [https://arxiv.org/pdf/2512.08223v1](https://arxiv.org/pdf/2512.08223v1)**

> **作者:** Ching-Hung Cheng; Hsiu-Fu Wu; Bing-Chen Wu; Khanh-Phong Bui; Van-Tin Luu; Ching-Chun Huang
>
> **摘要:** With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.
>
---
#### [new 027] A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation
- **分类: cs.CV**

- **简介: 该论文针对蘑菇分割中真实标注数据 costly 的问题，提出结合 3D 渲染与扩散模型的合成数据生成 pipeline，实现高逼真度与精确标注。用于训练的 Mask R-CNN 在零样本设置下达到 SOTA 分割性能。**

- **链接: [https://arxiv.org/pdf/2512.08747v1](https://arxiv.org/pdf/2512.08747v1)**

> **作者:** Artúr I. Károly; Péter Galambos
>
> **备注:** 20 pages, 8 figures
>
> **摘要:** Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.
>
---
#### [new 028] MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance
- **分类: cs.CV; cs.AI**

- **简介: 该论文研究文档图像中的阴影去除任务，旨在保留文本等高频细节。提出MatteViT框架，结合高频率增强模块与基于亮度的连续阴影蒙版引导，融合空域和频域信息，在公开数据集上实现最优性能，并提升OCR效果。**

- **链接: [https://arxiv.org/pdf/2512.08789v1](https://arxiv.org/pdf/2512.08789v1)**

> **作者:** Chaewon Kim; Seoyeon Lee; Jonghyuk Park
>
> **备注:** 10 pages, 7 figures, 5 tables
>
> **摘要:** Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.
>
---
#### [new 029] Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation
- **分类: cs.CV**

- **简介: 该论文研究自回归-扩散混合生成的加速问题，提出Fast-ARDiff框架。通过熵感知的推测解码降低AR部分延迟，并结合动态调度与联合蒸馏优化扩散解码，实现高效少步生成，在图像与文本条件生成中显著提速。**

- **链接: [https://arxiv.org/pdf/2512.08537v1](https://arxiv.org/pdf/2512.08537v1)**

> **作者:** Zhen Zou; Xiaoxiao Ma; Jie Huang; Zichao Yu; Feng Zhao
>
> **摘要:** Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.
>
---
#### [new 030] SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification
- **分类: cs.CV**

- **简介: 该论文属于医学图像解释任务，旨在解决现有模型解释方法在视网膜病变分类中缺乏平滑性和稀疏性的问题。作者提出SSplain方法，通过ADMM优化框架生成结构保持的像素级解释，提升了可解释性与临床一致性。**

- **链接: [https://arxiv.org/pdf/2512.08038v1](https://arxiv.org/pdf/2512.08038v1)**

> **作者:** Elifnur Sunger; Tales Imbiriba; Peter Campbell; Deniz Erdogmus; Stratis Ioannidis; Jennifer Dy
>
> **备注:** 20 pages, 16 figures
>
> **摘要:** Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.
>
---
#### [new 031] Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment
- **分类: cs.CV**

- **简介: 该论文研究手语翻译与对齐任务，旨在将连续手语视频转化为文本并实现时间对齐。提出轻量模型结合滑动感知网络与多任务训练，在BSL和ASL数据集上实现最先进性能，支持跨语言迁移与零样本泛化。**

- **链接: [https://arxiv.org/pdf/2512.08040v1](https://arxiv.org/pdf/2512.08040v1)**

> **作者:** Youngjoon Jang; Liliane Momeni; Zifan Jiang; Joon Son Chung; Gül Varol; Andrew Zisserman
>
> **摘要:** Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.
>
---
#### [new 032] Accelerated Rotation-Invariant Convolution for UAV Image Segmentation
- **分类: cs.CV; cs.RO**

- **简介: 该论文针对无人机图像分割中目标方向多变的问题，提出一种GPU优化的旋转不变卷积方法。通过共享旋转滤波器的数据，减少计算冗余和内存访问，提升效率与能效，并集成到U-Net中，在保持精度的同时显著加速训练并降低能耗。**

- **链接: [https://arxiv.org/pdf/2512.08888v1](https://arxiv.org/pdf/2512.08888v1)**

> **作者:** Manduhu Manduhu; Alexander Dow; Gerard Dooly; James Riordan
>
> **摘要:** Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles. Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\(\times\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\(\times\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.
>
---
#### [new 033] SFP: Real-World Scene Recovery Using Spatial and Frequency Priors
- **分类: cs.CV**

- **简介: 该论文针对真实场景恢复任务，解决现有方法因单一先验或合成数据导致的泛化不足问题。提出空间与频率双先验：空间先验估计传输图以去散射，频率先验自适应增强频谱，并融合空间、频率信息与显著特征，提升多退化条件下的恢复效果。**

- **链接: [https://arxiv.org/pdf/2512.08254v1](https://arxiv.org/pdf/2512.08254v1)**

> **作者:** Yun Liu; Tao Li; Cosmin Ancuti; Wenqi Ren; Weisi Lin
>
> **备注:** 10 pages, 13 figures
>
> **摘要:** Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.
>
---
#### [new 034] Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement
- **分类: cs.CV**

- **简介: 该论文属于3D生成任务，旨在解决现有3D生成模型在纹理细节上缺乏真实感的问题。作者提出Photo3D框架，利用GPT-4o生成图像数据，通过结构对齐的多视图合成与细节增强方法，提升3D模型外观的真实性和结构一致性。**

- **链接: [https://arxiv.org/pdf/2512.08535v1](https://arxiv.org/pdf/2512.08535v1)**

> **作者:** Xinyue Liang; Zhinyuan Ma; Lingchen Sun; Yanjun Guo; Lei Zhang
>
> **摘要:** Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.
>
---
#### [new 035] MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models
- **分类: cs.CV; cs.AI**

- **简介: 该论文聚焦多模态模型的视觉链式推理任务，旨在检验模型推理过程的视觉一致性和逻辑连贯性。提出MM-CoT基准，通过选择满足双重约束的事件链并引入对抗干扰项，揭示模型在真实推理能力上的不足。**

- **链接: [https://arxiv.org/pdf/2512.08228v1](https://arxiv.org/pdf/2512.08228v1)**

> **作者:** Jusheng Zhang; Kaitong Cai; Xiaoyang Guo; Sidi Liu; Qinhan Lv; Ruiqi Chen; Jing Yang; Yijia Fan; Xiaofei Sun; Jian Wang; Ziliang Chen; Liang Lin; Keze Wang
>
> **摘要:** The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.
>
---
#### [new 036] An Iteration-Free Fixed-Point Estimator for Diffusion Inversion
- **分类: cs.CV**

- **简介: 该论文研究扩散模型中的图像重构任务，旨在解决现有固定点迭代方法计算成本高、超参数难调的问题。作者提出一种无需迭代的固定点估计器，通过误差近似实现高效准确的逆扩散，理论分析表明其估计无偏且方差低，实验验证了优越的重构性能。**

- **链接: [https://arxiv.org/pdf/2512.08547v1](https://arxiv.org/pdf/2512.08547v1)**

> **作者:** Yifei Chen; Kaiyu Song; Yan Pan; Jianxing Yu; Jian Yin; Hanjiang Lai
>
> **摘要:** Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.
>
---
#### [new 037] Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance
- **分类: cs.CV**

- **简介: 该论文属于视频生成任务，旨在解决现有方法运动控制粗糙、扩展性差的问题。提出Wan-Move框架，通过潜在轨迹引导实现细粒度、高精度运动控制，无需修改模型结构，可扩展性强，生成视频质量媲美商用方案。**

- **链接: [https://arxiv.org/pdf/2512.08765v1](https://arxiv.org/pdf/2512.08765v1)**

> **作者:** Ruihang Chu; Yefei He; Zhekai Chen; Shiwei Zhang; Xiaogang Xu; Bin Xia; Dingdong Wang; Hongwei Yi; Xihui Liu; Hengshuang Zhao; Yu Liu; Yingya Zhang; Yujiu Yang
>
> **备注:** NeurlPS 2025. Code and data available at https://github.com/ali-vilab/Wan-Move
>
> **摘要:** We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.
>
---
#### [new 038] OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds
- **分类: cs.CV**

- **简介: 该论文属3D建筑重建任务，旨在从含噪声、密度不均的LiDAR点云中恢复高保真建筑表面。提出OCCDiff模型，结合潜在扩散与函数自编码器，通过点编码器引入条件特征，并采用多任务训练提升鲁棒性。**

- **链接: [https://arxiv.org/pdf/2512.08506v1](https://arxiv.org/pdf/2512.08506v1)**

> **作者:** Jialu Sui; Rui Liu; Hongsheng Zhang
>
> **摘要:** A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.
>
---
#### [new 039] Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems
- **分类: cs.CV; cs.RO**

- **简介: 该论文研究RGB-D深度补全任务，旨在解决现有方法中稀疏深度采样不真实的问题。作者提出一种几何感知的稀疏深度采样策略，利用PCA估计表面法向，生成更符合实际传感器特性的稀疏深度分布，并结合Marigold-DC模型提升补全精度与训练真实性。**

- **链接: [https://arxiv.org/pdf/2512.08229v1](https://arxiv.org/pdf/2512.08229v1)**

> **作者:** Tony Salloom; Dandi Zhou; Xinhai Sun
>
> **摘要:** Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.
>
---
#### [new 040] Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds
- **分类: cs.CV**

- **简介: 该论文研究3D点云的自监督学习，旨在解决现有生成式方法难以捕捉高阶判别特征的问题。作者提出双分支中心-周围对比（CSCon）框架，通过分区域掩码和局部对比损失，提升特征表示能力，在多种协议下取得领先性能。**

- **链接: [https://arxiv.org/pdf/2512.08673v1](https://arxiv.org/pdf/2512.08673v1)**

> **作者:** Shaofeng Zhang; Xuanqi Chen; Xiangdong Zhang; Sitong Wu; Junchi Yan
>
> **备注:** 16 pages, 6 figures
>
> **摘要:** Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \textbf{7.9\%}, \textbf{6.7\%}, and \textbf{10.3\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.
>
---
#### [new 041] PAVAS: Physics-Aware Video-to-Audio Synthesis
- **分类: cs.CV; cs.MM; cs.SD**

- **简介: 该论文研究视频到音频生成任务，旨在解决现有方法忽略物理因素导致声音不真实的问题。提出PAVAS模型，通过引入物理参数估计和物理感知适配器，结合质量、运动轨迹等物理信息生成更符合真实物理规律的音频，并构建新基准与指标验证效果。**

- **链接: [https://arxiv.org/pdf/2512.08282v1](https://arxiv.org/pdf/2512.08282v1)**

> **作者:** Oh Hyun-Bin; Yuhta Takida; Toshimitsu Uesaka; Tae-Hyun Oh; Yuki Mitsufuji
>
> **摘要:** Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.
>
---
#### [new 042] Trajectory Densification and Depth from Perspective-based Blur
- **分类: cs.CV**

- **简介: 该论文研究无稳定器拍摄下的视角模糊，提出通过分析视频模糊模式估计深度并稠密化轨迹。属于深度估计与运动恢复任务，解决手持拍摄中模糊导致的精度下降问题，结合光学设计与视觉语言模型实现高精度深度与轨迹重建。**

- **链接: [https://arxiv.org/pdf/2512.08627v1](https://arxiv.org/pdf/2512.08627v1)**

> **作者:** Tianchen Qiu; Qirun Zhang; Jiajian He; Zhengyue Zhuge; Jiahui Xu; Yueting Chen
>
> **摘要:** In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.
>
---
#### [new 043] Generation is Required for Data-Efficient Perception
- **分类: cs.CV; cs.LG**

- **简介: 该论文探讨视觉感知中生成式方法的必要性，聚焦于组合性泛化问题。通过理论与实验分析，证明生成式模型通过解码器约束和反演可有效实现组合泛化，而非生成式方法难以满足所需归纳偏置，泛化能力受限。**

- **链接: [https://arxiv.org/pdf/2512.08854v1](https://arxiv.org/pdf/2512.08854v1)**

> **作者:** Jack Brady; Bernhard Schölkopf; Thomas Kipf; Simon Buchholz; Wieland Brendel
>
> **备注:** Preprint
>
> **摘要:** It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.
>
---
#### [new 044] Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning
- **分类: cs.CV**

- **简介: 该论文研究持续测试时适应（CTTA）任务，解决测试阶段分布偏移导致的性能下降问题。提出简单随机掩码方法M2A，结合预测一致性与熵最小化目标，无需复杂设计即可有效提升模型鲁棒性。**

- **链接: [https://arxiv.org/pdf/2512.08048v1](https://arxiv.org/pdf/2512.08048v1)**

> **作者:** Chandler Timm C. Doloriel
>
> **备注:** ongoing work
>
> **摘要:** Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.
>
---
#### [new 045] Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening
- **分类: cs.CV**

- **简介: 该论文研究遥感全色锐化任务，旨在解决现有方法对特征区域异质性适应不足、计算成本高的问题。提出双模态双自适应掩膜卷积（Bi²MAC），通过软硬掩膜引导异质与冗余特征分路处理，在降低计算量的同时提升融合性能。**

- **链接: [https://arxiv.org/pdf/2512.08331v1](https://arxiv.org/pdf/2512.08331v1)**

> **作者:** Xianghong Xiao; Zeyu Xia; Zhou Fei; Jinliang Xiao; Haorui Chen; Liangjian Deng
>
> **摘要:** Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.
>
---
#### [new 046] Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference
- **分类: cs.CV**

- **简介: 该论文聚焦视觉语言模型（VLM）在空间推理中的可靠性问题，提出Tri-Bench基准，评估相机倾斜和物体干扰下VLM对平面三角形几何关系的理解。实验表明VLM依赖2D图像线索，难以利用参考框提示恢复3D几何，且对罕见三角形类别识别能力差。**

- **链接: [https://arxiv.org/pdf/2512.08860v1](https://arxiv.org/pdf/2512.08860v1)**

> **作者:** Amit Bendkhale
>
> **备注:** 6 pages, 3 figures. Code and data: https://github.com/Amiton7/Tri-Bench. Accepted to the AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)
>
> **摘要:** Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.
>
---
#### [new 047] OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation
- **分类: cs.CV**

- **简介: 该论文属于图像生成与编辑任务，旨在解决现有方法在多主体复杂场景中身份保持差的问题。作者提出OpenSubject数据集与构建流程，并设计基准评测模型，提升生成与编辑中的身份一致性和场景复杂性处理能力。**

- **链接: [https://arxiv.org/pdf/2512.08294v1](https://arxiv.org/pdf/2512.08294v1)**

> **作者:** Yexin Liu; Manyuan Zhang; Yueze Wang; Hongyu Li; Dian Zheng; Weiming Zhang; Changsheng Lu; Xunliang Cai; Yan Feng; Peng Pei; Harry Yang
>
> **摘要:** Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.
>
---
#### [new 048] RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于多传感器标定任务，旨在解决自动驾驶中LiDAR、RADAR与相机因振动和漂移导致的外参失准问题。提出RLCNet——端到端可训练的深度学习框架，实现三传感器在线联合标定，具备实时性、抗噪性和鲁棒性。**

- **链接: [https://arxiv.org/pdf/2512.08262v1](https://arxiv.org/pdf/2512.08262v1)**

> **作者:** Hafeez Husain Cholakkal; Stefano Arrigoni; Francesco Braghin
>
> **摘要:** Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.
>
---
#### [new 049] SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing
- **分类: cs.CV**

- **简介: 该论文研究遥感图像中的视觉定位任务，旨在提升视觉-语言模型对卫星图像中物体的精确定位能力。提出SATGround方法，通过引入结构化定位机制和专用控制令牌，实现语言与空间信息的联合推理，显著提升了定位性能。**

- **链接: [https://arxiv.org/pdf/2512.08881v1](https://arxiv.org/pdf/2512.08881v1)**

> **作者:** Aysim Toker; Andreea-Maria Oncescu; Roy Miles; Ismail Elezi; Jiankang Deng
>
> **摘要:** Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.
>
---
#### [new 050] Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices
- **分类: cs.CV; cs.DC**

- **简介: 该论文研究联邦学习下的皮肤病变分类任务，解决模型大、计算贵及数据隐私问题。提出基于输出分布偏度的剪枝方法，压缩多模态Swin Transformer，在边缘设备上实现高效部署，兼顾精度与隐私。**

- **链接: [https://arxiv.org/pdf/2512.08751v1](https://arxiv.org/pdf/2512.08751v1)**

> **作者:** Kuniko Paxton; Koorosh Aslansefat; Dhavalkumar Thakker; Yiannis Papadopoulos
>
> **摘要:** In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.
>
---
#### [new 051] Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models
- **分类: cs.CV**

- **简介: 该论文研究条件扩散模型中的图文对齐问题，旨在早期检测生成过程中的提示词与隐空间表征的不匹配。提出NoisyCLIP方法，在去噪过程中实时评估对齐性，减少50%计算成本并保持98%的CLIP性能，实现高效高质量图像生成。**

- **链接: [https://arxiv.org/pdf/2512.08505v1](https://arxiv.org/pdf/2512.08505v1)**

> **作者:** Vasco Ramos; Regev Cohen; Idan Szpektor; Joao Magalhaes
>
> **摘要:** Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.
>
---
#### [new 052] Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation
- **分类: cs.CV**

- **简介: 该论文研究少样本3D点云语义分割，旨在解决现有方法因忽略查询数据导致的原型偏差问题。提出查询感知枢纽原型学习（QHP），通过构建支持与查询点间的二部图生成相关原型，并优化原型分布，提升跨集语义一致性与分割性能。**

- **链接: [https://arxiv.org/pdf/2512.08253v1](https://arxiv.org/pdf/2512.08253v1)**

> **作者:** YiLin Zhou; Lili Wei; Zheming Xu; Ziyi Chen; Congyan Lang
>
> **摘要:** Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.
>
---
#### [new 053] Modular Neural Image Signal Processing
- **分类: cs.CV**

- **简介: 该论文研究神经图像信号处理（ISP）任务，旨在解决传统方法缺乏灵活性与可调试性的问题。提出一种模块化神经ISP框架，支持多阶段控制、跨相机泛化及风格定制，并开发可交互编辑工具，实现高质量、可重渲染的图像处理。**

- **链接: [https://arxiv.org/pdf/2512.08564v1](https://arxiv.org/pdf/2512.08564v1)**

> **作者:** Mahmoud Afifi; Zhongling Wang; Ran Zhang; Michael S. Brown
>
> **摘要:** This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM
>
---
#### [new 054] Leveraging Multispectral Sensors for Color Correction in Mobile Cameras
- **分类: cs.CV**

- **简介: 该论文属图像处理任务，旨在解决移动相机色彩校正精度不足问题。提出一种融合高分辨率RGB与低分辨率多光谱传感器的端到端学习框架，统一色彩校正流程，显著提升色彩准确性和稳定性。**

- **链接: [https://arxiv.org/pdf/2512.08441v1](https://arxiv.org/pdf/2512.08441v1)**

> **作者:** Luca Cogo; Marco Buzzelli; Simone Bianco; Javier Vazquez-Corral; Raimondo Schettini
>
> **摘要:** Recent advances in snapshot multispectral (MS) imaging have enabled compact, low-cost spectral sensors for consumer and mobile devices. By capturing richer spectral information than conventional RGB sensors, these systems can enhance key imaging tasks, including color correction. However, most existing methods treat the color correction pipeline in separate stages, often discarding MS data early in the process. We propose a unified, learning-based framework that (i) performs end-to-end color correction and (ii) jointly leverages data from a high-resolution RGB sensor and an auxiliary low-resolution MS sensor. Our approach integrates the full pipeline within a single model, producing coherent and color-accurate outputs. We demonstrate the flexibility and generality of our framework by refactoring two different state-of-the-art image-to-image architectures. To support training and evaluation, we construct a dedicated dataset by aggregating and repurposing publicly available spectral datasets, rendering under multiple RGB camera sensitivities. Extensive experiments show that our approach improves color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. Datasets, code, and models will be made available upon acceptance.
>
---
#### [new 055] Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation
- **分类: cs.CV**

- **简介: 该论文提出Chain-of-Image Generation（CoIG）框架，旨在提升文本到图像生成的可监控性与可控性。通过将生成过程分解为语义连贯的步骤，实现人类可理解的、逐步的图像合成，增强透明度与组成鲁棒性。**

- **链接: [https://arxiv.org/pdf/2512.08645v1](https://arxiv.org/pdf/2512.08645v1)**

> **作者:** Young Kyung Kim; Oded Schlesinger; Yuzhou Zhao; J. Matias Di Martino; Guillermo Sapiro
>
> **备注:** 19 pages, 13 figures
>
> **摘要:** While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.
>
---
#### [new 056] TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels
- **分类: cs.CV**

- **简介: 该论文研究单目视频中的3D跟踪任务，旨在解决现有方法难以分离相机运动与前景动态、且无法密集跟踪新出现物体的问题。作者提出TrackingWorld，通过跟踪上采样器生成稠密2D轨迹，并优化恢复世界坐标系下的3D轨迹，实现对几乎所有像素的稠密3D跟踪。**

- **链接: [https://arxiv.org/pdf/2512.08358v1](https://arxiv.org/pdf/2512.08358v1)**

> **作者:** Jiahao Lu; Weitao Xiong; Jiacheng Deng; Peng Li; Tianyu Huang; Zhiyang Dou; Cheng Lin; Sai-Kit Yeung; Yuan Liu
>
> **备注:** Accepted by NeurIPS 2025. Project Page: https://igl-hkust.github.io/TrackingWorld.github.io/
>
> **摘要:** Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.
>
---
#### [new 057] Thinking with Images via Self-Calling Agent
- **分类: cs.CV**

- **简介: 该论文研究视觉推理任务，旨在解决多模态思维链训练依赖稀缺高质量数据的问题。提出sCoT方法，将图像推理转化为语言为主的自调用思维链，通过子代理分治子任务，结合参数共享与策略优化，提升性能与训练效率。**

- **链接: [https://arxiv.org/pdf/2512.08511v1](https://arxiv.org/pdf/2512.08511v1)**

> **作者:** Wenxi Yang; Yuzhong Zhao; Fang Wan; Qixiang Ye
>
> **备注:** Code would be released at https://github.com/YWenxi/think-with-images-through-self-calling soon
>
> **摘要:** Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\%$ with $\sim 75\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.
>
---
#### [new 058] A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation
- **分类: cs.CV; cs.AI; math.NA**

- **简介: 该论文属于图像生成任务，旨在解决彩色图像生成中通道相关性被忽略及色彩偏差问题。提出四元数Wasserstein距离与对偶理论，构建新型生成对抗网络，提升生成效率与图像质量。**

- **链接: [https://arxiv.org/pdf/2512.08542v1](https://arxiv.org/pdf/2512.08542v1)**

> **作者:** Zhigang Jia; Duan Wang; Hengkai Wang; Yajun Xie; Meixiang Zhao; Xiaoyu Zhao
>
> **摘要:** Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.
>
---
#### [new 059] LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models
- **分类: cs.CV**

- **简介: 该论文属于视觉生成模型个性化任务，旨在解决现有方法需大量数据和耗时优化的问题。作者提出LoFA框架，通过发现LoRA参数变化的结构化分布模式，设计两阶段超网络快速预测个性化先验，实现秒级高效适配，性能超越传统LoRA。**

- **链接: [https://arxiv.org/pdf/2512.08785v1](https://arxiv.org/pdf/2512.08785v1)**

> **作者:** Yiming Hao; Mutian Xu; Chongjie Ye; Jie Qin; Shunlin Lu; Yipeng Qin; Xiaoguang Han
>
> **备注:** Project page: https://jaeger416.github.io/lofa/
>
> **摘要:** Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: https://jaeger416.github.io/lofa/.
>
---
#### [new 060] Near-real time fires detection using satellite imagery in Sudan conflict
- **分类: cs.CV; cs.AI**

- **简介: 该论文属遥感与冲突监测任务，旨在解决战争中火灾快速检测问题。利用Planet Labs的4-band卫星影像和深度学习模型，实现近实时火损监测，在苏丹五个案例中验证了方法的有效性，精度优于基线方法。**

- **链接: [https://arxiv.org/pdf/2512.07925v1](https://arxiv.org/pdf/2512.07925v1)**

> **作者:** Kuldip Singh Atwal; Dieter Pfoser; Daniel Rothbart
>
> **摘要:** The challenges of ongoing war in Sudan highlight the need for rapid monitoring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitoring. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our results indicate that using 8-band imagery or time series of such imagery only result in marginal gains.
>
---
#### [new 061] MVP: Multiple View Prediction Improves GUI Grounding
- **分类: cs.CV**

- **简介: 该论文研究GUI指代任务，解决现有模型坐标预测不稳定问题。提出无需训练的多视角预测框架MVP，通过注意力引导生成多视图，聚类融合预测结果，显著提升定位精度。**

- **链接: [https://arxiv.org/pdf/2512.08529v1](https://arxiv.org/pdf/2512.08529v1)**

> **作者:** Yunzhu Zhang; Zeyu Pan; Zhengwen Zeng; Shuheng Shen; Changhua Meng; Linchao Zhu
>
> **摘要:** GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.
>
---
#### [new 062] Astra: General Interactive World Model with Autoregressive Denoising
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文提出Astra，一种通用交互式世界模型，旨在解决长视野未来预测与多模态动作交互问题。通过自回归去噪架构、时序因果注意力和动作感知适配器，实现高保真、长时程、动作对齐的视频预测，支持自动驾驶、机器人操作等多样化场景。**

- **链接: [https://arxiv.org/pdf/2512.08931v1](https://arxiv.org/pdf/2512.08931v1)**

> **作者:** Yixuan Zhu; Jiaqi Feng; Wenzhao Zheng; Yuan Gao; Xin Tao; Pengfei Wan; Jie Zhou; Jiwen Lu
>
> **备注:** Code is available at: https://github.com/EternalEvan/Astra
>
> **摘要:** Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.
>
---
#### [new 063] Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts
- **分类: cs.CV; cs.LG**

- **简介: 该论文研究视觉模型解释方法在分布偏移下的鲁棒性问题，提出一种结合子模优化与梯度不确定性估计的子集选择框架，提升解释的稳定性和可信度，无需额外训练，增强了ID和OOD场景下的解释质量。**

- **链接: [https://arxiv.org/pdf/2512.08445v1](https://arxiv.org/pdf/2512.08445v1)**

> **作者:** Madhav Gupta; Vishak Prasad C; Ganesh Ramakrishnan
>
> **摘要:** Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.
>
---
#### [new 064] LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception
- **分类: cs.CV; cs.RO**

- **简介: 该论文研究夜间视觉感知任务，解决光照不足导致的感知性能下降问题。提出LiDAS系统，通过动态调控车灯照明增强关键区域，提升检测与分割效果，实现无需训练的零样本泛化，在同等或更低功耗下显著提升性能。**

- **链接: [https://arxiv.org/pdf/2512.08912v1](https://arxiv.org/pdf/2512.08912v1)**

> **作者:** Simon de Moreau; Andrei Bursuc; Hafid El-Idrissi; Fabien Moutarde
>
> **备注:** Preprint. 12 pages, 9 figures. Project page: https://simondemoreau.github.io/LiDAS/
>
> **摘要:** Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.
>
---
#### [new 065] New VVC profiles targeting Feature Coding for Machines
- **分类: cs.CV**

- **简介: 该论文针对机器视觉任务中的特征压缩问题，研究VVC在MPEG-AI FCM标准下的应用。通过分析编码工具对压缩效率和任务精度的影响，提出三种轻量级VVC配置，在显著降低编码时间的同时保持高压缩性能。**

- **链接: [https://arxiv.org/pdf/2512.08227v1](https://arxiv.org/pdf/2512.08227v1)**

> **作者:** Md Eimran Hossain Eimon; Ashan Perera; Juan Merlos; Velibor Adzic; Hari Kalva
>
> **备注:** Accepted for presentation at ICIP 2025 workshop on Coding for Machines
>
> **摘要:** Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.
>
---
#### [new 066] Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions
- **分类: cs.CV**

- **简介: 该论文属图像增强任务，旨在解决复杂光照下图像质量下降与噪声放大问题。提出一种新框架，结合梯度域滤波、Retinex分解、多曝光融合等技术，实现光照校正、细节增强与噪声抑制的协同优化。**

- **链接: [https://arxiv.org/pdf/2512.08378v1](https://arxiv.org/pdf/2512.08378v1)**

> **作者:** Jing Tao; You Li; Banglei Guan; Yang Shang; Qifeng Yu
>
> **备注:** The paper has been accepted and officially published by IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT
>
> **摘要:** Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.
>
---
#### [new 067] Beyond Real Weights: Hypercomplex Representations for Stable Quantization
- **分类: cs.CV; cs.CL**

- **简介: 该论文研究多模态语言模型的高效部署问题，提出用渐进式超复数乘法（PHM）层替代密集前馈网络，结合残差插值与知识蒸馏，实现模型压缩与加速，保持性能的同时显著降低参数量与推理延迟。**

- **链接: [https://arxiv.org/pdf/2512.08524v1](https://arxiv.org/pdf/2512.08524v1)**

> **作者:** Jawad Ibn Ahad; Maisha Rahman; Amrijit Biswas; Muhammad Rafsan Kabir; Robin Krambroeckers; Sifat Momen; Nabeel Mohammed; Shafin Rahman
>
> **备注:** Accepted in Winter Conference on Applications of Computer Vision (WACV) 2026
>
> **摘要:** Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.
>
---
#### [new 068] Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform
- **分类: cs.CV; cs.AI; cs.GR**

- **简介: 该论文聚焦神经渲染与世界模型构建，针对现有3D高斯点阵渲染工具部署复杂、动态支持弱的问题，提出Visionary——基于WebGPU的轻量级网页实时渲染平台，支持动态生成、多种高斯变体与即插即用算法，实现浏览器端高效统一的渲染与推理。**

- **链接: [https://arxiv.org/pdf/2512.08478v1](https://arxiv.org/pdf/2512.08478v1)**

> **作者:** Yuning Gong; Yifei Liu; Yifan Zhan; Muyao Niu; Xueying Li; Yuanjun Liao; Jiaming Chen; Yuanyuan Gao; Jiaqi Chen; Minming Chen; Li Zhou; Yuning Zhang; Wei Wang; Xiaoqing Hou; Huaxi Huang; Shixiang Tang; Le Ma; Dingwen Zhang; Xue Yang; Junchi Yan; Yanchi Zhang; Yinqiang Zheng; Xiao Sun; Zhihang Zhong
>
> **备注:** Project page: https://visionary-laboratory.github.io/visionary
>
> **摘要:** Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.
>
---
#### [new 069] SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation
- **分类: cs.CV**

- **简介: 该论文属图像生成与火灾检测任务，旨在解决火灾数据不足问题。提出SCU-CGAN模型，结合U-Net、CBAM和双判别器生成逼真火焰图像，提升数据集质量。实验表明生成图像更真实，增强后的数据使YOLOv5等检测模型性能显著提升。**

- **链接: [https://arxiv.org/pdf/2512.08362v1](https://arxiv.org/pdf/2512.08362v1)**

> **作者:** Ju-Young Kim; Ji-Hong Park; Gun-Woo Kim
>
> **备注:** Accepted for main track at MobieSec 2024 (not published in the proceedings)
>
> **摘要:** Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.
>
---
#### [new 070] C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition
- **分类: cs.CV**

- **简介: 该论文研究驾驶员行为识别任务，旨在解决轻量模型在跨域场景下精度低与计算成本高的问题。提出C-DIRA框架，结合动态ROI路由与域不变对抗学习，实现高效、鲁棒的细粒度行为识别。**

- **链接: [https://arxiv.org/pdf/2512.08647v1](https://arxiv.org/pdf/2512.08647v1)**

> **作者:** Keito Inoshita
>
> **摘要:** Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.
>
---
#### [new 071] Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth
- **分类: cs.CV**

- **简介: 该论文研究全环绕单目深度估计，解决现有基础模型计算量大、难以预测度量尺度深度的问题。提出一种新型知识蒸馏方法，结合尺度不变与视图关系蒸馏，提升轻量网络的深度估计精度与一致性。**

- **链接: [https://arxiv.org/pdf/2512.08700v1](https://arxiv.org/pdf/2512.08700v1)**

> **作者:** Kyumin Hwang; Wonhyeok Choi; Kiljoon Han; Wonjoon Choi; Minwoo Choi; Yongcheon Na; Minwoo Park; Sunghoon Im
>
> **备注:** Accepted at IEEE Robotics and Automation Letters (RA-L) 2026
>
> **摘要:** Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.
>
---
#### [new 072] Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval
- **分类: cs.CV**

- **简介: 该论文属多模态大模型的长视频理解任务，旨在解决因显存限制导致的处理时长受限问题。提出OneClip-RAG方法，通过单次检索关键视频片段增强理解，兼顾语义连贯与效率，并设计新数据集和训练策略提升指令跟随能力。**

- **链接: [https://arxiv.org/pdf/2512.08410v1](https://arxiv.org/pdf/2512.08410v1)**

> **作者:** Tao Chen; Shaobo Ju; Qiong Wu; Chenxin Fang; Kun Zhang; Jun Peng; Hui Li; Yiyi Zhou; Rongrong Ji
>
> **摘要:** Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.
>
---
#### [new 073] The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss
- **分类: cs.CV**

- **简介: 该论文研究多模态大模型中视觉与文本特征融合的偏差问题，发现Pre-Norm架构导致视觉信息丢失。作者提出通过在视觉投影后添加一层归一化来对齐模态间范数差异，显著提升模型性能。**

- **链接: [https://arxiv.org/pdf/2512.08374v1](https://arxiv.org/pdf/2512.08374v1)**

> **作者:** Bozhou Li; Xinda Xue; Sihan Yang; Yang Shi; Xinlong Chen; Yushuo Guan; Yuanxing Zhang; Wentao Zhang
>
> **摘要:** Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.
>
---
#### [new 074] From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis
- **分类: cs.CV**

- **简介: 该论文聚焦肺癌预后任务，旨在通过分析多重免疫荧光图像中的肿瘤微环境细胞互作关系，提升患者生存风险分层。提出HiGINE模型，结合细胞类型、形态及多尺度空间关系，并融合临床分期信息，实现更准确、鲁棒的预后预测。**

- **链接: [https://arxiv.org/pdf/2512.08572v1](https://arxiv.org/pdf/2512.08572v1)**

> **作者:** Olle Edgren Schüllerqvist; Jens Baumann; Joakim Lindblad; Love Nordling; Artur Mezheyeuski; Patrick Micke; Nataša Sladoje
>
> **备注:** 5 pages, 3 figures
>
> **摘要:** The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.
>
---
#### [new 075] Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection
- **分类: cs.CV; cs.AI**

- **简介: 该论文研究牙科影像中分层牙齿结构的语义分割任务，旨在解决现有方法对解剖层级监督弱的问题。提出一种显式嵌入层级结构的框架，通过递进预测、特征调制和一致性约束，提升分割精度与解剖合理性。**

- **链接: [https://arxiv.org/pdf/2512.07984v1](https://arxiv.org/pdf/2512.07984v1)**

> **作者:** Ryan Banks; Camila Lindoni Azevedo; Hongying Tang; Yunpeng Li
>
> **备注:** 13 pages, 7 figures, 3 tables
>
> **摘要:** Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.
>
---
#### [new 076] SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images
- **分类: cs.CV**

- **简介: 该论文研究遥感图像的开放词汇语义分割任务，旨在无需训练地实现精准分割。针对CLIP方法定位不准和流程复杂的问题，提出利用SAM 3的多头输出融合与存在性得分过滤，提升密集小目标的分割效果。**

- **链接: [https://arxiv.org/pdf/2512.08730v1](https://arxiv.org/pdf/2512.08730v1)**

> **作者:** Kaiyu Li; Shengqi Zhang; Yupeng Deng; Zhi Wang; Deyu Meng; Xiangyong Cao
>
> **摘要:** Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.
>
---
#### [new 077] Efficiently Reconstructing Dynamic Scenes One D4RT at a Time
- **分类: cs.CV**

- **简介: 该论文属于4D动态场景重建任务，旨在高效恢复视频中物体的几何与运动。提出D4RT模型，采用统一Transformer架构和新型查询机制，单次前向推理联合预测深度、时空对应与相机参数，实现轻量、可扩展的高效重建。**

- **链接: [https://arxiv.org/pdf/2512.08924v1](https://arxiv.org/pdf/2512.08924v1)**

> **作者:** Chuhan Zhang; Guillaume Le Moing; Skanda Koppula; Ignacio Rocco; Liliane Momeni; Junyu Xie; Shuyang Sun; Rahul Sukthankar; Joëlle K Barral; Raia Hadsell; Zoubin Ghahramani; Andrew Zisserman; Junlin Zhang; Mehdi SM Sajjadi
>
> **备注:** Project Page: https://d4rt-paper.github.io/
>
> **摘要:** Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.
>
---
#### [new 078] HybridToken-VLM: Hybrid Token Compression for Vision-Language Models
- **分类: cs.CV; cs.AI**

- **简介: 该论文研究视觉-语言模型中的视觉token压缩任务，旨在解决高效压缩与语义保留的矛盾。提出HTC-VLM框架，通过连续与离散双通道分别保留细节与语义，实现580-to-1压缩并保持87.2%性能。**

- **链接: [https://arxiv.org/pdf/2512.08240v1](https://arxiv.org/pdf/2512.08240v1)**

> **作者:** Jusheng Zhang; Xiaoyang Guo; Kaitong Cai; Qinhan Lv; Yijia Fan; Wenhao Chai; Jian Wang; Keze Wang
>
> **摘要:** Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.
>
---
#### [new 079] SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking
- **分类: cs.CV; cs.RO**

- **简介: 该论文研究工业多视角抓取中的6D姿态估计任务，旨在解决密集遮挡、无纹理等问题。提出全稀疏深度Transformer（SDT-6D），融合多视角深度图，通过分阶段热图机制与稀疏注意力，实现高分辨率下高效、精确的多物体位姿估计。**

- **链接: [https://arxiv.org/pdf/2512.08430v1](https://arxiv.org/pdf/2512.08430v1)**

> **作者:** Nico Leuze; Maximilian Hoh; Samed Doğan; Nicolas R. -Peña; Alfred Schoettl
>
> **备注:** Accepted to WACV 2026. Preprint version
>
> **摘要:** Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.
>
---
#### [new 080] Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators
- **分类: cs.CV**

- **简介: 该论文研究单目深度估计模型与人类感知的相似性问题。尽管模型在准确率上表现优异，但其误差模式与人类存在差异。作者在KITTI数据集上分析69个模型，发现高精度不等于更接近人类感知，需建立以人为本的多维度评估体系。**

- **链接: [https://arxiv.org/pdf/2512.08163v1](https://arxiv.org/pdf/2512.08163v1)**

> **作者:** Yuki Kubota; Taiki Fukiage
>
> **备注:** 22 pages, 12 figures, 1 table
>
> **摘要:** Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.
>
---
#### [new 081] Instance-Aware Test-Time Segmentation for Continual Domain Shifts
- **分类: cs.CV**

- **简介: 该论文研究持续测试时适应下的语义分割，旨在解决动态域变中伪标签不可靠与类别不均衡问题。提出实例感知方法，自适应调整伪标签并动态平衡受影响类别学习，提升模型鲁棒性。**

- **链接: [https://arxiv.org/pdf/2512.08569v1](https://arxiv.org/pdf/2512.08569v1)**

> **作者:** Seunghwan Lee; Inyoung Jung; Hojoon Lee; Eunil Park; Sungeun Hong
>
> **摘要:** Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.
>
---
#### [new 082] Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing
- **分类: cs.CV**

- **简介: 该论文研究图像去雾任务，旨在解决真实非均匀雾环境下去雾效果与计算效率难以兼顾的问题。提出Fourier-RWKV网络，通过空间、频率和语义三重感知状态，在线性复杂度下实现高效去雾，兼顾性能与速度。**

- **链接: [https://arxiv.org/pdf/2512.08161v1](https://arxiv.org/pdf/2512.08161v1)**

> **作者:** Lirong Zheng; Yanshan Li; Rui Yu; Kaihao Zhang
>
> **摘要:** Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.
>
---
#### [new 083] No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers
- **分类: cs.CV; cs.AI**

- **简介: 该论文针对视觉推理任务，解决现有方法依赖标注或存在逻辑错误的问题。提出无需标注的训练框架，利用LLM和VLM验证器通过强化学习与难负例挖掘，提升推理与视觉定位能力。**

- **链接: [https://arxiv.org/pdf/2512.08889v1](https://arxiv.org/pdf/2512.08889v1)**

> **作者:** Damiano Marsili; Georgia Gkioxari
>
> **备注:** Project webpage: https://glab-caltech.github.io/valor/
>
> **摘要:** Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/
>
---
#### [new 084] FastBEV++: Fast by Algorithm, Deployable by Design
- **分类: cs.CV**

- **简介: 该论文属于自动驾驶中基于纯视觉的BEV感知任务，旨在解决高性能模型难以部署的问题。作者提出FastBEV++，通过算法设计实现高效视图变换与深度感知融合，兼顾高精度与实时性，无需定制算子即可实现良好推理性能。**

- **链接: [https://arxiv.org/pdf/2512.08237v1](https://arxiv.org/pdf/2512.08237v1)**

> **作者:** Yuanpeng Chen; Hui Song; Wei Tao; ShanHui Mo; Shuang Zhang; Xiao Hua; TianKun Zhao
>
> **摘要:** The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev
>
---
#### [new 085] Detection of Cyberbullying in GIF using AI
- **分类: cs.CV; cs.LG; cs.MM**

- **简介: 该论文针对GIF中的网络欺凌检测问题，属于AI内容安全任务。作者收集了4100多个含标签的GIF数据集，利用VGG16模型实现97%准确率的欺凌识别，推动了非文本媒介中网络欺凌检测的研究。**

- **链接: [https://arxiv.org/pdf/2512.07838v1](https://arxiv.org/pdf/2512.07838v1)**

> **作者:** Pal Dave; Xiaohong Yuan; Madhuri Siddula; Kaushik Roy
>
> **摘要:** Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.
>
---
#### [new 086] Low Rank Support Quaternion Matrix Machine
- **分类: cs.CV; cs.LG; math.OC; stat.ML**

- **简介: 该论文针对彩色图像分类任务，提出低秩支持四元数矩阵机（LSQMM），利用四元数表示RGB通道以保留通道间耦合关系，并引入四元数核范数正则化增强低秩结构，提升分类精度与效率。**

- **链接: [https://arxiv.org/pdf/2512.08327v1](https://arxiv.org/pdf/2512.08327v1)**

> **作者:** Wang Chen; Ziyan Luo; Shuangyue Wang
>
> **摘要:** Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.
>
---
#### [new 087] Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking
- **分类: cs.CV**

- **简介: 该论文研究通用深度伪造图像检测，旨在提升对未见生成模型的泛化能力。提出频率域掩码训练策略，增强检测性能并支持模型压缩，实现高效、可持续的检测，适用于大规模筛查。**

- **链接: [https://arxiv.org/pdf/2512.08042v1](https://arxiv.org/pdf/2512.08042v1)**

> **作者:** Chandler Timm C. Doloriel; Habib Ullah; Kristian Hovde Liland; Fadi Al Machot; Ngai-Man Cheung
>
> **摘要:** Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).
>
---
#### [new 088] GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification
- **分类: cs.CV**

- **简介: 该论文研究视频运动放大任务，旨在解决微小运动放大中的噪声干扰问题。提出GeoDiffMM，利用光流作为几何引导条件，通过无噪光流增强和扩散模型实现结构一致的运动放大，有效分离真实微运动与噪声，提升放大效果。**

- **链接: [https://arxiv.org/pdf/2512.08325v1](https://arxiv.org/pdf/2512.08325v1)**

> **作者:** Xuedeng Liu; Jiabao Guo; Zheng Zhang; Fei Wang; Zhi Liu; Dan Guo
>
> **摘要:** Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.
>
---
#### [new 089] Detection of Digital Facial Retouching utilizing Face Beauty Information
- **分类: cs.CV**

- **简介: 该论文属图像取证任务，旨在检测数字面部修图。针对修图影响人脸识别的问题，提出利用面部 beauty 信息，分析美颜算法变化，结合AI特征提取方法提升检测效果，在未知修图算法场景下实现1.1% D-EER的检测性能。**

- **链接: [https://arxiv.org/pdf/2512.08397v1](https://arxiv.org/pdf/2512.08397v1)**

> **作者:** Philipp Srock; Juan E. Tapia; Christoph Busch
>
> **摘要:** Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.
>
---
#### [new 090] Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning
- **分类: cs.CV; cs.AI**

- **简介: 该论文研究空中视觉-语言导航任务，旨在让无人机仅凭单目RGB图像和自然语言指令完成城市环境导航。针对现有方法依赖多模态输入导致成本高的问题，提出统一框架，通过提示引导的多任务学习联合优化空间感知、轨迹推理与动作预测，并设计关键帧选择与标签重加权机制，实现在轻量设备上的高效导航。**

- **链接: [https://arxiv.org/pdf/2512.08639v1](https://arxiv.org/pdf/2512.08639v1)**

> **作者:** Huilin Xu; Zhuoyang Liu; Yixiang Luomei; Feng Xu
>
> **备注:** Under Review, 12 pages, 9 figures
>
> **摘要:** Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.
>
---
#### [new 091] BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain
- **分类: cs.CV**

- **简介: 该论文致力于发现并解释人脑视觉表征，提出BrainExplore框架，通过无监督分解fMRI信号、自动匹配自然图像与语言描述，大规模识别可解释的脑活动模式，揭示跨皮层的细粒度视觉概念。**

- **链接: [https://arxiv.org/pdf/2512.08560v1](https://arxiv.org/pdf/2512.08560v1)**

> **作者:** Navve Wasserman; Matias Cosarinsky; Yuval Golbari; Aude Oliva; Antonio Torralba; Tamar Rott Shaham; Michal Irani
>
> **摘要:** Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.
>
---
#### [new 092] Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration
- **分类: cs.CV**

- **简介: 该论文研究文本感知图像恢复（TAIR）任务，旨在解决扩散模型在修复含文本图像时易产生文字幻觉的问题。提出UniT框架，融合扩散Transformer、视觉语言模型与文本检测模块，通过迭代式文本引导抑制幻觉，实现高保真文本恢复。**

- **链接: [https://arxiv.org/pdf/2512.08922v1](https://arxiv.org/pdf/2512.08922v1)**

> **作者:** Jin Hyeon Kim; Paul Hyunbin Cho; Claire Kim; Jaewon Min; Jaeeun Lee; Jihye Park; Yeji Choi; Seungryong Kim
>
> **摘要:** Text-Aware Image Restoration (TAIR) aims to recover high-quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong generative priors for general image restoration, they often produce text hallucinations in text-centric tasks due to the absence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that integrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an iterative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermediate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine-grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.
>
---
#### [new 093] EgoX: Egocentric Video Generation from a Single Exocentric Video
- **分类: cs.CV**

- **简介: 该论文研究从单个第三方视角视频生成第一人称视角视频的任务，解决视角转换中姿态差异大、视野重叠少的问题。提出EgoX框架，结合LoRA适配、统一条件策略和几何引导注意力机制，实现高保真、几何一致的自中心视频生成。**

- **链接: [https://arxiv.org/pdf/2512.08269v1](https://arxiv.org/pdf/2512.08269v1)**

> **作者:** Taewoong Kang; Kinam Kim; Dohyeon Kim; Minho Park; Junha Hyung; Jaegul Choo
>
> **备注:** 21 pages, project page : https://keh0t0.github.io/EgoX
>
> **摘要:** Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.
>
---
#### [new 094] Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection
- **分类: cs.CV; cs.AI**

- **简介: 该论文研究相机-based 3D目标检测中的知识蒸馏，旨在将离线模型利用未来帧的知识有效迁移到在线模型。提出FTKD方法，通过未来感知特征重建和未来引导logit蒸馏，提升检测精度与速度估计，不增加推理成本。**

- **链接: [https://arxiv.org/pdf/2512.08247v1](https://arxiv.org/pdf/2512.08247v1)**

> **作者:** Haowen Zheng; Hu Zhu; Lu Deng; Weihao Gu; Yang Yang; Yanyan Liang
>
> **备注:** AAAI-26
>
> **摘要:** Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.
>
---
#### [new 095] HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting
- **分类: cs.CV**

- **简介: 该论文属于新视角合成任务，旨在解决3D高斯点阵渲染复杂反射场景时的速度与存储瓶颈。提出混合点阵机制，融合反射烘焙高斯追踪与层级剪枝，在保持质量的同时显著提升速度、降低内存。**

- **链接: [https://arxiv.org/pdf/2512.08334v1](https://arxiv.org/pdf/2512.08334v1)**

> **作者:** Chang Liu; Hongliang Yuan; Lianghao Zhang; Sichao Wang; Jianwei Guo; Shi-Sheng Huang
>
> **摘要:** Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.
>
---
#### [new 096] Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning
- **分类: cs.CV; cs.AI; cs.HC**

- **简介: 该论文研究图像描述生成任务，旨在解决低分辨率图像因细节不足导致的描述困难问题。提出SOLI方法，采用Siamese网络优化潜在嵌入，提升轻量模型在资源受限下的生成效率与准确性。**

- **链接: [https://arxiv.org/pdf/2512.08873v1](https://arxiv.org/pdf/2512.08873v1)**

> **作者:** Jing Jie Tan; Anissa Mokraoui; Ban-Hoe Kwan; Danny Wee-Kiat Ng; Yan-Chai Hum
>
> **备注:** 6 pages
>
> **摘要:** Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.
>
---
#### [new 097] Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文研究图像保护方法（如Glaze、Nightshade）的内部机制，旨在揭示其扰动的结构特性与可检测性。通过多域可解释性分析，发现扰动为低熵、内容耦合的结构性变化，而非随机噪声，解释了其视觉隐蔽但可检测的原因，属于生成模型安全防御的机理分析任务。**

- **链接: [https://arxiv.org/pdf/2512.08329v1](https://arxiv.org/pdf/2512.08329v1)**

> **作者:** Michael R. Martin; Garrick Chan; Kwan-Liu Ma
>
> **备注:** 32 pages, 17 figures, 1 table, 5 algorithms, preprint
>
> **摘要:** Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.
>
---
#### [new 098] Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality
- **分类: cs.CV**

- **简介: 该论文研究视频人脸替换任务，旨在解决长序列中保真度与时间一致性难题。提出LivingSwap模型，首次引入视频参考引导与关键帧控制，实现高保真、稳定的面部替换，并构建数据集Face2Face支持训练，显著提升影视级生成质量。**

- **链接: [https://arxiv.org/pdf/2512.07951v1](https://arxiv.org/pdf/2512.07951v1)**

> **作者:** Zekai Luo; Zongze Du; Zhouhang Zhu; Hao Zhong; Muzhi Zhu; Wen Wang; Yuling Xi; Chenchen Jing; Hao Chen; Chunhua Shen
>
> **备注:** Project webpage: https://aim-uofa.github.io/LivingSwap
>
> **摘要:** Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap
>
---
#### [new 099] GeoDM: Geometry-aware Distribution Matching for Dataset Distillation
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于数据蒸馏任务，旨在解决现有方法忽略数据内在几何结构的问题。作者提出GeoDM框架，通过欧氏、双曲与球面流形的乘积空间统一建模数据几何，引入可学习曲率与权重，并设计最优传输损失，提升蒸馏数据的分布保真度与模型性能。**

- **链接: [https://arxiv.org/pdf/2512.08317v1](https://arxiv.org/pdf/2512.08317v1)**

> **作者:** Xuhui Li; Zhengquan Luo; Zihui Cui; Zhiqiang Xu
>
> **摘要:** Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.
>
---
#### [new 100] Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning
- **分类: cs.CV; cs.AI**

- **简介: 该论文针对视觉语言模型在跨模态推理中领域迁移性能下降的问题，提出无需训练的双曲适配器（T-DHA），利用双曲空间建模语义层次结构，提升少样本图像识别与领域泛化能力。**

- **链接: [https://arxiv.org/pdf/2512.08820v1](https://arxiv.org/pdf/2512.08820v1)**

> **作者:** Yi Zhang; Chun-Wun Cheng; Junyi He; Ke Yu; Yushun Tang; Carola-Bibiane Schönlieb; Zhihai He; Angelica I. Aviles-Rivero
>
> **备注:** Accepted in IEEE Transactions on Multimedia (TMM)
>
> **摘要:** Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.
>
---
#### [new 101] Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment
- **分类: cs.CV; cs.GR**

- **简介: 该论文属于三维重建任务，旨在解决视觉基础模型缺乏多视图几何一致性的问题。作者提出Selfi方法，通过自监督特征对齐，利用重投影一致性损失优化特征空间，提升新视角合成与相机位姿估计性能。**

- **链接: [https://arxiv.org/pdf/2512.08930v1](https://arxiv.org/pdf/2512.08930v1)**

> **作者:** Youming Deng; Songyou Peng; Junyi Zhang; Kathryn Heal; Tiancheng Sun; John Flynn; Steve Marschner; Lucy Chai
>
> **备注:** Project Page: https://denghilbert.github.io/selfi/
>
> **摘要:** Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.
>
---
#### [new 102] Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属医学图像分割任务，旨在解决乳腺超声图像中恶性病变精准分割问题。提出Residual-SwinCA-Net模型，融合残差CNN与Swin Transformer，引入多尺度通道注意力和像素注意力机制，提升特征表达与边界识别，显著提高分割性能。**

- **链接: [https://arxiv.org/pdf/2512.08243v1](https://arxiv.org/pdf/2512.08243v1)**

> **作者:** Saeeda Naz; Saddam Hussain Khan
>
> **备注:** 26 Pages, 10 Figures, 4 Tables
>
> **摘要:** A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.
>
---
#### [new 103] Automated Pollen Recognition in Optical and Holographic Microscopy Images
- **分类: cs.CV**

- **简介: 该论文研究基于深度学习的花粉颗粒自动检测与分类，解决光学和全息显微图像中识别性能差异问题。采用YOLOv8s和MobileNetV3L模型，通过数据扩增提升全息图像性能，验证了低成本无透镜全息显微与深度学习结合的可行性。**

- **链接: [https://arxiv.org/pdf/2512.08589v1](https://arxiv.org/pdf/2512.08589v1)**

> **作者:** Swarn Singh Warshaneyan; Maksims Ivanovs; Blaž Cugmas; Inese Bērziņa; Laura Goldberga; Mindaugas Tamosiunas; Roberts Kadiķis
>
> **备注:** 08 pages, 10 figures, 04 tables, 20 references. Date of Conference: 13-14 June 2025 Date Added to IEEE Xplore: 10 July 2025 Electronic ISBN: 979-8-3315-0969-9 Print on Demand(PoD) ISBN: 979-8-3315-0970-5 DOI: 10.1109/AICCONF64766.2025.11064260 Conference Location: Prague, Czech Republic Online Access: https://ieeexplore.ieee.org/document/11064260
>
> **摘要:** This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.
>
---
#### [new 104] Self-Evolving 3D Scene Generation from a Single Image
- **分类: cs.CV**

- **简介: 该论文研究单图生成3D场景任务，旨在解决现有方法在复杂大场景中结构与纹理重建不完整的问题。提出EvoScene框架，无需训练，通过交替优化2D与3D表示，逐步提升几何稳定性与纹理一致性，实现高质量、可直接应用的3D网格重建。**

- **链接: [https://arxiv.org/pdf/2512.08905v1](https://arxiv.org/pdf/2512.08905v1)**

> **作者:** Kaizhi Zheng; Yue Fan; Jing Gu; Zishuo Xu; Xuehai He; Xin Eric Wang
>
> **摘要:** Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.
>
---
#### [new 105] OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics
- **分类: cs.CV**

- **简介: 该论文提出OpenMonoGS-SLAM，属于单目SLAM任务，旨在解决无深度传感器和开放语义场景下的三维重建与语义理解问题。作者融合3D高斯溅射与视觉基础模型，实现无需深度输入和语义标注的开放词汇语义SLAM。**

- **链接: [https://arxiv.org/pdf/2512.08625v1](https://arxiv.org/pdf/2512.08625v1)**

> **作者:** Jisang Yoo; Gyeongjin Kang; Hyun-kyu Ko; Hyeonwoo Yu; Eunbyung Park
>
> **备注:** 8 pages, 4 figures
>
> **摘要:** Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.
>
---
#### [new 106] Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models
- **分类: cs.CV**

- **简介: 该论文属于遥感图像变化检测任务，旨在解决亚马逊雨林毁林区域识别中模型效果差、缺乏标准化的问题。作者统一评估多种模型，引入自注意力机制与预后处理技术，并融合模型提升性能，达到80.41%的F1分数。**

- **链接: [https://arxiv.org/pdf/2512.08075v1](https://arxiv.org/pdf/2512.08075v1)**

> **作者:** Christian Massao Konishi; Helio Pedrini
>
> **摘要:** The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.
>
---
#### [new 107] Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture
- **分类: cs.CV; cs.CL**

- **简介: 该论文提出“手语检测”新任务，旨在从连续手语中识别特定手势是否存在。作者构建端到端编码器模型，直接基于姿态关键点进行二分类判断，避免依赖中间文本标注，降低计算成本与视觉噪声，验证了姿态表示在手语检索中的有效性。**

- **链接: [https://arxiv.org/pdf/2512.08738v1](https://arxiv.org/pdf/2512.08738v1)**

> **作者:** Samuel Ebimobowei Johnny; Blessed Guda; Emmanuel Enejo Aaron; Assane Gueye
>
> **备注:** To appear at AACL-IJCNLP 2025 Workshop WSLP
>
> **摘要:** Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting
>
---
#### [new 108] Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions
- **分类: cs.CV**

- **简介: 该论文研究扩散模型生成过程中概念形成的动态机制，提出无需训练的PCI框架，通过分析不同时间步插入概念的成功率（CIS），揭示概念固化时机，指导高效文本编辑。**

- **链接: [https://arxiv.org/pdf/2512.08486v1](https://arxiv.org/pdf/2512.08486v1)**

> **作者:** Ada Gorgun; Fawaz Sammani; Nikos Deligiannis; Bernt Schiele; Jonas Fischer
>
> **备注:** Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions
>
> **摘要:** Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions
>
---
#### [new 109] PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models
- **分类: cs.CV**

- **简介: 该论文研究3D点云表征学习，旨在解决现有对比和生成方法在处理无序、不均匀点云时的局限。提出PointDico，通过知识蒸馏融合扩散模型与对比学习，引入分层生成器和双通道结构，实现局部与全局信息融合，在ScanObjectNN和ShapeNetPart上达到SOTA性能。**

- **链接: [https://arxiv.org/pdf/2512.08330v1](https://arxiv.org/pdf/2512.08330v1)**

> **作者:** Pengbo Li; Yiding Sun; Haozhe Cheng
>
> **备注:** Accepted by IJCNN 2025
>
> **摘要:** Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.
>
---
#### [new 110] Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement
- **分类: cs.CV**

- **简介: 该论文属于人体图像生成任务，旨在解决多视角与新姿态合成中几何不一致和模糊问题。提出Blur2Sharp框架，结合3D人体NeRF与扩散模型，利用SMPL先验实现细节清晰、几何一致的视图生成。**

- **链接: [https://arxiv.org/pdf/2512.08215v1](https://arxiv.org/pdf/2512.08215v1)**

> **作者:** Chia-Hern Lai; I-Hsuan Lo; Yen-Ku Yeh; Thanh-Nguyen Truong; Ching-Chun Huang
>
> **摘要:** The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.
>
---
#### [new 111] RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features
- **分类: cs.RO; cs.CV**

- **简介: 该论文针对无标定物环境下LiDAR-相机外参自标定任务，解决初始位姿未知和特征质量不均导致的精度与鲁棒性问题。提出RAVES-Calib，利用Gluestick建立2D-3D点线特征对应，自适应加权优化外参，无需初值且兼容多传感器，实现高精度、强鲁棒的标定。**

- **链接: [https://arxiv.org/pdf/2512.08170v1](https://arxiv.org/pdf/2512.08170v1)**

> **作者:** Haoxin Zhang; Shuaixin Li; Xiaozhou Zhu; Hongbo Chen; Wen Yao
>
> **摘要:** In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.
>
---
#### [new 112] Generalizations of the Normalized Radon Cumulative Distribution Transform for Limited Data Recognition
- **分类: math.NA; cs.CV; cs.IT**

- **简介: 该论文研究图像识别中的小样本分类任务，旨在解决数据受仿射变换影响的问题。作者提出广义归一化方法和多维非欧扩展，增强Radon累积分布变换的不变性与分类性能，实现高精度识别。**

- **链接: [https://arxiv.org/pdf/2512.08099v1](https://arxiv.org/pdf/2512.08099v1)**

> **作者:** Matthias Beckmann; Robert Beinert; Jonas Bresch
>
> **摘要:** The Radon cumulative distribution transform (R-CDT) exploits one-dimensional Wasserstein transport and the Radon transform to represent prominent features in images. It is closely related to the sliced Wasserstein distance and facilitates classification tasks, especially in the small data regime, like the recognition of watermarks in filigranology. Here, a typical issue is that the given data may be subject to affine transformations caused by the measuring process. To make the R-CDT invariant under arbitrary affine transformations, a two-step normalization of the R-CDT has been proposed in our earlier works. The aim of this paper is twofold. First, we propose a family of generalized normalizations to enhance flexibility for applications. Second, we study multi-dimensional and non-Euclidean settings by making use of generalized Radon transforms. We prove that our novel feature representations are invariant under certain transformations and allow for linear separation in feature space. Our theoretical results are supported by numerical experiments based on 2d images, 3d shapes and 3d rotation matrices, showing near perfect classification accuracies and clustering results.
>
---
#### [new 113] Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation
- **分类: cs.RO; cs.CV; cs.LG; eess.IV**

- **简介: 该论文提出Zero-Splat TeleAssist，属于机器人遥操作中的姿态估计任务，旨在无需标记物和深度传感器的情况下，实现多机器人6自由度位姿实时估计。通过融合视觉-语言分割、单目深度与3D高斯溅射，构建共享世界模型，支持多方协同遥操作。**

- **链接: [https://arxiv.org/pdf/2512.08271v1](https://arxiv.org/pdf/2512.08271v1)**

> **作者:** Srijan Dokania; Dharini Raghavan
>
> **备注:** Published and Presented at 3rd Workshop on Human-Centric Multilateral Teleoperation in ICRA 2025
>
> **摘要:** We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.
>
---
#### [new 114] TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文针对扩散模型的强化学习后训练计算成本高的问题，提出TreeGRPO框架。通过将去噪过程建模为搜索树，实现高效样本利用、细粒度信用分配和摊销计算，显著提升训练效率，达成更优的效率-奖励权衡。**

- **链接: [https://arxiv.org/pdf/2512.08153v1](https://arxiv.org/pdf/2512.08153v1)**

> **作者:** Zheng Ding; Weirui Ye
>
> **摘要:** Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.
>
---
#### [new 115] LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model
- **分类: cs.LG; cs.CV**

- **简介: 该论文针对Transformer模型跨阶段动态稀疏加速难题，提出LAPA协同设计，通过ALOC、MRSA与DDF技术降低计算开销，并设计专用加速器。实验显示其能效显著优于现有SOTA方法。**

- **链接: [https://arxiv.org/pdf/2512.07855v1](https://arxiv.org/pdf/2512.07855v1)**

> **作者:** Huizheng Wang; Hongbin Wang; Shaojun Wei; Yang Hu; Shouyi Yin
>
> **摘要:** Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.
>
---
#### [new 116] Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization
- **分类: cs.RO; cs.CV**

- **简介: 该论文针对机器人感知中的非线性优化问题，利用变量可分性与稀疏性，提出一种适配规范对称的变量投影方法，构建无需显式存储的Schur补算子，显著加速求解过程，兼顾效率与精度。**

- **链接: [https://arxiv.org/pdf/2512.07969v1](https://arxiv.org/pdf/2512.07969v1)**

> **作者:** Alan Papalia; Nikolas Sanderson; Haoyu Han; Heng Yang; Hanumant Singh; Michael Everett
>
> **备注:** 8 pages, submitted for review
>
> **摘要:** Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \textbf{2$\times$--35$\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments.
>
---
#### [new 117] Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks
- **分类: cs.CL; cs.AI; cs.CV; cs.MA**

- **简介: 该论文研究长时序复杂任务求解，提出一种分层多智能体系统，通过空间课程学习和NLL置信度机制，引导64×64轻量代理网格协同推理，降低计算开销，提升长视野任务的稳定性和准确性。**

- **链接: [https://arxiv.org/pdf/2512.08545v1](https://arxiv.org/pdf/2512.08545v1)**

> **作者:** Indrajit Kar; Kalathur Chenchu Kishore Kumar
>
> **备注:** 22 pages, 2 tables, 9 figures
>
> **摘要:** Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.
>
---
#### [new 118] See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm
- **分类: cs.AI; cs.CV; cs.HC**

- **简介: 该论文提出“具身智能手机操作”（ESO）任务，旨在解决现有方法依赖ADB、仅限安卓的问题。作者设计See-Control框架，通过多模态智能体结合低自由度机械臂，实现跨平台物理交互操作手机，包含基准测试、智能体模型与标注数据集。**

- **链接: [https://arxiv.org/pdf/2512.08629v1](https://arxiv.org/pdf/2512.08629v1)**

> **作者:** Haoyu Zhao; Weizhong Ding; Yuhao Yang; Zheng Tian; Linyi Yang; Kun Shao; Jun Wang
>
> **摘要:** Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.
>
---
#### [new 119] Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model
- **分类: cs.RO; cs.AI; cs.CV**

- **简介: 该论文针对机器人操作规划中世界模型物理不准确的问题，提出Embodied Tree of Thoughts（EToT）框架。它结合物理仿真与视觉语言模型，通过先验与反思分支机制进行树搜索规划，提升长视野任务中的物理一致性与容错能力。**

- **链接: [https://arxiv.org/pdf/2512.08188v1](https://arxiv.org/pdf/2512.08188v1)**

> **作者:** Wenjiang Xu; Cindy Wang; Rui Fang; Mingkang Zhang; Lusong Li; Jing Xu; Jiayuan Gu; Zecui Zeng; Rui Chen
>
> **备注:** Website at https://embodied-tree-of-thoughts.github.io
>
> **摘要:** World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .
>
---
#### [new 120] CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space
- **分类: cs.LG; cs.CV**

- **简介: 该论文提出CLARITY，一种基于潜空间的医学世界模型，旨在解决现有AI在肿瘤临床决策中无法动态预测疾病演化的问题。它融合时间与患者特异性上下文，建模治疗条件下的疾病轨迹，并通过预测到决策框架生成个体化、可解释的治疗方案，显著提升治疗规划性能。**

- **链接: [https://arxiv.org/pdf/2512.08029v1](https://arxiv.org/pdf/2512.08029v1)**

> **作者:** Tianxingjian Ding; Yuanhao Zou; Chen Chen; Mubarak Shah; Yu Tian
>
> **摘要:** Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.
>
---
#### [new 121] VLD: Visual Language Goal Distance for Reinforcement Learning Navigation
- **分类: cs.RO; cs.CV**

- **简介: 该论文研究视觉语言导航任务，旨在解决端到端策略学习中仿真到现实的迁移难题及标注数据不足问题。提出VLD框架，通过解耦感知与策略学习，利用互联网视频自监督训练距离预测器，再在仿真中训练强化学习策略，实现对图像和文本目标的灵活导航。**

- **链接: [https://arxiv.org/pdf/2512.07976v1](https://arxiv.org/pdf/2512.07976v1)**

> **作者:** Lazar Milikic; Manthan Patel; Jonas Frey
>
> **摘要:** Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-"where to go"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.
>
---
#### [new 122] Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions
- **分类: cs.GR; cs.CV**

- **简介: 该论文研究从2D视频学习3D角色控制，解决现有方法依赖稀缺3D数据或生成不真实动作的问题。提出Mimic2DM框架，直接利用2D关键点训练控制策略，并结合生成模型实现多样、逼真的物理运动合成。**

- **链接: [https://arxiv.org/pdf/2512.08500v1](https://arxiv.org/pdf/2512.08500v1)**

> **作者:** Jianan Li; Xiao Chen; Tao Huang; Tien-Tsin Wong
>
> **摘要:** Video data is more cost-effective than motion capture data for learning 3D character motion controllers, yet synthesizing realistic and diverse behaviors directly from videos remains challenging. Previous approaches typically rely on off-the-shelf motion reconstruction techniques to obtain 3D trajectories for physics-based imitation. These reconstruction methods struggle with generalizability, as they either require 3D training data (potentially scarce) or fail to produce physically plausible poses, hindering their application to challenging scenarios like human-object interaction (HOI) or non-human characters. We tackle this challenge by introducing Mimic2DM, a novel motion imitation framework that learns the control policy directly and solely from widely available 2D keypoint trajectories extracted from videos. By minimizing the reprojection error, we train a general single-view 2D motion tracking policy capable of following arbitrary 2D reference motions in physics simulation, using only 2D motion data. The policy, when trained on diverse 2D motions captured from different or slightly different viewpoints, can further acquire 3D motion tracking capabilities by aggregating multiple views. Moreover, we develop a transformer-based autoregressive 2D motion generator and integrate it into a hierarchical control framework, where the generator produces high-quality 2D reference trajectories to guide the tracking policy. We show that the proposed approach is versatile and can effectively learn to synthesize physically plausible and diverse motions across a range of domains, including dancing, soccer dribbling, and animal movements, without any reliance on explicit 3D motion data. Project Website: https://jiann-li.github.io/mimic2dm/
>
---
#### [new 123] Fast and Robust Diffusion Posterior Sampling for MR Image Reconstruction Using the Preconditioned Unadjusted Langevin Algorithm
- **分类: physics.med-ph; cs.CV; cs.LG; math.PR**

- **简介: 该论文研究加速MRI重建中的扩散后验采样任务，旨在解决传统方法采样速度慢、需调参的问题。作者提出一种带预条件的未调整朗之万算法，结合精确似然与扩散先验，实现快速稳健的重建采样。**

- **链接: [https://arxiv.org/pdf/2512.05791v1](https://arxiv.org/pdf/2512.05791v1)**

> **作者:** Moritz Blumenthal; Tina Holliber; Jonathan I. Tamir; Martin Uecker
>
> **备注:** Submitted to Magnetic Resonance in Medicine
>
> **摘要:** Purpose: The Unadjusted Langevin Algorithm (ULA) in combination with diffusion models can generate high quality MRI reconstructions with uncertainty estimation from highly undersampled k-space data. However, sampling methods such as diffusion posterior sampling or likelihood annealing suffer from long reconstruction times and the need for parameter tuning. The purpose of this work is to develop a robust sampling algorithm with fast convergence. Theory and Methods: In the reverse diffusion process used for sampling the posterior, the exact likelihood is multiplied with the diffused prior at all noise scales. To overcome the issue of slow convergence, preconditioning is used. The method is trained on fastMRI data and tested on retrospectively undersampled brain data of a healthy volunteer. Results: For posterior sampling in Cartesian and non-Cartesian accelerated MRI the new approach outperforms annealed sampling in terms of reconstruction speed and sample quality. Conclusion: The proposed exact likelihood with preconditioning enables rapid and reliable posterior sampling across various MRI reconstruction tasks without the need for parameter tuning.
>
---
#### [new 124] CIP-Net: Continual Interpretable Prototype-based Network
- **分类: cs.LG; cs.CV**

- **简介: 该论文研究持续学习任务，旨在解决灾难性遗忘问题。作者提出CIP-Net，一种无需存储旧样本的自解释原型网络，通过可解释性机制提升模型记忆保持能力，在任务和类增量场景下实现高性能与低内存开销。**

- **链接: [https://arxiv.org/pdf/2512.07981v1](https://arxiv.org/pdf/2512.07981v1)**

> **作者:** Federico Di Valerio; Michela Proietti; Alessio Ragno; Roberto Capobianco
>
> **摘要:** Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.
>
---
#### [new 125] FlowSteer: Conditioning Flow Field for Consistent Image Restoration
- **分类: eess.IV; cs.CV**

- **简介: 该论文属图像恢复任务，旨在解决扩散模型在重建时偏离测量值的问题。提出FlowSteer方法，通过沿采样路径注入测量先验，在零样本下实现跨任务的高效一致性恢复，无需微调或适配器。**

- **链接: [https://arxiv.org/pdf/2512.08125v1](https://arxiv.org/pdf/2512.08125v1)**

> **作者:** Tharindu Wickremasinghe; Chenyang Qi; Harshana Weligampola; Zhengzhong Tu; Stanley H. Chan
>
> **摘要:** Flow-based text-to-image (T2I) models excel at prompt-driven image generation, but falter on Image Restoration (IR), often "drifting away" from being faithful to the measurement. Prior work mitigate this drift with data-specific flows or task-specific adapters that are computationally heavy and not scalable across tasks. This raises the question "Can't we efficiently manipulate the existing generative capabilities of a flow model?" To this end, we introduce FlowSteer (FS), an operator-aware conditioning scheme that injects measurement priors along the sampling path,coupling a frozed flow's implicit guidance with explicit measurement constraints. Across super-resolution, deblurring, denoising, and colorization, FS improves measurement consistency and identity preservation in a strictly zero-shot setting-no retrained models, no adapters. We show how the nature of flow models and their sensitivities to noise inform the design of such a scheduler. FlowSteer, although simple, achieves a higher fidelity of reconstructed images, while leveraging the rich generative priors of flow models.
>
---
#### [new 126] Self-Reinforced Deep Priors for Reparameterized Full Waveform Inversion
- **分类: physics.geo-ph; cs.CV**

- **简介: 该论文研究全波形反演（FWI）中的速度模型重建任务，旨在解决传统DIP-FWI因固定随机输入导致的病态性和局部极小问题。提出自增强框架SRDIP-FWI，通过联合迭代更新网络参数与输入，增强结构先验，实现更优正则化与多尺度重建。**

- **链接: [https://arxiv.org/pdf/2512.08284v1](https://arxiv.org/pdf/2512.08284v1)**

> **作者:** Guangyuan Zou; Junlun Li; Feng Liu; Xuejing Zheng; Jianjian Xie; Guoyi Chen
>
> **备注:** Submitted to GEOPHYSICS
>
> **摘要:** Full waveform inversion (FWI) has become a widely adopted technique for high-resolution subsurface imaging. However, its inherent strong nonlinearity often results in convergence toward local minima. Recently, deep image prior-based reparameterized FWI (DIP-FWI) has been proposed to alleviate the dependence on massive training data. By exploiting the spectral bias and implicit regularization in the neural network architecture, DIP-FWI can effectively avoid local minima and reconstruct more geologically plausible velocity models. Nevertheless, existing DIP-FWI typically use a fixed random input throughout the inversion process, which fails to utilize the mapping and correlation between the input and output of the network. Moreover, under complex geological conditions, the lack of informative prior in the input can exacerbate the ill-posedness of the inverse problem, leading to artifacts and unstable reconstructions. To address these limitations, we propose a self-reinforced DIP-FWI (SRDIP-FWI) framework, in which a steering algorithm alternately updates both the network parameters and the input at each iteration using feedback from the current network output. This design allows adaptive structural enhancement and improved regularization, thereby effectively mitigating the ill-posedness in FWI. Additionally, we analyze the spectral bias of the network in SRDIP-FWI and quantify its role in multiscale velocity model building. Synthetic tests and field land data application demonstrate that SRDIP-FWI achieves superior resolution, improved accuracy and greater depth penetration compared to multiscale FWI. More importantly, SRDIP-FWI eliminates the need for manual frequency-band selection and time-window picking, substantially simplifying the inversion workflow. Overall, the proposed method provides a novel, adaptive and robust framework for accurate subsurface velocity model reconstruction.
>
---
#### [new 127] DIJIT: A Robotic Head for an Active Observer
- **分类: cs.RO; cs.CV**

- **简介: 该论文提出DIJIT——一种具有九个机械自由度的双目机器人头，用于主动视觉研究。旨在模拟人类眼-头运动，探究其在视觉任务中的作用，并开发接近人类扫视运动的相机控制方法。**

- **链接: [https://arxiv.org/pdf/2512.07998v1](https://arxiv.org/pdf/2512.07998v1)**

> **作者:** Mostafa Kamali Tabrizi; Mingshi Chi; Bir Bikram Dey; Yu Qing Yuan; Markus D. Solbach; Yiqian Liu; Michael Jenkin; John K. Tsotsos
>
> **摘要:** We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.
>
---
#### [new 128] Multi-domain performance analysis with scores tailored to user preferences
- **分类: cs.PF; cs.AI; cs.CV; cs.LG**

- **简介: 该论文研究多域性能分析任务，解决不同应用领域下算法性能评估的加权平均问题。提出基于用户偏好的评分框架，定义四类关键域，并为二分类任务设计新可视化工具。**

- **链接: [https://arxiv.org/pdf/2512.08715v1](https://arxiv.org/pdf/2512.08715v1)**

> **作者:** Sébastien Piérard; Adrien Deliège; Marc Van Droogenbroeck
>
> **摘要:** The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.
>
---
#### [new 129] Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation
- **分类: eess.IV; cs.CV; cs.LG**

- **简介: 该论文针对肺癌CT图像分割中模型对分布外（OOD）输入易产生高置信度错误的问题，提出一种轻量、即插即用的随机森林框架RF-Deep。其利用肿瘤锚定的深度特征实现高效OOD检测，无需修改原模型，显著提升分割系统的鲁棒性与临床可靠性。**

- **链接: [https://arxiv.org/pdf/2512.08216v1](https://arxiv.org/pdf/2512.08216v1)**

> **作者:** Aneesh Rangnekar; Harini Veeraraghavan
>
> **摘要:** Accurate segmentation of cancerous lesions from 3D computed tomography (CT) scans is essential for automated treatment planning and response assessment. However, even state-of-the-art models combining self-supervised learning (SSL) pretrained transformers with convolutional decoders are susceptible to out-of-distribution (OOD) inputs, generating confidently incorrect tumor segmentations, posing risks for safe clinical deployment. Existing logit-based methods suffer from task-specific model biases, while architectural enhancements to explicitly detect OOD increase parameters and computational costs. Hence, we introduce a plug-and-play and lightweight post-hoc random forests-based OOD detection framework called RF-Deep that leverages deep features with limited outlier exposure. RF-Deep enhances generalization to imaging variations by repurposing the hierarchical features from the pretrained-then-finetuned backbone encoder, providing task-relevant OOD detection by extracting the features from multiple regions of interest anchored to the predicted tumor segmentations. Hence, it scales to images of varying fields-of-view. We compared RF-Deep against existing OOD detection methods using 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets. RF-Deep achieved AUROC > 93.50 for the challenging near-OOD datasets and near-perfect detection (AUROC > 99.00) for the far-OOD datasets, substantially outperforming logit-based and radiomics approaches. RF-Deep maintained similar performance consistency across networks of different depths and pretraining strategies, demonstrating its effectiveness as a lightweight, architecture-agnostic approach to enhance the reliability of tumor segmentation from CT volumes.
>
---
#### [new 130] Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata
- **分类: cs.NE; cs.AI; cs.CV; cs.LG**

- **简介: 该论文研究条件化形态生成任务，旨在解决神经细胞自动机（NCA）难以生成多样化结构的问题。作者提出条件化NCA（c-NCA），通过引入类别向量引导单一生长种子生成不同MNIST数字结构，实现局部规则下的条件形态自组织。**

- **链接: [https://arxiv.org/pdf/2512.08360v1](https://arxiv.org/pdf/2512.08360v1)**

> **作者:** Ali Sakour
>
> **备注:** 13 pages, 5 figures. Code available at: https://github.com/alisakour/Conditional-NCA-Digits
>
> **摘要:** Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.
>
---
#### [new 131] GSPN-2: Efficient Parallel Sequence Modeling
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文针对视觉Transformer处理高分辨率图像效率低的问题，提出GSPN-2，通过算法与系统联合优化，减少GPU计算开销与内存访问，提升序列建模效率，在保持精度的同时显著降低计算成本。**

- **链接: [https://arxiv.org/pdf/2512.07884v1](https://arxiv.org/pdf/2512.07884v1)**

> **作者:** Hongjun Wang; Yitong Jiang; Collin McCarthy; David Wehr; Hanrong Ye; Xinhao Li; Ka Chun Cheung; Wonmin Byeon; Jinwei Gu; Ke Chen; Kai Han; Hongxu Yin; Pavlo Molchanov; Jan Kautz; Sifei Liu
>
> **备注:** NeurIPS 2025
>
> **摘要:** Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/
>
---
## 更新

#### [replaced 001] PAGE-4D: Disentangled Pose and Geometry Estimation for VGGT-4D Perception
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2510.17568v3](https://arxiv.org/pdf/2510.17568v3)**

> **作者:** Kaichen Zhou; Yuhan Wang; Grace Chen; Xinhai Chang; Gaspard Beaudouin; Fangneng Zhan; Paul Pu Liang; Mengyu Wang
>
> **摘要:** Recent 3D feed-forward models, such as the Visual Geometry Grounded Transformer (VGGT), have shown strong capability in inferring 3D attributes of static scenes. However, since they are typically trained on static datasets, these models often struggle in real-world scenarios involving complex dynamic elements, such as moving humans or deformable objects like umbrellas. To address this limitation, we introduce PAGE-4D, a feedforward model that extends VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and point cloud reconstruction -- all without post-processing. A central challenge in multi-task 4D reconstruction is the inherent conflict between tasks: accurate camera pose estimation requires suppressing dynamic regions, while geometry reconstruction requires modeling them. To resolve this tension, we propose a dynamics-aware aggregator that disentangles static and dynamic information by predicting a dynamics-aware mask -- suppressing motion cues for pose estimation while amplifying them for geometry reconstruction. Extensive experiments show that PAGE-4D consistently outperforms the original VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.
>
---
#### [replaced 002] Fine-grained Spatiotemporal Grounding on Egocentric Videos
- **分类: cs.CV; cs.CL**

- **简介: 该论文研究第一人称视频中的细粒度时空定位任务，旨在解决现有方法在该场景下性能差的问题。作者分析了与第三人称视频的差异，构建了首个像素级基准EgoMask及大规模训练集EgoMask-Train，并验证了其有效性。**

- **链接: [https://arxiv.org/pdf/2508.00518v2](https://arxiv.org/pdf/2508.00518v2)**

> **作者:** Shuo Liang; Yiwu Zhong; Zi-Yuan Hu; Yeyao Tao; Liwei Wang
>
> **备注:** Accepted by ICCV 2025
>
> **摘要:** Spatiotemporal video grounding aims to localize target entities in videos based on textual queries. While existing research has made significant progress in exocentric videos, the egocentric setting remains relatively underexplored, despite its growing importance in applications such as augmented reality and robotics. In this work, we conduct a systematic analysis of the discrepancies between egocentric and exocentric videos, revealing key challenges such as shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts. To address these challenges, we introduce EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. It is constructed by our proposed automatic annotation pipeline, which annotates referring expressions and object masks across short-, medium-, and long-term videos. Additionally, we create EgoMask-Train, a large-scale training dataset to facilitate model development. Experiments demonstrate that the state-of-the-art spatiotemporal grounding models perform poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields significant improvements, while preserving performance on exocentric datasets. Our work thus provides essential resources and insights for advancing egocentric video understanding. Our code is available at https://github.com/LaVi-Lab/EgoMask .
>
---
#### [replaced 003] From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging
- **分类: eess.IV; cs.CV**

- **链接: [https://arxiv.org/pdf/2505.11394v2](https://arxiv.org/pdf/2505.11394v2)**

> **作者:** Alexander Oberstrass; Esteban Vaca; Eric Upschulte; Meiqi Niu; Nicola Palomero-Gallagher; David Graessel; Christian Schiffer; Markus Axer; Katrin Amunts; Timo Dickscheid
>
> **备注:** Revised version, accepted for publication
>
> **摘要:** Comprehensive assessment of the various aspects of the brain's microstructure requires the use of complementary imaging techniques. This includes measuring the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers (myeloarchitecture). The gold standard for cytoarchitectonic analysis is light microscopic imaging of cell-body stained tissue sections. To reveal the 3D orientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been introduced, a method that is label-free and allows subsequent staining of sections after 3D-PLI measurement. By post-staining for cell bodies, a direct link between fiber- and cytoarchitecture can potentially be established in the same section. However, inevitable distortions introduced during the staining process make a costly nonlinear and cross-modal registration necessary in order to study the detailed relationships between cells and fibers in the images. In addition, the complexity of processing histological sections for post-staining only allows for a limited number of such samples. In this work, we take advantage of deep learning methods for image-to-image translation to generate a virtual staining of 3D-PLI that is spatially aligned at the cellular level. We use a supervised setting, building on a unique dataset of brain sections, to which Cresyl violet staining has been applied after 3D-PLI measurement. To ensure high correspondence between both modalities, we address the misalignment of training data using Fourier-based registration. In this way, registration can be efficiently calculated during training for local image patches of target and predicted staining. We demonstrate that the proposed method can predict a Cresyl violet staining from 3D-PLI, resulting in a virtual staining that exhibits plausible patterns of cell organization in gray matter, with larger cell bodies being localized at their expected positions.
>
---
#### [replaced 004] DASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples
- **分类: cs.CV; cs.LG**

- **链接: [https://arxiv.org/pdf/2508.13309v2](https://arxiv.org/pdf/2508.13309v2)**

> **作者:** Abdullah Al Nomaan Nafi; Habibur Rahaman; Zafaryab Haider; Tanzim Mahfuz; Fnu Suya; Swarup Bhunia; Prabuddha Chakraborty
>
> **摘要:** Numerous techniques have been proposed for generating adversarial examples in white-box settings under strict Lp-norm constraints. However, such norm-bounded examples often fail to align well with human perception, and only recently have a few methods begun specifically exploring perceptually aligned adversarial examples. Moreover, it remains unclear whether insights from Lp-constrained attacks can be effectively leveraged to improve perceptual efficacy. In this paper, we introduce DAASH, a fully differentiable meta-attack framework that generates effective and perceptually aligned adversarial examples by strategically composing existing Lp-based attack methods. DAASH operates in a multi-stage fashion: at each stage, it aggregates candidate adversarial examples from multiple base attacks using learned, adaptive weights and propagates the result to the next stage. A novel meta-loss function guides this process by jointly minimizing misclassification loss and perceptual distortion, enabling the framework to dynamically modulate the contribution of each base attack throughout the stages. We evaluate DAASH on adversarially trained models across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on Lp-constrained based methods, DAASH significantly outperforms state-of-the-art perceptual attacks such as AdvAD -- achieving higher attack success rates (e.g., 20.63\% improvement) and superior visual quality, as measured by SSIM, LPIPS, and FID (improvements $\approx$ of 11, 0.015, and 5.7, respectively). Furthermore, DAASH generalizes well to unseen defenses, making it a practical and strong baseline for evaluating robustness without requiring handcrafted adaptive attacks for each new defense.
>
---
#### [replaced 005] End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2506.18331v4](https://arxiv.org/pdf/2506.18331v4)**

> **作者:** AmirHossein Zamani; Tianhao Xie; Amir G. Aghdam; Tiberiu Popa; Eugene Belilovsky
>
> **摘要:** While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches. Our implementation code is publicly available at: https://github.com/AHHHZ975/Differentiable-Texture-Learning
>
---
#### [replaced 006] 50 Years of Automated Face Recognition
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2505.24247v3](https://arxiv.org/pdf/2505.24247v3)**

> **作者:** Minchul Kim; Anil Jain; Xiaoming Liu
>
> **摘要:** Over the past five decades, automated face recognition (FR) has progressed from handcrafted geometric and statistical approaches to advanced deep learning architectures that now approach, and in many cases exceed, human performance. This paper traces the historical and technological evolution of FR, encompassing early algorithmic paradigms through to contemporary neural systems trained on extensive real and synthetically generated datasets. We examine pivotal innovations that have driven this progression, including advances in dataset construction, loss function formulation, network architecture design, and feature fusion strategies. Furthermore, we analyze the relationship between data scale, diversity, and model generalization, highlighting how dataset expansion correlates with benchmark performance gains. Recent systems have achieved near-perfect large-scale identification accuracy, with the leading algorithm in the latest NIST FRTE 1:N benchmark reporting a FNIR of 0.15 percent at FPIR of 0.001 on a gallery of over 10 million identities. We delineate key open problems and emerging directions, including scalable training, multi-modal fusion, synthetic data, and interpretable recognition frameworks.
>
---
#### [replaced 007] Harnessing Object Grounding for Time-Sensitive Video Understanding
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2509.06335v2](https://arxiv.org/pdf/2509.06335v2)**

> **作者:** Tz-Ying Wu; Sharath Nittur Sridhar; Subarna Tripathi
>
> **备注:** Accepted to WACV 2026
>
> **摘要:** We propose to improve the time-sensitive video understanding (TSV) capability of video large language models (Video-LLMs) with grounded objects (GO). We hypothesize that TSV tasks can benefit from GO within frames, which is supported by our preliminary experiments on LITA, a state-of-the-art Video-LLM for reasoning temporal localization. While augmenting prompts with textual descriptions of these object annotations improves the performance of LITA, it also introduces extra token length and susceptibility to the noise in object-level information. To address this, we propose GO-Tokenizer, a lightweight add-on module for Video-LLMs leveraging off-the-shelf object detectors to encode compact object information on the fly. Experimental results demonstrate that pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its counterpart, utilizing textual descriptions of objects in the prompt. The gain generalizes across different models, datasets, and video understanding tasks, such as reasoning temporal localization and dense captioning.
>
---
#### [replaced 008] HuPrior3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.06368v2](https://arxiv.org/pdf/2512.06368v2)**

> **作者:** Weitao Xiong; Zhiyuan Yuan; Jiahao Lu; Chengfeng Zhao; Peng Li; Yuan Liu
>
> **摘要:** Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce HuPrior3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.
>
---
#### [replaced 009] Evaluating and Preserving High-level Fidelity in Super-Resolution
- **分类: cs.CV; cs.LG**

- **链接: [https://arxiv.org/pdf/2512.07037v2](https://arxiv.org/pdf/2512.07037v2)**

> **作者:** Josep M. Rocafort; Shaolin Su; Alexandra Gomez-Villa; Javier Vazquez-Corral
>
> **摘要:** Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.
>
---
#### [replaced 010] Learning an Ensemble Token from Task-driven Priors in Facial Analysis
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2507.01290v3](https://arxiv.org/pdf/2507.01290v3)**

> **作者:** Sunyong Seo; Semin Kim; Jongha Lee
>
> **备注:** 10 pages, 7 figures, 7 tables
>
> **摘要:** Facial analysis exhibits task-specific feature variations. While Convolutional Neural Networks (CNNs) have enabled the fine-grained representation of spatial information, Vision Transformers (ViTs) have facilitated the representation of semantic information at the patch level. While advances in backbone architectures have improved over the past decade, combining high-fidelity models often incurs computational costs on feature representation perspective. In this work, we introduce KT-Adapter, a novel methodology for learning knowledge token which enables the integration of high-fidelity feature representation in computationally efficient manner. Specifically, we propose a robust prior unification learning method that generates a knowledge token within a self-attention mechanism, sharing the mutual information across the pre-trained encoders. This knowledge token approach offers high efficiency with negligible computational cost. Our results show improved performance across facial analysis, with statistically significant enhancements observed in the feature representations.
>
---
#### [replaced 011] Video Dataset Condensation with Diffusion Models
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2505.06670v2](https://arxiv.org/pdf/2505.06670v2)**

> **作者:** Zhe Li; Hadrien Reynaud; Mischa Dombrowski; Sarah Cechnicka; Franciskus Xaverius Erick; Bernhard Kainz
>
> **备注:** Accepted at BMVC 2025
>
> **摘要:** In recent years, the rapid expansion of dataset sizes and the increasing complexity of deep learning models have significantly escalated the demand for computational resources, both for data storage and model training. Dataset distillation has emerged as a promising solution to address this challenge by generating a compact synthetic dataset that retains the essential information from a large real dataset. However, existing methods often suffer from limited performance, particularly in the video domain. In this paper, we focus on video dataset distillation. We begin by employing a video diffusion model to generate synthetic videos. Since the videos are generated only once, this significantly reduces computational costs. Next, we introduce the Video Spatio-Temporal U-Net (VST-UNet), a model designed to select a diverse and informative subset of videos that effectively captures the characteristics of the original dataset. To further optimize computational efficiency, we explore a training-free clustering algorithm, Temporal-Aware Cluster-based Distillation (TAC-DT), to select representative videos without requiring additional training overhead. We validate the effectiveness of our approach through extensive experiments on four benchmark datasets, demonstrating performance improvements of up to \(10.61\%\) over the state-of-the-art. Our method consistently outperforms existing approaches across all datasets, establishing a new benchmark for video dataset distillation.
>
---
#### [replaced 012] VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning
- **分类: cs.SD; cs.AI; cs.CV; cs.MM; eess.AS**

- **简介: 该论文提出VoiceCloak，针对扩散模型驱动的语音克隆进行主动防御。通过在参考音频中引入对抗扰动，扰乱说话人身份表征与条件引导机制，并降低生成语音质量，有效遏制未经授权的语音克隆。**

- **链接: [https://arxiv.org/pdf/2505.12332v5](https://arxiv.org/pdf/2505.12332v5)**

> **作者:** Qianyue Hu; Junyan Wu; Wei Lu; Xiangyang Luo
>
> **备注:** 15 pages, 6 figures, 13 tables; Accepted by AAAI 2026
>
> **摘要:** Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at https://voice-cloak.github.io/VoiceCloak/.
>
---
#### [replaced 013] EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.04810v4](https://arxiv.org/pdf/2512.04810v4)**

> **作者:** Xin He; Longhui Wei; Jianbo Ouyang; Minghui Liao; Lingxi Xie; Qi Tian
>
> **备注:** Project Page: https://emma-umm.github.io/emma/
>
> **摘要:** We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.
>
---
#### [replaced 014] Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion
- **分类: cs.GR; cs.CV; cs.LG; eess.IV; stat.ML**

- **链接: [https://arxiv.org/pdf/2504.08937v4](https://arxiv.org/pdf/2504.08937v4)**

> **作者:** Minjie Deng; Yan Wei; An Wu; Yuncan Ouyang; Hao Zhai; Qianyao Peng
>
> **摘要:** In image fusion tasks, the absence of real fused images as priors forces most deep learning approaches to rely on large-scale paired datasets to extract global weighting features or to generate pseudo-supervised images through algorithmic constructions. Unlike previous methods, this work re-examines prior-guided learning under few-shot conditions by introducing rough set theory. We regard the traditional algorithm as a prior generator, while the network re-inferrs and adaptively optimizes the prior through a dynamic loss function, reducing the inference burden of the network and enabling effective few-shot learning.To provide the prior, we propose the Granular Ball Pixel Computation (GBPC) algorithm. GBPC models pixel pairs in a luminance subspace using meta-granular balls and mines intra-ball information at multiple granular levels. At the fine-grained level, sliding granular balls assign adaptive weights to individual pixels to produce pixel-level prior fusion. At the coarse-grained level, the algorithm performs split computation within a single image to estimate positive and boundary domain distributions, enabling modality awareness and prior confidence estimation, which dynamically guide the loss weighting.The network and the algorithmic prior are coupled through the loss function to form an integrated framework. Thanks to the dynamic weighting mechanism, the network can adaptively adjust to different priors during training, enhancing its perception and fusion capability across modalities. We name this framework GBFF (Granular Ball Fusion Framework). Experiments on four fusion tasks demonstrate that even with only ten training image pairs per task, GBFF achieves superior performance in both visual quality and model compactness. Code is available at: https://github.com/DMinjie/GBFF
>
---
#### [replaced 015] Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile Medical Segmentation
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2506.17159v2](https://arxiv.org/pdf/2506.17159v2)**

> **作者:** Qing Xu; Yuxiang Luo; Wenting Duan; Zhen Chen
>
> **备注:** Accepted by TMI
>
> **摘要:** Medical image analysis is critical yet challenged by the need of jointly segmenting organs or tissues, and numerous instances for anatomical structures and tumor microenvironment analysis. Existing studies typically formulated different segmentation tasks in isolation, which overlooks the fundamental interdependencies between these tasks, leading to suboptimal segmentation performance and insufficient medical image understanding. To address this issue, we propose a Co-Seg++ framework for versatile medical segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing semantic and instance segmentation tasks to mutually enhance each other. We first devise a spatio-sequential prompt encoder (SSP-Encoder) to capture long-range spatial and sequential relationships between segmentation regions and image embeddings as prior spatial constraints. Moreover, we devise a multi-task collaborative decoder (MTC-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, jointly computing semantic and instance segmentation masks. Extensive experiments on diverse CT and histopathology datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts in the semantic, instance, and panoptic segmentation of dental anatomical structures, histopathology tissues, and nuclei instances. The source code is available at https://github.com/xq141839/Co-Seg-Plus.
>
---
#### [replaced 016] Mixture of Contexts for Long Video Generation
- **分类: cs.GR; cs.AI; cs.CV**

- **链接: [https://arxiv.org/pdf/2508.21058v3](https://arxiv.org/pdf/2508.21058v3)**

> **作者:** Shengqu Cai; Ceyuan Yang; Lvmin Zhang; Yuwei Guo; Junfei Xiao; Ziyan Yang; Yinghao Xu; Zhenheng Yang; Alan Yuille; Leonidas Guibas; Maneesh Agrawala; Lu Jiang; Gordon Wetzstein
>
> **备注:** Project page: https://primecai.github.io/moc/
>
> **摘要:** Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.
>
---
#### [replaced 017] DIVER: Reinforced Diffusion Breaks Imitation Bottlenecks in End-to-End Autonomous Driving
- **分类: cs.CV; cs.RO**

- **简介: 该论文针对端到端自动驾驶中模仿学习因单一专家示范导致的保守与多样性不足问题，提出DIVER框架，结合强化学习与扩散模型生成多样化可行轨迹，并设计新多样性指标评估，提升复杂场景下的泛化能力。**

- **链接: [https://arxiv.org/pdf/2507.04049v3](https://arxiv.org/pdf/2507.04049v3)**

> **作者:** Ziying Song; Lin Liu; Hongyu Pan; Bencheng Liao; Mingzhe Guo; Lei Yang; Yongchang Zhang; Shaoqing Xu; Caiyan Jia; Yadan Luo
>
> **备注:** 18 pages, 8 figures
>
> **摘要:** Most end-to-end autonomous driving methods rely on imitation learning from single expert demonstrations, often leading to conservative and homogeneous behaviors that limit generalization in complex real-world scenarios. In this work, we propose DIVER, an end-to-end driving framework that integrates reinforcement learning with diffusion-based generation to produce diverse and feasible trajectories. At the core of DIVER lies a reinforced diffusion-based generation mechanism. First, the model conditions on map elements and surrounding agents to generate multiple reference trajectories from a single ground-truth trajectory, alleviating the limitations of imitation learning that arise from relying solely on single expert demonstrations. Second, reinforcement learning is employed to guide the diffusion process, where reward-based supervision enforces safety and diversity constraints on the generated trajectories, thereby enhancing their practicality and generalization capability. Furthermore, to address the limitations of L2-based open-loop metrics in capturing trajectory diversity, we propose a novel Diversity metric to evaluate the diversity of multi-mode predictions.Extensive experiments on the closed-loop NAVSIM and Bench2Drive benchmarks, as well as the open-loop nuScenes dataset, demonstrate that DIVER significantly improves trajectory diversity, effectively addressing the mode collapse problem inherent in imitation learning.
>
---
#### [replaced 018] Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases
- **分类: eess.IV; cs.CV**

- **链接: [https://arxiv.org/pdf/2504.07606v3](https://arxiv.org/pdf/2504.07606v3)**

> **作者:** Andrés Bell-Navas; María Villalba-Orero; Enrique Lara-Pezzi; Jesús Garicano-Mena; Soledad Le Clainche
>
> **备注:** 43 pages, 7 figures
>
> **摘要:** Heart diseases remain the leading cause of mortality worldwide, implying approximately 18 million deaths according to the WHO. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid, and effective prediction. This work presents an automatic system based on a novel framework which combines Modal Decomposition and Masked Autoencoders (MAE) to extend the application from heart disease classification to the more challenging and specific task of heart failure time prediction, not previously addressed to the best of authors' knowledge. This system comprises two stages. The first one transforms the data from a database of echocardiography video sequences into a large collection of annotated images compatible with the training phase of machine learning-based frameworks and deep learning-based ones. This stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage builds and trains a Vision Transformer (ViT). MAEs based on a combined scheme of self-supervised (SSL) and supervised learning, so far barely explored in the literature about heart failure prediction, are adopted to effectively train the ViT from scratch, even with scarce databases. The designed neural network analyses in real-time images from echocardiography sequences to estimate the time of happening a heart failure. This approach demonstrates to improve prediction accuracy from scarce databases and to be superior to several established ViT and Convolutional Neural Network (CNN) architectures. The source code will be incorporated into the next version release of the ModelFLOWs-app software (https://github.com/modelflows/ModelFLOWs-app).
>
---
#### [replaced 019] Make LVLMs Focus: Context-Aware Attention Modulation for Better Multimodal In-Context Learning
- **分类: cs.CV; cs.CL**

- **简介: 该论文研究多模态上下文学习中LVLMs注意力机制的局限性，提出无需训练的CAMA方法，通过上下文感知的注意力调制增强关键token的关注，提升模型对上下文的利用效率和推理稳定性。**

- **链接: [https://arxiv.org/pdf/2505.17097v3](https://arxiv.org/pdf/2505.17097v3)**

> **作者:** Yanshu Li; Jianjiang Yang; Ziteng Yang; Bozheng Li; Ligong Han; Hongyang He; Zhengtao Yao; Yingjie Victor Chen; Songlin Fei; Dongfang Liu; Ruixiang Tang
>
> **备注:** 14 pages, 8 figures, 5 tables
>
> **摘要:** Multimodal in-context learning (ICL) is becoming a key capability that allows large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, which expands their usefulness in many real-world applications. However, ICL performance remains unstable even when the in-context demonstrations (ICDs) are well matched, showing that LVLMs still struggle to make full use of the provided context. While existing work mainly focuses on prompt engineering or post-hoc logit calibration, we study the attention mechanisms inside LVLMs to address their inherent limitations. We identify two important weaknesses in their self-attention that hinder effective ICL. To address these weaknesses, we propose \textbf{Context-Aware Modulated Attention} (CAMA), a training-free and plug-and-play method that dynamically adjusts attention logits based on the input in-context sequence. CAMA uses a two-stage modulation process that strengthens attention to semantically important tokens, especially visual ones. Across four LVLMs and seven benchmarks, CAMA consistently outperforms vanilla models and baselines, showing clear effectiveness and generalization. It can also activate the intended benefits of prompt engineering methods and remains robust across different sequence configurations. Therefore, CAMA opens up new directions for improving multimodal reasoning through a deeper understanding of attention dynamics.
>
---
#### [replaced 020] Glass Surface Detection: Leveraging Reflection Dynamics in Flash/No-flash Imagery
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2511.16887v2](https://arxiv.org/pdf/2511.16887v2)**

> **作者:** Tao Yan; Hao Huang; Yiwei Lu; Zeyu Wang; Ke Xu; Yinghui Wang; Xiaojun Chang; Rynson W. H. Lau
>
> **备注:** 18 pages, 17 figures
>
> **摘要:** Glass surfaces are ubiquitous in daily life, typically appearing colorless, transparent, and lacking distinctive features. These characteristics make glass surface detection a challenging computer vision task. Existing glass surface detection methods always rely on boundary cues (e.g., window and door frames) or reflection cues to locate glass surfaces, but they fail to fully exploit the intrinsic properties of the glass itself for accurate localization. We observed that in most real-world scenes, the illumination intensity in front of the glass surface differs from that behind it, which results in variations in the reflections visible on the glass surface. Specifically, when standing on the brighter side of the glass and applying a flash towards the darker side, existing reflections on the glass surface tend to disappear. Conversely, while standing on the darker side and applying a flash towards the brighter side, distinct reflections will appear on the glass surface. Based on this phenomenon, we propose NFGlassNet, a novel method for glass surface detection that leverages the reflection dynamics present in flash/no-flash imagery. Specifically, we propose a Reflection Contrast Mining Module (RCMM) for extracting reflections, and a Reflection Guided Attention Module (RGAM) for fusing features from reflection and glass surface for accurate glass surface detection. For learning our network, we also construct a dataset consisting of 3.3K no-flash and flash image pairs captured from various scenes with corresponding ground truth annotations. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods. Our code, model, and dataset will be available upon acceptance of the manuscript.
>
---
#### [replaced 021] Random forest-based out-of-distribution detection for robust lung cancer segmentation
- **分类: eess.IV; cs.CV; cs.LG**

- **链接: [https://arxiv.org/pdf/2508.19112v3](https://arxiv.org/pdf/2508.19112v3)**

> **作者:** Aneesh Rangnekar; Harini Veeraraghavan
>
> **备注:** Accepted at SPIE Medical Imaging 2026
>
> **摘要:** Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. Transformer-based models with self-supervised pretraining can produce reliably accurate segmentation from in-distribution (ID) data but degrade when applied to out-of-distribution (OOD) datasets. We address this challenge with RF-Deep, a random forest classifier that utilizes deep features from a pretrained transformer encoder of the segmentation model to detect OOD scans and enhance segmentation reliability. The segmentation model comprises a Swin Transformer encoder, pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and non-cancerous conditions, with a convolution decoder, trained to segment lung cancers in 317 3D scans. Independent testing was performed on 603 3D CT public datasets that included one ID dataset and four OOD datasets comprising chest CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of 18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs, consistently outperforming established OOD approaches. The RF-Deep classifier provides a simple and effective approach to enhance reliability of cancer segmentation in ID and OOD scenarios.
>
---
#### [replaced 022] CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball
- **分类: cs.CV; cs.MA**

- **链接: [https://arxiv.org/pdf/2512.01478v2](https://arxiv.org/pdf/2512.01478v2)**

> **作者:** Omer Sela; Michael Chertok; Lior Wolf
>
> **摘要:** This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.
>
---
#### [replaced 023] CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale
- **分类: cs.AI; cs.CL; cs.CV**

- **简介: 该论文提出CLIBD，旨在融合图像与DNA条形码数据，解决大规模生物多样性监测中物种分类难题。通过对比学习构建多模态统一嵌入空间，实现无需微调的零样本物种识别，显著提升分类准确率。**

- **链接: [https://arxiv.org/pdf/2405.17537v5](https://arxiv.org/pdf/2405.17537v5)**

> **作者:** ZeMing Gong; Austin T. Wang; Xiaoliang Huo; Joakim Bruslund Haurum; Scott C. Lowe; Graham W. Taylor; Angel X. Chang
>
> **备注:** Add Variations of DNA encoding
>
> **摘要:** Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multimodal approach combining both, using CLIP-style contrastive learning to align images, barcode DNA, and text-based representations of taxonomic labels in a unified embedding space. This allows for accurate classification of both known and unknown insect species without task-specific fine-tuning, leveraging contrastive learning for the first time to fuse barcode DNA and image data. Our method surpasses previous single-modality approaches in accuracy by over 8% on zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.
>
---
#### [replaced 024] Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.04890v2](https://arxiv.org/pdf/2512.04890v2)**

> **作者:** Ramya Muthukrishnan; Borjan Gagoski; Aryn Lee; P. Ellen Grant; Elfar Adalsteinsson; Polina Golland; Benjamin Billot
>
> **摘要:** We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.
>
---
#### [replaced 025] On the Temporality for Sketch Representation Learning
- **分类: cs.CV; cs.AI**

- **链接: [https://arxiv.org/pdf/2512.04007v2](https://arxiv.org/pdf/2512.04007v2)**

> **作者:** Marcelo Isaias de Moraes Junior; Moacir Antonelli Ponti
>
> **备注:** Preprint submitted to Pattern Recognition Letters
>
> **摘要:** Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.
>
---
#### [replaced 026] Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文针对信息密集图像的视觉问答任务，解决现有大模型在细粒度图文定位与多步推理上的困难。提出无需训练的Speculative Verdict框架，利用小模型生成多样化推理路径，通过共识机制筛选后由大模型综合得出答案，提升准确率与效率。**

- **链接: [https://arxiv.org/pdf/2510.20812v3](https://arxiv.org/pdf/2510.20812v3)**

> **作者:** Yuhan Liu; Lianhui Qin; Shengjie Wang
>
> **摘要:** Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict.
>
---
#### [replaced 027] ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2510.11000v2](https://arxiv.org/pdf/2510.11000v2)**

> **作者:** Ruihang Xu; Dewei Zhou; Fan Ma; Yi Yang
>
> **备注:** Project Page: https://nenhang.github.io/ContextGen/
>
> **摘要:** Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. Recognizing the lack of large-scale, hierarchically-structured datasets for this task, we introduce IMIG-100K, the first dataset with detailed layout and identity annotations. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods in control precision, identity fidelity, and overall visual quality.
>
---
#### [replaced 028] MT-Depth: Multi-task Instance feature analysis for the Depth Completion
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.04734v2](https://arxiv.org/pdf/2512.04734v2)**

> **作者:** Abdul Haseeb Nizamani; Dandi Zhou; Xinhai Sun
>
> **摘要:** Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower Root Mean Squared Error (RMSE) compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive Mean Absolute Error (MAE). Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.
>
---
#### [replaced 029] SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass
- **分类: cs.CV; cs.AI**

- **链接: [https://arxiv.org/pdf/2508.15769v2](https://arxiv.org/pdf/2508.15769v2)**

> **作者:** Yanxu Meng; Haoning Wu; Ya Zhang; Weidi Xie
>
> **备注:** Accepted by 3DV 2026; Project Page: https://mengmouxu.github.io/SceneGen
>
> **摘要:** 3D content generation has recently attracted significant research interest, driven by its critical applications in VR/AR and embodied AI. In this work, we tackle the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for extra optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architecture yields improved generation performance when multiple images are provided; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robustness of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.
>
---
#### [replaced 030] Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2511.22425v2](https://arxiv.org/pdf/2511.22425v2)**

> **作者:** Minghao Yin; Yukang Cao; Kai Han
>
> **摘要:** We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This avoids common artifacts like oversmoothing while maintaining semantic fidelity. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.
>
---
#### [replaced 031] Beyond accuracy: quantifying the reliability of Multiple Instance Learning for Whole Slide Image classification
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2409.11110v3](https://arxiv.org/pdf/2409.11110v3)**

> **作者:** Hassan Keshvarikhojasteh; Marc Aubreville; Christof A. Bertram; Josien P. W. Pluim; Mitko Veta
>
> **摘要:** Machine learning models have become integral to many fields, but their reliability, defined as producing dependable, trustworthy, and domain-consistent predictions, remains a critical concern. Multiple Instance Learning (MIL) models designed for Whole Slide Image (WSI) classification in computational pathology are rarely evaluated in terms of reliability, leaving a key gap in understanding their suitability for high-stakes applications like clinical decision-making. In this paper, we address this gap by introducing three quantitative metrics for reliability assessment and applying them to several widely used MIL architectures across three region-wise annotated pathology datasets. Our findings indicate that the mean pooling instance (MEAN-POOL-INS)model demonstrates superior reliability compared to other networks, despite its simple architectural design and computational efficiency. These findings underscore the need of reliability evaluation alongside predictive performance in MIL models and establish MEAN-POOL-INS as a strong, trustworthy baseline for future research.
>
---
#### [replaced 032] MELLM: A Flow-Guided Large Language Model for Micro-Expression Understanding
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2505.07007v3](https://arxiv.org/pdf/2505.07007v3)**

> **作者:** Sirui Zhao; Zhengye Zhang; Shifeng Liu; Xinglong Mao; Shukang Yin; Chaoyou Fu; Tong Xu; Enhong Chen
>
> **摘要:** Micro-expressions (MEs), brief and low-intensity facial movements revealing concealed emotions, are crucial for affective computing. Despite notable progress in ME recognition, existing methods are largely confined to discrete emotion classification, lacking the capacity for comprehensive ME Understanding (MEU), particularly in interpreting subtle facial dynamics and underlying emotional cues. While Multimodal Large Language Models (MLLMs) offer potential for MEU with their advanced reasoning abilities, they still struggle to perceive such subtle facial affective behaviors. To bridge this gap, we propose a ME Large Language Model (MELLM) that integrates optical flow-based sensitivity to subtle facial motions with the powerful inference ability of LLMs. Specifically, an iterative, warping-based optical-flow estimator, named MEFlowNet, is introduced to precisely capture facial micro-movements. For its training and evaluation, we construct MEFlowDataset, a large-scale optical-flow dataset with 54,611 onset-apex image pairs spanning diverse identities and subtle facial motions. Subsequently, we design a Flow-Guided Micro-Expression Understanding paradigm. Under this framework, the optical flow signals extracted by MEFlowNet are leveraged to build MEU-Instruct, an instruction-tuning dataset for MEU. MELLM is then fine-tuned on MEU-Instruct, enabling it to translate subtle motion patterns into human-readable descriptions and generate corresponding emotional inferences. Experiments demonstrate that MEFlowNet significantly outperforms existing optical flow methods in facial and ME-flow estimation, while MELLM achieves state-of-the-art accuracy and generalization across multiple ME benchmarks. To the best of our knowledge, this work presents two key contributions: MEFlowNet as the first dedicated ME flow estimator, and MELLM as the first LLM tailored for MEU.
>
---
#### [replaced 033] You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction
- **分类: cs.CV; cs.CL**

- **简介: 该论文针对细粒度视觉分类中多选项问答的评估难题，提出nlg2choice方法。通过先生成开放式回答再进行约束解码匹配选项，提升MLLM在高数量级相似类别下的分类与检索性能，尤其优化了检索场景的计算效率。**

- **链接: [https://arxiv.org/pdf/2510.14885v2](https://arxiv.org/pdf/2510.14885v2)**

> **作者:** Logan Lawrence; Oindrila Saha; Megan Wei; Chen Sun; Subhransu Maji; Grant Van Horn
>
> **备注:** Accepted to WACV26. 12 pages, 8 tables, 5 figures
>
> **摘要:** Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.
>
---
#### [replaced 034] IRPO: Boosting Image Restoration via Post-training GRPO
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.00814v2](https://arxiv.org/pdf/2512.00814v2)**

> **作者:** Haoxuan Xu; Yi Liu; Boyuan Jiang; Jinlong Peng; Donghao Luo; Xiaobin Hu; Shuicheng Yan; Haoang Li
>
> **摘要:** Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in https://github.com/HaoxuanXU1024/IRPO.
>
---
#### [replaced 035] High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.06012v2](https://arxiv.org/pdf/2512.06012v2)**

> **作者:** Emmanuel Akeweje; Conall Kirk; Chi-Wai Chan; Denis Dowling; Mimi Zhang
>
> **摘要:** Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.
>
---
#### [replaced 036] The Missing Point in Vision Transformers for Universal Image Segmentation
- **分类: cs.CV; cs.AI; cs.LG; eess.IV**

- **链接: [https://arxiv.org/pdf/2505.19795v2](https://arxiv.org/pdf/2505.19795v2)**

> **作者:** Sajjad Shahabodini; Mobina Mansoori; Farnoush Bayatmakou; Jamshid Abouei; Konstantinos N. Plataniotis; Arash Mohammadi
>
> **摘要:** Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.
>
---
#### [replaced 037] Deep Learning, Machine Learning -- Digital Signal and Image Processing: From Theory to Application
- **分类: cs.CV; cs.GR; eess.IV; eess.SP**

- **链接: [https://arxiv.org/pdf/2410.20304v2](https://arxiv.org/pdf/2410.20304v2)**

> **作者:** Weiche Hsieh; Ziqian Bi; Junyu Liu; Benji Peng; Sen Zhang; Xuanhe Pan; Jiawei Xu; Jinlang Wang; Keyu Chen; Caitlyn Heqi Yin; Pohsun Feng; Yizhu Wen; Tianyang Wang; Ming Li; Jintao Ren; Xinyuan Song; Qian Niu; Silin Chen; Ming Liu
>
> **备注:** 293 pages
>
> **摘要:** Digital Signal Processing (DSP) and Digital Image Processing (DIP) with Machine Learning (ML) and Deep Learning (DL) are popular research areas in Computer Vision and related fields. We highlight transformative applications in image enhancement, filtering techniques, and pattern recognition. By integrating frameworks like the Discrete Fourier Transform (DFT), Z-Transform, and Fourier Transform methods, we enable robust data manipulation and feature extraction essential for AI-driven tasks. Using Python, we implement algorithms that optimize real-time data processing, forming a foundation for scalable, high-performance solutions in computer vision. This work illustrates the potential of ML and DL to advance DSP and DIP methodologies, contributing to artificial intelligence, automated feature extraction, and applications across diverse domains.
>
---
#### [replaced 038] MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2508.12522v2](https://arxiv.org/pdf/2508.12522v2)**

> **作者:** Muhammad Osama Zeeshan; Natacha Gillet; Alessandro Lameiras Koerich; Marco Pedersoli; Francois Bremond; Eric Granger
>
> **备注:** WACV 2026
>
> **摘要:** Personalized expression recognition (ER) involves adapting a machine learning model to subject-specific data for improved recognition of expressions with considerable interpersonal variability. Subject-specific ER can benefit significantly from multi-source domain adaptation (MSDA) methods, where each domain corresponds to a specific subject to improve model accuracy and robustness. Despite promising results, state-of-the-art MSDA approaches often overlook multimodal information or blend sources into a single domain, limiting subject diversity and failing to explicitly capture unique subject-specific characteristics. To address these limitations, we introduce MuSACo, a multimodal subject-specific selection and adaptation method for ER based on co-training. It leverages complementary information across multiple modalities and multiple source domains for subject-specific adaptation. This makes MuSACo particularly relevant for affective computing applications in digital health, such as patient-specific assessment for stress or pain, where subject-level nuances are crucial. MuSACo selects source subjects relevant to the target and generates pseudo-labels using the dominant modality for class-aware learning, in conjunction with a class-agnostic loss to learn from less confident target samples. Finally, source features from each modality are aligned, while only confident target features are combined. Experimental results on challenging multimodal ER datasets: BioVid, StressID, and BAH show that MuSACo outperforms UDA (blending) and state-of-the-art MSDA methods.
>
---
#### [replaced 039] MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens
- **分类: cs.CV; cs.AI**

- **链接: [https://arxiv.org/pdf/2310.02239v4](https://arxiv.org/pdf/2310.02239v4)**

> **作者:** Kaizhi Zheng; Xuehai He; Xin Eric Wang
>
> **摘要:** The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.
>
---
#### [replaced 040] Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects
- **分类: cs.GR; cs.CV**

- **链接: [https://arxiv.org/pdf/2510.02069v2](https://arxiv.org/pdf/2510.02069v2)**

> **作者:** Georgios Kouros; Minye Wu; Tinne Tuytelaars
>
> **摘要:** Accurate reconstruction and relighting of glossy objects remains a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restrict faithful material recovery and limit relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine environment map optimization accelerates convergence, and negative-only environment map clipping preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.
>
---
#### [replaced 041] FLAIR: Frequency- and Locality-Aware Implicit Neural Representations
- **分类: cs.CV; cs.AI**

- **链接: [https://arxiv.org/pdf/2508.13544v5](https://arxiv.org/pdf/2508.13544v5)**

> **作者:** Sukhun Ko; Seokhyun Yoon; Dahyeon Kye; Kyle Min; Chanho Eom; Jihyong Oh
>
> **备注:** Please visit our project page at https://cmlab-korea.github.io/FLAIR/
>
> **摘要:** Implicit Neural Representations (INRs) leverage neural networks to map coordinates to corresponding signals, enabling continuous and compact representations. This paradigm has driven significant advances in various vision tasks. However, existing INRs lack frequency selectivity and spatial localization, leading to an over-reliance on redundant signal components. Consequently, they exhibit spectral bias, tending to learn low-frequency components early while struggling to capture fine high-frequency details. To address these issues, we propose FLAIR (Frequency- and Locality-Aware Implicit Neural Representations), which incorporates two key innovations. The first is Band-Localized Activation (BLA), a novel activation designed for joint frequency selection and spatial localization under the constraints of the time-frequency uncertainty principle (TFUP). Through structured frequency control and spatially localized responses, BLA effectively mitigates spectral bias and enhances training stability. The second is Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet transform to compute energy scores and explicitly guide frequency information to the network, enabling precise frequency selection and adaptive band control. Our method consistently outperforms existing INRs in 2D image representation, as well as 3D shape reconstruction and novel view synthesis.
>
---
#### [replaced 042] CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2507.21888v3](https://arxiv.org/pdf/2507.21888v3)**

> **作者:** Fevziye Irem Eyiokur; Dogucan Yaman; Hazım Kemal Ekenel; Alexander Waibel
>
> **摘要:** We address Embodied Reference Understanding, the task of predicting the object a person in the scene refers to through pointing gesture and language. This requires multimodal reasoning over text, visual pointing cues, and scene context, yet existing methods often fail to fully exploit visual disambiguation signals. We also observe that while the referent often aligns with the head-to-fingertip direction, in many cases it aligns more closely with the wrist-to-fingertip direction, making a single-line assumption overly limiting. To address this, we propose a dual-model framework, where one model learns from the head-to-fingertip direction and the other from the wrist-to-fingertip direction. We introduce a Gaussian ray heatmap representation of these lines and use them as input to provide a strong supervisory signal that encourages the model to better attend to pointing cues. To fuse their complementary strengths, we present the CLIP-Aware Pointing Ensemble module, which performs a hybrid ensemble guided by CLIP features. We further incorporate an auxiliary object center prediction head to enhance referent localization. We validate our approach on YouRefIt, achieving 75.0 mAP at 0.25 IoU, alongside state-of-the-art CLIP and C_D scores, and demonstrate its generality on unseen CAESAR and ISL Pointing, showing robust performance across benchmarks.
>
---
#### [replaced 043] Spike-EVPR: Deep Spiking Residual Networks with SNN-Tailored Representations for Event-Based Visual Place Recognition
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2402.10476v2](https://arxiv.org/pdf/2402.10476v2)**

> **作者:** Zuntao Liu; Yaohui Li; Chenming Hu; Delei Kong; Junjie Jiang; Zheng Fang
>
> **备注:** 8 pages, 6 figures
>
> **摘要:** Event cameras are ideal for visual place recognition (VPR) in challenging environments due to their high temporal resolution and high dynamic range. However, existing methods convert sparse events into dense frame-like representations for Artificial Neural Networks (ANNs), ignoring event sparsity and incurring high computational cost. Spiking Neural Networks (SNNs) complement event data through discrete spike signals to enable energy-efficient VPR, but their application is hindered by the lack of effective spike-compatible representations and deep architectures capable of learning discriminative global descriptors. To address these limitations, we propose Spike-EVPR, a directly trained, end-to-end SNN framework tailored for event-based VPR. First, we introduce two complementary event representations, MCS-Tensor and TSS-Tensor, designed to reduce temporal redundancy while preserving essential spatio-temporal cues. Furthermore, we propose a deep spiking residual architecture that effectively aggregates these features to generate robust place descriptors. Extensive experiments on the Brisbane-Event-VPR and DDD20 datasets demonstrate that Spike-EVPR achieves state-of-the-art performance, improving Recall@1 by 7.61% and 13.20%, respectively, while significantly reducing energy consumption.
>
---
#### [replaced 044] Neural Radiance Fields for the Real World: A Survey
- **分类: cs.CV; cs.GR**

- **链接: [https://arxiv.org/pdf/2501.13104v2](https://arxiv.org/pdf/2501.13104v2)**

> **作者:** Wenhui Xiao; Remi Chierchia; Rodrigo Santa Cruz; Xuesong Li; David Ahmedt-Aristizabal; Olivier Salvado; Clinton Fookes; Leo Lebrat
>
> **备注:** Revised version
>
> **摘要:** Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since release. NeRFs can effectively reconstruct complex 3D scenes from 2D images, advancing different fields and applications such as scene understanding, 3D content generation, and robotics. Despite significant research progress, a thorough review of recent innovations, applications, and challenges is lacking. This survey compiles key theoretical advancements and alternative representations and investigates emerging challenges. It further explores applications on reconstruction, highlights NeRFs' impact on computer vision and robotics, and reviews essential datasets and toolkits. By identifying gaps in the literature, this survey discusses open challenges and offers directions for future research.
>
---
#### [replaced 045] Guiding WaveMamba with Frequency Maps for Image Debanding
- **分类: eess.IV; cs.CV**

- **链接: [https://arxiv.org/pdf/2508.11331v2](https://arxiv.org/pdf/2508.11331v2)**

> **作者:** Xinyi Wang; Smaranda Tasmoc; Nantheera Anantrasirichai; Angeliki Katsenou
>
> **备注:** 5 pages, 2 figures
>
> **摘要:** Compression at low bitrates in modern codecs often introduces banding artifacts, especially in smooth regions such as skies. These artifacts degrade visual quality and are common in user-generated content due to repeated transcoding. We propose a banding restoration method that employs the Wavelet State Space Model and a frequency masking map to preserve high-frequency details. Furthermore, we provide a benchmark of open-source banding restoration methods and evaluate their performance on two public banding image datasets. Experimentation on the available datasets suggests that the proposed post-processing approach effectively suppresses banding compared to the state-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving image textures. Visual inspections of the results confirm this. Code and supplementary material are available at: https://github.com/xinyiW915/Debanding-PCS2025.
>
---
#### [replaced 046] ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation
- **分类: cs.CV; cs.AI**

- **链接: [https://arxiv.org/pdf/2504.02316v2](https://arxiv.org/pdf/2504.02316v2)**

> **作者:** Yuan Zhou; Shilong Jin; Litao Hua; Wanjun Lv; Haoran Duan; Jungong Han
>
> **备注:** 13 pages, 14 figures, 3 tables
>
> **摘要:** Recent advances in zero-shot text-to-3D generation have revolutionized 3D content creation by enabling direct synthesis from textual descriptions. While state-of-the-art methods leverage 3D Gaussian Splatting with score distillation to enhance multi-view rendering through pre-trained text-to-image (T2I) models, they suffer from inherent prior view biases in T2I priors. These biases lead to inconsistent 3D generation, particularly manifesting as the multi-face Janus problem, where objects exhibit conflicting features across views. To address this fundamental challenge, we propose ConsDreamer, a novel method that mitigates view bias by refining both the conditional and unconditional terms in the score distillation process: (1) a View Disentanglement Module (VDM) that eliminates viewpoint biases in conditional prompts by decoupling irrelevant view components and injecting precise view control; and (2) a similarity-based partial order loss that enforces geometric consistency in the unconditional term by aligning cosine similarities with azimuth relationships. Extensive experiments demonstrate that ConsDreamer can be seamlessly integrated into various 3D representations and score distillation paradigms, effectively mitigating the multi-face Janus problem.
>
---
#### [replaced 047] Learning effective pruning at initialization from iterative pruning
- **分类: cs.CV; cs.LG**

- **链接: [https://arxiv.org/pdf/2408.14757v2](https://arxiv.org/pdf/2408.14757v2)**

> **作者:** Shengkai Liu; Yaofeng Cheng; Fusheng Zha; Wei Guo; Lining Sun; Zhenshan Bing; Chenguang Yang
>
> **摘要:** Pruning at initialization (PaI) reduces training costs by removing weights before training, which becomes increasingly crucial with the growing network size. However, current PaI methods still have a large accuracy gap with iterative pruning, especially at high sparsity levels. This raises an intriguing question: can we get inspiration from iterative pruning to improve the PaI performance? In the lottery ticket hypothesis, the iterative rewind pruning (IRP) finds subnetworks retroactively by rewinding the parameter to the original initialization in every pruning iteration, which means all the subnetworks are based on the initial state. Here, we hypothesise the surviving subnetworks are more important and bridge the initial feature and their surviving score as the PaI criterion. We employ an end-to-end neural network (\textbf{AutoS}parse) to learn this correlation, input the model's initial features, output their score and then prune the lowest score parameters before training. To validate the accuracy and generalization of our method, we performed PaI across various models. Results show that our approach outperforms existing methods in high-sparsity settings. Notably, as the underlying logic of model pruning is consistent in different models, only one-time IRP on one model is needed (e.g., once IRP on ResNet-18/CIFAR-10, AutoS can be generalized to VGG-16/CIFAR-10, ResNet-18/TinyImageNet, et al.). As the first neural network-based PaI method, we conduct extensive experiments to validate the factors influencing this approach. These results reveal the learning tendencies of neural networks and provide new insights into our understanding and research of PaI from a practical perspective. Our code is available at: https://github.com/ChengYaofeng/AutoSparse.git.
>
---
#### [replaced 048] Leveraging Multi-Modal Information to Enhance Dataset Distillation
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2505.08605v3](https://arxiv.org/pdf/2505.08605v3)**

> **作者:** Zhe Li; Hadrien Reynaud; Bernhard Kainz
>
> **备注:** Accepted at BMVC Workshop (Privacy, Fairness, Accountability and Transparency in Computer Vision)
>
> **摘要:** Dataset distillation aims to create a small and highly representative synthetic dataset that preserves the essential information of a larger real dataset. Beyond reducing storage and computational costs, related approaches offer a promising avenue for privacy preservation in computer vision by eliminating the need to store or share sensitive real-world images. Existing methods focus solely on optimizing visual representations, overlooking the potential of multi-modal information. In this work, we propose a multi-modal dataset distillation framework that incorporates two key enhancements: caption-guided supervision and object-centric masking. To leverage textual information, we introduce two strategies: caption concatenation, which fuses caption embeddings with visual features during classification, and caption matching, which enforces semantic alignment between real and synthetic data through a caption-based loss. To improve data utility and reduce unnecessary background noise, we employ segmentation masks to isolate target objects and introduce two novel losses: masked feature alignment and masked gradient matching, both aimed at promoting object-centric learning. Extensive evaluations demonstrate that our approach improves downstream performance while promoting privacy protection by minimizing exposure to real data.
>
---
#### [replaced 049] VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation
- **分类: cs.CV; cs.AI**

- **链接: [https://arxiv.org/pdf/2512.07215v2](https://arxiv.org/pdf/2512.07215v2)**

> **作者:** Md Selim Sarowar; Sungho Kim
>
> **摘要:** Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.
>
---
#### [replaced 050] Learning Geodesics of Geometric Shape Deformations From Images
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2410.18797v3](https://arxiv.org/pdf/2410.18797v3)**

> **作者:** Nian Wu; Miaomiao Zhang
>
> **备注:** Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:019
>
> **摘要:** This paper presents a novel method, named geodesic deformable networks (GDN), that for the first time enables the learning of geodesic flows of deformation fields derived from images. In particular, the capability of our proposed GDN being able to predict geodesics is important for quantifying and comparing deformable shape presented in images. The geodesic deformations, also known as optimal transformations that align pairwise images, are often parameterized by a time sequence of smooth vector fields governed by nonlinear differential equations. A bountiful literature has been focusing on learning the initial conditions (e.g., initial velocity fields) based on registration networks. However, the definition of geodesics central to deformation-based shape analysis is blind to the networks. To address this problem, we carefully develop an efficient neural operator to treat the geodesics as unknown mapping functions learned from the latent deformation spaces. A composition of integral operators and smooth activation functions is then formulated to effectively approximate such mappings. In contrast to previous works, our GDN jointly optimizes a newly defined geodesic loss, which adds additional benefits to promote the network regularizability and generalizability. We demonstrate the effectiveness of GDN on both 2D synthetic data and 3D real brain magnetic resonance imaging (MRI).
>
---
#### [replaced 051] ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.07720v2](https://arxiv.org/pdf/2512.07720v2)**

> **作者:** Fan Yang; Heyuan Li; Peihao Li; Weihao Yuan; Lingteng Qiu; Chaoyue Song; Cheng Chen; Yisheng He; Shifeng Zhang; Xiaoguang Han; Steven Hoi; Guosheng Lin
>
> **备注:** Project page: \url{https://lhyfst.github.io/visa}
>
> **摘要:** Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa
>
---
#### [replaced 052] Towards Task-Oriented Flying: Framework, Infrastructure, and Principles
- **分类: cs.RO; cs.AI; cs.CV; cs.LG**

- **简介: 该论文针对无人机在非结构化环境中的自主飞行任务，解决端到端学习控制缺乏系统设计与统一基础设施的问题。提出任务导向框架，整合仿真、训练与部署，支持可复现的深度强化学习，实现强鲁棒性和真实环境迁移。**

- **链接: [https://arxiv.org/pdf/2504.15129v2](https://arxiv.org/pdf/2504.15129v2)**

> **作者:** Kangyao Huang; Hao Wang; Jingyu Chen; Jintao Chen; Yu Luo; Di Guo; Xiangkui Zhang; Xiangyang Ji; Huaping Liu
>
> **摘要:** Deploying robot learning methods to aerial robots in unstructured environments remains both challenging and promising. While recent advances in deep reinforcement learning (DRL) have enabled end-to-end flight control, the field still lacks systematic design guidelines and a unified infrastructure to support reproducible training and real-world deployment. We present a task-oriented framework for end-to-end DRL in quadrotors that integrates design principles for complex task specification and reveals the interdependencies among simulated task definition, training design principles, and physical deployment. Our framework involves software infrastructure, hardware platforms, and open-source firmware to support a full-stack learning infrastructure and workflow. Extensive empirical results demonstrate robust flight and sim-to-real generalization under real-world disturbances. By reducing the entry barrier for deploying learning-based controllers on aerial robots, our work lays a practical foundation for advancing autonomous flight in dynamic and unstructured environments.
>
---
#### [replaced 053] CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2508.07871v2](https://arxiv.org/pdf/2508.07871v2)**

> **作者:** Yanshu Li; Jianjiang Yang; Zhennan Shen; Ligong Han; Haoyan Xu; Ruixiang Tang
>
> **备注:** 14 pages, 12 figures, 6 tables
>
> **摘要:** Modern large vision-language models (LVLMs) convert each input image into a large set of tokens that far outnumber the text tokens. Although this improves visual perception, it also introduces severe image token redundancy. Because image tokens contain sparse information, many contribute little to reasoning but greatly increase inference cost. Recent image token pruning methods address this issue by identifying important tokens and removing the rest. These methods improve efficiency with only small performance drops. However, most of them focus on single-image tasks and overlook multimodal in-context learning (ICL), where redundancy is higher and efficiency is more important. Redundant tokens weaken the advantage of multimodal ICL for rapid domain adaptation and lead to unstable performance. When existing pruning methods are applied in this setting, they cause large accuracy drops, which exposes a clear gap and the need for new approaches. To address this, we propose Contextually Adaptive Token Pruning (CATP), a training-free pruning method designed for multimodal ICL. CATP uses two stages of progressive pruning that fully reflect the complex cross-modal interactions in the input sequence. After removing 77.8% of the image tokens, CATP achieves an average performance gain of 0.6% over the vanilla model on four LVLMs and eight benchmarks, clearly outperforming all baselines. At the same time, it improves efficiency by reducing inference latency by an average of 10.78%. CATP strengthens the practical value of multimodal ICL and lays the foundation for future progress in interleaved image-text settings.
>
---
#### [replaced 054] Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models
- **分类: cs.AI; cs.CV**

- **链接: [https://arxiv.org/pdf/2511.09907v2](https://arxiv.org/pdf/2511.09907v2)**

> **作者:** Yongxian Wei; Yilin Zhao; Li Shen; Xinrui Chen; Runxi Cheng; Sinan Du; Hao Yu; Gang Liu; Jiahong Yan; Chun Yuan; Dian Li
>
> **摘要:** Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.
>
---
#### [replaced 055] Towards Explainable Bilingual Multimodal Misinformation Detection and Localization
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2506.22930v3](https://arxiv.org/pdf/2506.22930v3)**

> **作者:** Yiwei He; Zhenglin Huang; Haiquan Wen; Tianxiao Li; Yi Dong; Hao Fei; Baoyuan Wu; Guangliang Cheng
>
> **摘要:** The increasing realism of multimodal content has made misinformation more subtle and harder to detect, especially in news media where images are frequently paired with bilingual (e.g., Chinese-English) subtitles. Such content often includes localized image edits and cross-lingual inconsistencies that jointly distort meaning while remaining superficially plausible. We introduce BiMi, a bilingual multimodal framework that jointly performs region-level localization, cross-modal and cross-lingual consistency detection, and natural language explanation for misinformation analysis. To support generalization, BiMi integrates an online retrieval module that supplements model reasoning with up-to-date external context. We further release BiMiBench, a large-scale and comprehensive benchmark constructed by systematically editing real news images and subtitles, comprising 104,000 samples with realistic manipulations across visual and linguistic modalities. To enhance interpretability, we apply Group Relative Policy Optimization (GRPO) to improve explanation quality, marking the first use of GRPO in this domain. Extensive experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore, advancing state-of-the-art performance in realistic, multilingual misinformation detection. Code, models, and datasets will be released.
>
---
#### [replaced 056] OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows
- **分类: cs.AI; cs.CL; cs.CV; cs.HC**

- **简介: 该论文研究移动GUI智能体的安全增强，提出OS-Sentinel框架，结合形式化验证与基于VLM的上下文判断，检测操作中的系统风险与隐私泄露。构建了MobileRisk-Live沙箱环境与基准，实验证明其有效性。**

- **链接: [https://arxiv.org/pdf/2510.24411v2](https://arxiv.org/pdf/2510.24411v2)**

> **作者:** Qiushi Sun; Mukai Li; Zhoumianze Liu; Zhihui Xie; Fangzhi Xu; Zhangyue Yin; Kanzhi Cheng; Zehao Li; Zichen Ding; Qi Liu; Zhiyong Wu; Zhuosheng Zhang; Ben Kao; Lingpeng Kong
>
> **备注:** work in progress
>
> **摘要:** Computer-using agents powered by Vision-Language Models (VLMs) have demonstrated human-like capabilities in operating digital environments like mobile platforms. While these agents hold great promise for advancing digital automation, their potential for unsafe operations, such as system compromise and privacy leakage, is raising significant concerns. Detecting these safety concerns across the vast and complex operational space of mobile environments presents a formidable challenge that remains critically underexplored. To establish a foundation for mobile agent safety research, we introduce MobileRisk-Live, a dynamic sandbox environment accompanied by a safety detection benchmark comprising realistic trajectories with fine-grained annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety detection framework that synergistically combines a Formal Verifier for detecting explicit system-level violations with a VLM-based Contextual Judge for assessing contextual risks and agent actions. Experiments show that OS-Sentinel achieves 10%-30% improvements over existing approaches across multiple metrics. Further analysis provides critical insights that foster the development of safer and more reliable autonomous mobile agents. Our code and data are available at https://github.com/OS-Copilot/OS-Sentinel.
>
---
#### [replaced 057] AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery
- **分类: q-bio.QM; cs.AI; cs.CV; cs.LG**

- **链接: [https://arxiv.org/pdf/2501.06039v2](https://arxiv.org/pdf/2501.06039v2)**

> **作者:** Johann Wenckstern; Eeshaan Jain; Yexiang Cheng; Benedikt von Querfurth; Kiril Vasilev; Matteo Pariset; Phil F. Cheng; Petros Liakopoulos; Olivier Michielin; Andreas Wicki; Gabriele Gut; Charlotte Bunne
>
> **备注:** 25 pages, 5 figures
>
> **摘要:** Spatial proteomics technologies have transformed our understanding of complex tissue architecture in cancer but present unique challenges for computational analysis. Each study uses a different marker panel and protocol, and most methods are tailored to single cohorts, which limits knowledge transfer and robust biomarker discovery. Here we present Virtual Tissues (VirTues), a general-purpose foundation model for spatial proteomics that learns marker-aware, multi-scale representations of proteins, cells, niches and tissues directly from multiplex imaging data. From a single pretrained backbone, VirTues supports marker reconstruction, cell typing and niche annotation, spatial biomarker discovery, and patient stratification, including zero-shot annotation across heterogeneous panels and datasets. In triple-negative breast cancer, VirTues-derived biomarkers predict anti-PD-L1 chemo-immunotherapy response and stratify disease-free survival in an independent cohort, outperforming state-of-the-art biomarkers derived from the same datasets and current clinical stratification schemes.
>
---
#### [replaced 058] PET Image Reconstruction Using Deep Diffusion Image Prior
- **分类: eess.IV; cs.CV; physics.med-ph**

- **链接: [https://arxiv.org/pdf/2507.15078v2](https://arxiv.org/pdf/2507.15078v2)**

> **作者:** Fumio Hashimoto; Kuang Gong
>
> **备注:** 11 pages, 12 figures
>
> **摘要:** Diffusion models have shown great promise in medical image denoising and reconstruction, but their application to Positron Emission Tomography (PET) imaging remains limited by tracer-specific contrast variability and high computational demands. In this work, we proposed an anatomical prior-guided PET image reconstruction method based on diffusion models, inspired by the deep diffusion image prior (DDIP) framework. The proposed method alternated between diffusion sampling and model fine-tuning guided by the PET sinogram, enabling the reconstruction of high-quality images from various PET tracers using a score function pretrained on a dataset of another tracer. To improve computational efficiency, the half-quadratic splitting (HQS) algorithm was adopted to decouple network optimization from iterative PET reconstruction. The proposed method was evaluated using one simulation and two clinical datasets. For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested on [$^{18}$F]FDG data and amyloid-negative PET data to assess out-of-distribution (OOD) performance. For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one [$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from another tracer. Experiment results show that the proposed PET reconstruction method can generalize robustly across tracer distributions and scanner types, providing an efficient and versatile reconstruction framework for low-dose PET imaging.
>
---
#### [replaced 059] Zo3T: Zero-Shot 3D-Aware Trajectory-Guided Image-to-Video Generation via Test-Time Training
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2509.06723v3](https://arxiv.org/pdf/2509.06723v3)**

> **作者:** Ruicheng Zhang; Jun Zhou; Zunnan Xu; Zihao Liu; Jiehui Huang; Mingyang Zhang; Yu Sun; Xiu Li
>
> **摘要:** Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos that adhere to user-specified motion instructions. Existing methods typically rely on computationally expensive fine-tuning on scarce annotated datasets. Although some zero-shot methods attempt to trajectory control in the latent space, they may yield unrealistic motion by neglecting 3D perspective and creating a misalignment between the manipulated latents and the network's noise predictions. To address these challenges, we introduce Zo3T, a novel zero-shot test-time-training framework for trajectory-guided generation with three core innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging inferring scene depth to derive perspective-correct affine transformations for target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a mechanism that dynamically injects and optimizes ephemeral LoRA adapters into the denoising network alongside the latent state. Driven by a regional feature consistency loss, this co-adaptation effectively enforces motion constraints while allowing the pre-trained model to locally adapt its internal representations to the manipulated latent, thereby ensuring generative fidelity and on-manifold adherence. Finally, we develop Guidance Field Rectification, which refines the denoising evolutionary path by optimizing the conditional guidance field through a one-step lookahead strategy, ensuring efficient generative progression towards the target trajectory. Zo3T significantly enhances 3D realism and motion accuracy in trajectory-controlled I2V generation, demonstrating superior performance over existing training-based and zero-shot approaches.
>
---
#### [replaced 060] Domain-RAG: Retrieval-Guided Compositional Image Generation for Cross-Domain Few-Shot Object Detection
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2506.05872v2](https://arxiv.org/pdf/2506.05872v2)**

> **作者:** Yu Li; Xingyu Qiu; Yuqian Fu; Jie Chen; Tianwen Qian; Xu Zheng; Danda Pani Paudel; Yanwei Fu; Xuanjing Huang; Luc Van Gool; Yu-Gang Jiang
>
> **摘要:** Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel objects with only a handful of labeled samples from previously unseen domains. While data augmentation and generative methods have shown promise in few-shot learning, their effectiveness for CD-FSOD remains unclear due to the need for both visual realism and domain alignment. Existing strategies, such as copy-paste augmentation and text-to-image generation, often fail to preserve the correct object category or produce backgrounds coherent with the target domain, making them non-trivial to apply directly to CD-FSOD. To address these challenges, we propose Domain-RAG, a training-free, retrieval-guided compositional image generation framework tailored for CD-FSOD. Domain-RAG consists of three stages: domain-aware background retrieval, domain-guided background generation, and foreground-background composition. Specifically, the input image is first decomposed into foreground and background regions. We then retrieve semantically and stylistically similar images to guide a generative model in synthesizing a new background, conditioned on both the original and retrieved contexts. Finally, the preserved foreground is composed with the newly generated domain-aligned background to form the generated image. Without requiring any additional supervision or training, Domain-RAG produces high-quality, domain-consistent samples across diverse tasks, including CD-FSOD, remote sensing FSOD, and camouflaged FSOD. Extensive experiments show consistent improvements over strong baselines and establish new state-of-the-art results. Codes will be released upon acceptance.
>
---
#### [replaced 061] Bezier Splatting for Fast and Differentiable Vector Graphics Rendering
- **分类: cs.GR; cs.CV**

- **链接: [https://arxiv.org/pdf/2503.16424v4](https://arxiv.org/pdf/2503.16424v4)**

> **作者:** Xi Liu; Chaoyi Zhou; Nanxuan Zhao; Siyu Huang
>
> **备注:** NeurIPS 2025. Project page: https://xiliu8006.github.io/Bezier_splatting_project/
>
> **摘要:** Differentiable vector graphics (VGs) are widely used in image vectorization and vector synthesis, while existing representations are costly to optimize and struggle to achieve high-quality rendering results for high-resolution images. This work introduces a new differentiable VG representation, dubbed Bézier Splatting, that enables fast yet high-fidelity VG rasterization. Bézier Splatting samples 2D Gaussians along Bézier curves, which naturally provide positional gradients at object boundaries. Thanks to the efficient splatting-based differentiable rasterizer, Bézier Splatting achieves 30x and 150x faster per forward and backward rasterization step for open curves compared to DiffVG. Additionally, we introduce an adaptive pruning and densification strategy that dynamically adjusts the spatial distribution of curves to escape local minima, further improving VG quality. Furthermore, our new VG representation supports conversion to standard XML-based SVG format, enhancing interoperability with existing VG tools and pipelines. Experimental results show that Bézier Splatting significantly outperforms existing methods with better visual fidelity and significant optimization speedup.
>
---
#### [replaced 062] TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning
- **分类: cs.CV; cs.AI**

- **链接: [https://arxiv.org/pdf/2512.07135v2](https://arxiv.org/pdf/2512.07135v2)**

> **作者:** Zebin Xing; Pengxuan Yang; Linbo Wang; Yichen Zhang; Yiming Hu; Yupeng Zheng; Junli Wang; Yinfeng Gao; Guang Li; Kun Ma; Long Chen; Zhongpu Xia; Qichao Zhang; Hangjun Ye; Dongbin Zhao
>
> **摘要:** Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.
>
---
#### [replaced 063] A Data-driven Typology of Vision Models from Integrated Representational Metrics
- **分类: cs.CV; cs.AI**

- **链接: [https://arxiv.org/pdf/2509.21628v2](https://arxiv.org/pdf/2509.21628v2)**

> **作者:** Jialin Wu; Shreya Saha; Yiqing Bo; Meenakshi Khosla
>
> **摘要:** Large vision models differ widely in architecture and training paradigm, yet we lack principled methods to determine which aspects of their representations are shared across families and which reflect distinctive computational strategies. We leverage a suite of representational similarity metrics, each capturing a different facet-geometry, unit tuning, or linear decodability-and assess family separability using multiple complementary measures. Metrics preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family discrimination, whereas flexible mappings such as Linear Predictivity show weaker separation. These findings indicate that geometry and tuning carry family-specific signatures, while linearly decodable information is more broadly shared. To integrate these complementary facets, we adapt Similarity Network Fusion (SNF), a method inspired by multi-omics integration. SNF achieves substantially sharper family separation than any individual metric and produces robust composite signatures. Clustering of the fused similarity matrix recovers both expected and surprising patterns: supervised ResNets and ViTs form distinct clusters, yet all self-supervised models group together across architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with masked autoencoders, suggesting convergence between architectural modernization and reconstruction-based training. This biology-inspired framework provides a principled typology of vision models, showing that emergent computational strategies-shaped jointly by architecture and training objective-define representational structure beyond surface design categories.
>
---
#### [replaced 064] MeshRipple: Structured Autoregressive Generation of Artist-Meshes
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.07514v2](https://arxiv.org/pdf/2512.07514v2)**

> **作者:** Junkai Lin; Hang Long; Huipeng Guo; Jielei Zhang; JiaYi Yang; Tianle Guo; Yang Yang; Jianwen Li; Wenxiao Zhang; Matthias Nießner; Wei Yang
>
> **摘要:** Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface. MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies. This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.
>
---
#### [replaced 065] Enabling Validation for Robust Few-Shot Recognition
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2506.04713v3](https://arxiv.org/pdf/2506.04713v3)**

> **作者:** Hanxin Wang; Tian Liu; Shu Kong
>
> **备注:** Project website: https://hannawang09.github.io/projects/vest/
>
> **摘要:** Few-Shot Recognition (FSR) tackles classification tasks by training with minimal task-specific labeled data. Prevailing methods adapt or finetune a pretrained Vision-Language Model (VLM) and augment the scarce training data by retrieving task-relevant but noisy samples from open data sources. The finetuned VLM generalizes decently well to the task-specific in-distribution (ID) test data but struggles with out-of-distribution (OOD) test data. This motivates our study of robust FSR with VLM finetuning. The core challenge of FSR is data scarcity, extending beyond limited training data to a complete lack of validation data. We identify a key paradox as a potential solution: repurposing the retrieved open data for validation. As such retrieved data are inherently OOD compared with the task-specific ID training data, finetuned VLMs yield degraded performance on the retrieved data. This causes the validation logic to favor the pretrained model without any finetuning, hindering improvements w.r.t generalization. To resolve this dilemma, we introduce a novel validation strategy that harmonizes performance gain and degradation on the few-shot ID data and the retrieved data, respectively. Our validation enables parameter selection for partial finetuning and checkpoint selection, mitigating overfitting and improving test-data generalization. We unify this strategy with robust learning into a cohesive framework: Validation-Enabled Stage-wise Tuning (VEST). Extensive experiments on the established ImageNet OOD benchmarks show that VEST significantly outperforms existing VLM adaptation methods, achieving state-of-the-art FSR performance on both ID and OOD data.
>
---
#### [replaced 066] From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images
- **分类: cs.CV; cs.GR**

- **链接: [https://arxiv.org/pdf/2512.07527v2](https://arxiv.org/pdf/2512.07527v2)**

> **作者:** Fei Yu; Yu Liu; Luyang Tang; Mingchao Sun; Zengye Ge; Rui Bu; Yuchao Jin; Haisen Zhao; He Sun; Yangyan Li; Mu Xu; Wenzheng Chen; Baoquan Chen
>
> **摘要:** City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail. To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs. Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation. Project page can be found at https://pku-vcl-geometry.github.io/Orbit2Ground/.
>
---
#### [replaced 067] Shape and Texture Recognition in Large Vision-Language Models
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2503.23062v5](https://arxiv.org/pdf/2503.23062v5)**

> **作者:** Sagi Eppel; Mor Bismut; Alona Faktor-Strugatski
>
> **摘要:** Shapes and textures are the basic building blocks of visual perception. The ability to identify shapes regardless of orientation, texture, or context, and to recognize textures and materials independently of their associated objects, is essential for a general visual understanding of the world. This work introduces the Large Shapes and Textures dataset (LAS&T), a giant collection of highly diverse shapes and textures, created by unsupervised extraction of patterns from natural images. This dataset is used to benchmark how effectively leading Large Vision-Language Models (LVLM/VLM) recognize and represent shapes, textures, and materials in 2D and 3D scenes. For shape recognition, we test the models' ability to match images of identical shapes that differ in orientation, texture, color, or environment. Our results show that the shape-recognition capabilities of LVLMs remain well below human performance, especially when multiple transformations are applied. LVLMs rely predominantly on high-level and semantic features and struggle with abstract shapes lacking class associations. For texture and material recognition, we evaluated the models' ability to identify images with identical textures and materials across different objects and environments. Interestingly, leading LVLMs approach human-level performance in recognizing materials in 3D scenes, yet substantially underperform humans when identifying simpler, more abstract 2D textures and shapes. These results are consistent across a wide range of leading LVLMs (GPT/Gemini/LLama/Qwen) and foundation vision models (DINO/CLIP), exposing major deficiencies in the ability of VLMs to extract low-level visual features. In contrast, humans and simple nets trained directly for these tasks achieve high accuracy. The LAS&T dataset, featuring over 700,000 images for 2D/3D shape and textures recognition and retrieval, is freely available.
>
---
#### [replaced 068] COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.07107v2](https://arxiv.org/pdf/2512.07107v2)**

> **作者:** Jaeyoon Lee; Hojoon Jung; Sungtae Hwang; Jihyong Oh; Jongwon Choi
>
> **备注:** Project page: https://cau-vilab.github.io/COREA/
>
> **摘要:** We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.
>
---
#### [replaced 069] CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics
- **分类: cs.CV**

- **链接: [https://arxiv.org/pdf/2512.07155v2](https://arxiv.org/pdf/2512.07155v2)**

> **作者:** Dahyeon Kye; Jeahun Sung; Mingyu Jeon; Jihyong Oh
>
> **备注:** Please visit our project page at https://cmlab-korea.github.io/CHIMERA/
>
> **摘要:** Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.
>
---
