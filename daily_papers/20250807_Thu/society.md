# 计算机与社会 cs.CY

- **最新发布 14 篇**

- **更新 7 篇**

## 最新发布

#### [new 001] Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 该论文旨在诊断当前中心化AI会议的可持续性危机，分析其四个结构性压力（科学/环境/心理/物流）并提出Community-Federated Conference（CFC）模型作为替代方案。**

- **链接: [http://arxiv.org/pdf/2508.04586v1](http://arxiv.org/pdf/2508.04586v1)**

> **作者:** Nuo Chen; Moming Duan; Andre Huikai Lin; Qian Wang; Jiaying Wu; Bingsheng He
>
> **备注:** Preprint
>
> **摘要:** Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.
>
---
#### [new 002] Prompt Injection Vulnerability of Consensus Generating Applications in Digital Democracy
- **分类: cs.CY; cs.CR**

- **简介: 该论文探讨LLM在数字民主中的安全与攻击性问题，研究了提示注入攻击对共识生成系统的潜在影响，并提出Direct Preference Optimization（DPO）作为防御手段，旨在提升模型对模糊共识的鲁棒性。**

- **链接: [http://arxiv.org/pdf/2508.04281v1](http://arxiv.org/pdf/2508.04281v1)**

> **作者:** Jairo Gudiño-Rosero; Clément Contet; Umberto Grandi; César A. Hidalgo
>
> **备注:** 24 pages, 14 figures
>
> **摘要:** Large Language Models (LLMs) are gaining traction as a method to generate consensus statements and aggregate preferences in digital democracy experiments. Yet, LLMs may introduce critical vulnerabilities in these systems. Here, we explore the impact of prompt-injection attacks targeting consensus generating systems by introducing a four-dimensional taxonomy of attacks. We test these attacks using LLaMA 3.1 8B and Chat GPT 4.1 Nano finding the LLMs more vulnerable to criticism attacks -- attacks using disagreeable prompts -- and more effective at tilting ambiguous consensus statements. We also find evidence of more effective manipulation when using explicit imperatives and rational-sounding arguments compared to emotional language or fabricated statistics. To mitigate these vulnerabilities, we apply Direct Preference Optimization (DPO), an alignment method that fine-tunes LLMs to prefer unperturbed consensus statements. While DPO significantly improves robustness, it still offers limited protection against attacks targeting ambiguous consensus. These results advance our understanding of the vulnerability and robustness of consensus generating LLMs in digital democracy applications.
>
---
#### [new 003] Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding
- **分类: cs.CY; cs.AI; cs.CL; cs.LG**

- **简介: 该论文旨在构建健康保险规则理解语料库并开发预测模型，解决理解复杂政策与公平性问题，通过收集法律与医学文本及设计任务提升监管与患者自决能力。**

- **链接: [http://arxiv.org/pdf/2508.03718v1](http://arxiv.org/pdf/2508.03718v1)**

> **作者:** Mike Gartner
>
> **备注:** 22 pages, 7 figures
>
> **摘要:** U.S. health insurance is complex, and inadequate understanding and limited access to justice have dire implications for the most vulnerable. Advances in natural language processing present an opportunity to support efficient, case-specific understanding, and to improve access to justice and healthcare. Yet existing corpora lack context necessary for assessing even simple cases. We collect and release a corpus of reputable legal and medical text related to U.S. health insurance. We also introduce an outcome prediction task for health insurance appeals designed to support regulatory and patient self-help applications, and release a labeled benchmark for our task, and models trained on it.
>
---
#### [new 004] Moving beyond harm. A critical review of how NLP research approaches discrimination
- **分类: cs.CY**

- **简介: 该论文旨在批判当前NLP研究对算法歧视的认知框架（侧重技术解决与术语局限），通过分析ACL 2022案例揭示伦理词汇（如"harm"）的不足，提出以"injustice"替代"bias"作为新框架，解决系统性歧视问题。**

- **链接: [http://arxiv.org/pdf/2508.04504v1](http://arxiv.org/pdf/2508.04504v1)**

> **作者:** Katrin Schulz; Marjolein Lanzing; Giulia Martinez Brenner
>
> **摘要:** How to avoid discrimination in the context of NLP technology is one of the major challenges in the field. We propose that a different and more substantiated framing of the problem could help to find more productive approaches. In the first part of the paper we report on a case study: a qualitative review on papers published in the ACL anthologies 2022 on discriminatory behavior of NLP systems. We find that the field (i) still has a strong focus on a technological fix of algorithmic discrimination, and (ii) is struggling with a firm grounding of their ethical or normative vocabulary. Furthermore, this vocabulary is very limited, focusing mostly on the terms "harm" and "bias". In the second part of the paper we argue that addressing the latter problems might help with the former. The understanding of algorithmic discrimination as a technological problem is reflected in and reproduced by the vocabulary in use. The notions of "harm" and "bias" implicate a narrow framing of the issue of discrimination as one of the system-user interface. We argue that instead of "harm" the debate should make "injustice" the key notion. This would force us to understand the problem of algorithmic discrimination as a systemic problem. Thereby, it would broaden our perspective on the complex interactions that make NLP technology participate in discrimination. With that gain in perspective we can consider new angles for solutions.
>
---
#### [new 005] Development of management systems using artificial intelligence systems and machine learning methods for boards of directors (preprint, unofficial translation)
- **分类: cs.CY; cs.AI; cs.LG**

- **简介: 该论文旨在解决AI在董事会管理中的自主决策与法律伦理冲突问题，提出"参考模型"融合AI技术与法律框架，通过"计算法"和XAI确保透明性与合规性，构建了安全运营环境并优化算法训练。**

- **链接: [http://arxiv.org/pdf/2508.03769v1](http://arxiv.org/pdf/2508.03769v1)**

> **作者:** Anna Romanova
>
> **摘要:** The study addresses the paradigm shift in corporate management, where AI is moving from a decision support tool to an autonomous decision-maker, with some AI systems already appointed to leadership roles in companies. A central problem identified is that the development of AI technologies is far outpacing the creation of adequate legal and ethical guidelines. The research proposes a "reference model" for the development and implementation of autonomous AI systems in corporate management. This model is based on a synthesis of several key components to ensure legitimate and ethical decision-making. The model introduces the concept of "computational law" or "algorithmic law". This involves creating a separate legal framework for AI systems, with rules and regulations translated into a machine-readable, algorithmic format to avoid the ambiguity of natural language. The paper emphasises the need for a "dedicated operational context" for autonomous AI systems, analogous to the "operational design domain" for autonomous vehicles. This means creating a specific, clearly defined environment and set of rules within which the AI can operate safely and effectively. The model advocates for training AI systems on controlled, synthetically generated data to ensure fairness and ethical considerations are embedded from the start. Game theory is also proposed as a method for calculating the optimal strategy for the AI to achieve its goals within these ethical and legal constraints. The provided analysis highlights the importance of explainable AI (XAI) to ensure the transparency and accountability of decisions made by autonomous systems. This is crucial for building trust and for complying with the "right to explanation".
>
---
#### [new 006] Trustworthiness of Legal Considerations for the Use of LLMs in Education
- **分类: cs.CY; cs.AI**

- **简介: 该论文旨在分析全球AI法规对LLMs在教育领域的信任度保障，探讨其政策差异与挑战，并为GCC国家构建合规框架提供指导。任务解决如何平衡技术发展与伦理规范，工作包括比较多国体系、设计适应GCC需求的治理框架。**

- **链接: [http://arxiv.org/pdf/2508.03771v1](http://arxiv.org/pdf/2508.03771v1)**

> **作者:** Sara Alaswad; Tatiana Kalganova; Wasan Awad
>
> **备注:** 11 pages, 3 figures, 6 tables
>
> **摘要:** As Artificial Intelligence (AI), particularly Large Language Models (LLMs), becomes increasingly embedded in education systems worldwide, ensuring their ethical, legal, and contextually appropriate deployment has become a critical policy concern. This paper offers a comparative analysis of AI-related regulatory and ethical frameworks across key global regions, including the European Union, United Kingdom, United States, China, and Gulf Cooperation Council (GCC) countries. It maps how core trustworthiness principles, such as transparency, fairness, accountability, data privacy, and human oversight are embedded in regional legislation and AI governance structures. Special emphasis is placed on the evolving landscape in the GCC, where countries are rapidly advancing national AI strategies and education-sector innovation. To support this development, the paper introduces a Compliance-Centered AI Governance Framework tailored to the GCC context. This includes a tiered typology and institutional checklist designed to help regulators, educators, and developers align AI adoption with both international norms and local values. By synthesizing global best practices with region-specific challenges, the paper contributes practical guidance for building legally sound, ethically grounded, and culturally sensitive AI systems in education. These insights are intended to inform future regulatory harmonization and promote responsible AI integration across diverse educational environments.
>
---
#### [new 007] Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文旨在探讨多智能体协作对高质科研灵感的驱动作用，通过构建多代理系统验证其优势，对比不同配置（如团队规模、领导结构及专业性）并量化评估其性能，发现领导作用能提升创意质量，认知多样性与专业能力共同决定成果。**

- **链接: [http://arxiv.org/pdf/2508.04575v1](http://arxiv.org/pdf/2508.04575v1)**

> **作者:** Nuo Chen; Yicheng Tong; Jiaying Wu; Minh Duc Duong; Qian Wang; Qingyun Zou; Bryan Hooi; Bingsheng He
>
> **备注:** Preprint
>
> **摘要:** While AI agents show potential in scientific ideation, most existing frameworks rely on single-agent refinement, limiting creativity due to bounded knowledge and perspective. Inspired by real-world research dynamics, this paper investigates whether structured multi-agent discussions can surpass solitary ideation. We propose a cooperative multi-agent framework for generating research proposals and systematically compare configurations including group size, leaderled versus leaderless structures, and team compositions varying in interdisciplinarity and seniority. To assess idea quality, we employ a comprehensive protocol with agent-based scoring and human review across dimensions such as novelty, strategic vision, and integration depth. Our results show that multi-agent discussions substantially outperform solitary baselines. A designated leader acts as a catalyst, transforming discussion into more integrated and visionary proposals. Notably, we find that cognitive diversity is a primary driver of quality, yet expertise is a non-negotiable prerequisite, as teams lacking a foundation of senior knowledge fail to surpass even a single competent agent. These findings offer actionable insights for designing collaborative AI ideation systems and shed light on how team structure influences creative outcomes.
>
---
#### [new 008] SoK: Stablecoins for Digital Transformation -- Design, Metrics, and Application with Real World Asset Tokenization as a Case Study
- **分类: econ.GN; cs.CE; cs.CR; cs.CY; q-fin.CP; q-fin.EC**

- **简介: 该论文旨在填补稳定币系统设计、评估与应用的跨学科研究空白，构建了基于custodial结构、稳定机制和治理的系统分类框架，并开发了面向不同利益相关者的绩效评估模型，通过Real World Asset tokenization案例验证其在跨境数字金融中的实践价值，推动了可信、透明的数字货币基础设施建设。**

- **链接: [http://arxiv.org/pdf/2508.02403v1](http://arxiv.org/pdf/2508.02403v1)**

> **作者:** Luyao Zhang
>
> **摘要:** Stablecoins have become a foundational component of the digital asset ecosystem, with their market capitalization exceeding 230 billion USD as of May 2025. As fiat-referenced and programmable assets, stablecoins provide low-latency, globally interoperable infrastructure for payments, decentralized finance, DeFi, and tokenized commerce. Their accelerated adoption has prompted extensive regulatory engagement, exemplified by the European Union's Markets in Crypto-assets Regulation, MiCA, the US Guiding and Establishing National Innovation for US Stablecoins Act, GENIUS Act, and Hong Kong's Stablecoins Bill. Despite this momentum, academic research remains fragmented across economics, law, and computer science, lacking a unified framework for design, evaluation, and application. This study addresses that gap through a multi-method research design. First, it synthesizes cross-disciplinary literature to construct a taxonomy of stablecoin systems based on custodial structure, stabilization mechanism, and governance. Second, it develops a performance evaluation framework tailored to diverse stakeholder needs, supported by an open-source benchmarking pipeline to ensure transparency and reproducibility. Third, a case study on Real World Asset tokenization illustrates how stablecoins operate as programmable monetary infrastructure in cross-border digital systems. By integrating conceptual theory with empirical tools, the paper contributes: a unified taxonomy for stablecoin design; a stakeholder-oriented performance evaluation framework; an empirical case linking stablecoins to sectoral transformation; and reproducible methods and datasets to inform future research. These contributions support the development of trusted, inclusive, and transparent digital monetary infrastructure.
>
---
#### [new 009] Recommending With, Not For: Co-Designing Recommender Systems for Social Good
- **分类: cs.HC; cs.CY; cs.IR**

- **简介: 该论文探讨了推荐系统的设计应由其受益人（用户、创作者）共同参与而非仅由开发团队制定，旨在通过参与式民主机制实现社会价值导向，解决当前设计偏重技术性而忽视用户和社会利益的问题。**

- **链接: [http://arxiv.org/pdf/2508.03792v1](http://arxiv.org/pdf/2508.03792v1)**

> **作者:** Michael D. Ekstrand; Afsaneh Razi; Aleksandra Sarcevic; Maria Soledad Pera; Robin Burke; Katherine Landau Wright
>
> **备注:** Accepted to ACM TORS Special Issue on Recommender Systems for Social Good
>
> **摘要:** Recommender systems are usually designed by engineers, researchers, designers, and other members of development teams. These systems are then evaluated based on goals set by the aforementioned teams and other business units of the platforms operating the recommender systems. This design approach emphasizes the designers' vision for how the system can best serve the interests of users, providers, businesses, and other stakeholders. Although designers may be well-informed about user needs through user experience and market research, they are still the arbiters of the system's design and evaluation, with other stakeholders' interests less emphasized in user-centered design and evaluation. When extended to recommender systems for social good, this approach results in systems that reflect the social objectives as envisioned by the designers and evaluated as the designers understand them. Instead, social goals and operationalizations should be developed through participatory and democratic processes that are accountable to their stakeholders. We argue that recommender systems aimed at improving social good should be designed *by* and *with*, not just *for*, the people who will experience their benefits and harms. That is, they should be designed in collaboration with their users, creators, and other stakeholders as full co-designers, not only as user study participants.
>
---
#### [new 010] Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals
- **分类: cs.AI; cs.CY**

- **简介: 该论文旨在通过生成式AI实现个性化学习内容，解决传统教育与职业目标脱节的问题，探索其对学习效率的影响，并验证了跨学科知识适配的有效性。**

- **链接: [http://arxiv.org/pdf/2508.04070v1](http://arxiv.org/pdf/2508.04070v1)**

> **作者:** Ronja Mehlan; Claudia Hess; Quintus Stierstorfer; Kristina Schaaff
>
> **摘要:** As artificial intelligence becomes increasingly integrated into digital learning environments, the personalization of learning content to reflect learners' individual career goals offers promising potential to enhance engagement and long-term motivation. In our study, we investigate how career goal-based content adaptation in learning systems based on generative AI (GenAI) influences learner engagement, satisfaction, and study efficiency. The mixed-methods experiment involved more than 4,000 learners, with one group receiving learning scenarios tailored to their career goals and a control group. Quantitative results show increased session duration, higher satisfaction ratings, and a modest reduction in study duration compared to standard content. Qualitative analysis highlights that learners found the personalized material motivating and practical, enabling deep cognitive engagement and strong identification with the content. These findings underscore the value of aligning educational content with learners' career goals and suggest that scalable AI personalization can bridge academic knowledge and workplace applicability.
>
---
#### [new 011] "Think First, Verify Always": Training Humans to Face AI Risks
- **分类: cs.HC; cs.AI; cs.CR; cs.CY**

- **简介: 该论文旨在通过"Think First, Verify Always"（TFVA）协议训练人类应对AI风险，解决现有系统依赖设备而忽视人类因素的问题，提出基于五项原则的培训方案并验证其有效性，建议将该策略嵌入AI平台以提升信任与伦理应用。**

- **链接: [http://arxiv.org/pdf/2508.03714v1](http://arxiv.org/pdf/2508.03714v1)**

> **作者:** Yuksel Aydin
>
> **摘要:** Artificial intelligence enables unprecedented attacks on human cognition, yet cybersecurity remains predominantly device-centric. This paper introduces the "Think First, Verify Always" (TFVA) protocol, which repositions humans as 'Firewall Zero', the first line of defense against AI-enabled threats. The protocol is grounded in five operational principles: Awareness, Integrity, Judgment, Ethical Responsibility, and Transparency (AIJET). A randomized controlled trial (n=151) demonstrated that a minimal 3-minute intervention produced statistically significant improvements in cognitive security task performance, with participants showing an absolute +7.87% gains compared to controls. These results suggest that brief, principles-based training can rapidly enhance human resilience against AI-driven cognitive manipulation. We recommend that GenAI platforms embed "Think First, Verify Always" as a standard prompt, replacing passive warnings with actionable protocols to enhance trustworthy and ethical AI use. By bridging the gap between technical cybersecurity and human factors, the TFVA protocol establishes human-empowered security as a vital component of trustworthy AI systems.
>
---
#### [new 012] Inequality in the Age of Pseudonymity
- **分类: cs.GT; cs.CY; econ.TH**

- **简介: 该论文研究了Pseudonymity对不平等度量的影响，提出Sybil-证明方法以缓解其对数据精度的潜在损害。任务为揭示Pseudonymity下经济不平等的测量局限性，工作包括构建满足文献属性的Sybil-安全指标体系。**

- **链接: [http://arxiv.org/pdf/2508.04668v1](http://arxiv.org/pdf/2508.04668v1)**

> **作者:** Aviv Yaish; Nir Chemaya; Lin William Cong; Dahlia Malkhi
>
> **备注:** 38 pages, 1 figure
>
> **摘要:** Inequality measures such as the Gini coefficient are used to inform and motivate policymaking, and are increasingly applied to digital platforms. We analyze how measures fare in pseudonymous settings, as common to internet-based or blockchain-based platforms. One key challenge that arises is the ability of actors to create multiple fake identities under fictitious false names, also known as ``Sybils.'' While some actors may do so to preserve their privacy, we show that this can inadvertently distort inequality metrics. As we show, when using inequality measures that satisfy literature's canonical set of desired properties, the presence of Sybils in an economy implies that it is impossible to properly measure the economy's inequality. Then, we present several classes of Sybil-proof measures that satisfy relaxed versions of the aforementioned desired properties, and, by fully characterizing them, we prove that the structure imposed restricts their ability to assess inequality at a fine-grained level. In addition, we prove that popular inequality metrics, including the famous Gini coefficient, are vulnerable to Sybil manipulations, and examine the dynamics that result in the creation of Sybils, whether in pseudonymous settings or traditional ones.
>
---
#### [new 013] FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport
- **分类: cs.LG; cs.AI; cs.CY; stat.ML**

- **简介: 该论文提出一种模型无偏的Post-Processing框架FairPOT，旨在通过最优运输调整风险评分分布，解决高风险领域中严格公平可能导致AUC下降的问题。其工作包括：（1）设计可调λ参数平衡AUC与公平性；（2）扩展至partial AUC场景，集中干预高风险区域；（3）通过实验验证其有效性，证明具有计算效率和实用价值。**

- **链接: [http://arxiv.org/pdf/2508.03940v1](http://arxiv.org/pdf/2508.03940v1)**

> **作者:** Pengxi Liu; Yi Shen; Matthew M. Engelhard; Benjamin A. Goldstein; Michael J. Pencina; Nicoleta J. Economou-Zavlanos; Michael M. Zavlanos
>
> **摘要:** Fairness metrics utilizing the area under the receiver operator characteristic curve (AUC) have gained increasing attention in high-stakes domains such as healthcare, finance, and criminal justice. In these domains, fairness is often evaluated over risk scores rather than binary outcomes, and a common challenge is that enforcing strict fairness can significantly degrade AUC performance. To address this challenge, we propose Fair Proportional Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework that strategically aligns risk score distributions across different groups using optimal transport, but does so selectively by transforming a controllable proportion, i.e., the top-lambda quantile, of scores within the disadvantaged group. By varying lambda, our method allows for a tunable trade-off between reducing AUC disparities and maintaining overall AUC performance. Furthermore, we extend FairPOT to the partial AUC setting, enabling fairness interventions to concentrate on the highest-risk regions. Extensive experiments on synthetic, public, and clinical datasets show that FairPOT consistently outperforms existing post-processing techniques in both global and partial AUC scenarios, often achieving improved fairness with slight AUC degradation or even positive gains in utility. The computational efficiency and practical adaptability of FairPOT make it a promising solution for real-world deployment.
>
---
#### [new 014] Screen Matters: Cognitive and Behavioral Divergence Between Smartphone-Native and Computer-Native Youth
- **分类: cs.HC; cs.CY**

- **简介: 该研究探讨了手机与电脑作为数字交互模式对青少年认知与行为的影响，通过混合方法分析了数据，揭示了屏幕时间与注意力、挫败感及创造力之间的差异，并为教育设计提供了实证依据。**

- **链接: [http://arxiv.org/pdf/2508.03705v1](http://arxiv.org/pdf/2508.03705v1)**

> **作者:** Kanan Eldarov
>
> **摘要:** This study explores how different modes of digital interaction -- namely, computers versus smartphones -- affect attention, frustration, and creative performance in adolescents. Using a combination of digital task logs, webcam-based gaze estimation, and expert evaluation of task outcomes, we analyzed data from a diverse sample of 824 students aged 11-17. Participants were assigned to device groups in a randomized and stratified design to control for age, gender, and prior experience. Results suggest moderate but statistically significant differences in sustained attention, perceived frustration, and creative output. These findings indicate that the nature of digital interaction -- beyond mere screen time -- may influence cognitive and behavioral outcomes relevant to educational design. Practical implications for user interface development and learning environments are discussed.
>
---
## 更新

#### [replaced 001] The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory
- **分类: cs.CL; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2503.10533v2](http://arxiv.org/pdf/2503.10533v2)**

> **作者:** Robin Schmucker; Steven Moore
>
> **摘要:** High-quality test items are essential for educational assessments, particularly within Item Response Theory (IRT). Traditional validation methods rely on resource-intensive pilot testing to estimate item difficulty and discrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a domain-general approach for evaluating test items based on textual features. This method offers a scalable, pre-deployment evaluation without requiring student data, but its predictive validity concerning empirical IRT parameters is underexplored. To address this gap, we conducted a study involving 7,126 multiple-choice questions across various STEM subjects (physical science, mathematics, and life/earth sciences). Using an automated approach, we annotated each question with a 19-criteria IWF rubric and studied relationships to data-driven IRT parameters. Our analysis revealed statistically significant links between the number of IWFs and IRT difficulty and discrimination parameters, particularly in life/earth and physical science domains. We further observed how specific IWF criteria can impact item quality more and less severely (e.g., negative wording vs. implausible distractors) and how they might make a question more or less challenging. Overall, our findings establish automated IWF analysis as a valuable supplement to traditional validation, providing an efficient method for initial item screening, particularly for flagging low-difficulty MCQs. Our findings show the need for further research on domain-general evaluation rubrics and algorithms that understand domain-specific content for robust item validation.
>
---
#### [replaced 002] Authoritarian Recursions: How Fiction, History, and AI Reinforce Control in Education, Warfare, and Discourse
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2504.09030v4](http://arxiv.org/pdf/2504.09030v4)**

> **作者:** Hasan Oguz
>
> **备注:** 21 pages, 1 figure and 2 table, submitted to MetaScientia HPS
>
> **摘要:** This article introduces the concept of \textit{authoritarian recursion} to theorize how AI systems consolidate institutional control across education, warfare, and digital discourse. It identifies a shared recursive architecture in which algorithms mediate judgment, obscure accountability, and constrain moral and epistemic agency. Grounded in critical discourse analysis and sociotechnical ethics, the paper examines how AI systems normalize hierarchy through abstraction and feedback. Case studies -- automated proctoring, autonomous weapons, and content recommendation -- are analyzed alongside cultural imaginaries such as Orwell's \textit{Nineteen Eighty-Four}, Skynet, and \textit{Black Mirror}, used as heuristic tools to surface ethical blind spots. The analysis integrates Fairness, Accountability, and Transparency (FAccT), relational ethics, and data justice to explore how predictive infrastructures enable moral outsourcing and epistemic closure. By reframing AI as a communicative and institutional infrastructure, the article calls for governance approaches that center democratic refusal, epistemic plurality, and structural accountability.
>
---
#### [replaced 003] What Lives? A meta-analysis of diverse opinions on the definition of life
- **分类: q-bio.OT; cs.AI; cs.CY; q-bio.BM; q-bio.CB; q-bio.SC; stat.AP**

- **链接: [http://arxiv.org/pdf/2505.15849v2](http://arxiv.org/pdf/2505.15849v2)**

> **作者:** Reed Bender; Karina Kofman; Blaise Agüera y Arcas; Michael Levin
>
> **备注:** 54 pages, 4 figures, 2 tables, 11 supplemental figures, 3 supplemental tables
>
> **摘要:** The question of "what is life?" has challenged scientists and philosophers for centuries, producing an array of definitions that reflect both the mystery of its emergence and the diversity of disciplinary perspectives brought to bear on the question. Despite significant progress in our understanding of biological systems, psychology, computation, and information theory, no single definition for life has yet achieved universal acceptance. This challenge becomes increasingly urgent as advances in synthetic biology, artificial intelligence, and astrobiology challenge our traditional conceptions of what it means to be alive. We undertook a methodological approach that leverages large language models (LLMs) to analyze a set of definitions of life provided by a curated set of cross-disciplinary experts. We used a novel pairwise correlation analysis to map the definitions into distinct feature vectors, followed by agglomerative clustering, intra-cluster semantic analysis, and t-SNE projection to reveal underlying conceptual archetypes. This methodology revealed a continuous landscape of the themes relating to the definition of life, suggesting that what has historically been approached as a binary taxonomic problem should be instead conceived as differentiated perspectives within a unified conceptual latent space. We offer a new methodological bridge between reductionist and holistic approaches to fundamental questions in science and philosophy, demonstrating how computational semantic analysis can reveal conceptual patterns across disciplinary boundaries, and opening similar pathways for addressing other contested definitional territories across the sciences.
>
---
#### [replaced 004] Gender Bias in Perception of Human Managers Extends to AI Managers
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2502.17730v2](http://arxiv.org/pdf/2502.17730v2)**

> **作者:** Hao Cui; Taha Yasseri
>
> **备注:** 34 pages, 20 figures, 8 tables
>
> **摘要:** As AI becomes more embedded in workplaces, it is shifting from a tool for efficiency to an active force in organizational decision-making. Whether due to anthropomorphism or intentional design choices, people often assign human-like qualities, including gender, to AI systems. However, how AI managers are perceived in comparison to human managers and how gender influences these perceptions remains uncertain. To investigate this, we conducted randomized controlled trials (RCTs) where teams of three participants worked together under a randomly assigned manager. The manager was either a human or an AI and was presented as male, female, or gender-unspecified. The manager's role was to select the best-performing team member for an additional award. Our findings reveal that while participants initially showed no strong preference based on manager type or gender, their perceptions changed significantly after experiencing the award process. As expected, those who received awards rated their managers as more trustworthy, competent, fair, and were more willing to work with similar managers in the future, while those who were not selected viewed them less favorably. However, male managers, whether human or AI, were more positively received by awarded participants, whereas female managers, especially female AI managers, faced greater skepticism and negative judgments when they did not give awards. These results suggest that gender bias in leadership extends beyond human managers to include AI-driven decision-makers. As AI assumes more managerial responsibilities, understanding and addressing these biases will be crucial for designing fair and effective AI management systems.
>
---
#### [replaced 005] Don't Trust A Single Gerrymandering Metric
- **分类: physics.soc-ph; cs.CY; 60J20, 91C99, 91F99; J.4**

- **链接: [http://arxiv.org/pdf/2409.17186v2](http://arxiv.org/pdf/2409.17186v2)**

> **作者:** Thomas Ratliff; Stephanie Somersille; Ellen Veomett
>
> **备注:** 46 pages, 27 figures
>
> **摘要:** In recent years, in an effort to promote fairness in the election process, a wide variety of techniques and metrics have been proposed to determine whether a map is a partisan gerrymander. The most accessible measures, requiring easily obtained data, are metrics such as the Mean-Median Difference, Efficiency Gap, Declination, and GEO metric. But for most of these metrics, researchers have struggled to describe, given no additional information, how a value of that metric on a single map indicates the presence or absence of gerrymandering. Our main result is that each of these metrics is gameable when used as a single, isolated quantity to detect gerrymandering (or the lack thereof). That is, for each of the four metrics, we can find district plans for a given state with an extremely large number of Democratic-won (or Republican-won) districts while the metric value of that plan falls within a reasonable, predetermined bound. We do this by using a hill-climbing method to generate district plans that are constrained by the bounds on the metric but also maximize or nearly maximize the number of districts won by a party. In addition, extreme values of the Mean-Median Difference do not necessarily correspond to maps with an extreme number of districts won. Thus, the Mean- Median Difference metric is particularly misleading, as it cannot distinguish more extreme maps from less extreme maps. The other metrics are more nuanced, but when assessed on an ensemble, none perform substantially differently from simply measuring number of districts won by a fixed party. One clear consequence of these results is that they demonstrate the folly of specifying a priori bounds on a metric that a redistricting commission must meet in order to avoid gerrymandering.
>
---
#### [replaced 006] A Method for Assisting Novices Creating Class Diagrams Based on the Instructor's Class Layout
- **分类: cs.SE; cs.CY**

- **链接: [http://arxiv.org/pdf/2505.09116v2](http://arxiv.org/pdf/2505.09116v2)**

> **作者:** Yuta Saito; Takehiro Kokubu; Takafumi Tanaka; Atsuo Hazeyama; Hiroaki Hashiura
>
> **摘要:** Nowadays, modeling exercises on software development objects are conducted in higher education institutions for information technology. Not only are there many defects such as missing elements in the models created by learners during the exercises, but the layout of elements in the class diagrams often differs significantly from the correct answers created by the instructors. In this paper, we focus on the above problem and propose a method to provide effective support to learners during modeling exercises by automatically converting the layout of the learner's class diagram to that of the instructor, in addition to indicating the correctness of the artifacts to the learners during the exercises. The proposed method was implemented and evaluated as a tool, and the results indicate that the automatic layout conversion was an effective feedback to the learners.
>
---
#### [replaced 007] Beyond risk: A proto-framework for assessing the societal impact of AI systems
- **分类: cs.CY; cs.AI; cs.ET**

- **链接: [http://arxiv.org/pdf/2508.03666v2](http://arxiv.org/pdf/2508.03666v2)**

> **作者:** Willem Fourie
>
> **摘要:** In the discourse on AI regulation, 'responsible AI' is the dominant paradigm, with the focus on mitigating the risks related to AI systems. While this focus is important and necessary, it has limited use for a systematic consideration of AI's societal impact. This paper proposes a proto-framework for assessing the societal impact of AI systems by operationalising the concept of freedom. This proto-framework is intended as a step towards a fully operationalised framework to be used in policymaking contexts. By drawing on Kantian philosophy and related contemporary interpretations, freedom is developed as the counterpart to the concept of responsibility. Two dimensions of freedom are developed in further detail: freedom as capability and freedom as opportunity. These two dimensions of freedom are then applied in a proto-framework that systematically considers AI's impact on society using the Sustainable Development Goals. This proto-framework aims to complement current risk-based approaches and thereby offers a first step towards operationalising the concept of freedom in AI regulation.
>
---
