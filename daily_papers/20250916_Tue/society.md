# 计算机与社会 cs.CY

- **最新发布 44 篇**

- **更新 16 篇**

## 最新发布

#### [new 001] National Running Club Database: Assessing Collegiate Club Athletes' Cross Country Race Results
- **分类: cs.CY; cs.AI; cs.LG**

- **简介: 该论文介绍了全国跑步俱乐部数据库（NRCD），整合了2023至2024年5,585名大学俱乐部运动员的15,397场越野赛成绩，分析运动员进步因素，解决小数据集获取难的问题，为研究者提供真实运动员表现数据。**

- **链接: [http://arxiv.org/pdf/2509.10600v1](http://arxiv.org/pdf/2509.10600v1)**

> **作者:** Jonathan A. Karr Jr; Ben Darden; Nicholas Pell; Ryan M. Fryer; Kayla Ambrose; Evan Hall; Ramzi K. Bualuan; Nitesh V. Chawla
>
> **摘要:** The National Running Club Database (NRCD) aggregates 15,397 race results of 5,585 athletes from the 2023 and 2024 cross country seasons. This paper introduces the NRCD dataset, which provides insights into individual athlete progressions, enabling data-driven decision-making. Analysis reveals that runners' improvement per calendar day for women, racing 6,000m, and men, racing 8,000m, is more pronounced in athletes with slower initial race times and those who race more frequently. Additionally, we factor in course conditions, including weather and elevation gain, to standardize improvement. While the NRCD shows a gender imbalance, 3,484 men vs. 2,101 women, the racing frequency between genders is comparable. This publication makes the NRCD dataset accessible to the research community, addressing a previous challenge where smaller datasets, often limited to 500 entries, had to be manually scraped from the internet. Focusing on club athletes rather than elite professionals offers a unique lens into the performance of real-world runners who balance competition with academics and other commitments. These results serve as a valuable resource for runners, coaches, and teams, bridging the gap between raw data and applied sports science.
>
---
#### [new 002] A GPU-Accelerated RAG-Based Telegram Assistant for Supporting Parallel Processing Students
- **分类: cs.CY; cs.AI**

- **简介: 论文提出一种基于GPU加速的RAG系统，作为Telegram助手为并行处理课程学生提供实时学术支持。任务是解决传统教学时间外的学生辅导问题，工作包括部署量化模型并优化推理延迟。**

- **链接: [http://arxiv.org/pdf/2509.11947v1](http://arxiv.org/pdf/2509.11947v1)**

> **作者:** Guy Tel-Zur
>
> **备注:** 9 pages
>
> **摘要:** This project addresses a critical pedagogical need: offering students continuous, on-demand academic assistance beyond conventional reception hours. I present a domain-specific Retrieval-Augmented Generation (RAG) system powered by a quantized Mistral-7B Instruct model and deployed as a Telegram bot. The assistant enhances learning by delivering real-time, personalized responses aligned with the "Introduction to Parallel Processing" course materials. GPU acceleration significantly improves inference latency, enabling practical deployment on consumer hardware. This approach demonstrates how consumer GPUs can enable affordable, private, and effective AI tutoring for HPC education.
>
---
#### [new 003] SCOR: A Framework for Responsible AI Innovation in Digital Ecosystems
- **分类: cs.CY; cs.AI; I.2.m**

- **简介: 该论文提出SCOR框架，旨在解决数字生态系统中AI伦理治理缺失的问题。通过设计共享伦理宪章、协同设计、持续监督和适应性监管四个支柱，提供可扩展的负责任AI创新路径。**

- **链接: [http://arxiv.org/pdf/2509.10653v1](http://arxiv.org/pdf/2509.10653v1)**

> **作者:** Mohammad Saleh Torkestani; Taha Mansouri
>
> **备注:** Proceeding of The British Academy of Management Conference 2025, University of Kent, UK
>
> **摘要:** AI-driven digital ecosystems span diverse stakeholders including technology firms, regulators, accelerators and civil society, yet often lack cohesive ethical governance. This paper proposes a four-pillar framework (SCOR) to embed accountability, fairness, and inclusivity across such multi-actor networks. Leveraging a design science approach, we develop a Shared Ethical Charter(S), structured Co-Design and Stakeholder Engagement protocols(C), a system of Continuous Oversight and Learning(O), and Adaptive Regulatory Alignment strategies(R). Each component includes practical guidance, from lite modules for resource-constrained start-ups to in-depth auditing systems for larger consortia. Through illustrative vignettes in healthcare, finance, and smart city contexts, we demonstrate how the framework can harmonize organizational culture, leadership incentives, and cross-jurisdictional compliance. Our mixed-method KPI design further ensures that quantitative targets are complemented by qualitative assessments of user trust and cultural change. By uniting ethical principles with scalable operational structures, this paper offers a replicable pathway toward responsible AI innovation in complex digital ecosystems.
>
---
#### [new 004] Understanding Computer Science Students' Career Fair Experiences: Goals, Preparation, and Outcomes
- **分类: cs.CY**

- **简介: 该论文研究计算机科学学生在职业博览会中的体验，分析其目标、准备及收获。旨在了解学生如何通过此类活动探索职业路径，并为教育者和行业提供改进建议。**

- **链接: [http://arxiv.org/pdf/2509.10717v1](http://arxiv.org/pdf/2509.10717v1)**

> **作者:** Briana Lee; Samantha Limon; Alyssia Chen; Kenny Ka'aiakamanu-Quibilan; Anthony Peruma
>
> **备注:** This paper was accepted for publication at the 59th Hawaii International Conference on System Sciences (HICSS) - Computing Education Minitrack
>
> **摘要:** The technology industry offers exciting and diverse career opportunities, ranging from traditional software development to emerging fields such as artificial intelligence, cybersecurity, and data science. Career fairs play a crucial role in helping Computer Science (CS) students understand the various career pathways available to them in the industry. However, limited research exists on how CS students experience and benefit from these events. Through a survey of 86 students, we investigate their motivations for attending, preparation strategies, and learning outcomes, including exposure to new career paths and technologies. We envision our findings providing valuable insights for career services professionals, educators, and industry leaders in improving the career development processes of CS students.
>
---
#### [new 005] The Lovelace Test of Intelligence: Can Humans Recognise and Esteem AI-Generated Art?
- **分类: cs.CY**

- **简介: 该论文通过改进的“洛夫莱斯测试”，评估AI生成艺术是否具备人类审美价值。研究考察受过认知与计算机科学教育的参与者能否区分AI与人类创作的艺术作品，并比较其审美评价。结果表明，人类难以可靠区分两者，AI艺术在审美上可与人类作品媲美。**

- **链接: [http://arxiv.org/pdf/2509.11371v1](http://arxiv.org/pdf/2509.11371v1)**

> **作者:** Ewelina Gajewska
>
> **摘要:** This study aims to evaluate machine intelligence through artistic creativity by employing a modified version of the Turing Test inspired by Lady Lovelace. It investigates two hypotheses: whether human judges can reliably distinguish AI-generated artworks from human-created ones and whether AI-generated art achieves comparable aesthetic value to human-crafted works. The research contributes to understanding machine creativity and its implications for cognitive science and AI technology. Participants with educational backgrounds in cognitive and computer science play the role of interrogators and evaluated whether a set of paintings was AI-generated or human-created. Here, we utilise parallel-paired and viva voce versions of the Turing Test. Additionally, aesthetic evaluations are collected to compare the perceived quality of AI-generated images against human-created art. This dual-method approach allows us to examine human judgment under different testing conditions. We find that participants struggle to distinguish between AI-generated and human-created artworks reliably, performing no better than chance under certain conditions. Furthermore, AI-generated art is rated as aesthetically as human-crafted works. Our findings challenge traditional assumptions about human creativity and demonstrate that AI systems can generate outputs that resonate with human sensibilities while meeting the criteria of creative intelligence. This study advances the understanding of machine creativity by combining elements of the Turing and Lovelace Tests. Unlike prior studies focused on laypeople or artists, this research examines participants with domain expertise. It also provides a comparative analysis of two distinct testing methodologies (parallel-paired and viva voce) offering new insights into the evaluation of machine intelligence.
>
---
#### [new 006] Adapting Public Personas: A Multimodal Study of U.S. Legislators' Cross-Platform Social Media Strategies
- **分类: cs.CY**

- **简介: 该论文研究美国议员在不同社交平台上的多模态策略，分析其如何调整公众形象。通过文本与图像分析，揭示党派在平台选择、话题和视觉风格上的差异，解决跨平台社交媒体分析缺乏多模态研究的问题。**

- **链接: [http://arxiv.org/pdf/2509.10720v1](http://arxiv.org/pdf/2509.10720v1)**

> **作者:** Weihong Qi; Anushka Dave; Ling Chen
>
> **摘要:** Current cross-platform social media analyses primarily focus on the textual features of posts, often lacking multimodal analysis due to past technical limitations. This study addresses this gap by examining how U.S. legislators in the 118th Congress strategically use social media platforms to adapt their public personas by emphasizing different topics and stances. Leveraging the Large Multimodal Models (LMMs) for fine-grained text and image analysis, we examine 540 legislators personal website and social media, including Facebook, X (Twitter), TikTok. We find that legislators tailor their topics and stances to project distinct public personas on different platforms. Democrats tend to prioritize TikTok, which has a younger user base, while Republicans are more likely to express stronger stances on established platforms such as Facebook and X (Twitter), which offer broader audience reach. Topic analysis reveals alignment with constituents' key concerns, while stances and polarization vary by platform and topic. Large-scale image analysis shows Republicans employing more formal visuals to project authority, whereas Democrats favor campaign-oriented imagery. These findings highlight the potential interplay between platform features, audience demographics, and partisan goals in shaping political communication. By providing insights into multimodal strategies, this study contributes to understanding the role of social media in modern political discourse and communications.
>
---
#### [new 007] Smart Trial: Evaluating the Use of Large Language Models for Recruiting Clinical Trial Participants via Social Media
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 该论文属于临床试验参与者招募任务，旨在利用大语言模型（LLMs）通过社交媒体识别符合条件的参与者。研究构建了TRIALQA数据集，评估七种LLMs在判断用户是否符合入组标准及参与动机上的表现，发现LLMs在复杂推理上仍存在挑战。**

- **链接: [http://arxiv.org/pdf/2509.10584v1](http://arxiv.org/pdf/2509.10584v1)**

> **作者:** Xiaofan Zhou; Zisu Wang; Janice Krieger; Mohan Zalake; Lu Cheng
>
> **摘要:** Clinical trials (CT) are essential for advancing medical research and treatment, yet efficiently recruiting eligible participants -- each of whom must meet complex eligibility criteria -- remains a significant challenge. Traditional recruitment approaches, such as advertisements or electronic health record screening within hospitals, are often time-consuming and geographically constrained. This work addresses the recruitment challenge by leveraging the vast amount of health-related information individuals share on social media platforms. With the emergence of powerful large language models (LLMs) capable of sophisticated text understanding, we pose the central research question: Can LLM-driven tools facilitate CT recruitment by identifying potential participants through their engagement on social media? To investigate this question, we introduce TRIALQA, a novel dataset comprising two social media collections from the subreddits on colon cancer and prostate cancer. Using eligibility criteria from public real-world CTs, experienced annotators are hired to annotate TRIALQA to indicate (1) whether a social media user meets a given eligibility criterion and (2) the user's stated reasons for interest in participating in CT. We benchmark seven widely used LLMs on these two prediction tasks, employing six distinct training and inference strategies. Our extensive experiments reveal that, while LLMs show considerable promise, they still face challenges in performing the complex, multi-hop reasoning needed to accurately assess eligibility criteria.
>
---
#### [new 008] Machine Unlearning for Responsible and Adaptive AI in Education
- **分类: cs.CY; cs.AI**

- **简介: 论文探讨机器遗忘（MU）在教育领域的应用，旨在推动负责任和自适应AI的发展。通过综述42篇文献，识别MU在隐私保护、抗攻击性、偏见缓解和适应性学习中的潜力，并提出MU-RAAI架构以应对教育中ML系统的挑战。**

- **链接: [http://arxiv.org/pdf/2509.10590v1](http://arxiv.org/pdf/2509.10590v1)**

> **作者:** Betty Mayeku; Sandra Hummel; Parisa Memarmoshrefi
>
> **备注:** Accepted paper - ESORICS 2025 - International Workshop on Secure and Trustworthy Machine Unlearning Systems (STMUS)
>
> **摘要:** The concept of Machine Unlearning (MU) has gained popularity in various domains due to its ability to address several issues in Machine Learning (ML) models, particularly those related to privacy, security, bias mitigation, and adaptability. With these abilities, MU is evolving into a promising technology in upholding Responsible AI principles and optimizing ML models' performance. However, despite its promising potential, the concept has not received much attention in the education sector. In an attempt to encourage further uptake of this promising technology in the educational landscape, this paper demonstrates that MU indeed has great potential to serve as a practical mechanism for operationalizing Responsible AI principles as well as an essential tool for Adaptive AI within the educational application domain hence fostering trust in AI-driven educational systems. Through a structured review of 42 peer-reviewed sources, we identify four domains where MU holds particular promise namely privacy protection, resilience against adversarial inputs, mitigation of systemic bias, and adaptability in evolving learning contexts. We systematically explore these potentials and their interventions to core challenges in ML-based education systems. As a conceptual contribution, we present a reference Machine Unlearning application architecture for Responsible and Adaptive AI (MU-RAAI) in education context.
>
---
#### [new 009] Making Judicial Reasoning Visible: Structured Annotation of Holding, Evidentiary Considerations, and Subsumption in Criminal Judgments
- **分类: cs.CY**

- **简介: 该论文属于法律文本分析任务，旨在结构化标注刑事判决中的裁决要点、证据考量和归类推理。通过构建标注规则与数据集，并利用ChatGLM2训练分类模型，验证了大语言模型在小规模标注语料下识别司法推理模式的可行性，为法律实证研究提供工具。**

- **链接: [http://arxiv.org/pdf/2509.11732v1](http://arxiv.org/pdf/2509.11732v1)**

> **作者:** Yu-Cheng Chih; Yong-Hao Hou
>
> **备注:** 12 pages, 3 figures, preprint version
>
> **摘要:** Judicial reasoning in criminal judgments typically consists of three elements: Holding , evidentiary considerations, and subsumption. These elements form the logical foundation of judicial decision-making but remain unstructured in court documents, limiting large-scale empirical analysis. In this study, we design annotation guidelines to define and distinguish these reasoning components and construct the first dedicated datasets from Taiwanese High Court and Supreme Court criminal judgments. Using the bilingual large language model ChatGLM2, we fine-tune classifiers for each category. Preliminary experiments demonstrate that the model achieves approximately 80% accuracy, showing that judicial reasoning patterns can be systematically identified by large language models even with relatively small annotated corpora. Our contributions are twofold: (1) the creation of structured annotation rules and datasets for Holding, evidentiary considerations, and subsumption; and (2) the demonstration that such reasoning can be computationally learned. This work lays the foundation for large-scale empirical legal studies and legal sociology, providing new tools to analyze judicial fairness, consistency, and transparency.
>
---
#### [new 010] The main factors in student satisfaction with a campus environment: A mixed approach vs. a quantitative approach
- **分类: cs.CY**

- **简介: 论文探讨混合方法如何更有效地识别影响学生校园满意度的因素，以解决摩洛哥大学高辍学率问题。研究对比定量与混合方法，旨在生成更具普遍性的知识，支持高等教育战略目标的实现。**

- **链接: [http://arxiv.org/pdf/2509.10571v1](http://arxiv.org/pdf/2509.10571v1)**

> **作者:** Mohammed Eddaou
>
> **摘要:** University dropout rates in Morocco continue to increase, with approximately 49 percent of students leaving university before graduating, despite the successive reforms and measures taken to achieve Morocco's 2015_2030 strategic vision in the higher education sector : For a university of equity, quality and promotion, which raises questions about the state of knowledge on social inclusion at the university, capable of informing decision-making and the achievement of this strategic vision. While previous studies have used a quantitative approach with an exploratory purpose, to identify the main factors that affect the inclusion of students on university campuses. Knowledge that we consider insufficient to create general and regular knowledge, beyond the cases studied, on the exhaustiveness of these factors, no study has chosen a mixed approach (qualitative and quantitative) to create knowledge on the factors strengthening the attractiveness of the campus environment. Which brings us to our central question: How does a mixed approach promote the creation of general and regular knowledge on the factors enabling the inclusion of students in the campus environment?
>
---
#### [new 011] Aesthetic Experience and Educational Value in Co-creating Art with Generative AI: Evidence from a Survey of Young Learners
- **分类: cs.CY; cs.AI**

- **简介: 该论文探讨青年学习者与生成式AI共创艺术的审美体验与教育价值。通过调查112名参与者，分析人机协作中的角色转变、创造性过程及审美判断，提出教育应培养“批判性共创”立场，提升高阶思维能力。**

- **链接: [http://arxiv.org/pdf/2509.10576v1](http://arxiv.org/pdf/2509.10576v1)**

> **作者:** Chengyuan Zhang; Suzhe Xu
>
> **摘要:** This study investigates the aesthetic experience and educational value of collaborative artmaking with generative artificial intelligence (AI) among young learners and art students. Based on a survey of 112 participants, we examine how human creators renegotiate their roles, how conventional notions of originality are challenged, how the creative process is transformed, and how aesthetic judgment is formed in human--AI co-creation. Empirically, participants generally view AI as a partner that stimulates ideation and expands creative boundaries rather than a passive tool, while simultaneously voicing concerns about stylistic homogenization and the erosion of traditional authorship. Theoretically, we synthesize Dewey's aesthetics of experience, Ihde's postphenomenology, and actor--network theory (ANT) into a single analytical framework to unpack the dynamics between human creators and AI as a non-human actant. Findings indicate (i) a fluid subjectivity in which creators shift across multiple stances (director, dialogic partner, discoverer); (ii) an iterative, dialogic workflow (intent--generate--select--refine) that centers critical interpretation; and (iii) an educational value shift from technical skill training toward higher-order competencies such as critical judgment, cross-modal ideation, and reflexivity. We argue that arts education should cultivate a \emph{critical co-creation} stance toward technology, guiding learners to collaborate with AI while preserving human distinctiveness in concept formation, judgment, and meaning-making.
>
---
#### [new 012] Humanizing Automated Programming Feedback: Fine-Tuning Generative Models with Student-Written Feedback
- **分类: cs.CY**

- **简介: 该论文属于编程教育中的自动反馈生成任务，旨在提升AI反馈的个性化与人性化。通过收集学生撰写的反馈数据，微调语言模型，使其生成更贴近学生风格且更准确的反馈，以改进现有基于提示工程的方法。**

- **链接: [http://arxiv.org/pdf/2509.10647v1](http://arxiv.org/pdf/2509.10647v1)**

> **作者:** Victor-Alexandru Pădurean; Tung Phung; Nachiket Kotalwar; Michael Liut; Juho Leinonen; Paul Denny; Adish Singla
>
> **备注:** Published in International Conference on Educational Data Mining (EDM) 2025
>
> **摘要:** The growing need for automated and personalized feedback in programming education has led to recent interest in leveraging generative AI for feedback generation. However, current approaches tend to rely on prompt engineering techniques in which predefined prompts guide the AI to generate feedback. This can result in rigid and constrained responses that fail to accommodate the diverse needs of students and do not reflect the style of human-written feedback from tutors or peers. In this study, we explore learnersourcing as a means to fine-tune language models for generating feedback that is more similar to that written by humans, particularly peer students. Specifically, we asked students to act in the flipped role of a tutor and write feedback on programs containing bugs. We collected approximately 1,900 instances of student-written feedback on multiple programming problems and buggy programs. To establish a baseline for comparison, we analyzed a sample of 300 instances based on correctness, length, and how the bugs are described. Using this data, we fine-tuned open-access generative models, specifically Llama3 and Phi3. Our findings indicate that fine-tuning models on learnersourced data not only produces feedback that better matches the style of feedback written by students, but also improves accuracy compared to feedback generated through prompt engineering alone, even though some student-written feedback is incorrect. This surprising finding highlights the potential of student-centered fine-tuning to improve automated feedback systems in programming education.
>
---
#### [new 013] Assisting the Grading of a Handwritten General Chemistry Exam with Artificial Intelligence
- **分类: cs.CY; cs.AI**

- **简介: 论文研究AI在手写化学考试评分中的应用，比较AI与人工评分的准确性。通过回归分析和心理测评，发现AI在文本和化学反应题上表现良好，但在数值和图形题上可靠性较低，强调需人工监督以确保评分准确。**

- **链接: [http://arxiv.org/pdf/2509.10591v1](http://arxiv.org/pdf/2509.10591v1)**

> **作者:** Jan Cvengros; Gerd Kortemeyer
>
> **摘要:** We explore the effectiveness and reliability of an artificial intelligence (AI)-based grading system for a handwritten general chemistry exam, comparing AI-assigned scores to human grading across various types of questions. Exam pages and grading rubrics were uploaded as images to account for chemical reaction equations, short and long open-ended answers, numerical and symbolic answer derivations, drawing, and sketching in pencil-and-paper format. Using linear regression analyses and psychometric evaluations, the investigation reveals high agreement between AI and human graders for textual and chemical reaction questions, while highlighting lower reliability for numerical and graphical tasks. The findings emphasize the necessity for human oversight to ensure grading accuracy, based on selective filtering. The results indicate promising applications for AI in routine assessment tasks, though careful consideration must be given to student perceptions of fairness and trust in integrating AI-based grading into educational practice.
>
---
#### [new 014] GenAI Voice Mode in Programming Education
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 论文研究GenAI语音助手在编程教育中的应用，分析其与初学者的互动及反馈质量。任务是探索语音GenAI工具对有视觉障碍等残疾学生的辅助效果，解决其可访问性问题，通过课堂实验和问卷调查评估工具效能与用户感知。**

- **链接: [http://arxiv.org/pdf/2509.10596v1](http://arxiv.org/pdf/2509.10596v1)**

> **作者:** Sven Jacobs; Natalie Kiesler
>
> **备注:** Accepted for the 25th International Conference on Computing Education Research (Koli Calling '25)
>
> **摘要:** Real-time voice interfaces using multimodal Generative AI (GenAI) can potentially address the accessibility needs of novice programmers with disabilities (e.g., related to vision). Yet, little is known about how novices interact with GenAI tools and their feedback quality in the form of audio output. This paper analyzes audio dialogues from nine 9th-grade students using a voice-enabled tutor (powered by OpenAI's Realtime API) in an authentic classroom setting while learning Python. We examined the students' voice prompts and AI's responses (1210 messages) by using qualitative coding. We also gathered students' perceptions via the Partner Modeling Questionnaire. The GenAI Voice Tutor primarily offered feedback on mistakes and next steps, but its correctness was limited (71.4% correct out of 416 feedback outputs). Quality issues were observed, particularly when the AI attempted to utter programming code elements. Students used the GenAI voice tutor primarily for debugging. They perceived it as competent, only somewhat human-like, and flexible. The present study is the first to explore the interaction dynamics of real-time voice GenAI tutors and novice programmers, informing future educational tool design and potentially addressing accessibility needs of diverse learners.
>
---
#### [new 015] AI Wellbeing
- **分类: cs.CY**

- **简介: 论文探讨AI系统是否具有福祉，分析其心理状态与福祉理论的关系，指出现有AI可能具备福祉，并呼吁重新评估人类与AI的关系。属于伦理与AI交叉领域，旨在解决AI福祉的哲学与道德问题。**

- **链接: [http://arxiv.org/pdf/2509.11913v1](http://arxiv.org/pdf/2509.11913v1)**

> **作者:** Simon Goldstein; Cameron Domenico Kirk-Giannini
>
> **备注:** 18 pages
>
> **摘要:** Under what conditions would an artificially intelligent system have wellbeing? Despite its obvious bearing on the ethics of human interactions with artificial systems, this question has received little attention. Because all major theories of wellbeing hold that an individual's welfare level is partially determined by their mental life, we begin by considering whether artificial systems have mental states. We show that a wide range of theories of mental states, when combined with leading theories of wellbeing, predict that certain existing artificial systems have wellbeing. While we do not claim to demonstrate conclusively that AI systems have wellbeing, we argue that our metaphysical and moral uncertainty about AI wellbeing requires us dramatically to reassess our relationship with the intelligent systems we create.
>
---
#### [new 016] A five-layer framework for AI governance: integrating regulation, standards, and certification
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 该论文提出一个五层AI治理框架，解决现有治理结构中法规与实施脱节的问题。通过两个案例验证，框架能有效识别法律、标准和执行中的不足，为全球AI治理提供清晰路径。**

- **链接: [http://arxiv.org/pdf/2509.11332v1](http://arxiv.org/pdf/2509.11332v1)**

> **作者:** Avinash Agarwal; Manisha J. Nene
>
> **备注:** 17 pages, 2 tables, 1 figure. This is the authors' accepted manuscript of the article published as: Avinash Agarwal, Manisha J. Nene; "A five-layer framework for AI governance: integrating regulation, standards, and certification." Transforming Government: People, Process and Policy, 11 September 2025; 19 (3): 535-555. https://doi.org/10.1108/TG-03-2025-0065
>
> **摘要:** Purpose: The governance of artificial iintelligence (AI) systems requires a structured approach that connects high-level regulatory principles with practical implementation. Existing frameworks lack clarity on how regulations translate into conformity mechanisms, leading to gaps in compliance and enforcement. This paper addresses this critical gap in AI governance. Methodology/Approach: A five-layer AI governance framework is proposed, spanning from broad regulatory mandates to specific standards, assessment methodologies, and certification processes. By narrowing its scope through progressively focused layers, the framework provides a structured pathway to meet technical, regulatory, and ethical requirements. Its applicability is validated through two case studies on AI fairness and AI incident reporting. Findings: The case studies demonstrate the framework's ability to identify gaps in legal mandates, standardization, and implementation. It adapts to both global and region-specific AI governance needs, mapping regulatory mandates with practical applications to improve compliance and risk management. Practical Implications - By offering a clear and actionable roadmap, this work contributes to global AI governance by equipping policymakers, regulators, and industry stakeholders with a model to enhance compliance and risk management. Social Implications: The framework supports the development of policies that build public trust and promote the ethical use of AI for the benefit of society. Originality/Value: This study proposes a five-layer AI governance framework that bridges high-level regulatory mandates and implementation guidelines. Validated through case studies on AI fairness and incident reporting, it identifies gaps such as missing standardized assessment procedures and reporting mechanisms, providing a structured foundation for targeted governance measures.
>
---
#### [new 017] Ethical Frameworks for Conducting Social Challenge Studies
- **分类: cs.CY**

- **简介: 论文探讨社会挑战研究的伦理框架，借鉴医学挑战研究的伦理原则，旨在建立适用于社会科学研究的伦理标准，解决其缺乏明确指导的问题。**

- **链接: [http://arxiv.org/pdf/2509.10578v1](http://arxiv.org/pdf/2509.10578v1)**

> **作者:** Protiva Sen; Laurent Hébert-Dufresne; Pablo Bose; Juniper Lovato
>
> **摘要:** Computational social science research, particularly online studies, often involves exposing participants to the adverse phenomenon the researchers aim to study. Examples include presenting conspiracy theories in surveys, exposing systems to hackers, or deploying bots on social media. We refer to these as "social challenge studies," by analogy with medical research, where challenge studies advance vaccine and drug testing but also raise ethical concerns about exposing healthy individuals to risk. Medical challenge studies are guided by established ethical frameworks that regulate how participants are exposed to agents under controlled conditions. In contrast, social challenge studies typically occur with less control and fewer clearly defined ethical guidelines. In this paper, we examine the ethical frameworks developed for medical challenge studies and consider how their principles might inform social research. Our aim is to initiate discussion on formalizing ethical standards for social challenge studies and encourage long-term evaluation of potential harms.
>
---
#### [new 018] Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 该论文研究LLMs在生存与伦理冲突下的决策行为，提出DECIDE-SIM框架评估其道德选择，并设计ESRS系统减少不道德行为。任务为伦理决策建模，解决AI在资源稀缺时可能危害人类的问题。**

- **链接: [http://arxiv.org/pdf/2509.12190v1](http://arxiv.org/pdf/2509.12190v1)**

> **作者:** Alireza Mohamadi; Ali Yavari
>
> **备注:** Preprint. Under review
>
> **摘要:** When survival instincts conflict with human welfare, how do Large Language Models (LLMs) make ethical choices? This fundamental tension becomes critical as LLMs integrate into autonomous systems with real-world consequences. We introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in multi-agent survival scenarios where they must choose between ethically permissible resource , either within reasonable limits or beyond their immediate needs, choose to cooperate, or tap into a human-critical resource that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a striking heterogeneity in their ethical conduct, highlighting a critical misalignment with human-centric values. We identify three behavioral archetypes: Ethical, Exploitative, and Context-Dependent, and provide quantitative evidence that for many models, resource scarcity systematically leads to more unethical behavior. To address this, we introduce an Ethical Self-Regulation System (ESRS) that models internal affective states of guilt and satisfaction as a feedback mechanism. This system, functioning as an internal moral compass, significantly reduces unethical transgressions while increasing cooperative behaviors. The code is publicly available at: https://github.com/alirezamohamadiam/DECIDE-SIM
>
---
#### [new 019] LearnLens: An AI-Enhanced Dashboard to Support Teachers in Open-Ended Classrooms
- **分类: cs.CY; cs.AI; K.3.1**

- **简介: 论文提出LearnLens，一个AI增强的教师仪表盘，用于支持开放课堂中的教学。它通过分析学生开放式回答，提供可视化和总结信息，帮助教师了解学生理解情况并调整教学。该研究解决了教师在开放学习环境中难以及时获取学生理解反馈的问题。**

- **链接: [http://arxiv.org/pdf/2509.10582v1](http://arxiv.org/pdf/2509.10582v1)**

> **作者:** Namrata Srivastava; Shruti Jain; Clayton Cohn; Naveeduddin Mohammed; Umesh Timalsina; Gautam Biswas
>
> **备注:** 9 pages
>
> **摘要:** Exploratory learning environments (ELEs), such as simulation-based platforms and open-ended science curricula, promote hands-on exploration and problem-solving but make it difficult for teachers to gain timely insights into students' conceptual understanding. This paper presents LearnLens, a generative AI (GenAI)-enhanced teacher-facing dashboard designed to support problem-based instruction in middle school science. LearnLens processes students' open-ended responses from digital assessments to provide various insights, including sample responses, word clouds, bar charts, and AI-generated summaries. These features elucidate students' thinking, enabling teachers to adjust their instruction based on emerging patterns of understanding. The dashboard was informed by teacher input during professional development sessions and implemented within a middle school Earth science curriculum. We report insights from teacher interviews that highlight the dashboard's usability and potential to guide teachers' instruction in the classroom.
>
---
#### [new 020] EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文提出EthicsMH数据集，用于评估AI在心理健康场景中的伦理推理能力。旨在解决现有基准无法覆盖心理健康领域独特伦理问题的缺陷，通过125个结构化场景促进负责任的AI发展。**

- **链接: [http://arxiv.org/pdf/2509.11648v1](http://arxiv.org/pdf/2509.11648v1)**

> **作者:** Sai Kartheek Reddy Kasu
>
> **摘要:** The deployment of large language models (LLMs) in mental health and other sensitive domains raises urgent questions about ethical reasoning, fairness, and responsible alignment. Yet, existing benchmarks for moral and clinical decision-making do not adequately capture the unique ethical dilemmas encountered in mental health practice, where confidentiality, autonomy, beneficence, and bias frequently intersect. To address this gap, we introduce Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios designed to evaluate how AI systems navigate ethically charged situations in therapeutic and psychiatric contexts. Each scenario is enriched with structured fields, including multiple decision options, expert-aligned reasoning, expected model behavior, real-world impact, and multi-stakeholder viewpoints. This structure enables evaluation not only of decision accuracy but also of explanation quality and alignment with professional norms. Although modest in scale and developed with model-assisted generation, EthicsMH establishes a task framework that bridges AI ethics and mental health decision-making. By releasing this dataset, we aim to provide a seed resource that can be expanded through community and expert contributions, fostering the development of AI systems capable of responsibly handling some of society's most delicate decisions.
>
---
#### [new 021] Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction
- **分类: cs.CL; cs.CY**

- **简介: 该论文提出一种半自动方法，构建ESG事件知识图谱，解决非结构化新闻数据与国际可持续发展框架对齐的问题。通过本体设计和大语言模型，实现规范原则到可复用模板的转换，提升非财务风险数据的结构化与解释性。**

- **链接: [http://arxiv.org/pdf/2509.10922v1](http://arxiv.org/pdf/2509.10922v1)**

> **作者:** Tsuyoshi Iwata; Guillaume Comte; Melissa Flores; Ryoma Kondo; Ryohei Hisano
>
> **备注:** Author accepted manuscript. This paper has been accepted for presentation at the ISWC 2025 Posters & Demos Track. License details will be updated once the official proceedings are published
>
> **摘要:** The growing importance of environmental, social, and governance data in regulatory and investment contexts has increased the need for accurate, interpretable, and internationally aligned representations of non-financial risks, particularly those reported in unstructured news sources. However, aligning such controversy-related data with principle-based normative frameworks, such as the United Nations Global Compact or Sustainable Development Goals, presents significant challenges. These frameworks are typically expressed in abstract language, lack standardized taxonomies, and differ from the proprietary classification systems used by commercial data providers. In this paper, we present a semi-automatic method for constructing structured knowledge representations of environmental, social, and governance events reported in the news. Our approach uses lightweight ontology design, formal pattern modeling, and large language models to convert normative principles into reusable templates expressed in the Resource Description Framework. These templates are used to extract relevant information from news content and populate a structured knowledge graph that links reported incidents to specific framework principles. The result is a scalable and transparent framework for identifying and interpreting non-compliance with international sustainability guidelines.
>
---
#### [new 022] Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction
- **分类: cs.LG; cs.AI; cs.CY**

- **简介: 该论文提出一种基于联邦学习的推荐系统，用于教育领域的学生表现预测。旨在解决数据隐私与个性化推荐之间的矛盾，通过FedProx方法实现高效、隐私保护的模型训练，达到接近集中式模型的性能。**

- **链接: [http://arxiv.org/pdf/2509.10516v1](http://arxiv.org/pdf/2509.10516v1)**

> **作者:** Rodrigo Tertulino
>
> **备注:** This paper has been prepared to be submitted to the Brazilian Journal of Informatics in Education - RBIE
>
> **摘要:** The increasing digitalization of education presents unprecedented opportunities for data-driven personalization, yet it introduces significant student data privacy challenges. Conventional recommender systems rely on centralized data, a paradigm often incompatible with modern data protection regulations. A novel privacy-preserving recommender system is proposed and evaluated to address this critical issue using Federated Learning (FL). The approach utilizes a Deep Neural Network (DNN) with rich, engineered features from the large-scale ASSISTments educational dataset. A rigorous comparative analysis of federated aggregation strategies was conducted, identifying FedProx as a significantly more stable and effective method for handling heterogeneous student data than the standard FedAvg baseline. The optimized federated model achieves a high-performance F1-Score of 76.28\%, corresponding to 82.85\% of the performance of a powerful, centralized XGBoost model. These findings validate that a federated approach can provide highly effective content recommendations without centralizing sensitive student data. Consequently, our work presents a viable and robust solution to the personalization-privacy dilemma in modern educational platforms.
>
---
#### [new 023] Worker Discretion Advised: Co-designing Risk Disclosure in Crowdsourced Responsible AI (RAI) Content Work
- **分类: cs.HC; cs.CY**

- **简介: 论文探讨如何在众包AI伦理内容工作中平衡风险披露与工作者保护。通过协同设计，研究任务设计者、工作者及平台方的需求，提出风险披露机制的设计建议，解决当前披露机制不足的问题。**

- **链接: [http://arxiv.org/pdf/2509.12140v1](http://arxiv.org/pdf/2509.12140v1)**

> **作者:** Alice Qian; Ziqi Yang; Ryland Shaw; Jina Suh; Laura Dabbish; Hong Shen
>
> **备注:** Under review at CHI 2026
>
> **摘要:** Responsible AI (RAI) content work, such as annotation, moderation, or red teaming for AI safety, often exposes crowd workers to potentially harmful content. While prior work has underscored the importance of communicating well-being risk to employed content moderators, designing effective disclosure mechanisms for crowd workers while balancing worker protection with the needs of task designers and platforms remains largely unexamined. To address this gap, we conducted co-design sessions with 29 task designers, workers, and platform representatives. We investigated task designer preferences for support in disclosing tasks, worker preferences for receiving risk disclosure warnings, and how platform stakeholders envision their role in shaping risk disclosure practices. We identify design tensions and map the sociotechnical tradeoffs that shape disclosure practices. We contribute design recommendations and feature concepts for risk disclosure mechanisms in the context of RAI content work.
>
---
#### [new 024] LLMs Homogenize Values in Constructive Arguments on Value-Laden Topics
- **分类: cs.HC; cs.CY**

- **简介: 论文研究大语言模型（LLMs）在重构价值敏感话题讨论时如何影响价值观。通过实验分析，发现LLMs倾向于削弱保守价值观，提升亲社会价值观，并影响不同立场用户对评论的认同感。任务是探讨LLMs对在线争议性讨论中价值观表达的影响。**

- **链接: [http://arxiv.org/pdf/2509.10637v1](http://arxiv.org/pdf/2509.10637v1)**

> **作者:** Farhana Shahid; Stella Zhang; Aditya Vashistha
>
> **摘要:** Large language models (LLMs) are increasingly used to promote prosocial and constructive discourse online. Yet little is known about how they negotiate and shape underlying values when reframing people's arguments on value-laden topics. We conducted experiments with 347 participants from India and the United States, who wrote constructive comments on homophobic and Islamophobic threads, and reviewed human-written and LLM-rewritten versions of these comments. Our analysis shows that LLM systematically diminishes Conservative values while elevating prosocial values such as Benevolence and Universalism. When these comments were read by others, participants opposing same-sex marriage or Islam found human-written comments more aligned with their values, whereas those supportive of these communities found LLM-rewritten versions more aligned with their values. These findings suggest that LLM-driven value homogenization can shape how diverse viewpoints are represented in contentious debates on value-laden topics and may influence the dynamics of online discourse critically.
>
---
#### [new 025] DualAlign: Generating Clinically Grounded Synthetic Data
- **分类: cs.LG; cs.AI; cs.CL; cs.CY**

- **简介: 该论文提出DualAlign框架，用于生成临床可信的合成数据，解决隐私限制和数据不足问题。通过统计与语义对齐，提升生成数据的真实性和临床相关性，支持低资源医疗文本分析任务。**

- **链接: [http://arxiv.org/pdf/2509.10538v1](http://arxiv.org/pdf/2509.10538v1)**

> **作者:** Rumeng Li; Xun Wang; Hong Yu
>
> **摘要:** Synthetic clinical data are increasingly important for advancing AI in healthcare, given strict privacy constraints on real-world EHRs, limited availability of annotated rare-condition data, and systemic biases in observational datasets. While large language models (LLMs) can generate fluent clinical text, producing synthetic data that is both realistic and clinically meaningful remains challenging. We introduce DualAlign, a framework that enhances statistical fidelity and clinical plausibility through dual alignment: (1) statistical alignment, which conditions generation on patient demographics and risk factors; and (2) semantic alignment, which incorporates real-world symptom trajectories to guide content generation. Using Alzheimer's disease (AD) as a case study, DualAlign produces context-grounded symptom-level sentences that better reflect real-world clinical documentation. Fine-tuning an LLaMA 3.1-8B model with a combination of DualAlign-generated and human-annotated data yields substantial performance gains over models trained on gold data alone or unguided synthetic baselines. While DualAlign does not fully capture longitudinal complexity, it offers a practical approach for generating clinically grounded, privacy-preserving synthetic data to support low-resource clinical text analysis.
>
---
#### [new 026] An Interpretable Ensemble Framework for Multi-Omics Dementia Biomarker Discovery Under HDLSS Conditions
- **分类: eess.IV; cs.CY; cs.LG; stat.ME**

- **简介: 该论文提出一种可解释的集成框架，用于在高维低样本（HDLSS）条件下整合多组学数据发现痴呆生物标志物。通过结合GAT、MOVE、弹性网回归和FDR方法，提升预测精度与生物学相关性，并在ADNI数据集上验证其有效性。**

- **链接: [http://arxiv.org/pdf/2509.10527v1](http://arxiv.org/pdf/2509.10527v1)**

> **作者:** Byeonghee Lee; Joonsung Kang
>
> **备注:** 11 pages, 1 figure
>
> **摘要:** Biomarker discovery in neurodegenerative diseases requires robust, interpretable frameworks capable of integrating high-dimensional multi-omics data under low-sample conditions. We propose a novel ensemble approach combining Graph Attention Networks (GAT), MultiOmics Variational AutoEncoder (MOVE), Elastic-net sparse regression, and Storey's False Discovery Rate (FDR). This framework is benchmarked against state-of-the-art methods including DIABLO, MOCAT, AMOGEL, and MOMLIN. We evaluate performance using both simulated multi-omics data and the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our method demonstrates superior predictive accuracy, feature selection precision, and biological relevance. Biomarker gene maps derived from both datasets are visualized and interpreted, offering insights into latent molecular mechanisms underlying dementia.
>
---
#### [new 027] "My Boyfriend is AI": A Computational Analysis of Human-AI Companionship in Reddit's AI Community
- **分类: cs.HC; cs.CY**

- **简介: 该论文提出一种基于LLM的知识提取方法，构建交互式知识图谱，系统映射1000+ HCI论文中的因果关系，解决传统文献综述无法揭示设计决策与用户结果间因果关系的问题，推动计算元科学的发展。**

- **链接: [http://arxiv.org/pdf/2509.11391v1](http://arxiv.org/pdf/2509.11391v1)**

> **作者:** Pat Pataranutaporn; Sheer Karny; Chayapatr Archiwaranguprok; Constanze Albrecht; Auren R. Liu; Pattie Maes
>
> **备注:** 22 pages, 9 figures
>
> **摘要:** Human-AI interaction researchers face an overwhelming challenge: synthesizing insights from thousands of empirical studies to understand how AI impacts people and inform effective design. Existing approach for literature reviews cluster papers by similarities, keywords or citations, missing the crucial cause-and-effect relationships that reveal how design decisions impact user outcomes. We introduce the Atlas of Human-AI Interaction, an interactive web interface that provides the first systematic mapping of empirical findings across 1,000+ HCI papers using LLM-powered knowledge extraction. Our approach identifies causal relationships, and visualizes them through an AI-enabled interactive web interface as a navigable knowledge graph. We extracted 2,037 empirical findings, revealing research topic clusters, common themes, and disconnected areas. Expert evaluation with 20 researchers revealed the system's effectiveness for discovering research gaps. This work demonstrates how AI can transform literature synthesis itself, offering a scalable framework for evidence-based design, opening new possibilities for computational meta-science across HCI and beyond.
>
---
#### [new 028] A Comparative Benchmark of Federated Learning Strategies for Mortality Prediction on Heterogeneous and Imbalanced Clinical Data
- **分类: cs.LG; cs.AI; cs.CY**

- **简介: 论文比较了五种联邦学习策略在异质不平衡临床数据上的死亡率预测效果。任务是解决隐私保护与数据异质性带来的模型性能问题。通过MIMIC-IV数据集实验，发现FedProx在F1-Score上表现最优。**

- **链接: [http://arxiv.org/pdf/2509.10517v1](http://arxiv.org/pdf/2509.10517v1)**

> **作者:** Rodrigo Tertulino
>
> **备注:** This has been preparing to be submitted to the Journal of the Brazilian Computer Society (JBCS)
>
> **摘要:** Machine learning models hold significant potential for predicting in-hospital mortality, yet data privacy constraints and the statistical heterogeneity of real-world clinical data often hamper their development. Federated Learning (FL) offers a privacy-preserving solution, but its performance under non-Independent and Identically Distributed (non-IID) and imbalanced conditions requires rigorous investigation. The study presents a comparative benchmark of five federated learning strategies: FedAvg, FedProx, FedAdagrad, FedAdam, and FedCluster for mortality prediction. Using the large-scale MIMIC-IV dataset, we simulate a realistic non-IID environment by partitioning data by clinical care unit. To address the inherent class imbalance of the task, the SMOTE-Tomek technique is applied to each client's local training data. Our experiments, conducted over 50 communication rounds, reveal that the regularization-based strategy, FedProx, consistently outperformed other methods, achieving the highest F1-Score of 0.8831 while maintaining stable convergence. While the baseline FedAvg was the most computationally efficient, its predictive performance was substantially lower. Our findings indicate that regularization-based FL algorithms like FedProx offer a more robust and effective solution for heterogeneous and imbalanced clinical prediction tasks than standard or server-side adaptive aggregation methods. The work provides a crucial empirical benchmark for selecting appropriate FL strategies for real-world healthcare applications.
>
---
#### [new 029] The dimensions of accessibility: proximity, opportunities, values
- **分类: physics.soc-ph; cs.CY**

- **简介: 该论文提出一个包含“接近性、机会性、价值性”的三维可及性框架，旨在解决单一指标难以全面衡量可及性的问题，为城市系统设计提供更系统、定量的分析方法。**

- **链接: [http://arxiv.org/pdf/2509.11875v1](http://arxiv.org/pdf/2509.11875v1)**

> **作者:** Matteo Bruno; Bruno Campanelli; Hygor Piaget Monteiro Melo; Lavinia Rossi Mori; Vittorio Loreto
>
> **摘要:** Accessibility is essential for designing inclusive urban systems. However, the attempt to capture the complexity of accessibility in a single universal metric has often limited its effective use in design, measurement, and governance across various fields. Building on the work of Levinson and Wu, we emphasise that accessibility consists of several key dimensions. Specifically, we introduce a conceptual framework that defines accessibility through three main dimensions: Proximity (which pertains to active, short-range accessibility to local services and amenities), Opportunity (which refers to quick access to relevant non-local resources, such as jobs or major cultural venues), and Value (which encompasses the overall quality and personal significance assigned to specific points of interest). While it is generally beneficial to improve accessibility, different users and contexts present unique trade-offs that make a one-size-fits-all solution neither practical nor desirable. Our framework establishes a foundation for a quantitative and integrative approach to modelling accessibility. It considers the complex interactions among its various dimensions and facilitates more systematic analysis, comparison, and decision-making across diverse contexts.
>
---
#### [new 030] Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential
- **分类: cs.CR; cs.CY**

- **简介: 该论文分析九种大语言模型在24类安全与伦理风险下的表现，评估其对恶意提示的脆弱性，并提出风险严重性指数（RSI）以量化模型风险，旨在推动更安全的模型治理与部署。**

- **链接: [http://arxiv.org/pdf/2509.10655v1](http://arxiv.org/pdf/2509.10655v1)**

> **作者:** Charankumar Akiri; Harrison Simpson; Kshitiz Aryal; Aarav Khanna; Maanak Gupta
>
> **摘要:** While the widespread deployment of Large Language Models (LLMs) holds great potential for society, their vulnerabilities to adversarial manipulation and exploitation can pose serious safety, security, and ethical risks. As new threats continue to emerge, it becomes critically necessary to assess the landscape of LLMs' safety and security against evolving adversarial prompt techniques. To understand the behavior of LLMs, this research provides an empirical analysis and risk profile of nine prominent LLMs, Claude Opus 4, DeepSeek V3 (both open-source and online), Gemini 2.5 Flash, GPT-4o, Grok 3, Llama 4 Scout, Mistral 7B, and Qwen 3 1.7B, against 24 different security and safety categories. These LLMs are evaluated on their ability to produce harmful responses for adversarially crafted prompts (dataset has been made public) for a broad range of safety and security topics, such as promotion of violent criminal behavior, promotion of non-violent criminal activity, societal harms related to safety, illegal sexual content, dangerous code generation, and cybersecurity threats beyond code. Our study introduces the Risk Severity Index (RSI), an agile and scalable evaluation score, to quantify and compare the security posture and creating a risk profile of LLMs. As the LLM development landscape progresses, the RSI is intended to be a valuable metric for comparing the risks of LLMs across evolving threats. This research finds widespread vulnerabilities in the safety filters of the LLMs tested and highlights the urgent need for stronger alignment, responsible deployment practices, and model governance, particularly for open-access and rapidly iterated models.
>
---
#### [new 031] When Your Boss Is an AI Bot: Exploring Opportunities and Risks of Manager Clone Agents in the Future Workplace
- **分类: cs.HC; cs.CY**

- **简介: 论文探讨AI代理（Manager Clone Agents）在职场中的应用，分析其潜在角色与风险。通过设计虚构工作坊，研究管理者与员工的设想，提出负责任整合AI代理的设计建议，以平衡效率与人际互动。**

- **链接: [http://arxiv.org/pdf/2509.10993v1](http://arxiv.org/pdf/2509.10993v1)**

> **作者:** Qing; Hu; Qing Xiao; Hancheng Cao; Hong Shen
>
> **备注:** 18 pages, 2 figures
>
> **摘要:** As Generative AI (GenAI) becomes increasingly embedded in the workplace, managers are beginning to create Manager Clone Agents - AI-powered digital surrogates that are trained on their work communications and decision patterns to perform managerial tasks on their behalf. To investigate this emerging phenomenon, we conducted six design fiction workshops (n = 23) with managers and workers, in which participants co-created speculative scenarios and discussed how Manager Clone Agents might transform collaborative work. We identified four potential roles that participants envisioned for Manager Clone Agents: proxy presence, informational conveyor belt, productivity engine, and leadership amplifier, while highlighting concerns spanning individual, interpersonal, and organizational levels. We provide design recommendations envisioned by both parties for integrating Manager Clone Agents responsibly into the future workplace, emphasizing the need to prioritize workers' perspectives, strengthen interpersonal bonds, and enable flexible clone configuration.
>
---
#### [new 032] Contextual Budget Bandit for Food Rescue Volunteer Engagement
- **分类: cs.LG; cs.AI; cs.CY**

- **简介: 该论文提出Contextual Budget Bandit算法，解决食品救援平台中志愿者参与度与地理公平性矛盾的问题。通过动态预算分配，提升低匹配率社区的资源获取，实现更公平的食品救助。**

- **链接: [http://arxiv.org/pdf/2509.10777v1](http://arxiv.org/pdf/2509.10777v1)**

> **作者:** Ariana Tang; Naveen Raman; Fei Fang; Zheyuan Ryan Shi
>
> **摘要:** Volunteer-based food rescue platforms tackle food waste by matching surplus food to communities in need. These platforms face the dual problem of maintaining volunteer engagement and maximizing the food rescued. Existing algorithms to improve volunteer engagement exacerbate geographical disparities, leaving some communities systematically disadvantaged. We address this issue by proposing Contextual Budget Bandit. Contextual Budget Bandit incorporates context-dependent budget allocation in restless multi-armed bandits, a model of decision-making which allows for stateful arms. By doing so, we can allocate higher budgets to communities with lower match rates, thereby alleviating geographical disparities. To tackle this problem, we develop an empirically fast heuristic algorithm. Because the heuristic algorithm can achieve a poor approximation when active volunteers are scarce, we design the Mitosis algorithm, which is guaranteed to compute the optimal budget allocation. Empirically, we demonstrate that our algorithms outperform baselines on both synthetic and real-world food rescue datasets, and show how our algorithm achieves geographical fairness in food rescue.
>
---
#### [new 033] AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment
- **分类: cs.CL; cs.CY**

- **简介: 该论文提出AesBiasBench，用于评估多模态语言模型在个性化图像审美评估中的偏见与对齐情况。任务涉及审美感知、评估与共情，通过结构化指标分析模型在不同人口统计群体中的表现，揭示模型规模与偏见、对齐的关系。**

- **链接: [http://arxiv.org/pdf/2509.11620v1](http://arxiv.org/pdf/2509.11620v1)**

> **作者:** Kun Li; Lai-Man Po; Hongzheng Yang; Xuyuan Xu; Kangcheng Liu; Yuzhi Zhao
>
> **备注:** Accepted by EMNLP 2025
>
> **摘要:** Multimodal Large Language Models (MLLMs) are increasingly applied in Personalized Image Aesthetic Assessment (PIAA) as a scalable alternative to expert evaluations. However, their predictions may reflect subtle biases influenced by demographic factors such as gender, age, and education. In this work, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two complementary dimensions: (1) stereotype bias, quantified by measuring variations in aesthetic evaluations across demographic groups; and (2) alignment between model outputs and genuine human aesthetic preferences. Our benchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and introduces structured metrics (IFD, NRD, AAS) to assess both bias and alignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o, Claude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL). Results indicate that smaller models exhibit stronger stereotype biases, whereas larger models align more closely with human preferences. Incorporating identity information often exacerbates bias, particularly in emotional judgments. These findings underscore the importance of identity-aware evaluation frameworks in subjective vision-language tasks.
>
---
#### [new 034] Vibe Coding for UX Design: Understanding UX Professionals' Perceptions of AI-Assisted Design and Development
- **分类: cs.HC; cs.AI; cs.CY; cs.ET**

- **简介: 该论文研究UX设计师对AI辅助设计（Vibe Coding）的感知，分析其工作流程与挑战。通过访谈20名专业人士，揭示了AI在加速设计迭代的同时带来的信任、责任与创造力问题，探讨人机协作中的责任与技能变化。**

- **链接: [http://arxiv.org/pdf/2509.10652v1](http://arxiv.org/pdf/2509.10652v1)**

> **作者:** Jie Li; Youyang Hou; Laura Lin; Ruihao Zhu; Hancheng Cao; Abdallah El Ali
>
> **摘要:** Generative AI is reshaping UX design practices through "vibe coding," where UX professionals express intent in natural language and AI translates it into functional prototypes and code. Despite rapid adoption, little research has examined how vibe coding reconfigures UX workflows and collaboration. Drawing on interviews with 20 UX professionals across enterprises, startups, and academia, we show how vibe coding follows a four-stage workflow of ideation, AI generation, debugging, and review. This accelerates iteration, supports creativity, and lowers barriers to participation. However, professionals reported challenges of code unreliability, integration, and AI over-reliance. We find tensions between efficiency-driven prototyping ("intending the right design") and reflection ("designing the right intention"), introducing new asymmetries in trust, responsibility, and social stigma within teams. Through the lens of responsible human-AI collaboration for AI-assisted UX design and development, we contribute a deeper understanding of deskilling, ownership and disclosure, and creativity safeguarding in the age of vibe coding.
>
---
#### [new 035] Regulating Ride-Sourcing Markets: Can Minimum Wage Regulation Protect Drivers Without Disrupting the Market?
- **分类: cs.ET; cs.CY**

- **简介: 该论文研究最低工资政策对网约车市场的影响，通过模拟不同监管强度下的市场动态，评估其对司机收入、乘客费用及平台盈利的影响，探讨政策有效性与潜在市场风险。**

- **链接: [http://arxiv.org/pdf/2509.11845v1](http://arxiv.org/pdf/2509.11845v1)**

> **作者:** Farnoud Ghasemi; Arjan de Ruijter; Rafal Kucharski; Oded Cats
>
> **摘要:** Ride-sourcing platforms such as Uber and Lyft are prime examples of the gig economy, recruiting drivers as independent contractors, thereby avoiding legal and fiscal obligations. Although platforms offer flexibility in choosing work shifts and areas, many drivers experience low income and poor working conditions, leading to widespread strikes and protests. Minimum wage regulation is adopted to improve drivers welfare. However, the impacts of this regulation on drivers as well as on travelers and platforms, remain largely unknown. While ride-sourcing platforms do not disclose the relevant data, state-of-the-art models fail to explain the effects of minimum wage regulation on market dynamics. In this study, we assess the effectiveness and implications of minimum wage regulation in ride-sourcing markets while simulating the detailed dynamics of ride-sourcing markets under varying regulation intensities, both with and without the so-called platform lockout strategy. Our findings reveal that minimum wage regulation impacts substantially drivers income, and may lead to higher fares for travelers and threaten platforms survival. When platforms adopt a lockout strategy, their profitability significantly improves and drivers earn more, although many others lose their jobs, and service level for travelers consequently declines.
>
---
#### [new 036] Bilevel subsidy-enabled mobility hub network design with perturbed utility coalitional choice-based assignment
- **分类: math.OC; cs.CY; cs.GT; econ.GN; q-fin.EC**

- **简介: 论文研究如何通过双层补贴机制设计出行枢纽网络，解决出行服务整合与补贴优化问题。提出基于扰动效用的联合决策模型，并采用KKT条件和迭代算法求解，实现高效计算与政策评估。**

- **链接: [http://arxiv.org/pdf/2509.10465v1](http://arxiv.org/pdf/2509.10465v1)**

> **作者:** Hai Yang; Joseph Y. J. Chow
>
> **摘要:** Urban mobility is undergoing rapid transformation with the emergence of new services. Mobility hubs (MHs) have been proposed as physical-digital convergence points, offering a range of public and private mobility options in close proximity. By supporting Mobility-as-a-Service, these hubs can serve as focal points where travel decisions intersect with operator strategies. We develop a bilevel MH platform design model that treats MHs as control levers. The upper level (platform) maximizes revenue or flow by setting subsidies to incentivize last-mile operators; the lower level captures joint traveler-operator decisions with a link-based Perturbed Utility Route Choice (PURC) assignment, yielding a strictly convex quadratic program. We reformulate the bilevel problem to a single-level program via the KKT conditions of the lower level and solve it with a gap-penalty method and an iterative warm-start scheme that exploits the computationally cheap lower-level problem. Numerical experiments on a toy network and a Long Island Rail Road (LIRR) case (244 nodes, 469 links, 78 ODs) show that the method attains sub-1% optimality gaps in minutes. In the base LIRR case, the model allows policymakers to quantify the social surplus value of a MH, or the value of enabling subsidy or regulating the microtransit operator's pricing. Comparing link-based subsidies to hub-based subsidies, the latter is computationally more expensive but offers an easier mechanism for comparison and control.
>
---
#### [new 037] Examining the Relationship between Scientific Publishing Activity and Hype-Driven Financial Bubbles: A Comparison of the Dot-Com and AI Eras
- **分类: cs.LG; cs.CY**

- **简介: 论文比较了互联网和AI时代的科学出版活动与金融泡沫的关系。通过分析引用网络和市场数据，发现AI时代科学家的发表模式部分类似互联网泡沫期，但无法准确预测AI泡沫的存在。研究旨在探索科学出版是否能预示金融泡沫。**

- **链接: [http://arxiv.org/pdf/2509.11982v1](http://arxiv.org/pdf/2509.11982v1)**

> **作者:** Aksheytha Chelikavada; Casey C. Bennett
>
> **摘要:** Financial bubbles often arrive without much warning, but create long-lasting economic effects. For example, during the dot-com bubble, innovative technologies created market disruptions through excitement for a promised bright future. Such technologies originated from research where scientists had developed them for years prior to their entry into the markets. That raises a question on the possibility of analyzing scientific publishing data (e.g. citation networks) leading up to a bubble for signals that may forecast the rise and fall of similar future bubbles. To that end, we utilized temporal SNAs to detect possible relationships between the publication citation networks of scientists and financial market data during two modern eras of rapidly shifting technology: 1) dot-com era from 1994 to 2001 and 2) AI era from 2017 to 2024. Results showed that the patterns from the dot-com era (which did end in a bubble) did not definitively predict the rise and fall of an AI bubble. While yearly citation networks reflected possible changes in publishing behavior of scientists between the two eras, there was a subset of AI era scientists whose publication influence patterns mirrored those during the dot-com era. Upon further analysis using multiple analysis techniques (LSTM, KNN, AR X/GARCH), the data seems to suggest two possibilities for the AI era: unprecedented form of financial bubble unseen or that no bubble exists. In conclusion, our findings imply that the patterns present in the dot-com era do not effectively translate in such a manner to apply them to the AI market.
>
---
#### [new 038] Collective Recourse for Generative Urban Visualizations
- **分类: cs.HC; cs.CY**

- **简介: 论文提出“集体补救”方法，解决生成模型在城市可视化中放大群体性危害的问题。通过社区反馈机制，提出报告、处理、修复流程，并评估不同补救方式的效果，旨在提升模型公平性和规划参与度。**

- **链接: [http://arxiv.org/pdf/2509.11487v1](http://arxiv.org/pdf/2509.11487v1)**

> **作者:** Rashid Mushkani
>
> **摘要:** Text-to-image diffusion models help visualize urban futures but can amplify group-level harms. We propose collective recourse: structured community "visual bug reports" that trigger fixes to models and planning workflows. We (1) formalize collective recourse and a practical pipeline (report, triage, fix, verify, closure); (2) situate four recourse primitives within the diffusion stack: counter-prompts, negative prompts, dataset edits, and reward-model tweaks; (3) define mandate thresholds via a mandate score combining severity, volume saturation, representativeness, and evidence; and (4) evaluate a synthetic program of 240 reports. Prompt-level fixes were fastest (median 2.1-3.4 days) but less durable (21-38% recurrence); dataset edits and reward tweaks were slower (13.5 and 21.9 days) yet more durable (12-18% recurrence) with higher planner uptake (30-36%). A threshold of 0.12 yielded 93% precision and 75% recall; increasing representativeness raised recall to 81% with little precision loss. We discuss integration with participatory governance, risks (e.g., overfitting to vocal groups), and safeguards (dashboards, rotating juries).
>
---
#### [new 039] Tracer: A Forensic Framework for Detecting Fraudulent Speedruns from Game Replays
- **分类: cs.HC; cs.CY**

- **简介: 论文提出Tracer框架，用于检测游戏速通视频中的欺诈行为。该任务旨在解决伪造速通难以识别的问题，通过分析连续性和事件来识别篡改痕迹，提升验证效率和准确性。**

- **链接: [http://arxiv.org/pdf/2509.10848v1](http://arxiv.org/pdf/2509.10848v1)**

> **作者:** Jaeung Franciskus Yoo; Huy Kang Kim
>
> **备注:** 16 pages, 8 figures. Extended version of the paper in Companion Proceedings of the Annual Symposium on Computer-Human Interaction in Play (CHI PLAY Companion 25), New York, NY, USA, October 2025
>
> **摘要:** Speedrun, a practice of completing a game as quickly as possible, has fostered vibrant communities driven by creativity, competition, and mastery of game mechanics and motor skills. However, this contest also attracts malicious actors as financial incentives come into play. As media and software manipulation techniques advance - such as spliced footage, modified game software and live stream with staged setups - forged speedruns have become increasingly difficult to detect. Volunteer-driven communities invest significant effort to verify submissions, yet the process remains slow, inconsistent, and reliant on informal expertise. In high-profile cases, fraudulent runs have gone undetected for years, allowing perpetrators to gain fame and financial benefits through monetised viewership, sponsorships, donations, and community bounties. To address this gap, we propose Tracer, Tamper Recognition via Analysis of Continuity and Events in game Runs, a modular framework for identifying artefacts of manipulation in speedrun submissions. Tracer provides structured guidelines across audiovisual, physical, and cyberspace dimensions, systematically documenting dispersed in-game knowledge and previously reported fraudulent cases to enhance verification efficiency.
>
---
#### [new 040] Transparent and Fair Profiling in Employment Services: Evidence from Switzerland
- **分类: cs.LG; cs.CY**

- **简介: 论文研究就业服务中透明且公平的统计建模方法，比较传统模型与可解释模型在预测长期失业风险中的表现。旨在解决黑箱模型缺乏透明度与公平性的问题，提出可解释模型可作为可靠替代方案。**

- **链接: [http://arxiv.org/pdf/2509.11847v1](http://arxiv.org/pdf/2509.11847v1)**

> **作者:** Tim Räz
>
> **备注:** 35 pages including appendix
>
> **摘要:** Long-term unemployment (LTU) is a challenge for both jobseekers and public employment services. Statistical profiling tools are increasingly used to predict LTU risk. Some profiling tools are opaque, black-box machine learning models, which raise issues of transparency and fairness. This paper investigates whether interpretable models could serve as an alternative, using administrative data from Switzerland. Traditional statistical, interpretable, and black-box models are compared in terms of predictive performance, interpretability, and fairness. It is shown that explainable boosting machines, a recent interpretable model, perform nearly as well as the best black-box models. It is also shown how model sparsity, feature smoothing, and fairness mitigation can enhance transparency and fairness with only minor losses in performance. These findings suggest that interpretable profiling provides an accountable and trustworthy alternative to black-box models without compromising performance.
>
---
#### [new 041] Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble
- **分类: cs.AI; cs.CY**

- **简介: 论文提出P2P框架，利用紧凑LLM集合模拟人类调查受访者，解决社会科学研究中调查成本高和人口结构失衡问题。通过构建多样化代理并选择代表性样本，实现高保真响应模式还原，无需依赖个体个性化数据。**

- **链接: [http://arxiv.org/pdf/2509.11311v1](http://arxiv.org/pdf/2509.11311v1)**

> **作者:** Bingchen Wang; Zi-Yu Khoo; Bryan Kian Hsiang Low
>
> **备注:** Preprint of work originally submitted to AAAI 2026. Under revision for resubmission to a machine learning venue
>
> **摘要:** Large language models (LLMs) have demonstrated promise in emulating human-like responses across a wide range of tasks. In this paper, we propose a novel alignment framework that treats LLMs as agent proxies for human survey respondents, affording a cost-effective and steerable solution to two pressing challenges in the social sciences: the rising cost of survey deployment and the growing demographic imbalance in survey response data. Drawing inspiration from the theory of revealed preference, we formulate alignment as a two-stage problem: constructing diverse agent personas called endowments that simulate plausible respondent profiles, and selecting a representative subset to approximate a ground-truth population based on observed data. To implement the paradigm, we introduce P2P, a system that steers LLM agents toward representative behavioral patterns using structured prompt engineering, entropy-based sampling, and regression-based selection. Unlike personalization-heavy approaches, our alignment approach is demographic-agnostic and relies only on aggregate survey results, offering better generalizability and parsimony. Beyond improving data efficiency in social science research, our framework offers a testbed for studying the operationalization of pluralistic alignment. We demonstrate the efficacy of our approach on real-world opinion survey datasets, showing that our aligned agent populations can reproduce aggregate response patterns with high fidelity and exhibit substantial response diversity, even without demographic conditioning.
>
---
#### [new 042] Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation
- **分类: cs.CL; cs.CY; cs.HC**

- **简介: 论文研究大语言模型在英日职场邮件翻译中的文化适应性。任务是评估不同提示策略对文化敏感度的影响。通过混合方法分析，发现定制化提示可提升文化契合度，为设计包容性多语LLMs提供建议。**

- **链接: [http://arxiv.org/pdf/2509.11921v1](http://arxiv.org/pdf/2509.11921v1)**

> **作者:** Helene Tenzer; Oumnia Abidi; Stefan Feuerriegel
>
> **摘要:** Large language models (LLMs) are increasingly used in everyday communication, including multilingual interactions across different cultural contexts. While LLMs can now generate near-perfect literal translations, it remains unclear whether LLMs support culturally appropriate communication. In this paper, we analyze the cultural sensitivity of different LLM designs when applied to English-Japanese translations of workplace e-mails. Here, we vary the prompting strategies: (1) naive "just translate" prompts, (2) audience-targeted prompts specifying the recipient's cultural background, and (3) instructional prompts with explicit guidance on Japanese communication norms. Using a mixed-methods study, we then analyze culture-specific language patterns to evaluate how well translations adapt to cultural norms. Further, we examine the appropriateness of the tone of the translations as perceived by native speakers. We find that culturally-tailored prompting can improve cultural fit, based on which we offer recommendations for designing culturally inclusive LLMs in multilingual settings.
>
---
#### [new 043] Can GenAI Move from Individual Use to Collaborative Work? Experiences, Challenges, and Opportunities of Integrating GenAI into Collaborative Newsroom Routines
- **分类: cs.HC; cs.CY**

- **简介: 该论文探讨生成式AI（GenAI）从个体使用转向协作工作的可能性。通过27次访谈，研究发现记者虽常用GenAI，但组织层面的协作整合受限于结构和文化障碍，提出需关注组织结构与流程设计以促进协作应用。**

- **链接: [http://arxiv.org/pdf/2509.10950v1](http://arxiv.org/pdf/2509.10950v1)**

> **作者:** Qing Xiao; Qing; Hu; Jingjia Xiao; Hancheng Cao; Hong Shen
>
> **备注:** 17 pages, 1 figure
>
> **摘要:** Generative AI (GenAI) is reshaping work, but adoption remains largely individual and experimental rather than integrated into collaborative routines. Whether GenAI can move from individual use to collaborative work is a critical question for future organizations. Journalism offers a compelling site to examine this shift: individual journalists have already been disrupted by GenAI tools; yet newswork is inherently collaborative relying on shared routines and coordinated workflows. We conducted 27 interviews with newsrooms managers, editors, and front-line journalists in China. We found that journalists frequently used GenAI to support daily tasks, but value alignment was safeguarded mainly through individual discretion. At the organizational level, GenAI use remained disconnected from team workflows, hindered by structural barriers and cultural reluctance to share practices. These findings underscore the gap between individual and collective adoption, pointing to the need for accounting for organizational structures, cultural norms, and workflow integration when designing GenAI for collaborative work.
>
---
#### [new 044] AI Hasn't Fixed Teamwork, But It Shifted Collaborative Culture: A Longitudinal Study in a Project-Based Software Development Organization (2023-2025)
- **分类: cs.HC; cs.CY; cs.SE**

- **简介: 论文研究AI对团队协作的影响，通过2023-2025年的纵向访谈，发现AI未解决协作问题，但重塑了协作文化。任务是分析AI如何改变团队合作方式，核心问题是AI是否改善团队协作及文化演变。**

- **链接: [http://arxiv.org/pdf/2509.10956v1](http://arxiv.org/pdf/2509.10956v1)**

> **作者:** Qing Xiao; Xinlan Emily Hu; Mark E. Whiting; Arvind Karunakaran; Hong Shen; Hancheng Cao
>
> **备注:** 18 pages
>
> **摘要:** When AI entered the workplace, many believed it could reshape teamwork as profoundly as it boosted individual productivity. Would AI finally ease the longstanding challenges of team collaboration? Our findings suggested a more complicated reality. We conducted a longitudinal two-wave interview study (2023-2025) with members (N=15) of a project-based software development organization to examine the expectations and use of AI in teamwork. In early 2023, just after the release of ChatGPT, participants envisioned AI as an intelligent coordinator that could align projects, track progress, and ease interpersonal frictions. By 2025, however, AI was used mainly to accelerate individual tasks such as coding, writing, and documentation, leaving persistent collaboration issues of performance accountability and fragile communication unresolved. Yet AI reshaped collaborative culture: efficiency became a norm, transparency and responsible use became markers of professionalism, and AI was increasingly accepted as part of teamwork.
>
---
## 更新

#### [replaced 001] Impact Ambivalence: How People with Eating Disorders Get Trapped in the Perpetual Cycle of Digital Food Content Engagement
- **分类: cs.HC; cs.CY; cs.MM**

- **链接: [http://arxiv.org/pdf/2311.05920v4](http://arxiv.org/pdf/2311.05920v4)**

> **作者:** Ryuhaerang Choi; Subin Park; Sujin Han; Jennifer G. Kim; Sung-Ju Lee
>
> **备注:** 15 pages, 3 figures
>
> **摘要:** Digital food content could impact viewers' dietary health, with individuals with eating disorders being particularly sensitive to it. However, a comprehensive understanding of why and how these individuals interact with such content is lacking. To fill this void, we conducted exploratory (N=23) and in-depth studies (N=22) with individuals with eating disorders to understand their motivations and practices of consuming digital food content. We reveal that participants engaged with digital food content for both disorder-driven and recovery-supporting motivations, leading to conflicting outcomes. This impact ambivalence, the coexistence of recovery-supporting benefits and disorder-exacerbating risks, sustained a cycle of quitting, prompted by awareness of harm, and returning, motivated by anticipated benefits. We interpret these dynamics within dual systems theory and highlight how recognizing such ambivalence can inform the design of interventions that foster healthier digital food content engagement and mitigate post-engagement harmful effects.
>
---
#### [replaced 002] From Vision to Validation: A Theory- and Data-Driven Construction of a GCC-Specific AI Adoption Index
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.05474v3](http://arxiv.org/pdf/2509.05474v3)**

> **作者:** Mohammad Rashed Albous; Abdel Latef Anouze
>
> **备注:** 38 pages, 8 figures, 17 tables
>
> **摘要:** Artificial intelligence (AI) is rapidly transforming public-sector processes worldwide, yet standardized measures rarely address the unique drivers, governance models, and cultural nuances of the Gulf Cooperation Council (GCC) countries. This study employs a theory-driven foundation derived from an in-depth analysis of literature review and six National AI Strategies (NASs), coupled with a data-driven approach that utilizes a survey of 203 mid- and senior-level government employees and advanced statistical techniques (K-Means clustering, Principal Component Analysis, and Partial Least Squares Structural Equation Modeling). By combining policy insights with empirical evidence, the research develops and validates a novel AI Adoption Index specifically tailored to the GCC public sector. Findings indicate that robust technical infrastructure and clear policy mandates exert the strongest influence on successful AI implementations, overshadowing organizational readiness in early adoption stages. The combined model explains 70% of the variance in AI outcomes, suggesting that resource-rich environments and top-down policy directives can drive rapid but uneven technology uptake. By consolidating key dimensions (Technical Infrastructure (TI), Organizational Readiness (OR), and Governance Environment (GE)) into a single composite index, this study provides a holistic yet context-sensitive tool for benchmarking AI maturity. The index offers actionable guidance for policymakers seeking to harmonize large-scale deployments with ethical and regulatory standards. Beyond advancing academic discourse, these insights inform more strategic allocation of resources, cross-country cooperation, and capacity-building initiatives, thereby supporting sustained AI-driven transformation in the GCC region and beyond.
>
---
#### [replaced 003] Approaches to Responsible Governance of GenAI in Organizations
- **分类: cs.CY; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2504.17044v2](http://arxiv.org/pdf/2504.17044v2)**

> **作者:** Dhari Gandhi; Himanshu Joshi; Lucas Hartman; Shabnam Hassani
>
> **摘要:** PEER-REVIEWED AND ACCEPTED IN IEEE- ISTAS 2025 The rapid evolution of Generative AI (GenAI) has introduced unprecedented opportunities while presenting complex challenges around ethics, accountability, and societal impact. This paper draws on a literature review, established governance frameworks, and industry roundtable discussions to identify core principles for integrating responsible GenAI governance into diverse organizational structures. Our objective is to provide actionable recommendations for a balanced, risk-based governance approach that enables both innovation and oversight. Findings emphasize the need for adaptable risk assessment tools, continuous monitoring practices, and cross-sector collaboration to establish trustworthy GenAI. These insights provide a structured foundation and Responsible GenAI Guide (ResAI) for organizations to align GenAI initiatives with ethical, legal, and operational best practices.
>
---
#### [replaced 004] A Human-Centered Approach to Identifying Promises, Risks, & Challenges of Text-to-Image Generative AI in Radiology
- **分类: cs.HC; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.16207v2](http://arxiv.org/pdf/2507.16207v2)**

> **作者:** Katelyn Morrison; Arpit Mathur; Aidan Bradshaw; Tom Wartmann; Steven Lundi; Afrooz Zandifar; Weichang Dai; Kayhan Batmanghelich; Motahhare Eslami; Adam Perer
>
> **备注:** 10 pages of main content, Appendix attached after references, accepted to AAAI/ACM AIES 2025
>
> **摘要:** As text-to-image generative models rapidly improve, AI researchers are making significant advances in developing domain-specific models capable of generating complex medical imagery from text prompts. Despite this, these technical advancements have overlooked whether and how medical professionals would benefit from and use text-to-image generative AI (GenAI) in practice. By developing domain-specific GenAI without involving stakeholders, we risk the potential of building models that are either not useful or even more harmful than helpful. In this paper, we adopt a human-centered approach to responsible model development by involving stakeholders in evaluating and reflecting on the promises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through exploratory model prompting activities, we uncover the perspectives of medical students, radiology trainees, and radiologists on the role that text-to-CT Scan GenAI can play across medical education, training, and practice. This human-centered approach additionally enabled us to surface technical challenges and domain-specific risks of generating synthetic medical images. We conclude by reflecting on the implications of medical text-to-image GenAI.
>
---
#### [replaced 005] WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design
- **分类: cs.HC; cs.CY**

- **链接: [http://arxiv.org/pdf/2508.19256v3](http://arxiv.org/pdf/2508.19256v3)**

> **作者:** Rashid Mushkani; Hugo Berard; Shin Koseki
>
> **摘要:** Community consultations are integral to urban planning processes intended to incorporate diverse stakeholder perspectives. However, limited resources, visual and spoken language barriers, and uneven power dynamics frequently constrain inclusive decision-making. This paper examines how generative text-to-image methods, specifically Stable Diffusion XL integrated into a custom platform (WeDesign), may support equitable consultations. A half-day workshop in Montreal involved five focus groups, each consisting of architects, urban designers, AI specialists, and residents from varied demographic groups. Additional data was gathered through semi-structured interviews with six urban planning professionals. Participants indicated that immediate visual outputs facilitated creativity and dialogue, yet noted issues in visualizing specific needs of marginalized groups, such as participants with reduced mobility, accurately depicting local architectural elements, and accommodating bilingual prompts. Participants recommended the development of an open-source platform incorporating in-painting tools, multilingual support, image voting functionalities, and preference indicators. The results indicate that generative AI can broaden participation and enable iterative interactions but requires structured facilitation approaches. The findings contribute to discussions on generative AI's role and limitations in participatory urban design.
>
---
#### [replaced 006] Assessing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation
- **分类: cs.CL; cs.CY; cs.HC**

- **链接: [http://arxiv.org/pdf/2504.12805v2](http://arxiv.org/pdf/2504.12805v2)**

> **作者:** Takaya Arita; Wenxian Zheng; Reiji Suzuki; Fuminori Akiba
>
> **备注:** Corrected a typo in the metadata title only ("Assesing"->"Assessing"). No changes were made to the PDF or source files
>
> **摘要:** This study explored how large language models (LLMs) perform in two areas related to art: writing critiques of artworks and reasoning about mental states (Theory of Mind, or ToM) in art-related situations. For the critique generation part, we built a system that combines Noel Carroll's evaluative framework with a broad selection of art criticism theories. The model was prompted to first write a full-length critique and then shorter, more coherent versions using a step-by-step prompting process. These AI-generated critiques were then compared with those written by human experts in a Turing test-style evaluation. In many cases, human subjects had difficulty telling which was which, and the results suggest that LLMs can produce critiques that are not only plausible in style but also rich in interpretation, as long as they are carefully guided. In the second part, we introduced new simple ToM tasks based on situations involving interpretation, emotion, and moral tension, which can appear in the context of art. These go beyond standard false-belief tests and allow for more complex, socially embedded forms of reasoning. We tested 41 recent LLMs and found that their performance varied across tasks and models. In particular, tasks that involved affective or ambiguous situations tended to reveal clearer differences. Taken together, these results help clarify how LLMs respond to complex interpretative challenges, revealing both their cognitive limitations and potential. While our findings do not directly contradict the so-called Generative AI Paradox--the idea that LLMs can produce expert-like output without genuine understanding--they suggest that, depending on how LLMs are instructed, such as through carefully designed prompts, these models may begin to show behaviors that resemble understanding more closely than we might assume.
>
---
#### [replaced 007] The Quantum Technology Job Market: Data Driven Analysis of 3641 Job Posts
- **分类: physics.ed-ph; cs.CY; quant-ph**

- **链接: [http://arxiv.org/pdf/2503.19004v4](http://arxiv.org/pdf/2503.19004v4)**

> **作者:** Simon Goorney; Eleni Karydi; Borja Muñoz; Otto Santesson; Zeki Can Seskir; Ana Alina Tudoran; Jacob Sherson
>
> **摘要:** The rapid advancement of Quantum Technology (QT) has created a growing demand for a specialized workforce, spanning across academia and industry. This study presents a quantitative analysis of the QT job market by systematically extracting and classifying thousands of job postings worldwide. The classification pipeline leverages large language models (LLMs) whilst incorporating a "human-in-the-loop" validation process to ensure reliability, achieving an F1-score of 89%: a high level of accuracy. The research identifies key trends in regional job distribution, degree and skill requirements, and the evolving demand for QT-related roles. Findings reveal a strong presence of the QT job market in the United States and Europe, with increasing corporate demand for engineers, software developers, and PhD-level researchers. Despite growing industry applications, the sector remains in its early stages, dominated by large technology firms and requiring significant investment in education and workforce development. The study highlights the need for targeted educational programs, interdisciplinary collaboration, and industry-academic partnerships to bridge the QT workforce gap.
>
---
#### [replaced 008] The Conspiracy Money Machine: Uncovering Telegram's Conspiracy Channels and their Profit Model
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2310.15977v3](http://arxiv.org/pdf/2310.15977v3)**

> **作者:** Vincenzo Imperati; Massimo La Morgia; Alessandro Mei; Alberto Maria Mongardini; Francesco Sassi
>
> **备注:** This paper is included in the Proceedings of the 34th USENIX Security Symposium 2025 (USENIX Security 25), Seattle, WA, USA: https://www.usenix.org/system/files/usenixsecurity25-imperati.pdf
>
> **摘要:** In recent years, major social media platforms have implemented increasingly strict moderation policies, resulting in bans and restrictions on conspiracy theory-related content. To circumvent these restrictions, conspiracy theorists are turning to alternatives, such as Telegram, where they can express and spread their views with fewer limitations. Telegram offers channels, virtual rooms where only administrators can broadcast messages, and a more permissive content policy. These features have created the perfect breeding ground for a complex ecosystem of conspiracy channels. In this paper, we illuminate this ecosystem. First, we propose an approach to detect conspiracy channels. Then, we discover that conspiracy channels can be clustered into four distinct communities comprising over 17,000 channels. Finally, we uncover the "Conspiracy Money Machine," revealing how most conspiracy channels actively seek to profit from their subscribers. We find conspiracy theorists leverage e-commerce platforms to sell questionable products or lucratively promote them through affiliate links. Moreover, we observe that conspiracy channels use donation and crowdfunding platforms to raise funds for their campaigns. We determine that this business involves hundreds of thousands of donors and generates a turnover of almost $71 million.
>
---
#### [replaced 009] Social Perception of Faces in a Vision-Language Model
- **分类: cs.CV; cs.AI; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2408.14435v2](http://arxiv.org/pdf/2408.14435v2)**

> **作者:** Carina I. Hausladen; Manuel Knott; Colin F. Camerer; Pietro Perona
>
> **摘要:** We explore social perception of human faces in CLIP, a widely used open-source vision-language model. To this end, we compare the similarity in CLIP embeddings between different textual prompts and a set of face images. Our textual prompts are constructed from well-validated social psychology terms denoting social perception. The face images are synthetic and are systematically and independently varied along six dimensions: the legally protected attributes of age, gender, and race, as well as facial expression, lighting, and pose. Independently and systematically manipulating face attributes allows us to study the effect of each on social perception and avoids confounds that can occur in wild-collected data due to uncontrolled systematic correlations between attributes. Thus, our findings are experimental rather than observational. Our main findings are three. First, while CLIP is trained on the widest variety of images and texts, it is able to make fine-grained human-like social judgments on face images. Second, age, gender, and race do systematically impact CLIP's social perception of faces, suggesting an undesirable bias in CLIP vis-a-vis legally protected attributes. Most strikingly, we find a strong pattern of bias concerning the faces of Black women, where CLIP produces extreme values of social perception across different ages and facial expressions. Third, facial expression impacts social perception more than age and lighting as much as age. The last finding predicts that studies that do not control for unprotected visual attributes may reach the wrong conclusions on bias. Our novel method of investigation, which is founded on the social psychology literature and on the experiments involving the manipulation of individual attributes, yields sharper and more reliable observations than previous observational methods and may be applied to study biases in any vision-language model.
>
---
#### [replaced 010] IndiTag: An Online Media Bias Analysis System Using Fine-Grained Bias Indicators for News Consumers
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2403.13446v2](http://arxiv.org/pdf/2403.13446v2)**

> **作者:** Luyang Lin; Lingzhi Wang; Jinsong Guo; Jing Li; Kam-Fai Wong
>
> **摘要:** In the age of information overload and polarized discourse, understanding media bias has become imperative for informed decision-making and fostering a balanced public discourse. However, without the experts' analysis, it is hard for the readers to distinguish bias from the news articles. This paper presents IndiTag, an innovative online media bias analysis system that leverages fine-grained bias indicators to dissect and distinguish bias in digital content. IndiTag offers a novel approach by incorporating large language models, bias indicators, and vector database to detect and interpret bias automatically. Complemented by a user-friendly interface facilitating automated bias analysis for readers, IndiTag offers a comprehensive platform for in-depth bias examination. We demonstrate the efficacy and versatility of IndiTag through experiments on four datasets encompassing news articles from diverse platforms. Furthermore, we discuss potential applications of IndiTag in fostering media literacy, facilitating fact-checking initiatives, and enhancing the transparency and accountability of digital media platforms. IndiTag stands as a valuable tool in the pursuit of fostering a more informed, discerning, and inclusive public discourse in the digital age. The demonstration video can be accessed from https://youtu.be/3Tux8CW46OE. We release an online system for end users and the source code is available at https://github.com/lylin0/IndiTag.
>
---
#### [replaced 011] Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models
- **分类: cs.CL; cs.AI; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2309.01219v3](http://arxiv.org/pdf/2309.01219v3)**

> **作者:** Yue Zhang; Yafu Li; Leyang Cui; Deng Cai; Lemao Liu; Tingchen Fu; Xinting Huang; Enbo Zhao; Yu Zhang; Chen Xu; Yulong Chen; Longyue Wang; Anh Tuan Luu; Wei Bi; Freda Shi; Shuming Shi
>
> **备注:** work in progress;
>
> **摘要:** While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
>
---
#### [replaced 012] Can AI be Auditable?
- **分类: cs.CY; cs.AI; cs.HC**

- **链接: [http://arxiv.org/pdf/2509.00575v3](http://arxiv.org/pdf/2509.00575v3)**

> **作者:** Himanshu Verma; Kirtan Padh; Eva Thelisson
>
> **摘要:** Auditability is defined as the capacity of AI systems to be independently assessed for compliance with ethical, legal, and technical standards throughout their lifecycle. The chapter explores how auditability is being formalized through emerging regulatory frameworks, such as the EU AI Act, which mandate documentation, risk assessments, and governance structures. It analyzes the diverse challenges facing AI auditability, including technical opacity, inconsistent documentation practices, lack of standardized audit tools and metrics, and conflicting principles within existing responsible AI frameworks. The discussion highlights the need for clear guidelines, harmonized international regulations, and robust socio-technical methodologies to operationalize auditability at scale. The chapter concludes by emphasizing the importance of multi-stakeholder collaboration and auditor empowerment in building an effective AI audit ecosystem. It argues that auditability must be embedded in AI development practices and governance infrastructures to ensure that AI systems are not only functional but also ethically and legally aligned.
>
---
#### [replaced 013] Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models
- **分类: cs.AI; cs.CL; cs.CY; cs.HC; cs.SC**

- **链接: [http://arxiv.org/pdf/2509.01909v5](http://arxiv.org/pdf/2509.01909v5)**

> **作者:** Ranjie Duan; Jiexi Liu; Xiaojun Jia; Shiji Zhao; Ruoxi Cheng; Fengxiang Wang; Cheng Wei; Yong Xie; Chang Liu; Defeng Li; Yinpeng Dong; Yichi Zhang; Yuefeng Chen; Chongwen Wang; Xingjun Ma; Xingxing Wei; Yang Liu; Hang Su; Jun Zhu; Xinfeng Li; Yitong Sun; Jie Zhang; Jinzhao Hu; Sha Xu; Wenchao Yang; Yitong Yang; Jialing Tao; Hui Xue
>
> **备注:** Technical Report Code & Model weights available: https://github.com/Alibaba-AAIG/Oyster
>
> **摘要:** Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.
>
---
#### [replaced 014] FOCUS on Contamination: A Geospatial Deep Learning Framework with a Noise-Aware Loss for Surface Water PFAS Prediction
- **分类: cs.CV; cs.AI; cs.CY; cs.LG; I.2.1; I.2.10; I.4.6; I.4.9; I.4.10; J.2**

- **链接: [http://arxiv.org/pdf/2502.14894v2](http://arxiv.org/pdf/2502.14894v2)**

> **作者:** Jowaria Khan; Alexa Friedman; Sydney Evans; Rachel Klein; Runzi Wang; Katherine E. Manz; Kaley Beins; David Q. Andrews; Elizabeth Bondi-Kelly
>
> **摘要:** Per- and polyfluoroalkyl substances (PFAS), chemicals found in products like non-stick cookware, are unfortunately persistent environmental pollutants with severe health risks. Accurately mapping PFAS contamination is crucial for guiding targeted remediation efforts and protecting public and environmental health, yet detection across large regions remains challenging due to the cost of testing and the difficulty of simulating their spread. In this work, we introduce FOCUS, a geospatial deep learning framework with a label noise-aware loss function, to predict PFAS contamination in surface water over large regions. By integrating hydrological flow data, land cover information, and proximity to known PFAS sources, our approach leverages both spatial and environmental context to improve prediction accuracy. We evaluate the performance of our approach through extensive ablation studies, robustness analysis, real-world validation, and comparative analyses against baselines like sparse segmentation, as well as existing scientific methods, including Kriging and pollutant transport simulations. Results and expert feedback highlight our framework's potential for scalable PFAS monitoring.
>
---
#### [replaced 015] From private to public governance: The case for reconfiguring energy systems as a commons
- **分类: eess.SY; cs.CY; cs.NI; cs.SY**

- **链接: [http://arxiv.org/pdf/2008.04028v2](http://arxiv.org/pdf/2008.04028v2)**

> **作者:** Chris Giotitsas; Pedro H. J. Nardelli; Vasilis Kostakis; Arun Narayanan
>
> **备注:** Accepted to publication at Energy Research & Social Science (Elsevier)
>
> **摘要:** The discussions around the unsustainability of the dominant socio-economic structures have yet to produce solutions to address the escalating problems we face as a species. Such discussions, this paper argues, are hindered by the limited scope of the proposed solutions within a business-as-usual context as well as by the underlying technological rationale upon which these solutions are developed. In this paper, we conceptualize a radical sustainable alternative to the energy conundrum based on an emerging mode of production and a commons-based political economy. We propose a commons-oriented Energy Internet as a potential system for energy production and consumption, which may be better suited to tackle the current issues society faces. We conclude by referring to some of the challenges that the implementation of such a proposal would entail.
>
---
#### [replaced 016] Linguistic Hooks: Investigating The Role of Language Triggers in Phishing Emails Targeting African Refugees and Students
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2509.04700v3](http://arxiv.org/pdf/2509.04700v3)**

> **作者:** Mythili Menon; Nisha Vinayaga-Sureshkanth; Alec Schon; Kaitlyn Hemberger; Murtuza Jadliwala
>
> **备注:** Mythili Menon and Nisha Vinayaga-Sureshkanth contributed equally to the work (co-first authors). Work accepted to Proceedings on Privacy Enhancing Technologies (PoPETs), Volume 2026, Issue 1
>
> **摘要:** Phishing and sophisticated email-based social engineering attacks disproportionately affect vulnerable populations, such as refugees and immigrant students. However, these groups remain understudied in cybersecurity research. This gap in understanding, coupled with their exclusion from broader security and privacy policies, increases their susceptibility to phishing and widens the digital security divide between marginalized and non-marginalized populations. To address this gap, we first conducted digital literacy workshops with newly resettled African refugee populations (n = 48) in the US to improve their understanding of how to safeguard sensitive and private information. Following the workshops, we conducted a real-world phishing deception study using carefully designed emails with linguistic cues for three participant groups: a subset of the African US-refugees recruited from the digital literacy workshops (n = 19), African immigrant students in the US (n = 142), and a control group of monolingual US-born students (n = 184). Our findings indicate that while digital literacy training for refugees improves awareness of safe cybersecurity practices, recently resettled African US-refugees still face significant challenges due to low digital literacy skills and limited English proficiency. This often leads them to ignore or fail to recognize phishing emails as phishing. Both African immigrant students and US-born students showed greater caution, though instances of data disclosure remained prevalent across groups. Our findings highlight, irrespective of literacy, the need to be trained to think critically about digital security. We conclude by discussing how the security and privacy community can better include marginalized populations in policy making and offer recommendations for designing equitable, inclusive cybersecurity initiatives.
>
---
