# 机器人 cs.RO

- **最新发布 55 篇**

- **更新 45 篇**

## 最新发布

#### [new 001] A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0
- **分类: cs.RO**

- **简介: 论文设计了一款21自由度的人形灵巧手CYJ Hand-0，采用SMA与电机混合驱动，结合仿生结构实现手指屈伸与外展。该研究属于机器人灵巧手设计任务，旨在提升机械手灵活性与仿生性。工作包括结构设计、驱动系统集成及实验验证。**

- **链接: [http://arxiv.org/pdf/2507.14538v1](http://arxiv.org/pdf/2507.14538v1)**

> **作者:** Jin Chai; Xiang Yao; Mengfan Hou; Yanghong Li; Erbao Dong
>
> **摘要:** CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid tendon-driven actuation system that combines shape memory alloys (SMAs) and DC motors. The hand employs high-strength fishing line as artificial tendons and uses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal and tendon-muscle structure of the human hand. A linear motor-driven module controls finger flexion, while an SMA-based module enables finger extension and lateral abduction. These modules are integrated into a compact hybrid actuation unit mounted on a custom rear support structure. Mechanical and kinematic experiments, conducted under an Arduino Mega 2560-based control system, validate the effectiveness of the design and demonstrate its biomimetic dexterity.
>
---
#### [new 002] Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support
- **分类: cs.RO**

- **简介: 该论文属于人机交互与心理健康支持任务，旨在解决现有社交辅助机器人在实时对话、个性化反馈和非言语行为同步方面的不足。研究通过集成端到端语音语言模型，开展用户研究评估系统表现，并根据反馈提出改进方向。**

- **链接: [http://arxiv.org/pdf/2507.14412v1](http://arxiv.org/pdf/2507.14412v1)**

> **作者:** Mengxue Fu; Zhonghao Shi; Minyu Huang; Siqi Liu; Mina Kian; Yirui Song; Maja J. Matarić
>
> **摘要:** Socially assistive robots (SARs) have shown great potential for supplementing well-being support. However, prior studies have found that existing dialogue pipelines for SARs remain limited in real-time latency, back-channeling, and personalized speech dialogue. Toward addressing these limitations, we propose using integrated end-to-end speech-language models (SLMs) with SARs. This work 1) evaluated the usability of an SLM-enabled SAR dialogue system through a small user study, and 2) identified remaining limitations through study user feedback to inform future improvements. We conducted a small within-participant user study with university students (N = 11) whose results showed that participants perceived an SLM-enabled SAR system as capable of providing empathetic feedback, natural turn-taking, back-channeling, and adaptive responses. We also found that participants reported the robot's nonverbal behaviors as lacking variability and synchronization with conversation, and the SLM's verbal feedback as generic and repetitive. These findings highlighted the need for real-time robot movement synchronized with conversation, improved prompting or fine-tuning to generate outputs better aligned with mental health practices, and more expressive, adaptive vocal generation.
>
---
#### [new 003] Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction
- **分类: cs.RO; cs.HC**

- **简介: 该论文属于人机交互任务，旨在解决现有HRI系统在双向、多模态和情境感知支持方面的不足。论文提出了一种结合注视和语音的LLM框架，实现环境感知与动态任务支持，验证了其适应性和实时性，并通过实验比较了LLM与传统脚本方法的优劣。**

- **链接: [http://arxiv.org/pdf/2507.15729v1](http://arxiv.org/pdf/2507.15729v1)**

> **作者:** Jens V. Rüppel; Andrey Rudenko; Tim Schreiter; Martin Magnusson; Achim J. Lilienthal
>
> **备注:** This paper has been accepted to the 34th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), which will be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses
>
> **摘要:** The rapid development of Large Language Models (LLMs) creates an exciting potential for flexible, general knowledge-driven Human-Robot Interaction (HRI) systems for assistive robots. Existing HRI systems demonstrate great progress in interpreting and following user instructions, action generation, and robot task solving. On the other hand, bi-directional, multi-modal, and context-aware support of the user in collaborative tasks still remains an open challenge. In this paper, we present a gaze- and speech-informed interface to the assistive robot, which is able to perceive the working environment from multiple vision inputs and support the dynamic user in their tasks. Our system is designed to be modular and transferable to adapt to diverse tasks and robots, and it is capable of real-time use of language-based interaction state representation and fast on board perception modules. Its development was supported by multiple public dissemination events, contributing important considerations for improved robustness and user experience. Furthermore, in two lab studies, we compare the performance and user ratings of our system with those of a traditional scripted HRI pipeline. Our findings indicate that an LLM-based approach enhances adaptability and marginally improves user engagement and task execution metrics but may produce redundant output, while a scripted pipeline is well suited for more straightforward tasks.
>
---
#### [new 004] Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking
- **分类: cs.RO**

- **简介: 论文研究用时延嵌入方法建立非光滑或混杂系统的线性模型，并设计基于状态历史的LQR控制器。任务是建模与控制周期性混杂系统，解决其非线性和非光滑带来的建模难题。工作包括对摆球和最简行走模型进行时延嵌入建模，并实现反馈控制。**

- **链接: [http://arxiv.org/pdf/2507.14455v1](http://arxiv.org/pdf/2507.14455v1)**

> **作者:** Chun-Ming Yang; Pranav A. Bhounsule
>
> **摘要:** Time-delay embedding is a technique that uses snapshots of state history over time to build a linear state space model of a nonlinear smooth system. We demonstrate that periodic non-smooth or hybrid system can also be modeled as a linear state space system using this approach as long as its behavior is consistent in modes and timings. We extended time-delay embeddings to generate a linear model of two periodic hybrid systems: the bouncing pendulum and the simplest walker with control inputs. This leads to a novel state history augmented linear quadratic regulator (LQR) which uses current and past state history for feedback control.
>
---
#### [new 005] All-UWB SLAM Using UWB Radar and UWB AOA
- **分类: cs.RO**

- **简介: 该论文属于SLAM任务，旨在解决在视觉受限且特征匮乏的环境中（如烟雾、灰尘）实现高精度定位与建图的问题。论文提出了一种结合UWB雷达与UWB AOA测量的新方法，通过动态部署UWB锚点以增强环境感知能力，提升了SLAM在低特征环境中的准确性与扩展性。**

- **链接: [http://arxiv.org/pdf/2507.15474v1](http://arxiv.org/pdf/2507.15474v1)**

> **作者:** Charith Premachandra; Achala Athukorala; U-Xuan Tan
>
> **摘要:** There has been a growing interest in autonomous systems designed to operate in adverse conditions (e.g. smoke, dust), where the visible light spectrum fails. In this context, Ultra-wideband (UWB) radar is capable of penetrating through such challenging environmental conditions due to the lower frequency components within its broad bandwidth. Therefore, UWB radar has emerged as a potential sensing technology for Simultaneous Localization and Mapping (SLAM) in vision-denied environments where optical sensors (e.g. LiDAR, Camera) are prone to failure. Existing approaches involving UWB radar as the primary exteroceptive sensor generally extract features in the environment, which are later initialized as landmarks in a map. However, these methods are constrained by the number of distinguishable features in the environment. Hence, this paper proposes a novel method incorporating UWB Angle of Arrival (AOA) measurements into UWB radar-based SLAM systems to improve the accuracy and scalability of SLAM in feature-deficient environments. The AOA measurements are obtained using UWB anchor-tag units which are dynamically deployed by the robot in featureless areas during mapping of the environment. This paper thoroughly discusses prevailing constraints associated with UWB AOA measurement units and presents solutions to overcome them. Our experimental results show that integrating UWB AOA units with UWB radar enables SLAM in vision-denied feature-deficient environments.
>
---
#### [new 006] Strong, Accurate, and Low-Cost Robot Manipulator
- **分类: cs.RO**

- **简介: 论文提出了一种低成本、高性能的6自由度3D打印机械臂Forte，旨在解决教育和研究中低成本机器人性能不足的问题。通过优化机械设计和驱动系统，实现高重复精度和负载能力，适用于教学与AI实验。**

- **链接: [http://arxiv.org/pdf/2507.15693v1](http://arxiv.org/pdf/2507.15693v1)**

> **作者:** Georges Chebly; Spencer Little; Nisal Perera; Aliya Abedeen; Ken Suzuki; Donghyun Kim
>
> **摘要:** This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed to achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach, and sub-millimeter repeatability - at a material cost under $215. As an accessible robot for broad applications across classroom education to AI experiments, Forte pushes forward the performance limitations of existing low-cost educational arms. We introduce a cost-effective mechanical design that combines capstan-based cable drives, timing belts, simple tensioning mechanisms, and lightweight 3D-printed structures, along with topology optimization for structural stiffness. Through careful drivetrain engineering, we minimize backlash and maintain control fidelity without relying on high-power electronics or expensive manufacturing processes. Experimental validation demonstrates that Forte achieves high repeatability and load capacity, offering a compelling robotic platform for both classroom instruction and advanced robotics research.
>
---
#### [new 007] A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators
- **分类: cs.RO; cs.NA; math.DG; math.DS; math.GR; math.NA**

- **简介: 论文研究并解决了并联运动机械臂（PKM）中串联弹性驱动器（SEA）轨迹控制所需的二阶逆动力学时间导数计算问题。利用李群递归算法，首次实现了对PKM的二阶时间导数高效计算，并通过6自由度Gough-Stewart平台和二维PKM验证了方法的有效性。属于机器人动力学建模与控制任务。**

- **链接: [http://arxiv.org/pdf/2507.14274v1](http://arxiv.org/pdf/2507.14274v1)**

> **作者:** Andreas Mueller; Shivesh Kumar; Thomas Kordik
>
> **摘要:** Series elastic actuators (SEA) were introduced for serial robotic arms. Their model-based trajectory tracking control requires the second time derivatives of the inverse dynamics solution, for which algorithms were proposed. Trajectory control of parallel kinematics manipulators (PKM) equipped with SEAs has not yet been pursued. Key element for this is the computationally efficient evaluation of the second time derivative of the inverse dynamics solution. This has not been presented in the literature, and is addressed in the present paper for the first time. The special topology of PKM is exploited reusing the recursive algorithms for evaluating the inverse dynamics of serial robots. A Lie group formulation is used and all relations are derived within this framework. Numerical results are presented for a 6-DOF Gough-Stewart platform (as part of an exoskeleton), and for a planar PKM when a flatness-based control scheme is applied.
>
---
#### [new 008] Search-Based Autonomous Vehicle Motion Planning Using Game Theory
- **分类: cs.RO; cs.GT**

- **简介: 该论文属于自动驾驶路径规划任务，旨在解决如何让自动驾驶车辆在复杂交通环境中生成更真实的行驶路径。论文提出了一种基于搜索和博弈论的交互式运动规划方法，将其他道路使用者视为智能体，提高了路径规划的实时性与实用性，并通过实验验证了其性能。**

- **链接: [http://arxiv.org/pdf/2507.15088v1](http://arxiv.org/pdf/2507.15088v1)**

> **作者:** Pouya Panahandeh; Mohammad Pirani; Baris Fidan; Amir Khajepour
>
> **摘要:** In this paper, we propose a search-based interactive motion planning scheme for autonomous vehicles (AVs), using a game-theoretic approach. In contrast to traditional search-based approaches, the newly developed approach considers other road users (e.g. drivers and pedestrians) as intelligent agents rather than static obstacles. This leads to the generation of a more realistic path for the AV. Due to the low computational time, the proposed motion planning scheme is implementable in real-time applications. The performance of the developed motion planning scheme is compared with existing motion planning techniques and validated through experiments using WATonoBus, an electrical all-weather autonomous shuttle bus.
>
---
#### [new 009] FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models
- **分类: cs.RO; cs.AI**

- **简介: 该论文属于机器人任务规划领域，旨在解决长时程任务中自主纠错能力不足的问题。作者提出FCRF框架，通过灵活的自我反思机制，使大语言模型能根据任务难度调整策略，并结合历史经验提升纠错效果。实验表明该方法在模拟和真实环境中均有效提升任务完成性能。**

- **链接: [http://arxiv.org/pdf/2507.14975v1](http://arxiv.org/pdf/2507.14975v1)**

> **作者:** Yufan Song; Jiatao Zhang; Zeng Gu; Qingmiao Liang; Tuocheng Hu; Wei Song; Shiqiang Zhu
>
> **备注:** 8 pages, 6 figures, IROS 2025
>
> **摘要:** Autonomous error correction is critical for domestic robots to achieve reliable execution of complex long-horizon tasks. Prior work has explored self-reflection in Large Language Models (LLMs) for task planning error correction; however, existing methods are constrained by inflexible self-reflection mechanisms that limit their effectiveness. Motivated by these limitations and inspired by human cognitive adaptation, we propose the Flexible Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture that enables LLMs to perform flexible self-reflection based on task difficulty, while constructively integrating historical valuable experience with failure lessons. We evaluated FCRF on diverse domestic tasks through simulation in AlfWorld and physical deployment in the real-world environment. Experimental results demonstrate that FCRF significantly improves overall performance and self-reflection flexibility in complex long-horizon robotic tasks.
>
---
#### [new 010] The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents
- **分类: cs.RO; cs.AI; cs.LG**

- **简介: 该论文属于智能系统与机器人控制任务，旨在解决自主智能体在不确定环境中安全合规行为的问题。作者提出了基于神经符号系统的“宪法控制器”（CoCo）框架，结合符号推理与深度学习，并引入“自我怀疑”机制，以提升智能体在复杂环境中的决策安全性与规则遵循能力。**

- **链接: [http://arxiv.org/pdf/2507.15478v1](http://arxiv.org/pdf/2507.15478v1)**

> **作者:** Simon Kohaut; Felix Divo; Navid Hamid; Benedict Flade; Julian Eggert; Devendra Singh Dhami; Kristian Kersting
>
> **摘要:** Ensuring reliable and rule-compliant behavior of autonomous agents in uncertain environments remains a fundamental challenge in modern robotics. Our work shows how neuro-symbolic systems, which integrate probabilistic, symbolic white-box reasoning models with deep learning methods, offer a powerful solution to this challenge. This enables the simultaneous consideration of explicit rules and neural models trained on noisy data, combining the strength of structured reasoning with flexible representations. To this end, we introduce the Constitutional Controller (CoCo), a novel framework designed to enhance the safety and reliability of agents by reasoning over deep probabilistic logic programs representing constraints such as those found in shared traffic spaces. Furthermore, we propose the concept of self-doubt, implemented as a probability density conditioned on doubt features such as travel velocity, employed sensors, or health factors. In a real-world aerial mobility study, we demonstrate CoCo's advantages for intelligent autonomous systems to learn appropriate doubts and navigate complex and uncertain environments safely and compliantly.
>
---
#### [new 011] VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving
- **分类: cs.RO; cs.SY; eess.SY**

- **简介: 论文提出VLM-UDMC框架，用于城市自动驾驶的统一决策与运动控制。该工作结合视觉语言模型与场景理解，解决复杂城市环境中安全、可解释的驾驶决策问题。通过上层慢速系统进行风险感知推理，并动态调整下层快速系统的运动规划，提升整体驾驶性能。**

- **链接: [http://arxiv.org/pdf/2507.15266v1](http://arxiv.org/pdf/2507.15266v1)**

> **作者:** Haichao Liu; Haoren Guo; Pei Liu; Benshan Ma; Yuxiang Zhang; Jun Ma; Tong Heng Lee
>
> **备注:** 14 pages, 12 figures
>
> **摘要:** Scene understanding and risk-aware attentions are crucial for human drivers to make safe and effective driving decisions. To imitate this cognitive ability in urban autonomous driving while ensuring the transparency and interpretability, we propose a vision-language model (VLM)-enhanced unified decision-making and motion control framework, named VLM-UDMC. This framework incorporates scene reasoning and risk-aware insights into an upper-level slow system, which dynamically reconfigures the optimal motion planning for the downstream fast system. The reconfiguration is based on real-time environmental changes, which are encoded through context-aware potential functions. More specifically, the upper-level slow system employs a two-step reasoning policy with Retrieval-Augmented Generation (RAG), leveraging foundation models to process multimodal inputs and retrieve contextual knowledge, thereby generating risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM provides real-time trajectory predictions for heterogeneous traffic participants by extracting smoother trend representations for short-horizon trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is verified via both simulations and real-world experiments with a full-size autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively leverages scene understanding and attention decomposition for rational driving decisions, thus improving the overall urban driving performance. Our open-source project is available at https://github.com/henryhcliu/vlmudmc.git.
>
---
#### [new 012] Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper
- **分类: cs.RO; cs.AI; cs.LG**

- **简介: 该论文属于机器人操控任务，旨在解决现有手持夹持器缺乏触觉感知的问题。作者设计了一款便携式视觉-触觉夹持器，并提出跨模态学习框架，融合视觉与触觉数据，提升机器人精细操作的准确性与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.15062v1](http://arxiv.org/pdf/2507.15062v1)**

> **作者:** Xinyue Zhu; Binghao Huang; Yunzhu Li
>
> **备注:** More videos can be found on our website:https://binghao-huang.github.io/touch_in_the_wild/
>
> **摘要:** Handheld grippers are increasingly used to collect human demonstrations due to their ease of deployment and versatility. However, most existing designs lack tactile sensing, despite the critical role of tactile feedback in precise manipulation. We present a portable, lightweight gripper with integrated tactile sensors that enables synchronized collection of visual and tactile data in diverse, real-world, and in-the-wild settings. Building on this hardware, we propose a cross-modal representation learning framework that integrates visual and tactile signals while preserving their distinct characteristics. The learning procedure allows the emergence of interpretable representations that consistently focus on contacting regions relevant for physical interactions. When used for downstream manipulation tasks, these representations enable more efficient and effective policy learning, supporting precise robotic manipulation based on multimodal feedback. We validate our approach on fine-grained tasks such as test tube insertion and pipette-based fluid transfer, demonstrating improved accuracy and robustness under external disturbances. Our project page is available at https://binghao-huang.github.io/touch_in_the_wild/ .
>
---
#### [new 013] The Emergence of Deep Reinforcement Learning for Path Planning
- **分类: cs.RO; cs.AI**

- **简介: 该论文属于路径规划任务，旨在解决复杂动态环境中自主系统的导航问题。论文综述了传统方法及深度强化学习（DRL）在路径规划中的应用，比较了各类算法的优势与局限，探讨了DRL与经典方法结合的潜力，为未来研究提供方向。**

- **链接: [http://arxiv.org/pdf/2507.15469v1](http://arxiv.org/pdf/2507.15469v1)**

> **作者:** Thanh Thi Nguyen; Saeid Nahavandi; Imran Razzak; Dung Nguyen; Nhat Truong Pham; Quoc Viet Hung Nguyen
>
> **备注:** Accepted for publication in the Proceedings of the 2025 IEEE International Conference on Systems, Man, and Cybernetics (SMC)
>
> **摘要:** The increasing demand for autonomous systems in complex and dynamic environments has driven significant research into intelligent path planning methodologies. For decades, graph-based search algorithms, linear programming techniques, and evolutionary computation methods have served as foundational approaches in this domain. Recently, deep reinforcement learning (DRL) has emerged as a powerful method for enabling autonomous agents to learn optimal navigation strategies through interaction with their environments. This survey provides a comprehensive overview of traditional approaches as well as the recent advancements in DRL applied to path planning tasks, focusing on autonomous vehicles, drones, and robotic platforms. Key algorithms across both conventional and learning-based paradigms are categorized, with their innovations and practical implementations highlighted. This is followed by a thorough discussion of their respective strengths and limitations in terms of computational efficiency, scalability, adaptability, and robustness. The survey concludes by identifying key open challenges and outlining promising avenues for future research. Special attention is given to hybrid approaches that integrate DRL with classical planning techniques to leverage the benefits of both learning-based adaptability and deterministic reliability, offering promising directions for robust and resilient autonomous navigation.
>
---
#### [new 014] DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models
- **分类: cs.RO**

- **简介: 该论文属于状态估计任务，旨在解决传统粒子滤波在复杂、高维和多模态分布下采样效率低、估计不准的问题。论文提出了DiffPF，首次将条件扩散模型引入可微分粒子滤波，通过生成采样提升后验估计质量，显著提高了状态估计精度。**

- **链接: [http://arxiv.org/pdf/2507.15716v1](http://arxiv.org/pdf/2507.15716v1)**

> **作者:** Ziyu Wan; Lin Zhao
>
> **摘要:** This paper proposes DiffPF, a differentiable particle filter that leverages diffusion models for state estimation in dynamic systems. Unlike conventional differentiable particle filters, which require importance weighting and typically rely on predefined or low-capacity proposal distributions. DiffPF learns a flexible posterior sampler by conditioning a diffusion model on predicted particles and the current observation. This enables accurate, equally-weighted sampling from complex, high-dimensional, and multimodal filtering distributions. We evaluate DiffPF across a range of scenarios, including both unimodal and highly multimodal distributions, and test it on simulated as well as real-world tasks, where it consistently outperforms existing filtering baselines. In particular, DiffPF achieves an 82.8% improvement in estimation accuracy on a highly multimodal global localization benchmark, and a 26% improvement on the real-world KITTI visual odometry benchmark, compared to state-of-the-art differentiable filters. To the best of our knowledge, DiffPF is the first method to integrate conditional diffusion models into particle filtering, enabling high-quality posterior sampling that produces more informative particles and significantly improves state estimation.
>
---
#### [new 015] EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation
- **分类: cs.RO**

- **简介: 该论文属于人形机器人控制任务，旨在解决机器人在站立状态下模仿人类上半身动作时的稳定性问题。作者提出了一种基于强化学习的框架，并设计了可执行运动先验（EMP）模块，调整动作输入以提升稳定性，同时保留动作幅度。通过模拟和实验证明了方法的有效性。**

- **链接: [http://arxiv.org/pdf/2507.15649v1](http://arxiv.org/pdf/2507.15649v1)**

> **作者:** Haocheng Xu; Haodong Zhang; Zhenghan Chen; Rong Xiong
>
> **摘要:** To support humanoid robots in performing manipulation tasks, it is essential to study stable standing while accommodating upper-body motions. However, the limited controllable range of humanoid robots in a standing position affects the stability of the entire body. Thus we introduce a reinforcement learning based framework for humanoid robots to imitate human upper-body motions while maintaining overall stability. Our approach begins with designing a retargeting network that generates a large-scale upper-body motion dataset for training the reinforcement learning (RL) policy, which enables the humanoid robot to track upper-body motion targets, employing domain randomization for enhanced robustness. To avoid exceeding the robot's execution capability and ensure safety and stability, we propose an Executable Motion Prior (EMP) module, which adjusts the input target movements based on the robot's current state. This adjustment improves standing stability while minimizing changes to motion amplitude. We evaluate our framework through simulation and real-world tests, demonstrating its practical applicability.
>
---
#### [new 016] Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks
- **分类: cs.RO; cs.CV**

- **简介: 该论文属于3D人体运动预测任务，旨在解决预测中的不确定性建模问题。现有方法难以量化预测置信度，影响安全关键场景应用。论文提出ProbHMI，利用可逆网络在解耦隐空间中建模姿态，显式预测未来隐分布，实现有效不确定性估计，提升风险感知决策能力。**

- **链接: [http://arxiv.org/pdf/2507.14694v1](http://arxiv.org/pdf/2507.14694v1)**

> **作者:** Yue Ma; Kanglei Zhou; Fuyang Yu; Frederick W. B. Li; Xiaohui Liang
>
> **摘要:** 3D human motion forecasting aims to enable autonomous applications. Estimating uncertainty for each prediction (i.e., confidence based on probability density or quantile) is essential for safety-critical contexts like human-robot collaboration to minimize risks. However, existing diverse motion forecasting approaches struggle with uncertainty quantification due to implicit probabilistic representations hindering uncertainty modeling. We propose ProbHMI, which introduces invertible networks to parameterize poses in a disentangled latent space, enabling probabilistic dynamics modeling. A forecasting module then explicitly predicts future latent distributions, allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI achieves strong performance for both deterministic and diverse prediction while validating uncertainty calibration, critical for risk-aware decision making.
>
---
#### [new 017] Optimizing Force Signals from Human Demonstrations of In-Contact Motions
- **分类: cs.RO**

- **简介: 该论文旨在优化人类示教中的力信号，以更准确反映操作意图。针对非专家用户通过示教输入的信号存在噪声和偏差问题，论文比较了多种滤波方法，并提出峰值检测方法来修正首次接触偏差。通过特定误差准则评估优化效果，结果显示误差可降低20%。该研究提升了机器人编程及人机交互的可用性。**

- **链接: [http://arxiv.org/pdf/2507.15608v1](http://arxiv.org/pdf/2507.15608v1)**

> **作者:** Johannes Hartwig; Fabian Viessmann; Dominik Henrich
>
> **备注:** Accepted for publication in Annals of Scientific Society for Assembly, Handling and Industrial Robotics 2024 (to appear)
>
> **摘要:** For non-robot-programming experts, kinesthetic guiding can be an intuitive input method, as robot programming of in-contact tasks is becoming more prominent. However, imprecise and noisy input signals from human demonstrations pose problems when reproducing motions directly or using the signal as input for machine learning methods. This paper explores optimizing force signals to correspond better to the human intention of the demonstrated signal. We compare different signal filtering methods and propose a peak detection method for dealing with first-contact deviations in the signal. The evaluation of these methods considers a specialized error criterion between the input and the human-intended signal. In addition, we analyze the critical parameters' influence on the filtering methods. The quality for an individual motion could be increased by up to \SI{20}{\percent} concerning the error criterion. The proposed contribution can improve the usability of robot programming and the interaction between humans and robots.
>
---
#### [new 018] KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning
- **分类: cs.RO**

- **简介: 该论文属于机器人抓取任务，旨在解决6自由度抓取估计中的小物体和噪声问题。论文提出KGN-Pro方法，通过概率PnP层实现2D-3D关键点学习，结合RGB-D图像生成关键点图和置信度图，优化重投影误差，实现端到端训练，在模拟和真实场景中均表现出更优的抓取成功率。**

- **链接: [http://arxiv.org/pdf/2507.14820v1](http://arxiv.org/pdf/2507.14820v1)**

> **作者:** Bingran Chen; Baorun Li; Jian Yang; Yong Liu; Guangyao Zhai
>
> **摘要:** High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation to serve as a basic function. Previous approaches either directly generate grasps from point-cloud data, suffering from challenges with small objects and sensor noise, or infer 3D information from RGB images, which introduces expensive annotation requirements and discretization issues. Recent methods mitigate some challenges by retaining a 2D representation to estimate grasp keypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF poses. However, these methods are limited by their non-differentiable nature and reliance solely on 2D supervision, which hinders the full exploitation of rich 3D information. In this work, we present KGN-Pro, a novel grasping network that preserves the efficiency and fine-grained object grasping of previous KGNs while integrating direct 3D optimization through probabilistic PnP layers. KGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further outputs a 2D confidence map to weight keypoint contributions during re-projection error minimization. By modeling the weighted sum of squared re-projection errors probabilistically, the network effectively transmits 3D supervision to its 2D keypoint predictions, enabling end-to-end learning. Experiments on both simulated and real-world platforms demonstrate that KGN-Pro outperforms existing methods in terms of grasp cover rate and success rate.
>
---
#### [new 019] CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2507.15189v1](http://arxiv.org/pdf/2507.15189v1)**

> **作者:** Kevin Christiansen Marsim; Jinwoo Jeon; Yeeun Kim; Myeongwoo Jeong; Hyun Myung
>
> **摘要:** Depth information which specifies the distance between objects and current position of the robot is essential for many robot tasks such as navigation. Recently, researchers have proposed depth completion frameworks to provide dense depth maps that offer comprehensive information about the surrounding environment. However, existing methods show significant trade-offs between computational efficiency and accuracy during inference. The substantial memory and computational requirements make them unsuitable for real-time applications, highlighting the need to improve the completeness and accuracy of depth information while improving processing speed to enhance robot performance in various tasks. To address these challenges, in this paper, we propose CHADET(cross-hierarchical-attention depth-completion transformer), a lightweight depth-completion network that can generate accurate dense depth maps from RGB images and sparse depth points. For each pair, its feature is extracted from the depthwise blocks and passed to the equally lightweight transformer-based decoder. In the decoder, we utilize the novel cross-hierarchical-attention module that refines the image features from the depth information. Our approach improves the quality and reduces memory usage of the depth map prediction, as validated in both KITTI, NYUv2, and VOID datasets.
>
---
#### [new 020] Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs
- **分类: cs.RO**

- **简介: 该论文属于机器人任务规划与人工智能结合的交叉任务，旨在解决大型场景图中多目标收集问题。论文提出Inter-LLM算法，通过融合大语言模型与运动规划，优化长视野下的动作序列，提升家庭机器人在复杂环境中的任务完成率与效率。**

- **链接: [http://arxiv.org/pdf/2507.15782v1](http://arxiv.org/pdf/2507.15782v1)**

> **作者:** Ruochu Yang; Yu Zhou; Fumin Zhang; Mengxue Hou
>
> **摘要:** Household robots have been a longstanding research topic, but they still lack human-like intelligence, particularly in manipulating open-set objects and navigating large environments efficiently and accurately. To push this boundary, we consider a generalized multi-object collection problem in large scene graphs, where the robot needs to pick up and place multiple objects across multiple locations in a long mission of multiple human commands. This problem is extremely challenging since it requires long-horizon planning in a vast action-state space under high uncertainties. To this end, we propose a novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a multimodal action cost similarity function, our algorithm can both reflect the history and look into the future to optimize plans, striking a good balance of quality and efficiency. Simulation experiments demonstrate that compared with latest works, our algorithm improves the overall mission performance by 30% in terms of fulfilling human commands, maximizing mission success rates, and minimizing mission costs.
>
---
#### [new 021] A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning
- **分类: cs.RO**

- **简介: 论文研究自动驾驶车辆-挂车系统的导航问题，旨在解决不同挂车类型与负载条件下的建模与控制难题。作者提出融合神经网络与在线残差学习的混合运动学模型，并结合模型预测控制实现精确导航。实验验证了方法在多种挂车上的通用性与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.15607v1](http://arxiv.org/pdf/2507.15607v1)**

> **作者:** Yanbo Chen; Yunzhe Tan; Yaojia Wang; Zhengzhe Xu; Junbo Tan; Xueqian Wang
>
> **备注:** 8 pages, 10 figures
>
> **摘要:** Autonomous navigation of vehicle-trailer systems is crucial in environments like airports, supermarkets, and concert venues, where various types of trailers are needed to navigate with different payloads and conditions. However, accurately modeling such systems remains challenging, especially for trailers with castor wheels. In this work, we propose a novel universal vehicle-trailer navigation system that integrates a hybrid nominal kinematic model--combining classical nonholonomic constraints for vehicles and neural network-based trailer kinematics--with a lightweight online residual learning module to correct real-time modeling discrepancies and disturbances. Additionally, we develop a model predictive control framework with a weighted model combination strategy that improves long-horizon prediction accuracy and ensures safer motion planning. Our approach is validated through extensive real-world experiments involving multiple trailer types and varying payload conditions, demonstrating robust performance without manual tuning or trailer-specific calibration.
>
---
#### [new 022] Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition
- **分类: cs.RO**

- **简介: 该论文属于机器人控制任务，旨在解决四足机器人在线最优控制问题。通过Koopman算子理论和EDMD方法，建立包含非线性运动特性的线性模型，实现跳跃、奔跑及步态转换的实时控制，提升复杂地形下的运动性能。**

- **链接: [http://arxiv.org/pdf/2507.14605v1](http://arxiv.org/pdf/2507.14605v1)**

> **作者:** Chun-Ming Yang; Pranav A. Bhounsule
>
> **摘要:** Online optimal control of quadrupedal robots would enable them to plan their movement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged as a practical approach for real-time control. In LMPC, an optimization problem with a quadratic cost and linear constraints is formulated over a finite horizon and solved on the fly. However, LMPC relies on linearizing the equations of motion (EOM), which may lead to poor solution quality. In this paper, we use Koopman operator theory and the Extended Dynamic Mode Decomposition (EDMD) to create a linear model of the system in high dimensional space, thus retaining the nonlinearity of the EOM. We model the aerial phase and ground contact phases using different linear models. Then, using LMPC, we demonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait transitions in level and rough terrains. The main novelty is the use of Koopman operator theory to create hybrid models of a quadrupedal system and demonstrate the online generation of multiple gaits and gaits transitions.
>
---
#### [new 023] One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner
- **分类: cs.RO; cs.AI**

- **简介: 该论文属于芯片物理设计任务，旨在解决传统布局方法与后续设计阶段脱节导致的模块内元件放置不佳和模块间馈通过多问题。论文提出Flora，通过三阶段方法优化馈通和元件放置，实现更优的芯片布局效果。**

- **链接: [http://arxiv.org/pdf/2507.14914v1](http://arxiv.org/pdf/2507.14914v1)**

> **作者:** Zhexuan Xu; Jie Wang; Siyuan Xu; Zijie Geng; Mingxuan Yuan; Feng Wu
>
> **摘要:** Floorplanning determines the shapes and locations of modules on a chip canvas and plays a critical role in optimizing the chip's Power, Performance, and Area (PPA) metrics. However, existing floorplanning approaches often fail to integrate with subsequent physical design stages, leading to suboptimal in-module component placement and excessive inter-module feedthrough. To tackle this challenge, we propose Flora, a three-stage feedthrough and placement aware rectilinear floorplanner. In the first stage, Flora employs wiremask and position mask techniques to achieve coarse-grained optimization of HPWL and feedthrough. In the second stage, under the constraint of a fixed outline, Flora achieves a zero-whitespace layout by locally resizing module shapes, thereby performing fine-grained optimization of feedthrough and improving component placement. In the third stage, Flora utilizes a fast tree search-based method to efficiently place components-including macros and standard cells-within each module, subsequently adjusting module boundaries based on the placement results to enable cross-stage optimization. Experimental results show that Flora outperforms recent state-of-the-art floorplanning approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin, 29.15% in FTmod, and a 14% improvement in component placement performance.
>
---
#### [new 024] Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers
- **分类: cs.RO; cs.AI; cs.CV**

- **简介: 该论文属于机器人视觉任务，旨在解决机器人视觉处理效率低、注意力不集中问题。受人类主动视觉启发，作者提出利用注视信息与渐进式视觉Transformer，构建主动视觉机器人系统，提升处理效率与精度。**

- **链接: [http://arxiv.org/pdf/2507.15833v1](http://arxiv.org/pdf/2507.15833v1)**

> **作者:** Ian Chuang; Andrew Lee; Dechen Gao; Jinyu Zou; Iman Soltani
>
> **备注:** 13 pages, 10 figures
>
> **摘要:** Human vision is a highly active process driven by gaze, which directs attention and fixation to task-relevant regions and dramatically reduces visual processing. In contrast, robot learning systems typically rely on passive, uniform processing of raw camera images. In this work, we explore how incorporating human-like active gaze into robotic policies can enhance both efficiency and performance. We build on recent advances in foveated image processing and apply them to an Active Vision robot system that emulates both human head movement and eye tracking. Extending prior work on the AV-ALOHA robot simulation platform, we introduce a framework for simultaneously collecting eye-tracking data and robot demonstrations from a human operator as well as a simulation benchmark and dataset for training robot policies that incorporate human gaze. Given the widespread use of Vision Transformers (ViTs) in robot learning, we integrate gaze information into ViTs using a foveated patch tokenization scheme inspired by recent work in image segmentation. Compared to uniform patch tokenization, this significantly reduces the number of tokens-and thus computation-without sacrificing visual fidelity near regions of interest. We also explore two approaches to gaze imitation and prediction from human data. The first is a two-stage model that predicts gaze to guide foveation and action; the second integrates gaze into the action space, allowing the policy to jointly predict gaze and actions end-to-end. Our results show that our method for foveated robot vision not only drastically reduces computational overhead, but also improves performance for high precision tasks and robustness to unseen distractors. Together, these findings suggest that human-inspired visual processing offers a useful inductive bias for robotic vision systems. https://ian-chuang.github.io/gaze-av-aloha/
>
---
#### [new 025] RepILN: Reparameterized Inertial Localization Network
- **分类: cs.RO**

- **简介: 论文提出RepILN，一种面向物联网设备的惯性定位网络。针对惯性定位中模型复杂、难以捕捉长期依赖的问题，设计多分支训练结构和单路径推理结构，并引入时间稀疏注意力与门控卷积单元，提升精度与参数效率。在公共数据集上验证了方法的有效性。**

- **链接: [http://arxiv.org/pdf/2507.15293v1](http://arxiv.org/pdf/2507.15293v1)**

> **作者:** Shanshan Zhang; Tianshui Wen; Siyue Wang; Qi Zhang; Ziheng Zhou; Lingxiang Zheng; Yu Yang
>
> **摘要:** Inertial localization is regarded as a promising positioning solution for consumer-grade IoT devices due to its cost-effectiveness and independence from external infrastructure. However, data-driven inertial localization methods often rely on increasingly complex network architectures to improve accuracy, which challenges the limited computational resources of IoT devices. Moreover, these methods frequently overlook the importance of modeling long-term dependencies in inertial measurements - a critical factor for accurate trajectory reconstruction - thereby limiting localization performance. To address these challenges, we propose a reparameterized inertial localization network that uses a multi-branch structure during training to enhance feature extraction. At inference time, this structure is transformed into an equivalent single-path architecture to improve parameter efficiency. To further capture long-term dependencies in motion trajectories, we introduce a temporal-scale sparse attention mechanism that selectively emphasizes key trajectory segments while suppressing noise. Additionally, a gated convolutional unit is incorporated to effectively integrate long-range dependencies with local fine-grained features. Extensive experiments on public benchmarks demonstrate that our method achieves a favorable trade-off between accuracy and model compactness. For example, on the RoNIN dataset, our approach reduces the Absolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while reducing the number of parameters by 3.86%.
>
---
#### [new 026] CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions
- **分类: cs.RO; cs.SY; eess.SY**

- **简介: 该论文属于安全控制任务，旨在解决神经控制屏障函数（NCBFs）的安全性验证问题。现有方法保守性强，验证不准确。作者提出CPED-NCBFs方法，基于分裂共形预测验证NCBF，提升安全性保障，并在点质量系统和单车模型上验证了效果。**

- **链接: [http://arxiv.org/pdf/2507.15022v1](http://arxiv.org/pdf/2507.15022v1)**

> **作者:** Sumeadh MS; Kevin Dsouza; Ravi Prakash
>
> **备注:** 6pages, 4figures, Submitted to the prestigious Indian Control Conference (ICC), 2025
>
> **摘要:** Among the promising approaches to enforce safety in control systems, learning Control Barrier Functions (CBFs) from expert demonstrations has emerged as an effective strategy. However, a critical challenge remains: verifying that the learned CBFs truly enforce safety across the entire state space. This is especially difficult when CBF is represented using neural networks (NCBFs). Several existing verification techniques attempt to address this problem including SMT-based solvers, mixed-integer programming (MIP), and interval or bound-propagation methods but these approaches often introduce loose, conservative bounds. To overcome these limitations, in this work we use CPED-NCBFs a split-conformal prediction based verification strategy to verify the learned NCBF from the expert demonstrations. We further validate our method on point mass systems and unicycle models to demonstrate the effectiveness of the proposed theory.
>
---
#### [new 027] Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls
- **分类: cs.RO**

- **简介: 该论文属于机器人抓取任务，旨在解决因环境遮挡导致物体难以被抓取的问题。通过结合外在灵巧操作与层次强化学习框架，使机器人能选择合适动作并精确定位执行。利用Q-learning训练高层策略，低层技能通过CVAE生成具体动作，并采用领域随机化提升泛化能力，实现在真实场景中的有效抓取。**

- **链接: [http://arxiv.org/pdf/2507.14721v1](http://arxiv.org/pdf/2507.14721v1)**

> **作者:** Keita Kobashi; Masayoshi Tomizuka
>
> **备注:** 7 pages, 7 figures
>
> **摘要:** This study addresses the problem of occluded grasping, where primary grasp configurations of an object are not available due to occlusion with environment. Simple parallel grippers often struggle with such tasks due to limited dexterity and actuation constraints. Prior works have explored object pose reorientation such as pivoting by utilizing extrinsic contacts between an object and an environment feature like a wall, to make the object graspable. However, such works often assume the presence of a short wall, and this assumption may not always hold in real-world scenarios. If the wall available for interaction is too large or too tall, the robot may still fail to grasp the object even after pivoting, and the robot must combine different types of actions to grasp. To address this, we propose a hierarchical reinforcement learning (RL) framework. We use Q-learning to train a high-level policy that selects the type of action expected to yield the highest reward. The selected low-level skill then samples a specific robot action in continuous space. To guide the robot to an appropriate location for executing the selected action, we adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on the object point cloud and the skill ID, enabling it to infer a suitable location based on the object geometry and the selected skill. To promote generalization, we apply domain randomization during the training of low-level skills. The RL policy is trained entirely in simulation with a box-like object and deployed to six objects in real world. We conduct experiments to evaluate our method and demonstrate both its generalizability and robust sim-to-real transfer performance with promising success rates.
>
---
#### [new 028] Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions
- **分类: cs.RO**

- **简介: 该论文属于医学机器人任务，旨在解决磁控软体吸管设备在微创神经外科手术中的形状建模问题。作者提出一种基于机器学习的建模方法，使用随机森林和神经网络对设备形变进行高精度预测，实现亚毫米级实时形状重建，提升手术中对软体机器人控制的准确性。**

- **链接: [http://arxiv.org/pdf/2507.15155v1](http://arxiv.org/pdf/2507.15155v1)**

> **作者:** Majid Roshanfar; Alex Zhang; Changyan He; Amir Hooshiar; Dale J. Podolsky; Thomas Looi; Eric Diller
>
> **摘要:** This letter introduces a novel learning-based modeling framework for a magnetically steerable soft suction device designed for endoscopic endonasal brain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm inner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material, and integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape feedback. Shape reconstruction is represented using four Bezier control points, enabling a compact and smooth model of the device's deformation. A data-driven model was trained on 5,097 experimental samples covering a range of magnetic field magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical tip distances (90-100 mm), using both Neural Network (NN) and Random Forest (RF) architectures. The RF model outperformed the NN across all metrics, achieving a mean root mean square error of 0.087 mm in control point prediction and a mean shape reconstruction error of 0.064 mm. Feature importance analysis further revealed that magnetic field components predominantly influence distal control points, while frequency and distance affect the base configuration. This learning-based approach effectively models the complex nonlinear behavior of hyperelastic soft robots under magnetic actuation without relying on simplified physical assumptions. By enabling sub-millimeter shape prediction accuracy and real-time inference, this work represents an advancement toward the intelligent control of magnetically actuated soft robotic tools in minimally invasive neurosurgery.
>
---
#### [new 029] Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry
- **分类: cs.RO**

- **简介: 该论文旨在通过协作设计（co-design）开发一款陪伴机器人，帮助监测和调节精神病患者的应激反应，同时记录其行为以用于长期干预。研究在法医精神病诊所开展，通过四次协作设计工作坊，邀请患者、护理人员和治疗师参与设计过程，强调患者赋权与伦理考量。论文任务是探索如何与患者共同设计而非为患者设计，解决其在高压环境中缺乏自主性与控制感的问题。**

- **链接: [http://arxiv.org/pdf/2507.14931v1](http://arxiv.org/pdf/2507.14931v1)**

> **作者:** Qiaoqiao Ren; Remko Proesmans; Arend Pissens; Lara Dehandschutter; William Denecker; Lotte Rouckhout; Joke Carrette; Peter Vanhopplinus; Tony Belpaeme; Francis wyffels
>
> **摘要:** Forensic mental health care involves the treatment of individuals with severe mental disorders who have committed violent offences. These settings are often characterized by high levels of bureaucracy, risk avoidance, and restricted autonomy. Patients frequently experience a profound loss of control over their lives, leading to heightened psychological stress-sometimes resulting in isolation as a safety measure. In this study, we explore how co-design can be used to collaboratively develop a companion robot that helps monitor and regulate stress while maintaining tracking of the patients' interaction behaviours for long-term intervention. We conducted four co-design workshops in a forensic psychiatric clinic with patients, caregivers, and therapists. Our process began with the presentation of an initial speculative prototype to therapists, enabling reflection on shared concerns, ethical risks, and desirable features. This was followed by a creative ideation session with patients, a third workshop focused on defining desired functions and emotional responses, and we are planning a final prototype demo to gather direct patient feedback. Our findings emphasize the importance of empowering patients in the design process and adapting proposals based on their current emotional state. The goal was to empower the patient in the design process and ensure each patient's voice was heard.
>
---
#### [new 030] Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation
- **分类: cs.RO; cs.SY; eess.SY**

- **简介: 该论文属于移动机器人安全导航任务，旨在解决未知密集环境中导航的安全性与可行性问题。论文结合控制李雅普诺夫函数（CLF）与控制屏障函数（CBF），提出一种基于模型预测轮廓控制（MPCC）的新框架，并通过软演员-评论家（SAC）策略动态调整CBF参数，以增强系统适应性和安全性。**

- **链接: [http://arxiv.org/pdf/2507.14700v1](http://arxiv.org/pdf/2507.14700v1)**

> **作者:** Nicholas Mohammad; Nicola Bezzo
>
> **备注:** To be presented in the 64th IEEE Conference on Decision and Control (CDC 25)
>
> **摘要:** Safe navigation in unknown and cluttered environments remains a challenging problem in robotics. Model Predictive Contour Control (MPCC) has shown promise for performant obstacle avoidance by enabling precise and agile trajectory tracking, however, existing methods lack formal safety assurances. To address this issue, we propose a general Control Lyapunov Function (CLF) and Control Barrier Function (CBF) enabled MPCC framework that enforces safety constraints derived from a free-space corridor around the planned trajectory. To enhance feasibility, we dynamically adapt the CBF parameters at runtime using a Soft Actor-Critic (SAC) policy. The approach is validated with extensive simulations and an experiment on mobile robot navigation in unknown cluttered environments.
>
---
#### [new 031] Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach
- **分类: cs.RO; cs.AI**

- **简介: 该论文属于城市空中交通路径规划任务，旨在解决实时通信感知与乘客需求动态变化下的路径规划问题。作者提出构建无线电图评估通信质量，并设计多源混合注意力强化学习框架（MSHA-RL），实现高效、安全的空中出租车共享路径规划。**

- **链接: [http://arxiv.org/pdf/2507.14249v1](http://arxiv.org/pdf/2507.14249v1)**

> **作者:** Yuejiao Xie; Maonan Wang; Di Zhou; Man-On Pun; Zhu Han
>
> **摘要:** Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions to alleviate urban congestion, with path planning becoming a key focus area. Unlike ground transportation, UAM trajectory planning has to prioritize communication quality for accurate location tracking in constantly changing environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi, requires adaptive planning to respond to real-time passenger requests, especially in ride-sharing scenarios where passenger demands are unpredictable and dynamic. However, conventional trajectory planning strategies based on predefined routes lack the flexibility to meet varied passenger ride demands. To address these challenges, this work first proposes constructing a radio map to evaluate the communication quality of urban airspace. Building on this, we introduce a novel Multi-Source Hybrid Attention Reinforcement Learning (MSHA-RL) framework for the challenge of effectively focusing on passengers and UAM locations, which arises from the significant dimensional disparity between the representations. This model first generates the alignment among diverse data sources with large gap dimensions before employing hybrid attention to balance global and local insights, thereby facilitating responsive, real-time path planning. Extensive experimental results demonstrate that the approach enables communication-compliant trajectory planning, reducing travel time and enhancing operational efficiency while prioritizing passenger safety.
>
---
#### [new 032] CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning
- **分类: cs.RO; I.2.9; I.2.10; I.2.11**

- **简介: 论文提出CDGMP框架，结合决策与运动规划，解决自动驾驶中车道选择与轨迹控制问题。采用多策略强化学习与专家混合架构，提升灵活性与安全性，适应复杂交通场景。**

- **链接: [http://arxiv.org/pdf/2507.14903v1](http://arxiv.org/pdf/2507.14903v1)**

> **作者:** Pan Hu
>
> **备注:** 8 pages, 5 figures
>
> **摘要:** Autonomous driving demands reliable and efficient solutions to closely related problems such as decision-making and motion planning. In this work, decision-making refers specifically to highway lane selection, while motion planning involves generating control commands (such as speed and steering) to reach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs), achieving both flexible and safe lane selection alongside precise trajectory execution remains a significant challenge. This paper proposes a framework called Cohesive Decision-Guided Motion Planning (CDGMP), which tightly integrates decision-making and motion planning using a Mixture of Experts (MoE) inspired architecture combined with multi-policy reinforcement learning. By coordinating multiple specialized sub-networks through a gating mechanism, the method decomposes the complex driving task into modular components. Each sub-network focuses on a specific aspect of driving, improving efficiency by activating only the most relevant modules during inference. This design also enhances safety through modular specialization. CDGMP improves the adaptability and robustness of CAVs across diverse traffic scenarios, offering a scalable solution to real-world autonomy challenges. The architectural principles behind CDGMP, especially the use of MoE, also provide a strong foundation for other high-dimensional decision and control tasks. Simulation results (available at https://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane selection and motion planning.
>
---
#### [new 033] GR-3 Technical Report
- **分类: cs.RO; cs.AI; cs.CV**

- **简介: 该论文提出GR-3，一种大规模视觉-语言-动作模型，旨在构建通用机器人策略。它通过多阶段训练实现对新任务的快速适应，具备处理复杂操作和移动任务的能力。实验表明其性能优于现有方法，结合ByteMini机器人可实现多样化任务执行。**

- **链接: [http://arxiv.org/pdf/2507.15493v1](http://arxiv.org/pdf/2507.15493v1)**

> **作者:** Chilam Cheang; Sijin Chen; Zhongren Cui; Yingdong Hu; Liqun Huang; Tao Kong; Hang Li; Yifeng Li; Yuxiao Liu; Xiao Ma; Hao Niu; Wenxuan Ou; Wanli Peng; Zeyu Ren; Haixin Shi; Jiawen Tian; Hongtao Wu; Xin Xiao; Yuyang Xiao; Jiafeng Xu; Yichu Yang
>
> **备注:** Tech report. Authors are listed in alphabetical order. Project page: https://seed.bytedance.com/GR3/
>
> **摘要:** We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life.
>
---
#### [new 034] Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly
- **分类: cs.RO**

- **简介: 论文提出了一种基于数字孪生和扩展现实的遥操作系统，用于电动汽车电池的安全拆解与分类。该工作旨在解决人工拆解中的安全隐患问题，通过结合遥操作与自动化，提升拆解效率与适应性，并实现未来自动化序列的创建。**

- **链接: [http://arxiv.org/pdf/2507.14929v1](http://arxiv.org/pdf/2507.14929v1)**

> **作者:** Tero Kaarlela; Sami Salo; Jose Outeiro
>
> **摘要:** Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a sustainable transition to electric vehicles by enabling a closed-loop supply chain. Currently, the manual disassembly process exposes workers to hazards, including electrocution and toxic chemicals. We propose a teleoperated system for the safe disassembly and sorting of EVBs. A human-in-the-loop can create and save disassembly sequences for unknown EVB types, enabling future automation. An RGB camera aligns the physical and digital twins of the EVB, and the digital twin of the robot is based on the Robot Operating System (ROS) middleware. This hybrid approach combines teleoperation and automation to improve safety, adaptability, and efficiency in EVB disassembly and sorting. The economic contribution is realized by reducing labor dependency and increasing throughput in battery recycling. An online pilot study was set up to evaluate the usability of the presented approach, and the results demonstrate the potential as a user-friendly solution.
>
---
#### [new 035] Heterogeneous object manipulation on nonlinear soft surface through linear controller
- **分类: cs.RO**

- **简介: 该论文属于软体机器人操作任务，旨在解决高自由度控制复杂性问题。提出了一种基于几何变换的PID控制器，无需大量训练即可实现对不同物体的精确操作，适用于现实应用。**

- **链接: [http://arxiv.org/pdf/2507.14967v1](http://arxiv.org/pdf/2507.14967v1)**

> **作者:** Pratik Ingle; Kasper Støy; Andres Faiña
>
> **备注:** 8 pages, 3 figures
>
> **摘要:** Manipulation surfaces indirectly control and reposition objects by actively modifying their shape or properties rather than directly gripping objects. These surfaces, equipped with dense actuator arrays, generate dynamic deformations. However, a high-density actuator array introduces considerable complexity due to increased degrees of freedom (DOF), complicating control tasks. High DOF restrict the implementation and utilization of manipulation surfaces in real-world applications as the maintenance and control of such systems exponentially increase with array/surface size. Learning-based control approaches may ease the control complexity, but they require extensive training samples and struggle to generalize for heterogeneous objects. In this study, we introduce a simple, precise and robust PID-based linear close-loop feedback control strategy for heterogeneous object manipulation on MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation density). Our approach employs a geometric transformation-driven PID controller, directly mapping tilt angle control outputs(1D/2D) to actuator commands to eliminate the need for extensive black-box training. We validate the proposed method through simulations and experiments on a physical system, successfully manipulating objects with diverse geometries, weights and textures, including fragile objects like eggs and apples. The outcomes demonstrate that our approach is highly generalized and offers a practical and reliable solution for object manipulation on soft robotic manipulation, facilitating real-world implementation without prohibitive training demands.
>
---
#### [new 036] BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives
- **分类: cs.RO**

- **简介: 该论文属于机器人任务与运动规划（TAMP）任务，旨在解决机器人在复杂、长周期任务中难以泛化和适应新环境的问题。作者提出BT-TL-DMPs框架，结合行为树、时序逻辑和动态运动基元，实现任务结构的高层决策与底层运动控制的统一，优化运动基元以满足时空约束并保持动力学特性。通过仿真和真实实验验证了方法的有效性。**

- **链接: [http://arxiv.org/pdf/2507.14582v1](http://arxiv.org/pdf/2507.14582v1)**

> **作者:** Zezhi Liu; Shizhen Wu; Hanqian Luo; Deyun Qin; Yongchun Fang
>
> **备注:** 11 pages, 8 figures
>
> **摘要:** In the field of Learning from Demonstration (LfD), enabling robots to generalize learned manipulation skills to novel scenarios for long-horizon tasks remains challenging. Specifically, it is still difficult for robots to adapt the learned skills to new environments with different task and motion requirements, especially in long-horizon, multi-stage scenarios with intricate constraints. This paper proposes a novel hierarchical framework, called BT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and Dynamical Movement Primitives (DMPs) to address this problem. Within this framework, Signal Temporal Logic (STL) is employed to formally specify complex, long-horizon task requirements and constraints. These STL specifications are systematically transformed to generate reactive and modular BTs for high-level decision-making task structure. An STL-constrained DMP optimization method is proposed to optimize the DMP forcing term, allowing the learned motion primitives to adapt flexibly while satisfying intricate spatiotemporal requirements and, crucially, preserving the essential dynamics learned from demonstrations. The framework is validated through simulations demonstrating generalization capabilities under various STL constraints and real-world experiments on several long-horizon robotic manipulation tasks. The results demonstrate that the proposed framework effectively bridges the symbolic-motion gap, enabling more reliable and generalizable autonomous manipulation for complex robotic tasks.
>
---
#### [new 037] X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots
- **分类: cs.RO**

- **简介: 该论文属于机器人导航任务，旨在解决不同机器人形态下的导航泛化问题。作者提出了X-Nav框架，通过深度强化学习与知识蒸馏，训练出可跨形态迁移的通用导航策略，并在仿真与真实环境中验证了其有效性。**

- **链接: [http://arxiv.org/pdf/2507.14731v1](http://arxiv.org/pdf/2507.14731v1)**

> **作者:** Haitong Wang; Aaron Hao Tan; Angus Fung; Goldie Nejat
>
> **摘要:** Existing navigation methods are primarily designed for specific robot embodiments, limiting their generalizability across diverse robot platforms. In this paper, we introduce X-Nav, a novel framework for end-to-end cross-embodiment navigation where a single unified policy can be deployed across various embodiments for both wheeled and quadrupedal robots. X-Nav consists of two learning stages: 1) multiple expert policies are trained using deep reinforcement learning with privileged observations on a wide range of randomly generated robot embodiments; and 2) a single general policy is distilled from the expert policies via navigation action chunking with transformer (Nav-ACT). The general policy directly maps visual and proprioceptive observations to low-level control commands, enabling generalization to novel robot embodiments. Simulated experiments demonstrated that X-Nav achieved zero-shot transfer to both unseen embodiments and photorealistic environments. A scalability study showed that the performance of X-Nav improves when trained with an increasing number of randomly generated embodiments. An ablation study confirmed the design choices of X-Nav. Furthermore, real-world experiments were conducted to validate the generalizability of X-Nav in real-world environments.
>
---
#### [new 038] Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding
- **分类: cs.RO**

- **简介: 该论文属于机器人编程任务，旨在解决非专业用户在使用协作机器人时需手动标定负载惯性参数的问题。通过手引导示教，利用非接触阶段的运动数据估算负载的质量、质心和惯性张量，实验表明该方法可行，但需足够加速度以提高精度。**

- **链接: [http://arxiv.org/pdf/2507.15604v1](http://arxiv.org/pdf/2507.15604v1)**

> **作者:** Johannes Hartwig; Philipp Lienhardt; Dominik Henrich
>
> **备注:** Accepted for publication in Annals of Scientific Society for Assembly, Handling and Industrial Robotics 2025 (to appear)
>
> **摘要:** As the availability of cobots increases, it is essential to address the needs of users with little to no programming knowledge to operate such systems efficiently. Programming concepts often use intuitive interaction modalities, such as hand guiding, to address this. When programming in-contact motions, such frameworks require knowledge of the robot tool's payload inertial parameters (PIP) in addition to the demonstrated velocities and forces to ensure effective hybrid motion-force control. This paper aims to enable non-expert users to program in-contact motions more efficiently by eliminating the need for a dedicated PIP calibration, thereby enabling flexible robot tool changes. Since demonstrated tasks generally also contain motions with non-contact, our approach uses these parts to estimate the robot's PIP using established estimation techniques. The results show that the estimation of the payload's mass is accurate, whereas the center of mass and the inertia tensor are affected by noise and a lack of excitation. Overall, these findings show the feasibility of PIP estimation during hand guiding but also highlight the need for sufficient payload accelerations for an accurate estimation.
>
---
#### [new 039] Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages
- **分类: cs.RO**

- **简介: 该论文属于机器人运动规划任务，旨在解决高维空间中狭窄通道的规划效率低问题。现有方法依赖手工设计或训练的启发式策略，泛化性差或需要大量训练。论文提出一种多分辨率在线采样方法，在稀疏与密集采样间自适应切换，提高复杂环境中的规划速度与完整性。实验验证了其在多种高维空间及实际机器人上的优越性。**

- **链接: [http://arxiv.org/pdf/2507.15710v1](http://arxiv.org/pdf/2507.15710v1)**

> **作者:** Lu Huang; Lingxiao Meng; Jiankun Wang; Xingjian Jing
>
> **摘要:** Sampling-based algorithms are widely used for motion planning in high-dimensional configuration spaces. However, due to low sampling efficiency, their performance often diminishes in complex configuration spaces with narrow corridors. Existing approaches address this issue using handcrafted or learned heuristics to guide sampling toward useful regions. Unfortunately, these strategies often lack generalizability to various problems or require extensive prior training. In this paper, we propose a simple yet efficient sampling-based planning framework along with its bidirectional version that overcomes these issues by integrating different levels of planning granularity. Our approach probes configuration spaces with uniform random samples at varying resolutions and explores these multi-resolution samples online with a bias towards sparse samples when traveling large free configuration spaces. By seamlessly transitioning between sparse and dense samples, our approach can navigate complex configuration spaces while maintaining planning speed and completeness. The simulation results demonstrate that our approach outperforms several state-of-the-art sampling-based planners in $\mathbb{SE}(2)$, $\mathbb{SE}(3)$, and $\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments conducted with the Franka Emika Panda robot operating in a constrained workspace provide additional evidence of the superiority of the proposed method.
>
---
#### [new 040] Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms
- **分类: cs.RO**

- **简介: 该论文属于机器人控制任务，旨在解决柔性缆索驱动机械臂（FCRA）因电缆特性导致的建模与控制难题。作者提出了一种基于输入-输出数据的模型预测控制（MPC）方法，并引入数据选择算法（DSA）以提升计算效率。实验验证了该方法在定位和轨迹跟踪上的高精度表现。**

- **链接: [http://arxiv.org/pdf/2507.15677v1](http://arxiv.org/pdf/2507.15677v1)**

> **作者:** Huayue Liang; Yanbo Chen; Hongyang Cheng; Yanzhao Yu; Shoujie Li; Junbo Tan; Xueqian Wang; Long Zeng
>
> **摘要:** Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant motion. Still, the inherent properties of cables, such as resilience, hysteresis, and friction, often lead to particular difficulties in modeling and control. This paper proposes a model predictive control (MPC) method that relies exclusively on input-output data, without a physical model, to improve the control accuracy of FCRAs. First, we develop an implicit model based on input-output data and integrate it into an MPC optimization framework. Second, a data selection algorithm (DSA) is introduced to filter the data that best characterize the system, thereby reducing the solution time per step to approximately 4 ms, which is an improvement of nearly 80%. Lastly, the influence of hyperparameters on tracking error is investigated through simulation. The proposed method has been validated on a real FCRA platform, including five-point positioning accuracy tests, a five-point response tracking test, and trajectory tracking for letter drawing. The results demonstrate that the average positioning accuracy is approximately 2.070 mm. Moreover, compared to the PID method with an average tracking error of 1.418{\deg}, the proposed method achieves an average tracking error of 0.541{\deg}.
>
---
#### [new 041] Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe
- **分类: cs.RO; cs.CV**

- **简介: 该论文属于无人机控制任务，旨在解决四旋翼飞行器在狭窄管道内悬停时受气流扰动不稳定的问题。工作包括：提出低延迟烟雾测速方法，结合神经网络估计气流扰动，并通过强化学习控制器实现实时反馈控制，有效应对管道内复杂气流，首次实现闭环控制飞行。**

- **链接: [http://arxiv.org/pdf/2507.15444v1](http://arxiv.org/pdf/2507.15444v1)**

> **作者:** Leonard Bauersfeld; Davide Scaramuzza
>
> **备注:** 17 pages
>
> **摘要:** Autonomous quadrotor flight in confined spaces such as pipes and tunnels presents significant challenges due to unsteady, self-induced aerodynamic disturbances. Very recent advances have enabled flight in such conditions, but they either rely on constant motion through the pipe to mitigate airflow recirculation effects or suffer from limited stability during hovering. In this work, we present the first closed-loop control system for quadrotors for hovering in narrow pipes that leverages real-time flow field measurements. We develop a low-latency, event-based smoke velocimetry method that estimates local airflow at high temporal resolution. This flow information is used by a disturbance estimator based on a recurrent convolutional neural network, which infers force and torque disturbances in real time. The estimated disturbances are integrated into a learning-based controller trained via reinforcement learning. The flow-feedback control proves particularly effective during lateral translation maneuvers in the pipe cross-section. There, the real-time disturbance information enables the controller to effectively counteract transient aerodynamic effects, thereby preventing collisions with the pipe wall. To the best of our knowledge, this work represents the first demonstration of an aerial robot with closed-loop control informed by real-time flow field measurements. This opens new directions for research on flight in aerodynamically complex environments. In addition, our work also sheds light on the characteristic flow structures that emerge during flight in narrow, circular pipes, providing new insights at the intersection of robotics and fluid dynamics.
>
---
#### [new 042] Robots for Kiwifruit Harvesting and Pollination
- **分类: cs.RO**

- **简介: 该论文属于农业机器人任务，旨在解决猕猴桃果园中自动化授粉与采摘的问题。研究设计了高效的猕猴桃采摘机制和定点授粉系统，并利用3D激光雷达和计算机视觉实现机器人在复杂果园环境中的导航与路径跟随，提升了作业效率与自主性。**

- **链接: [http://arxiv.org/pdf/2507.15484v1](http://arxiv.org/pdf/2507.15484v1)**

> **作者:** Jamie Bell
>
> **摘要:** This research was a part of a project that developed mobile robots that performed targeted pollen spraying and automated harvesting in pergola structured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were designed and field testing of one of the concepts showed that the mechanism could reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism was able to reach over 80 percent of fruit in the cluttered kiwifruit canopy, whereas the previous state of the art mechanism was only able to reach less than 70 percent of the fruit. Artificial pollination was performed by detecting flowers and then spraying pollen in solution onto the detected flowers from a line of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the height of the canopy was measured and the spray boom was moved up and down to keep the boom close enough to the flowers for the spray to reach the flowers, while minimising collisions with the canopy. Mobile robot navigation was performed using a 2D lidar in apple orchards and vineyards. Lidar navigation in kiwifruit orchards was more challenging because the pergola structure only provides a small amount of data for the direction of rows, compared to the amount of data from the overhead canopy, the undulating ground and other objects in the orchards. Multiple methods are presented here for extracting structure defining features from 3D lidar data in kiwifruit orchards. In addition, a 3D lidar navigation system -- which performed row following, row end detection and row end turns -- was tested for over 30 km of autonomous driving in kiwifruit orchards. Computer vision algorithms for row detection and row following were also tested. The computer vision algorithm worked as well as the 3D lidar row following method in testing.
>
---
#### [new 043] CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions
- **分类: cs.RO**

- **简介: 该论文提出CLEVER系统，属于流式主动学习任务，旨在解决数据流中基于深度神经网络的语义感知鲁棒性问题。通过在遇到失败时寻求人类帮助，并在线根据人类指令调整模型，结合贝叶斯方法融合先验知识，提升实际中语义感知系统的鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.15499v1](http://arxiv.org/pdf/2507.15499v1)**

> **作者:** Jongseok Lee; Timo Birr; Rudolph Triebel; Tamim Asfour
>
> **备注:** 8 pages. Accepted to IEEE RAL
>
> **摘要:** We propose CLEVER, an active learning system for robust semantic perception with Deep Neural Networks (DNNs). For data arriving in streams, our system seeks human support when encountering failures and adapts DNNs online based on human instructions. In this way, CLEVER can eventually accomplish the given semantic perception tasks. Our main contribution is the design of a system that meets several desiderata of realizing the aforementioned capabilities. The key enabler herein is our Bayesian formulation that encodes domain knowledge through priors. Empirically, we not only motivate CLEVER's design but further demonstrate its capabilities with a user validation study as well as experiments on humanoid and deformable objects. To our knowledge, we are the first to realize stream-based active learning on a real robot, providing evidence that the robustness of the DNN-based semantic perception can be improved in practice. The project website can be accessed at https://sites.google.com/view/thecleversystem.
>
---
#### [new 044] Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos
- **分类: cs.CV; cs.LG; cs.RO**

- **简介: 该论文属于视觉-语言-动作（VLA）模型任务，旨在解决现有模型在复杂操作任务中灵活性差、泛化能力弱的问题。作者提出了Being-H0，通过大规模人类视频进行预训练，引入物理指令调优和部分动作分层编码，提升手部动作生成与指令跟随能力，并支持机器人实际操作。**

- **链接: [http://arxiv.org/pdf/2507.15597v1](http://arxiv.org/pdf/2507.15597v1)**

> **作者:** Hao Luo; Yicheng Feng; Wanpeng Zhang; Sipeng Zheng; Ye Wang; Haoqi Yuan; Jiazheng Liu; Chaoyi Xu; Qin Jin; Zongqing Lu
>
> **备注:** 37 pages
>
> **摘要:** We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0.
>
---
#### [new 045] Improving Functional Reliability of Near-Field Monitoring for Emergency Braking in Autonomous Vehicles
- **分类: eess.SY; cs.RO; cs.SY**

- **简介: 该论文属于自动驾驶安全任务，旨在解决紧急制动时近场监测系统误报率高的问题。作者提出了三种基于动态空间特性、物体尺寸和运动预测的监测策略，并通过仿真实验验证其有效性，显著提升了系统的可靠性。**

- **链接: [http://arxiv.org/pdf/2507.15594v1](http://arxiv.org/pdf/2507.15594v1)**

> **作者:** Junnan Pan; Prodromos Sotiriadis; Vladislav Nenchev; Ferdinand Englberger
>
> **备注:** 6 pages, 3 figures, conference paper
>
> **摘要:** Autonomous vehicles require reliable hazard detection. However, primary sensor systems may miss near-field obstacles, resulting in safety risks. Although a dedicated fast-reacting near-field monitoring system can mitigate this, it typically suffers from false positives. To mitigate these, in this paper, we introduce three monitoring strategies based on dynamic spatial properties, relevant object sizes, and motion-aware prediction. In experiments in a validated simulation, we compare the initial monitoring strategy against the proposed improvements. The results demonstrate that the proposed strategies can significantly improve the reliability of near-field monitoring systems.
>
---
#### [new 046] Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images
- **分类: cs.CV; cs.LG; cs.RO**

- **简介: 该论文属于定位与导航任务，旨在解决自主系统中姿态估计不准确的问题。通过融合激光雷达点云和图像，引入稠密深度图与注意力机制，优化光流估计与姿态预测，提升了定位精度与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.15496v1](http://arxiv.org/pdf/2507.15496v1)**

> **作者:** JunYing Huang; Ao Xu; DongSun Yong; KeRen Li; YuanFeng Wang; Qi Qin
>
> **摘要:** Odometry is a critical task for autonomous systems for self-localization and navigation. We propose a novel LiDAR-Visual odometry framework that integrates LiDAR point clouds and images for accurate and robust pose estimation. Our method utilizes a dense-depth map estimated from point clouds and images through depth completion, and incorporates a multi-scale feature extraction network with attention mechanisms, enabling adaptive depth-aware representations. Furthermore, we leverage dense depth information to refine flow estimation and mitigate errors in occlusion-prone regions. Our hierarchical pose refinement module optimizes motion estimation progressively, ensuring robust predictions against dynamic environments and scale ambiguities. Comprehensive experiments on the KITTI odometry benchmark demonstrate that our approach achieves similar or superior accuracy and robustness compared to state-of-the-art visual and LiDAR odometry methods.
>
---
#### [new 047] Traffic Signal Phase and Timing Estimation with Large-Scale Floating Car Data
- **分类: eess.SP; cs.RO**

- **简介: 该论文属于交通信号分析任务，旨在解决准确估计信号相位与配时（SPaT）的问题。由于传统方法依赖固定假设且适用性有限，论文提出了一种基于大规模浮动车数据（FCD）的工业级分析框架，实现从数据预处理到SPaT估计的全流程处理。该方法适应不同路口结构和周期性变化，具备强鲁棒性，并已应用于实际导航平台。**

- **链接: [http://arxiv.org/pdf/2507.14190v1](http://arxiv.org/pdf/2507.14190v1)**

> **作者:** Mingcheng Liao; Zebang Feng; Miao Fan; Shengtong Xu; Haoyi Xiong
>
> **备注:** Accepted by ITSC'25
>
> **摘要:** Effective modern transportation systems depend critically on accurate Signal Phase and Timing (SPaT) estimation. However, acquiring ground-truth SPaT information faces significant hurdles due to communication challenges with transportation departments and signal installers. As a result, Floating Car Data (FCD) has become the primary source for large-scale SPaT analyses. Current FCD approaches often simplify the problem by assuming fixed schedules and basic intersection designs for specific times and locations. These methods fail to account for periodic signal changes, diverse intersection structures, and the inherent limitations of real-world data, thus lacking a comprehensive framework that is universally applicable. Addressing this limitation, we propose an industrial-grade FCD analysis suite that manages the entire process, from initial data preprocessing to final SPaT estimation. Our approach estimates signal phases, identifies time-of-day (TOD) periods, and determines the durations of red and green lights. The framework's notable stability and robustness across diverse conditions, regardless of road geometry, is a key feature. Furthermore, we provide a cleaned, de-identified FCD dataset and supporting parameters to facilitate future research. Currently operational within our navigation platform, the system analyses over 15 million FCD records daily, supporting over two million traffic signals in mainland China, with more than 75\% of estimations demonstrating less than five seconds of error.
>
---
#### [new 048] Diffusion Beats Autoregressive in Data-Constrained Settings
- **分类: cs.LG; cs.AI; cs.CV; cs.RO**

- **简介: 论文研究在数据受限场景下扩散模型与自回归（AR）模型的性能对比，发现扩散模型在计算资源充足但数据稀缺时显著优于AR模型。扩散模型通过隐式数据增强，更好地利用重复数据，实现更低验证损失和更优下游任务表现，并提出了扩散模型的新扩展规律和计算阈值公式。**

- **链接: [http://arxiv.org/pdf/2507.15857v1](http://arxiv.org/pdf/2507.15857v1)**

> **作者:** Mihir Prabhudesai; Menging Wu; Amir Zadeh; Katerina Fragkiadaki; Deepak Pathak
>
> **备注:** Project Webpage: https://diffusion-scaling.github.io
>
> **摘要:** Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We interpret this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: https://diffusion-scaling.github.io.
>
---
#### [new 049] EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于水下图像增强任务，旨在解决AI模型在珊瑚礁监测中面临的数据集偏差、计算成本高和可解释性差的问题。作者提出了EBA-AI框架，利用CLIP嵌入减轻数据偏差，引入自适应处理降低能耗，并结合不确定性估计与可解释性技术，提升模型公平性、效率与可信度。**

- **链接: [http://arxiv.org/pdf/2507.15036v1](http://arxiv.org/pdf/2507.15036v1)**

> **作者:** Lyes Saad Saoud; Irfan Hussain
>
> **摘要:** Underwater image enhancement is vital for marine conservation, particularly coral reef monitoring. However, AI-based enhancement models often face dataset bias, high computational costs, and lack of transparency, leading to potential misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware AI framework to address these challenges. EBA-AI leverages CLIP embeddings to detect and mitigate dataset bias, ensuring balanced representation across varied underwater environments. It also integrates adaptive processing to optimize energy efficiency, significantly reducing GPU usage while maintaining competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100 show that while PSNR drops by a controlled 1.0 dB, computational savings enable real-time feasibility for large-scale marine monitoring. Additionally, uncertainty estimation and explainability techniques enhance trust in AI-driven environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet, WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing efficiency, fairness, and interpretability in underwater image processing. By addressing key limitations of AI-driven enhancement, this work contributes to sustainable, bias-aware, and computationally efficient marine conservation efforts. For interactive visualizations, animations, source code, and access to the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/
>
---
#### [new 050] From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward
- **分类: cs.AI; cs.RO; F.2.2**

- **简介: 该论文属于人工智能与认知科学交叉任务，旨在解决强化学习代理在噪声环境下难以准确识别自身行为因果效应的问题。作者提出了一种基于因果推理的新型内在奖励机制CAIS，通过测量行为对感知结果分布的影响，使代理能排除环境干扰，识别自身因果效力，并在模拟婴儿行为场景中展现出优于传统方法的效果。**

- **链接: [http://arxiv.org/pdf/2507.15106v1](http://arxiv.org/pdf/2507.15106v1)**

> **作者:** Xia Xu; Jochen Triesch
>
> **备注:** 13 pages, 5 figures
>
> **摘要:** While human infants robustly discover their own causal efficacy, standard reinforcement learning agents remain brittle, as their reliance on correlation-based rewards fails in noisy, ecologically valid scenarios. To address this, we introduce the Causal Action Influence Score (CAIS), a novel intrinsic reward rooted in causal inference. CAIS quantifies an action's influence by measuring the 1-Wasserstein distance between the learned distribution of sensory outcomes conditional on that action, $p(h|a)$, and the baseline outcome distribution, $p(h)$. This divergence provides a robust reward that isolates the agent's causal impact from confounding environmental noise. We test our approach in a simulated infant-mobile environment where correlation-based perceptual rewards fail completely when the mobile is subjected to external forces. In stark contrast, CAIS enables the agent to filter this noise, identify its influence, and learn the correct policy. Furthermore, the high-quality predictive model learned for CAIS allows our agent, when augmented with a surprise signal, to successfully reproduce the "extinction burst" phenomenon. We conclude that explicitly inferring causality is a crucial mechanism for developing a robust sense of agency, offering a psychologically plausible framework for more adaptive autonomous systems.
>
---
#### [new 051] GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving
- **分类: cs.CV; cs.RO**

- **简介: 论文提出GEMINUS，用于端到端自动驾驶的双感知全局与场景自适应专家混合框架。该工作旨在解决复杂交通环境中单一策略难以适应多样场景的问题，通过结合全局专家与场景专家，并设计双感知路由机制，实现更优的驾驶性能。**

- **链接: [http://arxiv.org/pdf/2507.14456v1](http://arxiv.org/pdf/2507.14456v1)**

> **作者:** Chi Wan; Yixin Cui; Jiatong Du; Shuo Yang; Yulong Bai; Yanjun Huang
>
> **摘要:** End-to-end autonomous driving requires adaptive and robust handling of complex and diverse traffic environments. However, prevalent single-mode planning methods attempt to learn an overall policy while struggling to acquire diversified driving skills to handle diverse scenarios. Therefore, this paper proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a Dual-aware Router. Specifically, the Global Expert is trained on the overall dataset, possessing robust performance. The Scene-Adaptive Experts are trained on corresponding scene subsets, achieving adaptive performance. The Dual-aware Router simultaneously considers scenario-level features and routing uncertainty to dynamically activate expert modules. Through the effective coupling of the Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router, GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS outperforms existing methods in the Bench2Drive closed-loop benchmark and achieves state-of-the-art performance in Driving Score and Success Rate, even with only monocular vision input. Furthermore, ablation studies demonstrate significant improvements over the original single-expert baseline: 7.67% in Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The code will be available at https://github.com/newbrains1/GEMINUS.
>
---
#### [new 052] Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于计算机视觉与机器人领域，旨在解决基于事件的运动分割与自运动估计问题。利用神经形态视觉传感器的稀疏事件数据，结合几何约束和惯性测量，提出一种优化框架，实现无需光流计算的准确运动分割与运动估计，适用于实时机器人应用。**

- **链接: [http://arxiv.org/pdf/2507.14500v1](http://arxiv.org/pdf/2507.14500v1)**

> **作者:** Zhiyuan Hua; Dehao Yuan; Cornelia Fermüller
>
> **摘要:** This paper introduces a robust framework for motion segmentation and egomotion estimation using event-based normal flow, tailored specifically for neuromorphic vision sensors. In contrast to traditional methods that rely heavily on optical flow or explicit depth estimation, our approach exploits the sparse, high-temporal-resolution event data and incorporates geometric constraints between normal flow, scene structure, and inertial measurements. The proposed optimization-based pipeline iteratively performs event over-segmentation, isolates independently moving objects via residual analysis, and refines segmentations using hierarchical clustering informed by motion similarity and temporal consistency. Experimental results on the EVIMO2v2 dataset validate that our method achieves accurate segmentation and translational motion estimation without requiring full optical flow computation. This approach demonstrates significant advantages at object boundaries and offers considerable potential for scalable, real-time robotic and navigation applications.
>
---
#### [new 053] Visual Place Recognition for Large-Scale UAV Applications
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于视觉地点识别（vPR）任务，旨在解决无人机在大范围、高海拔环境下定位难的问题。作者提出了LASED大规模数据集，并引入可转向卷积神经网络，以提升模型对旋转变化的鲁棒性，显著提高了识别准确率。**

- **链接: [http://arxiv.org/pdf/2507.15089v1](http://arxiv.org/pdf/2507.15089v1)**

> **作者:** Ioannis Tsampikos Papapetros; Ioannis Kansizoglou; Antonios Gasteratos
>
> **摘要:** Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial Vehicle (UAV) navigation, enabling robust localization across diverse environments. Despite significant advancements, aerial vPR faces unique challenges due to the limited availability of large-scale, high-altitude datasets, which limits model generalization, along with the inherent rotational ambiguity in UAV imagery. To address these challenges, we introduce LASED, a large-scale aerial dataset with approximately one million images, systematically sampled from 170,000 unique locations throughout Estonia over a decade, offering extensive geographic and temporal diversity. Its structured design ensures clear place separation significantly enhancing model training for aerial scenarios. Furthermore, we propose the integration of steerable Convolutional Neural Networks (CNNs) to explicitly handle rotational variance, leveraging their inherent rotational equivariance to produce robust, orientation-invariant feature representations. Our extensive benchmarking demonstrates that models trained on LASED achieve significantly higher recall compared to those trained on smaller, less diverse datasets, highlighting the benefits of extensive geographic coverage and temporal diversity. Moreover, steerable CNNs effectively address rotational ambiguity inherent in aerial imagery, consistently outperforming conventional convolutional architectures, achieving on average 12\% recall improvement over the best-performing non-steerable network. By combining structured, large-scale datasets with rotation-equivariant neural networks, our approach significantly enhances model robustness and generalization for aerial vPR.
>
---
#### [new 054] Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems
- **分类: cs.LG; cs.AI; cs.RO**

- **简介: 该论文属于多智能体强化学习任务，旨在解决安全关键系统中的安全策略学习问题。论文提出了一种基于控制屏障函数的分层多智能体强化学习方法，通过高层学习合作策略、底层结合CBF保障个体行为安全，实现高效且安全的多智能体协同导航。**

- **链接: [http://arxiv.org/pdf/2507.14850v1](http://arxiv.org/pdf/2507.14850v1)**

> **作者:** H. M. Sabbir Ahmad; Ehsan Sabouni; Alexander Wasilkoff; Param Budhraja; Zijian Guo; Songyuan Zhang; Chuchu Fan; Christos Cassandras; Wenchao Li
>
> **摘要:** We address the problem of safe policy learning in multi-agent safety-critical autonomous systems. In such systems, it is necessary for each agent to meet the safety requirements at all times while also cooperating with other agents to accomplish the task. Toward this end, we propose a safe Hierarchical Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier Functions (CBFs). Our proposed hierarchical approach decomposes the overall reinforcement learning problem into two levels learning joint cooperative behavior at the higher level and learning safe individual behavior at the lower or agent level conditioned on the high-level policy. Specifically, we propose a skill-based HMARL-CBF algorithm in which the higher level problem involves learning a joint policy over the skills for all the agents and the lower-level problem involves learning policies to execute the skills safely with CBFs. We validate our approach on challenging environment scenarios whereby a large number of agents have to safely navigate through conflicting road networks. Compared with existing state of the art methods, our approach significantly improves the safety achieving near perfect (within 5%) success/safety rate while also improving performance across all the environments.
>
---
#### [new 055] Light Future: Multimodal Action Frame Prediction via InstructPix2Pix
- **分类: cs.CV; cs.MM; cs.RO; I.2.10; I.4.8**

- **简介: 该论文属于机器人动作预测任务，旨在解决未来视觉帧预测的高效性与轻量化问题。论文提出基于InstructPix2Pix的多模态预测框架，仅需单张图像和文本指令即可预测未来100帧的视觉变化。通过微调InstructPix2Pix模型实现图文联合输入，在RoboTWin数据集上取得了优于现有方法的性能，具备低计算成本和快速推理优势。**

- **链接: [http://arxiv.org/pdf/2507.14809v1](http://arxiv.org/pdf/2507.14809v1)**

> **作者:** Zesen Zhong; Duomin Zhang; Yijia Li
>
> **备注:** 9 pages including appendix, 5 tables, 8 figures, to be submitted to WACV 2026
>
> **摘要:** Predicting future motion trajectories is a critical capability across domains such as robotics, autonomous systems, and human activity forecasting, enabling safer and more intelligent decision-making. This paper proposes a novel, efficient, and lightweight approach for robot action prediction, offering significantly reduced computational cost and inference latency compared to conventional video prediction models. Importantly, it pioneers the adaptation of the InstructPix2Pix model for forecasting future visual frames in robotic tasks, extending its utility beyond static image editing. We implement a deep learning-based visual prediction framework that forecasts what a robot will observe 100 frames (10 seconds) into the future, given a current image and a textual instruction. We repurpose and fine-tune the InstructPix2Pix model to accept both visual and textual inputs, enabling multimodal future frame prediction. Experiments on the RoboTWin dataset (generated based on real-world scenarios) demonstrate that our method achieves superior SSIM and PSNR compared to state-of-the-art baselines in robot action prediction tasks. Unlike conventional video prediction models that require multiple input frames, heavy computation, and slow inference latency, our approach only needs a single image and a text prompt as input. This lightweight design enables faster inference, reduced GPU demands, and flexible multimodal control, particularly valuable for applications like robotics and sports motion trajectory analytics, where motion trajectory precision is prioritized over visual fidelity.
>
---
## 更新

#### [replaced 001] Design and Characterization of a Micro-Vibration Adhesion System
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2504.05351v2](http://arxiv.org/pdf/2504.05351v2)**

> **作者:** Siqian Li; Xi Wang; Jung-Che Chang; Xin Dong
>
> **摘要:** In recent years, miniature wall-climbing robots have attracted widespread attention due to their significant potential in equipment inspection and in-situ repair applications. Traditional wall-climbing systems typically rely on electromagnetic, electrostatic, vacuum suction, or van der Waals forces for controllable adhesion. However, these conventional methods impose limitations when striving for both a compact design and high-speed mobility. This paper proposes a novel Vibration-Based Adhesion (VBA) technique, which utilizes a flexible disk vibrating near a surface to generate a strong and controllable attractive force without direct contact. By employing an electric motor as the vibration source, the constructed VBA system was experimentally evaluated, achieving an adhesion-to-weight ratio exceeding 51 times. The experimental results demonstrate that this adhesion mechanism not only provides a high normal force but also maintains minimal shear force, making it particularly suitable for high-speed movement and heavy load applications in miniature wall-climbing robots.
>
---
#### [replaced 002] Efficient Multi-Camera Tokenization with Triplanes for End-to-End Driving
- **分类: cs.CV; cs.LG; cs.RO**

- **链接: [http://arxiv.org/pdf/2506.12251v2](http://arxiv.org/pdf/2506.12251v2)**

> **作者:** Boris Ivanovic; Cristiano Saltori; Yurong You; Yan Wang; Wenjie Luo; Marco Pavone
>
> **备注:** 12 pages, 10 figures, 5 tables
>
> **摘要:** Autoregressive Transformers are increasingly being deployed as end-to-end robot and autonomous vehicle (AV) policy architectures, owing to their scalability and potential to leverage internet-scale pretraining for generalization. Accordingly, tokenizing sensor data efficiently is paramount to ensuring the real-time feasibility of such architectures on embedded hardware. To this end, we present an efficient triplane-based multi-camera tokenization strategy that leverages recent advances in 3D neural reconstruction and rendering to produce sensor tokens that are agnostic to the number of input cameras and their resolution, while explicitly accounting for their geometry around an AV. Experiments on a large-scale AV dataset and state-of-the-art neural simulator demonstrate that our approach yields significant savings over current image patch-based tokenization strategies, producing up to 72% fewer tokens, resulting in up to 50% faster policy inference while achieving the same open-loop motion planning accuracy and improved offroad rates in closed-loop driving simulations.
>
---
#### [replaced 003] Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2504.06513v4](http://arxiv.org/pdf/2504.06513v4)**

> **作者:** Xinyi Wang; Taekyung Kim; Bardh Hoxha; Georgios Fainekos; Dimitra Panagou
>
> **备注:** 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Project page: {https://lawliet9666.github.io/cvarbf/}
>
> **摘要:** Robot navigation in dynamic, crowded environments poses a significant challenge due to the inherent uncertainties in the obstacle model. In this work, we propose a risk-adaptive approach based on the Conditional Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically adjusted to accept the minimum necessary risk, achieving a good performance in terms of safety and optimization feasibility under uncertainty. Additionally, we introduce a dynamic zone-based barrier function which characterizes the collision likelihood by evaluating the relative state between the robot and the obstacle. By integrating risk adaptation with this new function, our approach adaptively expands the safety margin, enabling the robot to proactively avoid obstacles in highly dynamic environments. Comparisons and ablation studies demonstrate that our method outperforms existing social navigation approaches, and validate the effectiveness of our proposed framework.
>
---
#### [replaced 004] The Duality of Generative AI and Reinforcement Learning in Robotics: A Review
- **分类: cs.RO; cs.LG**

- **链接: [http://arxiv.org/pdf/2410.16411v2](http://arxiv.org/pdf/2410.16411v2)**

> **作者:** Angelo Moroncelli; Vishal Soni; Marco Forgione; Dario Piga; Blerina Spahiu; Loris Roveda
>
> **备注:** Submitted for publication to Information Fusion
>
> **摘要:** Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers. Lastly, we identify open challenges accounting for model scalability, adaptation and grounding, giving recommendations and insights on future research directions. We reflect on which generative AI models best fit the RL tasks and why. On the other side, we reflect on important issues inherent to RL-enhanced generative policies, such as safety concerns and failure modes, and what are the limitations of current methods. A curated collection of relevant research papers is maintained on our GitHub repository, serving as a resource for ongoing research and development in this field: https://github.com/clmoro/Robotics-RL-FMs-Integration.
>
---
#### [replaced 005] GraspMAS: Zero-Shot Language-driven Grasp Detection with Multi-Agent System
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2506.18448v2](http://arxiv.org/pdf/2506.18448v2)**

> **作者:** Quang Nguyen; Tri Le; Huy Nguyen; Thieu Vo; Tung D. Ta; Baoru Huang; Minh N. Vu; Anh Nguyen
>
> **备注:** Accepted to IROS 2025. Webpage: https://zquang2202.github.io/GraspMAS/
>
> **摘要:** Language-driven grasp detection has the potential to revolutionize human-robot interaction by allowing robots to understand and execute grasping tasks based on natural language commands. However, existing approaches face two key challenges. First, they often struggle to interpret complex text instructions or operate ineffectively in densely cluttered environments. Second, most methods require a training or finetuning step to adapt to new domains, limiting their generation in real-world applications. In this paper, we introduce GraspMAS, a new multi-agent system framework for language-driven grasp detection. GraspMAS is designed to reason through ambiguities and improve decision-making in real-world scenarios. Our framework consists of three specialized agents: Planner, responsible for strategizing complex queries; Coder, which generates and executes source code; and Observer, which evaluates the outcomes and provides feedback. Intensive experiments on two large-scale datasets demonstrate that our GraspMAS significantly outperforms existing baselines. Additionally, robot experiments conducted in both simulation and real-world settings further validate the effectiveness of our approach. Our project page is available at https://zquang2202.github.io/GraspMAS
>
---
#### [replaced 006] Model-Free and Real-Time Bioinspired Unicycle-Based Source Seeking: Differential Wheeled Robotic Experiments
- **分类: cs.RO; math.OC**

- **链接: [http://arxiv.org/pdf/2501.02184v3](http://arxiv.org/pdf/2501.02184v3)**

> **作者:** Ahmed A. Elgohary; Sameh A. Eisa; Shivam Bajpai
>
> **摘要:** Many autonomous robots aimed at source-seeking are studied, and their controls designed, using unicycle modeling and formulation. This is true not only for model-based controllers, but also for model-free, real-time control methods such as extremum seeking control (ESC). In this paper, we propose a unicycle-based ESC design applicable to differential wheeled robots that: (1) is very simple design, based on one simple control-affine law, and without state integrators; (2) attenuates oscillations known to persist in ESC designs (i.e., fully stop at the source); and (3) operates in a model-free, real-time setting, tolerating environmental/sensor noise. We provide simulation and real-world robotic experimental results for fixed and moving light source seeking by a differential wheeled robot using our proposed design. Results indicate clear advantages of our proposed design when compared to the literature, including attenuation of undesired oscillations, improved convergence speed, and better handling of noise.
>
---
#### [replaced 007] EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera
- **分类: cs.CV; cs.RO; eess.IV; physics.optics**

- **链接: [http://arxiv.org/pdf/2503.12419v3](http://arxiv.org/pdf/2503.12419v3)**

> **作者:** Luming Wang; Hao Shi; Xiaoting Yin; Kailun Yang; Kaiwei Wang; Jian Bai
>
> **备注:** Accepted to SMC 2025. The dataset and models are made available at https://github.com/3190105222/EgoEv_Gesture
>
> **摘要:** Egocentric gesture recognition is a pivotal technology for enhancing natural human-computer interaction, yet traditional RGB-based solutions suffer from motion blur and illumination variations in dynamic scenarios. While event cameras show distinct advantages in handling high dynamic range with ultra-low power consumption, existing RGB-based architectures face inherent limitations in processing asynchronous event streams due to their synchronous frame-based nature. Moreover, from an egocentric perspective, event cameras record data that includes events generated by both head movements and hand gestures, thereby increasing the complexity of gesture recognition. To address this, we propose a novel network architecture specifically designed for event data processing, incorporating (1) a lightweight CNN with asymmetric depthwise convolutions to reduce parameters while preserving spatiotemporal features, (2) a plug-and-play state-space model as context block that decouples head movement noise from gesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BTSM) that shifts features along bins and temporal dimensions to fuse sparse events efficiently. We further establish the EgoEvGesture dataset, the first large-scale dataset for egocentric gesture recognition using event cameras. Experimental results demonstrate that our method achieves 62.7% accuracy tested on unseen subjects with only 7M parameters, 3.1% higher than state-of-the-art approaches. Notable misclassifications in freestyle motions stem from high inter-personal variability and unseen test patterns differing from training data. Moreover, our approach achieved a remarkable accuracy of 97.0% on the DVS128 Gesture, demonstrating the effectiveness and generalization capability of our method on public datasets. The dataset and models are made available at https://github.com/3190105222/EgoEv_Gesture.
>
---
#### [replaced 008] CoordField: Coordination Field for Agentic UAV Task Allocation In Low-altitude Urban Scenarios
- **分类: cs.RO; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.00091v4](http://arxiv.org/pdf/2505.00091v4)**

> **作者:** Tengchao Zhang; Yonglin Tian; Fei Lin; Jun Huang; Patrik P. Süli; Qinghua Ni; Rui Qin; Xiao Wang; Fei-Yue Wang
>
> **摘要:** With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV) swarms to perform complex tasks in urban environments, system design now faces major challenges, including efficient semantic understanding, flexible task planning, and the ability to dynamically adjust coordination strategies in response to evolving environmental conditions and continuously changing task requirements. To address the limitations of existing methods, this paper proposes CoordField, a coordination field agent system for coordinating heterogeneous drone swarms in complex urban scenarios. In this system, large language models (LLMs) is responsible for interpreting high-level human instructions and converting them into executable commands for the UAV swarms, such as patrol and target tracking. Subsequently, a Coordination field mechanism is proposed to guide UAV motion and task selection, enabling decentralized and adaptive allocation of emergent tasks. A total of 50 rounds of comparative testing were conducted across different models in a 2D simulation space to evaluate their performance. Experimental results demonstrate that the proposed system achieves superior performance in terms of task coverage, response time, and adaptability to dynamic changes.
>
---
#### [replaced 009] Predictive Planner for Autonomous Driving with Consistency Models
- **分类: cs.RO; cs.LG**

- **链接: [http://arxiv.org/pdf/2502.08033v3](http://arxiv.org/pdf/2502.08033v3)**

> **作者:** Anjian Li; Sangjae Bae; David Isele; Ryne Beeson; Faizan M. Tariq
>
> **备注:** Accepted at the 28th IEEE International Conference on Intelligent Transportation Systems (ITSC) 2025
>
> **摘要:** Trajectory prediction and planning are essential for autonomous vehicles to navigate safely and efficiently in dynamic environments. Traditional approaches often treat them separately, limiting the ability for interactive planning. While recent diffusion-based generative models have shown promise in multi-agent trajectory generation, their slow sampling is less suitable for high-frequency planning tasks. In this paper, we leverage the consistency model to build a predictive planner that samples from a joint distribution of ego and surrounding agents, conditioned on the ego vehicle's navigational goal. Trained on real-world human driving datasets, our consistency model generates higher-quality trajectories with fewer sampling steps than standard diffusion models, making it more suitable for real-time deployment. To enforce multiple planning constraints simultaneously on the ego trajectory, a novel online guided sampling approach inspired by the Alternating Direction Method of Multipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset (WOMD), our method enables proactive behavior such as nudging and yielding, and also demonstrates smoother, safer, and more efficient trajectories and satisfaction of multiple constraints under a limited computational budget.
>
---
#### [replaced 010] J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities Effectively in Inverse Kinematic Control of Serial Manipulators
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2505.00306v3](http://arxiv.org/pdf/2505.00306v3)**

> **作者:** Shivani Guptasarma; Matthew Strong; Honghao Zhen; Monroe Kennedy III
>
> **备注:** 18 pages, 25 figures. v1: Fig. 1 replaced with faster-loading version. v2: Website at https://jparse-manip.github.io/
>
> **摘要:** J-PARSE is a method for smooth first-order inverse kinematic control of a serial manipulator near kinematic singularities. The commanded end-effector velocity is interpreted component-wise, according to the available mobility in each dimension of the task space. First, a substitute "Safety" Jacobian matrix is created, keeping the aspect ratio of the manipulability ellipsoid above a threshold value. The desired motion is then projected onto non-singular and singular directions, and the latter projection scaled down by a factor informed by the threshold value. A right-inverse of the non-singular Safety Jacobian is applied to the modified command. In the absence of joint limits and collisions, this ensures smooth transition into and out of low-rank poses, guaranteeing asymptotic stability for target poses within the workspace, and stability for those outside. Velocity control with J-PARSE is benchmarked against the Least-Squares and Damped Least-Squares inversions of the Jacobian, and shows high accuracy in reaching and leaving singular target poses. By expanding the available workspace of manipulators, the method finds applications in servoing, teleoperation, and learning. Videos and code are available at https://jparse-manip.github.io/.
>
---
#### [replaced 011] Interactive Navigation for Legged Manipulators with Learned Arm-Pushing Controller
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2503.01474v2](http://arxiv.org/pdf/2503.01474v2)**

> **作者:** Zhihai Bi; Kai Chen; Chunxin Zheng; Yulin Li; Haoang Li; Jun Ma
>
> **摘要:** Interactive navigation is crucial in scenarios where proactively interacting with objects can yield shorter paths, thus significantly improving traversal efficiency. Existing methods primarily focus on using the robot body to relocate large obstacles (which could be comparable to the size of a robot). However, they prove ineffective in narrow or constrained spaces where the robot's dimensions restrict its manipulation capabilities. This paper introduces a novel interactive navigation framework for legged manipulators, featuring an active arm-pushing mechanism that enables the robot to reposition movable obstacles in space-constrained environments. To this end, we develop a reinforcement learning-based arm-pushing controller with a two-stage reward strategy for large-object manipulation. Specifically, this strategy first directs the manipulator to a designated pushing zone to achieve a kinematically feasible contact configuration. Then, the end effector is guided to maintain its position at appropriate contact points for stable object displacement while preventing toppling. The simulations validate the robustness of the arm-pushing controller, showing that the two-stage reward strategy improves policy convergence and long-term performance. Real-world experiments further demonstrate the effectiveness of the proposed navigation framework, which achieves shorter paths and reduced traversal time. The open-source project can be found at https://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator.git.
>
---
#### [replaced 012] Resource-Efficient Affordance Grounding with Complementary Depth and Semantic Prompts
- **分类: cs.CV; cs.RO; eess.IV**

- **链接: [http://arxiv.org/pdf/2503.02600v2](http://arxiv.org/pdf/2503.02600v2)**

> **作者:** Yizhou Huang; Fan Yang; Guoliang Zhu; Gen Li; Hao Shi; Yukun Zuo; Wenrui Chen; Zhiyong Li; Kailun Yang
>
> **备注:** Accepted to IROS 2025. The source code will be made publicly available at https://github.com/DAWDSE/BiT-Align
>
> **摘要:** Affordance refers to the functional properties that an agent perceives and utilizes from its environment, and is key perceptual information required for robots to perform actions. This information is rich and multimodal in nature. Existing multimodal affordance methods face limitations in extracting useful information, mainly due to simple structural designs, basic fusion methods, and large model parameters, making it difficult to meet the performance requirements for practical deployment. To address these issues, this paper proposes the BiT-Align image-depth-text affordance mapping framework. The framework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance (TFG) attention selection mechanism. BPM integrates the auxiliary modality depth image directly as a prompt to the primary modality RGB image, embedding it into the primary modality encoder without introducing additional encoders. This reduces the model's parameter count and effectively improves functional region localization accuracy. The TFG mechanism guides the selection and enhancement of attention heads in the image encoder using textual features, improving the understanding of affordance characteristics. Experimental results demonstrate that the proposed method achieves significant performance improvements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset, compared with the current state-of-the-art method, we achieve a 6.0% improvement in the KLD metric, while reducing model parameters by 88.8%, demonstrating practical application values. The source code will be made publicly available at https://github.com/DAWDSE/BiT-Align.
>
---
#### [replaced 013] Stimulating Imagination: Towards General-purpose "Something Something Placement"
- **分类: cs.RO; cs.AI**

- **链接: [http://arxiv.org/pdf/2408.01655v2](http://arxiv.org/pdf/2408.01655v2)**

> **作者:** Jianyang Wu; Jie Gu; Xiaokang Ma; Fangzhou Qiu; Chu Tang; Jingmin Chen
>
> **备注:** 7 pages, accepted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)
>
> **摘要:** General-purpose object placement is a fundamental capability of an intelligent generalist robot: being capable of rearranging objects following precise human instructions even in novel environments. This work is dedicated to achieving general-purpose object placement with ``something something'' instructions. Specifically, we break the entire process down into three parts, including object localization, goal imagination and robot control, and propose a method named SPORT. SPORT leverages a pre-trained large vision model for broad semantic reasoning about objects, and learns a diffusion-based pose estimator to ensure physically-realistic results in 3D space. Only object types (movable or reference) are communicated between these two parts, which brings two benefits. One is that we can fully leverage the powerful ability of open-set object recognition and localization since no specific fine-tuning is needed for the robotic scenario. Moreover, the diffusion-based estimator only need to ``imagine" the object poses after the placement, while no necessity for their semantic information. Thus the training burden is greatly reduced and no massive training is required. The training data for the goal pose estimation is collected in simulation and annotated by using GPT-4. Experimental results demonstrate the effectiveness of our approach. SPORT can not only generate promising 3D goal poses for unseen simulated objects, but also be seamlessly applied to real-world settings.
>
---
#### [replaced 014] Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning
- **分类: cs.RO; cs.AI; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.04790v2](http://arxiv.org/pdf/2507.04790v2)**

> **作者:** Giwon Lee; Wooseong Jeong; Daehee Park; Jaewoo Jeong; Kuk-Jin Yoon
>
> **备注:** Accepted at ICCV 2025
>
> **摘要:** Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches.
>
---
#### [replaced 015] CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network for Sampling-Based Path Planning
- **分类: cs.RO; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2305.10442v3](http://arxiv.org/pdf/2305.10442v3)**

> **作者:** Abhinav Sagar; Sai Teja Gilukara
>
> **摘要:** Sampling-based path planning algorithms play an important role in autonomous robotics. However, a common problem among these algorithms is that the initial path generated is not optimal, and the convergence is too slow for real-world applications. In this paper, we propose a novel image-based learning algorithm using a Convolutional Block Attention Generative Adversarial Network (CBAGAN-RRT) with a combination of spatial and channel attention and a novel loss function to design the heuristics, find a better optimal path, and improve the convergence of the algorithm, both concerning time and speed. The probability distribution of the paths generated from our GAN model is used to guide the sampling process for the RRT algorithm. We demonstrate that our algorithm outperforms the previous state-of-the-art algorithms using both the image quality generation metrics, like IOU Score, Dice Score, FID score, and path planning metrics like time cost and the number of nodes. Ablation studies show the effectiveness of various components in our network architecture. The advantage of our approach is that we can avoid the complicated preprocessing in the state space, our model can be generalized to complex environments like those containing turns and narrow passages without loss of accuracy, and our model can be easily integrated with other sampling-based path planning algorithms.
>
---
#### [replaced 016] G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior Retrieval and Prior-Assisted Generation
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2503.19457v2](http://arxiv.org/pdf/2503.19457v2)**

> **作者:** Juntao Jian; Xiuping Liu; Zixuan Chen; Manyi Li; Jian Liu; Ruizhen Hu
>
> **备注:** Accepted by ICCV 2025
>
> **摘要:** Recent advances in dexterous grasping synthesis have demonstrated significant progress in producing reasonable and plausible grasps for many task purposes. But it remains challenging to generalize to unseen object categories and diverse task instructions. In this paper, we propose G-DexGrasp, a retrieval-augmented generation approach that can produce high-quality dexterous hand configurations for unseen object categories and language-based task instructions. The key is to retrieve generalizable grasping priors, including the fine-grained contact part and the affordance-related distribution of relevant grasping instances, for the following synthesis pipeline. Specifically, the fine-grained contact part and affordance act as generalizable guidance to infer reasonable grasping configurations for unseen objects with a generative model, while the relevant grasping distribution plays as regularization to guarantee the plausibility of synthesized grasps during the subsequent refinement optimization. Our comparison experiments validate the effectiveness of our key designs for generalization and demonstrate the remarkable performance against the existing approaches. Project page: https://g-dexgrasp.github.io/
>
---
#### [replaced 017] WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2503.02247v5](http://arxiv.org/pdf/2503.02247v5)**

> **作者:** Dujun Nie; Xianda Guo; Yiqun Duan; Ruijun Zhang; Long Chen
>
> **备注:** 8 pages, 5 figures
>
> **摘要:** Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.
>
---
#### [replaced 018] Leveraging Semantic Graphs for Efficient and Robust LiDAR SLAM
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2503.11145v2](http://arxiv.org/pdf/2503.11145v2)**

> **作者:** Neng Wang; Huimin Lu; Zhiqiang Zheng; Hesheng Wang; Yun-Hui Liu; Xieyuanli Chen
>
> **备注:** 8 pages, 4 figures,Accpted for IROS 2025
>
> **摘要:** Accurate and robust simultaneous localization and mapping (SLAM) is crucial for autonomous mobile systems, typically achieved by leveraging the geometric features of the environment. Incorporating semantics provides a richer scene representation that not only enhances localization accuracy in SLAM but also enables advanced cognitive functionalities for downstream navigation and planning tasks. Existing point-wise semantic LiDAR SLAM methods often suffer from poor efficiency and generalization, making them less robust in diverse real-world scenarios. In this paper, we propose a semantic graph-enhanced SLAM framework, named SG-SLAM, which effectively leverages the geometric, semantic, and topological characteristics inherent in environmental structures. The semantic graph serves as a fundamental component that facilitates critical functionalities of SLAM, including robust relocalization during odometry failures, accurate loop closing, and semantic graph map construction. Our method employs a dual-threaded architecture, with one thread dedicated to online odometry and relocalization, while the other handles loop closure, pose graph optimization, and map update. This design enables our method to operate in real time and generate globally consistent semantic graph maps and point cloud maps. We extensively evaluate our method across the KITTI, MulRAN, and Apollo datasets, and the results demonstrate its superiority compared to state-of-the-art methods. Our method has been released at https://github.com/nubot-nudt/SG-SLAM.
>
---
#### [replaced 019] Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2505.13982v2](http://arxiv.org/pdf/2505.13982v2)**

> **作者:** Jinzhou Li; Tianhao Wu; Jiyao Zhang; Zeyuan Chen; Haotian Jin; Mingdong Wu; Yujun Shen; Yaodong Yang; Hao Dong
>
> **摘要:** Effectively utilizing multi-sensory data is important for robots to generalize across diverse tasks. However, the heterogeneous nature of these modalities makes fusion challenging. Existing methods propose strategies to obtain comprehensively fused features but often ignore the fact that each modality requires different levels of attention at different manipulation stages. To address this, we propose a force-guided attention fusion module that adaptively adjusts the weights of visual and tactile features without human labeling. We also introduce a self-supervised future force prediction auxiliary task to reinforce the tactile modality, improve data imbalance, and encourage proper adjustment. Our method achieves an average success rate of 93% across three fine-grained, contactrich tasks in real-world experiments. Further analysis shows that our policy appropriately adjusts attention to each modality at different manipulation stages. The videos can be viewed at https://adaptac-dex.github.io/.
>
---
#### [replaced 020] Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2507.08224v2](http://arxiv.org/pdf/2507.08224v2)**

> **作者:** Chan Young Park; Jillian Fisher; Marius Memmel; Dipika Khullar; Seoho Yun; Abhishek Gupta; Yejin Choi
>
> **备注:** Code Available: https://github.com/chan0park/SelfReVision
>
> **摘要:** Large language models (LLMs) have shown promise in robotic procedural planning, yet their human-centric reasoning often omits the low-level, grounded details needed for robotic execution. Vision-language models (VLMs) offer a path toward more perceptually grounded plans, but current methods either rely on expensive, large-scale models or are constrained to narrow simulation settings. We introduce SelfReVision, a lightweight and scalable self-improvement framework for vision-language procedural planning. SelfReVision enables small VLMs to iteratively critique, revise, and verify their own plans-without external supervision or teacher models-drawing inspiration from chain-of-thought prompting and self-instruct paradigms. Through this self-distillation loop, models generate higher-quality, execution-ready plans that can be used both at inference and for continued fine-tuning. Using models varying from 3B to 72B, our results show that SelfReVision not only boosts performance over weak base VLMs but also outperforms models 100X the size, yielding improved control in downstream embodied tasks.
>
---
#### [replaced 021] LaViPlan : Language-Guided Visual Path Planning with RLVR
- **分类: cs.RO; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.12911v2](http://arxiv.org/pdf/2507.12911v2)**

> **作者:** Hayeon Oh
>
> **备注:** 11 pages, 6 figures
>
> **摘要:** Out-of-distribution (OOD) scenarios in autonomous driving refer to situations that deviate from the training domain, often leading to unexpected and potentially hazardous behavior from planners that lack prior exposure to such cases. Recently, Vision-Language Models (VLMs) have been introduced into autonomous driving research for their promising generalization capabilities in OOD settings. Early studies demonstrated that VLMs could recognize OOD scenarios and generate user-level decisions such as "go straight" or "turn right." However, a new challenge has emerged due to the misalignment between the VLM's high-level decisions or visual reasoning expressed in language, and the low-level predicted trajectories interpreted as actions. In this paper, we propose LaViPlan, a framework that leverages Reinforcement Learning with Verifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics. This approach addresses the vision-language-action misalignment observed in existing VLMs fine-tuned via supervised learning, which can recognize driving scenarios but often produce context-unaware decisions. Experimental results demonstrate that our method improves situational awareness and decision-making under OOD conditions, highlighting its potential to mitigate the misalignment issue. This work introduces a promising post-training paradigm for VLM agents in the context of autonomous driving.
>
---
#### [replaced 022] Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model
- **分类: cs.RO; cs.AI; cs.SY; eess.SY**

- **链接: [http://arxiv.org/pdf/2507.06174v4](http://arxiv.org/pdf/2507.06174v4)**

> **作者:** Koki Yamane; Yunhan Li; Masashi Konosu; Koki Inami; Junji Oaki; Sho Sakaino; Toshiaki Tsuji
>
> **备注:** 20 pages, 9 figures, Submitted to CoRL 2025
>
> **摘要:** In recent years, the advancement of imitation learning has led to increased interest in teleoperating low-cost manipulators to collect demonstration data. However, most existing systems rely on unilateral control, which only transmits target position values. While this approach is easy to implement and suitable for slow, non-contact tasks, it struggles with fast or contact-rich operations due to the absence of force feedback. This work demonstrates that fast teleoperation with force feedback is feasible even with force-sensorless, low-cost manipulators by leveraging 4-channel bilateral control. Based on accurately identified manipulator dynamics, our method integrates nonlinear terms compensation, velocity and external force estimation, and variable gain corresponding to inertial variation. Furthermore, using data collected by 4-channel bilateral control, we show that incorporating force information into both the input and output of learned policies improves performance in imitation learning. These results highlight the practical effectiveness of our system for high-fidelity teleoperation and data collection on affordable hardware.
>
---
#### [replaced 023] CROSS-GAiT: Cross-Attention-Based Multimodal Representation Fusion for Parametric Gait Adaptation in Complex Terrains
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2409.17262v3](http://arxiv.org/pdf/2409.17262v3)**

> **作者:** Gershom Seneviratne; Kasun Weerakoon; Mohamed Elnoor; Vignesh Rajgopal; Harshavarthan Varatharajan; Mohamed Khalid M Jaffar; Jason Pusey; Dinesh Manocha
>
> **摘要:** We present CROSS-GAiT, a novel algorithm for quadruped robots that uses Cross Attention to fuse terrain representations derived from visual and time-series inputs; including linear accelerations, angular velocities, and joint efforts. These fused representations are used to continuously adjust two critical gait parameters (step height and hip splay), enabling adaptive gaits that respond dynamically to varying terrain conditions. To generate terrain representations, we process visual inputs through a masked Vision Transformer (ViT) encoder and time-series data through a dilated causal convolutional encoder. The Cross Attention mechanism then selects and integrates the most relevant features from each modality, combining terrain characteristics with robot dynamics for informed gait adaptation. This fused representation allows CROSS-GAiT to continuously adjust gait parameters in response to unpredictable terrain conditions in real-time. We train CROSS-GAiT on a diverse set of terrains including asphalt, concrete, brick pavements, grass, dense vegetation, pebbles, gravel, and sand and validate its generalization ability on unseen environments. Our hardware implementation on the Ghost Robotics Vision 60 demonstrates superior performance in challenging terrains, such as high-density vegetation, unstable surfaces, sandbanks, and deformable substrates. We observe at least a 7.04% reduction in IMU energy density and a 27.3% reduction in total joint effort, which directly correlates with increased stability and reduced energy usage when compared to state-of-the-art methods. Furthermore, CROSS-GAiT demonstrates at least a 64.5% increase in success rate and a 4.91% reduction in time to reach the goal in four complex scenarios. Additionally, the learned representations perform 4.48% better than the state-of-the-art on a terrain classification task.
>
---
#### [replaced 024] LSTP-Nav: Lightweight Spatiotemporal Policy for Map-free Multi-agent Navigation with LiDAR
- **分类: cs.RO; cs.SY; eess.SY**

- **链接: [http://arxiv.org/pdf/2408.16370v5](http://arxiv.org/pdf/2408.16370v5)**

> **作者:** Xingrong Diao; Zhirui Sun; Jianwei Peng; Jiankun Wang
>
> **摘要:** Safe and efficient multi-agent navigation in dynamic environments remains inherently challenging, particularly when real-time decision-making is required on resource-constrained platforms. Ensuring collision-free trajectories while adapting to uncertainties without relying on pre-built maps further complicates real-world deployment. To address these challenges, we propose LSTP-Nav, a lightweight end-to-end policy for multi-agent navigation that enables map-free collision avoidance in complex environments by directly mapping raw LiDAR point clouds to motion commands. At the core of this framework lies LSTP-Net, an efficient network that processes raw LiDAR data using a GRU architecture, enhanced with attention mechanisms to dynamically focus on critical environmental features while minimizing computational overhead. Additionally, a novel HS reward optimizes collision avoidance by incorporating angular velocity, prioritizing obstacles along the predicted heading, and enhancing training stability. To narrow the sim-to-real gap, we develop PhysReplay-Simlab, a physics-realistic multi-agent simulator, employs localized replay to mine near-failure experiences. Relying solely on LiDA, LSTP-Nav achieves efficient zero-shot sim-to-real transfer on a CPU-only robotic platform, enabling robust navigation in dynamic environments while maintaining computation frequencies above 40 Hz. Extensive experiments demonstrate that LSTP-Nav outperforms baselines with a 9.58% higher success rate and a 12.30% lower collision rate, underscoring its practicality and robustness for real-world applications.
>
---
#### [replaced 025] Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI
- **分类: cs.LG; cs.AI; cs.RO**

- **链接: [http://arxiv.org/pdf/2403.10559v2](http://arxiv.org/pdf/2403.10559v2)**

> **作者:** Bo Shu; Yiting Zhang; Dong Shu
>
> **备注:** 9 pages, 2 figures
>
> **摘要:** This report investigates the history and impact of Generative Models and Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing progress in technology and transportation. By focusing on the application of generative models within the context of CAVs, the study aims to unravel how this integration could enhance predictive modeling, simulation accuracy, and decision-making processes in autonomous vehicles. This thesis discusses the benefits and challenges of integrating generative models and CAV technology in transportation. It aims to highlight the progress made, the remaining obstacles, and the potential for advancements in safety and innovation.
>
---
#### [replaced 026] Intelligent LiDAR Navigation: Leveraging External Information and Semantic Maps with LLM as Copilot
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2409.08493v3](http://arxiv.org/pdf/2409.08493v3)**

> **作者:** Fujing Xie; Jiajie Zhang; Sören Schwertfeger
>
> **备注:** Accepted at IROS 2025
>
> **摘要:** Traditional robot navigation systems primarily utilize occupancy grid maps and laser-based sensing technologies, as demonstrated by the popular move_base package in ROS. Unlike robots, humans navigate not only through spatial awareness and physical distances but also by integrating external information, such as elevator maintenance updates from public notification boards and experiential knowledge, like the need for special access through certain doors. With the development of Large Language Models (LLMs), which possesses text understanding and intelligence close to human performance, there is now an opportunity to infuse robot navigation systems with a level of understanding akin to human cognition. In this study, we propose using osmAG (Area Graph in OpensStreetMap textual format), an innovative semantic topometric hierarchical map representation, to bridge the gap between the capabilities of ROS move_base and the contextual understanding offered by LLMs. Our methodology employs LLMs as an actual copilot in robot navigation, enabling the integration of a broader range of informational inputs while maintaining the robustness of traditional robotic navigation systems. Our code, demo, map, experiment results can be accessed at https://github.com/xiexiexiaoxiexie/Intelligent-LiDAR-Navigation-LLM-as-Copilot.
>
---
#### [replaced 027] GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2505.03233v2](http://arxiv.org/pdf/2505.03233v2)**

> **作者:** Shengliang Deng; Mi Yan; Songlin Wei; Haixin Ma; Yuxin Yang; Jiayi Chen; Zhiqi Zhang; Taoyu Yang; Xuheng Zhang; Heming Cui; Zhizheng Zhang; He Wang
>
> **摘要:** Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA's advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.
>
---
#### [replaced 028] The Constitutional Filter: Bayesian Estimation of Compliant Agents
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2412.18347v3](http://arxiv.org/pdf/2412.18347v3)**

> **作者:** Simon Kohaut; Felix Divo; Benedict Flade; Devendra Singh Dhami; Julian Eggert; Kristian Kersting
>
> **摘要:** Predicting agents impacted by legal policies, physical limitations, and operational preferences is inherently difficult. In recent years, neuro-symbolic methods have emerged, integrating machine learning and symbolic reasoning models into end-to-end learnable systems. Hereby, a promising avenue for expressing high-level constraints over multi-modal input data in robotics has opened up. This work introduces an approach for Bayesian estimation of agents expected to comply with a human-interpretable neuro-symbolic model we call its Constitution. Hence, we present the Constitutional Filter (CoFi), leading to improved tracking of agents by leveraging expert knowledge, incorporating deep learning architectures, and accounting for environmental uncertainties. CoFi extends the general, recursive Bayesian estimation setting, ensuring compatibility with a vast landscape of established techniques such as Particle Filters. To underpin the advantages of CoFi, we evaluate its performance on real-world marine traffic data. Beyond improved performance, we show how CoFi can learn to trust and adapt to the level of compliance of an agent, recovering baseline performance even if the assumed Constitution clashes with reality.
>
---
#### [replaced 029] MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2506.06535v2](http://arxiv.org/pdf/2506.06535v2)**

> **作者:** Vineet Bhat; Naman Patel; Prashanth Krishnamurthy; Ramesh Karri; Farshad Khorrami
>
> **摘要:** Robotic manipulation of unseen objects via natural language commands remains challenging. Language driven robotic grasping (LDRG) predicts stable grasp poses from natural language queries and RGB-D images. We propose MapleGrasp, a novel framework that leverages mask-guided feature pooling for efficient vision-language driven grasping. Our two-stage training first predicts segmentation masks from CLIP-based vision-language features. The second stage pools features within these masks to generate pixel-level grasp predictions, improving efficiency, and reducing computation. Incorporating mask pooling results in a 7% improvement over prior approaches on the OCID-VLG benchmark. Furthermore, we introduce RefGraspNet, an open-source dataset eight times larger than existing alternatives, significantly enhancing model generalization for open-vocabulary grasping. MapleGrasp scores a strong grasping accuracy of 89\% when compared with competing methods in the RefGraspNet benchmark. Our method achieves comparable performance to larger Vision-Language-Action models on the LIBERO benchmark, and shows significantly better generalization to unseen tasks. Real-world experiments on a Franka arm demonstrate 73% success rate with unseen objects, surpassing competitive baselines by 11%. Code will be released after publication.
>
---
#### [replaced 030] Improving AEBS Validation Through Objective Intervention Classification Leveraging the Prediction Divergence Principle
- **分类: cs.RO; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.07872v2](http://arxiv.org/pdf/2507.07872v2)**

> **作者:** Daniel Betschinske; Steven Peters
>
> **备注:** This work has been accepted for publication at the 2025 IEEE International Automated Vehicle Validation Conference (IAVVC)
>
> **摘要:** The safety validation of automatic emergency braking system (AEBS) requires accurately distinguishing between false positive (FP) and true positive (TP) system activations. While simulations allow straightforward differentiation by comparing scenarios with and without interventions, analyzing activations from open-loop resimulations - such as those from field operational testing (FOT) - is more complex. This complexity arises from scenario parameter uncertainty and the influence of driver interventions in the recorded data. Human labeling is frequently used to address these challenges, relying on subjective assessments of intervention necessity or situational criticality, potentially introducing biases and limitations. This work proposes a rule-based classification approach leveraging the Prediction Divergence Principle (PDP) to address those issues. Applied to a simplified AEBS, the proposed method reveals key strengths, limitations, and system requirements for effective implementation. The findings suggest that combining this approach with human labeling may enhance the transparency and consistency of classification, thereby improving the overall validation process. While the rule set for classification derived in this work adopts a conservative approach, the paper outlines future directions for refinement and broader applicability. Finally, this work highlights the potential of such methods to complement existing practices, paving the way for more reliable and reproducible AEBS validation frameworks.
>
---
#### [replaced 031] Joint Travel Route Optimization Framework for Platooning
- **分类: cs.ET; cs.RO; cs.SY; eess.SY**

- **链接: [http://arxiv.org/pdf/2504.07623v2](http://arxiv.org/pdf/2504.07623v2)**

> **作者:** Akif Adas; Stefano Arrigoni; Mattia Brambilla; Monica Barbara Nicoli; Edoardo Sabbioni
>
> **摘要:** Platooning represents an advanced driving technology designed to assist drivers in traffic convoys of varying lengths, enhancing road safety, reducing driver fatigue, and improving fuel efficiency. Sophisticated automated driving assistance systems have facilitated this innovation. Recent advancements in platooning emphasize cooperative mechanisms within both centralized and decentralized architectures enabled by vehicular communication technologies. This study introduces a cooperative route planning optimization framework aimed at promoting the adoption of platooning through a centralized platoon formation strategy at the system level. This approach is envisioned as a transitional phase from individual (ego) driving to fully collaborative driving. Additionally, this research formulates and incorporates travel cost metrics related to fuel consumption, driver fatigue, and travel time, considering regulatory constraints on consecutive driving durations. The performance of these cost metrics has been evaluated using Dijkstra's and A* shortest path algorithms within a network graph framework. The results indicate that the proposed architecture achieves an average cost improvement of 14 % compared to individual route planning for long road trips.
>
---
#### [replaced 032] 3D Active Metric-Semantic SLAM
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2309.06950v4](http://arxiv.org/pdf/2309.06950v4)**

> **作者:** Yuezhan Tao; Xu Liu; Igor Spasojevic; Saurav Agarwal; Vijay Kumar
>
> **摘要:** In this letter, we address the problem of exploration and metric-semantic mapping of multi-floor GPS-denied indoor environments using Size Weight and Power (SWaP) constrained aerial robots. Most previous work in exploration assumes that robot localization is solved. However, neglecting the state uncertainty of the agent can ultimately lead to cascading errors both in the resulting map and in the state of the agent itself. Furthermore, actions that reduce localization errors may be at direct odds with the exploration task. We propose a framework that balances the efficiency of exploration with actions that reduce the state uncertainty of the agent. In particular, our algorithmic approach for active metric-semantic SLAM is built upon sparse information abstracted from raw problem data, to make it suitable for SWaP-constrained robots. Furthermore, we integrate this framework within a fully autonomous aerial robotic system that achieves autonomous exploration in cluttered, 3D environments. From extensive real-world experiments, we showed that by including Semantic Loop Closure (SLC), we can reduce the robot pose estimation errors by over 90% in translation and approximately 75% in yaw, and the uncertainties in pose estimates and semantic maps by over 70% and 65%, respectively. Although discussed in the context of indoor multi-floor exploration, our system can be used for various other applications, such as infrastructure inspection and precision agriculture where reliable GPS data may not be available.
>
---
#### [replaced 033] Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive Survey
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2503.14537v4](http://arxiv.org/pdf/2503.14537v4)**

> **作者:** Liewen Liao; Weihao Yan; Ming Yang; Songan Zhang
>
> **摘要:** Learning-based 3D reconstruction has emerged as a transformative technique in autonomous driving, enabling precise modeling of both dynamic and static environments through advanced neural representations. Despite data augmentation, 3D reconstruction inspires pioneering solution for vital tasks in the field of autonomous driving, such as scene understanding and closed-loop simulation. We investigates the details of 3D reconstruction and conducts a multi-perspective, in-depth analysis of recent advancements. Specifically, we first provide a systematic introduction of preliminaries, including data modalities, benchmarks and technical preliminaries of learning-based 3D reconstruction, facilitating instant identification of suitable methods according to sensor suites. Then, we systematically review learning-based 3D reconstruction methods in autonomous driving, categorizing approaches by subtasks and conducting multi-dimensional analysis and summary to establish a comprehensive technical reference. The development trends and existing challenges are summarized in the context of learning-based 3D reconstruction in autonomous driving. We hope that our review will inspire future researches.
>
---
#### [replaced 034] How to Adapt Control Barrier Functions? A Learning-Based Approach with Applications to a VTOL Quadplane
- **分类: cs.RO; cs.SY; eess.SY**

- **链接: [http://arxiv.org/pdf/2504.03038v2](http://arxiv.org/pdf/2504.03038v2)**

> **作者:** Taekyung Kim; Randal W. Beard; Dimitra Panagou
>
> **备注:** 2025 IEEE Conference on Decision and Control (CDC). Project page: https://www.taekyung.me/how-to-adapt-cbf
>
> **摘要:** In this paper, we present a novel theoretical framework for online adaptation of Control Barrier Function (CBF) parameters, i.e., of the class K functions included in the CBF condition, under input constraints. We introduce the concept of locally validated CBF parameters, which are adapted online to guarantee finite-horizon safety, based on conditions derived from Nagumo's theorem and tangent cone analysis. To identify these parameters online, we integrate a learning-based approach with an uncertainty-aware verification process that account for both epistemic and aleatoric uncertainties inherent in neural network predictions. Our method is demonstrated on a VTOL quadplane model during challenging transition and landing maneuvers, showcasing enhanced performance while maintaining safety.
>
---
#### [replaced 035] ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions
- **分类: cs.RO; cs.AI; cs.SY; eess.IV; eess.SY**

- **链接: [http://arxiv.org/pdf/2409.10283v4](http://arxiv.org/pdf/2409.10283v4)**

> **作者:** Sourav Sanyal; Kaushik Roy
>
> **备注:** Accepted for publication in IEEE Robotics and Automation Letters (RA-L)
>
> **摘要:** In the rapidly evolving field of vision-language navigation (VLN), ensuring safety for physical agents remains an open challenge. For a human-in-the-loop language-operated drone to navigate safely, it must understand natural language commands, perceive the environment, and simultaneously avoid hazards in real time. Control Barrier Functions (CBFs) are formal methods that enforce safe operating conditions. Model Predictive Control (MPC) is an optimization framework that plans a sequence of future actions over a prediction horizon, ensuring smooth trajectory tracking while obeying constraints. In this work, we consider a VLN-operated drone platform and enhance its safety by formulating a novel scene-aware CBF that leverages ego-centric observations from a camera which has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less baseline system uses a Vision-Language Encoder with cross-modal attention to convert commands into an ordered sequence of landmarks. An object detection model identifies and verifies these landmarks in the captured images to generate a planned path. To further enhance safety, an Adaptive Safety Margin Algorithm (ASMA) is proposed. ASMA tracks moving objects and performs scene-aware CBF evaluation on-the-fly, which serves as an additional constraint within the MPC framework. By continuously identifying potentially risky observations, the system performs prediction in real time about unsafe conditions and proactively adjusts its control actions to maintain safe navigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in the Gazebo environment using the Robot Operating System (ROS), ASMA achieves 64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in trajectory lengths compared to the baseline CBF-less VLN.
>
---
#### [replaced 036] ALLO: A Photorealistic Dataset and Data Generation Pipeline for Anomaly Detection During Robotic Proximity Operations in Lunar Orbit
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2409.20435v3](http://arxiv.org/pdf/2409.20435v3)**

> **作者:** Selina Leveugle; Chang Won Lee; Svetlana Stolpner; Chris Langley; Paul Grouchy; Steven Waslander; Jonathan Kelly
>
> **备注:** Submitted to International Conference on Robotics and Automation (ICRA'25), Atlanta, USA, May 19-23, 2025
>
> **摘要:** NASA's forthcoming Lunar Gateway space station, which will be uncrewed most of the time, will need to operate with an unprecedented level of autonomy. Enhancing autonomy on the Gateway presents several unique challenges, one of which is to equip the Canadarm3, the Gateway's external robotic system, with the capability to perform worksite monitoring. Monitoring will involve using the arm's inspection cameras to detect any anomalies within the operating environment, a task complicated by the widely-varying lighting conditions in space. In this paper, we introduce the visual anomaly detection and localization task for space applications and establish a benchmark with our novel synthetic dataset called ALLO (for Anomaly Localization in Lunar Orbit). We develop a complete data generation pipeline to create ALLO, which we use to evaluate the performance of state-of-the-art visual anomaly detection algorithms. Given the low tolerance for risk during space operations and the lack of relevant data, we emphasize the need for novel, robust, and accurate anomaly detection methods to handle the challenging visual conditions found in lunar orbit and beyond.
>
---
#### [replaced 037] Informed Hybrid Zonotope-based Motion Planning Algorithm
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2507.09309v2](http://arxiv.org/pdf/2507.09309v2)**

> **作者:** Peng Xie; Johannes Betz; Amr Alanwar
>
> **摘要:** Optimal path planning in nonconvex free spaces is notoriously challenging, as formulating such problems as mixed-integer linear programs (MILPs) is NP-hard. We propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an alternative approach that decomposes the obstacle-free space and performs low-dimensional face sampling guided by an ellipsotope heuristic, enabling focused exploration along promising transit regions. This structured exploration eliminates the excessive, unreachable sampling that degrades existing informed planners such as AIT* and EIT* in narrow gaps or boxed-goal scenarios. We prove that HZ-MP is probabilistically complete and asymptotically optimal. It converges to near-optimal trajectories in finite time and scales to high-dimensional cluttered scenes.
>
---
#### [replaced 038] Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2403.15333v4](http://arxiv.org/pdf/2403.15333v4)**

> **作者:** Vít Krátký; Giuseppe Silano; Matouš Vrba; Christos Papaioannidis; Ioannis Mademlis; Robert Pěnička; Ioannis Pitas; Martin Saska
>
> **备注:** 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works
>
> **摘要:** This paper presents a formation control approach for contactless gesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor Unmanned Aerial Vehicles (UAVs) and a human worker. The approach is designed to monitor the safety of human workers, particularly those operating at heights. In the proposed dynamic formation scheme, one UAV acts as the formation leader, equipped with sensors for detecting human workers and recognizing gestures. The follower UAVs maintain a predetermined formation relative to the worker's position, providing additional perspectives of the monitored scene. Hand gestures enable the human worker to specify movement and action commands for the UAV team and to initiate other mission-related tasks without requiring additional communication channels or specific markers. Combined with a novel unified human detection and tracking algorithm, a human position estimation method, and a gesture detection pipeline, the proposed approach represents the first instance of an HSI system incorporating all these modules onboard real-world UAVs. Simulations and field experiments involving three UAVs and a human worker in a mock-up scenario demonstrate the effectiveness and responsiveness of the proposed approach.
>
---
#### [replaced 039] Six-DoF Hand-Based Teleoperation for Omnidirectional Aerial Robots
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2506.15009v2](http://arxiv.org/pdf/2506.15009v2)**

> **作者:** Jinjie Li; Jiaxuan Li; Kotaro Kaneko; Haokun Liu; Liming Shu; Moju Zhao
>
> **备注:** 7 pages, 10 figures. This work has been accepted to IROS 2025. The video is released in https://youtu.be/n0IQEnjPzrw?si=Zp3kb3ss-D_AySOE
>
> **摘要:** Omnidirectional aerial robots offer full 6-DoF independent control over position and orientation, making them popular for aerial manipulation. Although advancements in robotic autonomy, human operation remains essential in complex aerial environments. Existing teleoperation approaches for multirotors fail to fully leverage the additional DoFs provided by omnidirectional rotation. Additionally, the dexterity of human fingers should be exploited for more engaged interaction. In this work, we propose an aerial teleoperation system that brings the rotational flexibility of human hands into the unbounded aerial workspace. Our system includes two motion-tracking marker sets--one on the shoulder and one on the hand--along with a data glove to capture hand gestures. Using these inputs, we design four interaction modes for different tasks, including Spherical Mode and Cartesian Mode for long-range moving, Operation Mode for precise manipulation, as well as Locking Mode for temporary pauses, where the hand gestures are utilized for seamless mode switching. We evaluate our system on a vertically mounted valve-turning task in the real world, demonstrating how each mode contributes to effective aerial manipulation. This interaction framework bridges human dexterity with aerial robotics, paving the way for enhanced aerial teleoperation in unstructured environments.
>
---
#### [replaced 040] Learning Granularity-Aware Affordances from Human-Object Interaction for Tool-Based Functional Dexterous Grasping
- **分类: cs.RO; cs.CV; eess.IV**

- **链接: [http://arxiv.org/pdf/2407.00614v2](http://arxiv.org/pdf/2407.00614v2)**

> **作者:** Fan Yang; Wenrui Chen; Kailun Yang; Haoran Lin; Dongsheng Luo; Conghui Tang; Zhiyong Li; Yaonan Wang
>
> **备注:** Accepted to IEEE Transactions on Neural Networks and Learning Systems (TNNLS). The source code and the established dataset are available at https://github.com/yangfan293/GAAF-DEX
>
> **摘要:** To enable robots to use tools, the initial step is teaching robots to employ dexterous gestures for touching specific areas precisely where tasks are performed. Affordance features of objects serve as a bridge in the functional interaction between agents and objects. However, leveraging these affordance cues to help robots achieve functional tool grasping remains unresolved. To address this, we propose a granularity-aware affordance feature extraction method for locating functional affordance areas and predicting dexterous coarse gestures. We study the intrinsic mechanisms of human tool use. On one hand, we use fine-grained affordance features of object-functional finger contact areas to locate functional affordance regions. On the other hand, we use highly activated coarse-grained affordance features in hand-object interaction regions to predict grasp gestures. Additionally, we introduce a model-based post-processing module that transforms affordance localization and gesture prediction into executable robotic actions. This forms GAAF-Dex, a complete framework that learns Granularity-Aware Affordances from human-object interaction to enable tool-based functional grasping with dexterous hands. Unlike fully-supervised methods that require extensive data annotation, we employ a weakly supervised approach to extract relevant cues from exocentric (Exo) images of hand-object interactions to supervise feature extraction in egocentric (Ego) images. To support this approach, we have constructed a small-scale dataset, Functional Affordance Hand-object Interaction Dataset (FAH), which includes nearly 6K images of functional hand-object interaction Exo images and Ego images. Extensive experiments on the dataset demonstrate that our method outperforms state-of-the-art methods. The source code and the established dataset are available at https://github.com/yangfan293/GAAF-DEX.
>
---
#### [replaced 041] Advancing Object Goal Navigation Through LLM-enhanced Object Affinities Transfer
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2403.09971v3](http://arxiv.org/pdf/2403.09971v3)**

> **作者:** Mengying Lin; Shugao Liu; Dingxi Zhang; Yaran Chen; Zhaoran Wang; Haoran Li; Dongbin Zhao
>
> **摘要:** Object-goal navigation requires mobile robots to efficiently locate targets with visual and spatial information, yet existing methods struggle with generalization in unseen environments. Heuristic approaches with naive metrics fail in complex layouts, while graph-based and learning-based methods suffer from environmental biases and limited generalization. Although Large Language Models (LLMs) as planners or agents offer a rich knowledge base, they are cost-inefficient and lack targeted historical experience. To address these challenges, we propose the LLM-enhanced Object Affinities Transfer (LOAT) framework, integrating LLM-derived semantics with learning-based approaches to leverage experiential object affinities for better generalization in unseen settings. LOAT employs a dual-module strategy: one module accesses LLMs' vast knowledge, and the other applies learned object semantic relationships, dynamically fusing these sources based on context. Evaluations in AI2-THOR and Habitat simulators show significant improvements in navigation success and efficiency, and real-world deployment demonstrates the zero-shot ability of LOAT to enhance object-goal navigation systems.
>
---
#### [replaced 042] Why Automate This? Exploring the Connection between Time Use, Well-being and Robot Automation Across Social Groups
- **分类: cs.HC; cs.RO**

- **链接: [http://arxiv.org/pdf/2501.06348v2](http://arxiv.org/pdf/2501.06348v2)**

> **作者:** Ruchira Ray; Leona Pang; Sanjana Srivastava; Li Fei-Fei; Samantha Shorey; Roberto Martín-Martín
>
> **备注:** 20 pages, 14 figures
>
> **摘要:** Understanding the motivations underlying the human inclination to automate tasks is vital to developing truly helpful robots integrated into daily life. Accordingly, we ask: are individuals more inclined to automate chores based on the time they consume or the feelings experienced while performing them? This study explores these preferences and whether they vary across different social groups (i.e., gender category and income level). Leveraging data from the BEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use Survey Well-Being Module, we investigate the relationship between the desire for automation, time spent on daily activities, and their associated feelings - Happiness, Meaningfulness, Sadness, Painfulness, Stressfulness, or Tiredness. Our key findings show that, despite common assumptions, time spent does not strongly relate to the desire for automation for the general population. For the feelings analyzed, only happiness and pain are key indicators. Significant differences by gender and economic level also emerged: Women prefer to automate stressful activities, whereas men prefer to automate those that make them unhappy; mid-income individuals prioritize automating less enjoyable and meaningful activities, while low and high-income show no significant correlations. We hope our research helps motivate technologies to develop robots that match the priorities of potential users, moving domestic robotics toward more socially relevant solutions. We open-source all the data, including an online tool that enables the community to replicate our analysis and explore additional trends at https://hri1260.github.io/why-automate-this.
>
---
#### [replaced 043] To Lead or to Follow? Adaptive Robot Task Planning in Human-Robot Collaboration
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2401.01483v2](http://arxiv.org/pdf/2401.01483v2)**

> **作者:** Ali Noormohammadi-Asl; Stephen L. Smith; Kerstin Dautenhahn
>
> **摘要:** Adaptive task planning is fundamental to ensuring effective and seamless human-robot collaboration. This paper introduces a robot task planning framework that takes into account both human leading/following preferences and performance, specifically focusing on task allocation and scheduling in collaborative settings. We present a proactive task allocation approach with three primary objectives: enhancing team performance, incorporating human preferences, and upholding a positive human perception of the robot and the collaborative experience. Through a user study, involving an autonomous mobile manipulator robot working alongside participants in a collaborative scenario, we confirm that the task planning framework successfully attains all three intended goals, thereby contributing to the advancement of adaptive task planning in human-robot collaboration. This paper mainly focuses on the first two objectives, and we discuss the third objective, participants' perception of the robot, tasks, and collaboration in a companion paper.
>
---
#### [replaced 044] Active Probing with Multimodal Predictions for Motion Planning
- **分类: cs.RO; cs.SY; eess.SY**

- **链接: [http://arxiv.org/pdf/2507.09822v3](http://arxiv.org/pdf/2507.09822v3)**

> **作者:** Darshan Gadginmath; Farhad Nawaz; Minjun Sung; Faizan M Tariq; Sangjae Bae; David Isele; Fabio Pasqualetti; Jovin D'sa
>
> **备注:** To appear at IROS '25. 8 pages. 3 tables. 6 figures. Project page: https://darshangm.github.io/papers/active-probing-multimodal-predictions/
>
> **摘要:** Navigation in dynamic environments requires autonomous systems to reason about uncertainties in the behavior of other agents. In this paper, we introduce a unified framework that combines trajectory planning with multimodal predictions and active probing to enhance decision-making under uncertainty. We develop a novel risk metric that seamlessly integrates multimodal prediction uncertainties through mixture models. When these uncertainties follow a Gaussian mixture distribution, we prove that our risk metric admits a closed-form solution, and is always finite, thus ensuring analytical tractability. To reduce prediction ambiguity, we incorporate an active probing mechanism that strategically selects actions to improve its estimates of behavioral parameters of other agents, while simultaneously handling multimodal uncertainties. We extensively evaluate our framework in autonomous navigation scenarios using the MetaDrive simulation environment. Results demonstrate that our active probing approach successfully navigates complex traffic scenarios with uncertain predictions. Additionally, our framework shows robust performance across diverse traffic agent behavior models, indicating its broad applicability to real-world autonomous navigation challenges. Code and videos are available at https://darshangm.github.io/papers/active-probing-multimodal-predictions/.
>
---
#### [replaced 045] How Cars Move: Analyzing Driving Dynamics for Safer Urban Traffic
- **分类: cs.CV; cs.PF; cs.RO**

- **链接: [http://arxiv.org/pdf/2412.04020v3](http://arxiv.org/pdf/2412.04020v3)**

> **作者:** Kangan Qian; Jinyu Miao; Xinyu Jiao; Ziang Luo; Zheng Fu; Yining Shi; Yunlong Wang; Kun Jiang; Diange Yang
>
> **备注:** 8 pages, 5 figures
>
> **摘要:** Understanding the spatial dynamics of cars within urban systems is essential for optimizing infrastructure management and resource allocation. Recent empirical approaches for analyzing traffic patterns have gained traction due to their applicability to city-scale policy development. However, conventional methodologies often rely on fragmented grid-based techniques, which may overlook critical interdependencies among spatial elements and temporal continuity. These limitations can compromise analytical effectiveness in complex urban environments. To address these challenges, we propose PriorMotion, a data integration framework designed to systematically uncover movement patterns through driving dynamics analysis. Our approach combines multi-scale empirical observations with customized analytical tools to capture evolving spatial-temporal trends in urban traffic. Comprehensive evaluations demonstrate that PriorMotion significantly enhances analytical outcomes, including increased accuracy in traffic pattern analysis, improved adaptability to heterogeneous data environments, and reduced long-term projection errors. Validation confirms its effectiveness for urban infrastructure management applications requiring precise characterization of complex spatial-temporal interactions.
>
---
