# 计算机视觉 cs.CV

- **最新发布 86 篇**

- **更新 69 篇**

## 最新发布

#### [new 001] CATCH: A Modular Cross-domain Adaptive Template with Hook
- **分类: cs.CV**

- **简介: 该论文针对视觉问答（VQA）模型在跨域场景下泛化能力差的问题，提出CATCH框架。通过引入轻量级领域分类器与双适配器模块，实现视觉与语言的解耦适应，利用统一钩子接口动态注入，无需重训练主干模型。实验表明，该方法在多个跨域VQA任务上显著提升性能，具备良好可扩展性。**

- **链接: [http://arxiv.org/pdf/2510.26582v1](http://arxiv.org/pdf/2510.26582v1)**

> **作者:** Xinjin Li; Yulie Lu; Jinghan Cao; Yu Ma; Zhenglin Li; Yeyang Zhou
>
> **摘要:** Recent advances in Visual Question Answering (VQA) have demonstrated impressive performance in natural image domains, with models like LLaVA leveraging large language models (LLMs) for open-ended reasoning. However, their generalization degrades significantly when transferred to out-of-domain scenarios such as remote sensing, medical imaging, or math diagrams, due to large distributional shifts and the lack of effective domain adaptation mechanisms. Existing approaches typically rely on per-domain fine-tuning or bespoke pipelines, which are costly, inflexible, and not scalable across diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for cross-domain adaptation that improves the generalization of VQA models while requiring minimal changes to their core architecture. Our key idea is to decouple visual and linguistic adaptation by introducing two lightweight modules: a domain classifier to identify the input image type, and a dual adapter mechanism comprising a Prompt Adapter for language modulation and a Visual Adapter for vision feature adjustment. Both modules are dynamically injected via a unified hook interface, requiring no retraining of the backbone model. Experimental results across four domain-specific VQA benchmarks demonstrate that our framework achieves consistent performance gains without retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH provides a scalable and extensible approach to multi-domain VQA, enabling practical deployment across diverse application domains.
>
---
#### [new 002] The Quest for Generalizable Motion Generation: Data, Model, and Evaluation
- **分类: cs.CV**

- **简介: 该论文聚焦3D人体动作生成任务，针对现有模型泛化能力不足的问题，提出融合视频生成知识的框架。构建大规模多模态数据集ViMoGen-228K，设计基于流匹配的扩散变换器模型ViMoGen及轻量版ViMoGen-light，并建立层次化评估基准MBench，显著提升生成质量与泛化性能。**

- **链接: [http://arxiv.org/pdf/2510.26794v1](http://arxiv.org/pdf/2510.26794v1)**

> **作者:** Jing Lin; Ruisi Wang; Junzhe Lu; Ziqi Huang; Guorui Song; Ailing Zeng; Xian Liu; Chen Wei; Wanqi Yin; Qingping Sun; Zhongang Cai; Lei Yang; Ziwei Liu
>
> **摘要:** Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.
>
---
#### [new 003] GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?
- **分类: cs.CV; cs.AI**

- **简介: 该论文针对图像超分辨率任务，解决现有方法忽视场景文本可读性的问题。提出GLYPH-SR框架，通过视觉-语言引导的扩散模型与文本-场景交替调度，联合优化文本可读性与视觉质量，在多个数据集上显著提升OCR性能，同时保持高感知质量。**

- **链接: [http://arxiv.org/pdf/2510.26339v1](http://arxiv.org/pdf/2510.26339v1)**

> **作者:** Mingyu Sung; Seungjae Ham; Kangwoo Kim; Yeokyoung Yoon; Sangseok Yun; Il-Min Kim; Jae-Mo Kang
>
> **备注:** 11 pages, 6 figures. Includes supplementary material. Under review as a conference paper at ICLR 2026
>
> **摘要:** Image super-resolution(SR) is fundamental to many vision system-from surveillance and autonomy to document analysis and retail analytics-because recovering high-frequency details, especially scene-text, enables reliable downstream perception. Scene-text, i.e., text embedded in natural images such as signs, product labels, and storefronts, often carries the most actionable information; when characters are blurred or hallucinated, optical character recognition(OCR) and subsequent decisions fail even if the rest of the image appears sharp. Yet previous SR research has often been tuned to distortion (PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that are largely insensitive to character-level errors. Furthermore, studies that do address text SR often focus on simplified benchmarks with isolated characters, overlooking the challenges of text within complex natural scenes. As a result, scene-text is effectively treated as generic texture. For SR to be effective in practical deployments, it is therefore essential to explicitly optimize for both text legibility and perceptual quality. We present GLYPH-SR, a vision-language-guided diffusion framework that aims to achieve both objectives jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by OCR data, and a ping-pong scheduler that alternates between text- and scene-centric guidance. To enable targeted text restoration, we train these components on a synthetic corpus while keeping the main SR branch frozen. Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR) while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed to satisfy both objectives simultaneously-high readability and high visual realism-delivering SR that looks right and reds right.
>
---
#### [new 004] Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management
- **分类: cs.CV**

- **简介: 该论文提出一种多任务集成几何深度网络（Ch-EGN），用于供应链可持续性与风险管理。针对供应链中风险预测、产品分类与关系识别问题，融合几何与卷积深度学习，提升数据依赖建模能力。在两个数据集上实现高精度预测，显著优于现有方法。**

- **链接: [http://arxiv.org/pdf/2510.26203v1](http://arxiv.org/pdf/2510.26203v1)**

> **作者:** Mehdi Khaleghi; Nastaran Khaleghi; Sobhan Sheykhivand; Sebelan Danishvar
>
> **摘要:** The sustainability of supply chain plays a key role in achieving optimal performance in controlling the supply chain. The management of risks that occur in a supply chain is a fundamental problem for the purpose of developing the sustainability of the network and elevating the performance efficiency of the supply chain. The correct classification of products is another essential element in a sustainable supply chain. Acknowledging recent breakthroughs in the context of deep networks, several architectural options have been deployed to analyze supply chain datasets. A novel geometric deep network is used to propose an ensemble deep network. The proposed Chebyshev ensemble geometric network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This network is proposed to leverage the information dependencies in supply chain to derive invisible states of samples in the database. The functionality of the proposed deep network is assessed on the two different databases. The SupplyGraph Dataset and DataCo are considered in this research. The prediction of delivery status of DataCo supply chain is done for risk administration. The product classification and edge classification are performed using the SupplyGraph database to enhance the sustainability of the supply network. An average accuracy of 98.95% is obtained for the ensemble network for risk management. The average accuracy of 100% and 98.07% are obtained for sustainable supply chain in terms of 5 product group classification and 4 product relation classification, respectively. The average accuracy of 92.37% is attained for 25 company relation classification. The results confirm an average improvement and efficiency of the proposed method compared to the state-of-the-art approaches.
>
---
#### [new 005] Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement
- **分类: cs.CV**

- **简介: 该论文针对低光图像增强任务，提出通过提升扫描模式的豪斯多夫维数来优化Mamba框架。引入希尔伯特选择性扫描机制，增强特征空间探索能力，更好捕捉细节与局部关联，同时保持长程依赖建模。实验表明，该方法在提升图像质量的同时降低计算开销和推理时间。**

- **链接: [http://arxiv.org/pdf/2510.26001v1](http://arxiv.org/pdf/2510.26001v1)**

> **作者:** Xinhua Wang; Caibo Feng; Xiangjun Fu; Chunxiao Liu
>
> **摘要:** We propose an innovative enhancement to the Mamba framework by increasing the Hausdorff dimension of its scanning pattern through a novel Hilbert Selective Scan mechanism. This mechanism explores the feature space more effectively, capturing intricate fine-scale details and improving overall coverage. As a result, it mitigates information inconsistencies while refining spatial locality to better capture subtle local interactions without sacrificing the model's ability to handle long-range dependencies. Extensive experiments on publicly available benchmarks demonstrate that our approach significantly improves both the quantitative metrics and qualitative visual fidelity of existing Mamba-based low-light image enhancement methods, all while reducing computational resource consumption and shortening inference time. We believe that this refined strategy not only advances the state-of-the-art in low-light image enhancement but also holds promise for broader applications in fields that leverage Mamba-based techniques.
>
---
#### [new 006] Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm
- **分类: cs.CV**

- **简介: 该论文针对图像边缘检测任务，提出一种基于二维细胞自动机并用粒子群优化的自适应检测器。旨在解决传统检测器难以识别松散边缘及缺乏上下文适应性的问题。研究通过扩展优化搜索空间和引入迁移学习提升性能，但实验表明二者均未显著改善效果，模型仍具备良好自适应能力。**

- **链接: [http://arxiv.org/pdf/2510.26509v1](http://arxiv.org/pdf/2510.26509v1)**

> **作者:** Vinícius Ferraria; Eurico Ruivo
>
> **摘要:** The edge detection task is essential in image processing aiming to extract relevant information from an image. One recurring problem in this task is the weaknesses found in some detectors, such as the difficulty in detecting loose edges and the lack of context to extract relevant information from specific problems. To address these weaknesses and adapt the detector to the properties of an image, an adaptable detector described by two-dimensional cellular automaton and optimized by meta-heuristic combined with transfer learning techniques was developed. This study aims to analyze the impact of expanding the search space of the optimization phase and the robustness of the adaptability of the detector in identifying edges of a set of natural images and specialized subsets extracted from the same image set. The results obtained prove that expanding the search space of the optimization phase was not effective for the chosen image set. The study also analyzed the adaptability of the model through a series of experiments and validation techniques and found that, regardless of the validation, the model was able to adapt to the input and the transfer learning techniques applied to the model showed no significant improvements.
>
---
#### [new 007] Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning
- **分类: cs.CV; cs.AI**

- **简介: 该论文针对海岸城市洪涝预测任务，解决传统模型计算成本高、数据稀缺等问题。提出一种轻量级CNN模型，融合海平面上升与岸线适应情景，实现高效精准的洪水深度预测，并在阿布扎比和旧金山数据上验证其跨区域泛化能力，显著降低预测误差。**

- **链接: [http://arxiv.org/pdf/2510.26017v1](http://arxiv.org/pdf/2510.26017v1)**

> **作者:** Bilal Hassan; Areg Karapetyan; Aaron Chung Hin Chow; Samer Madanat
>
> **备注:** Submitted to Hydrology and Earth System Sciences
>
> **摘要:** Climate change and sea-level rise (SLR) pose escalating threats to coastal cities, intensifying the need for efficient and accurate methods to predict potential flood hazards. Traditional physics-based hydrodynamic simulators, although precise, are computationally expensive and impractical for city-scale coastal planning applications. Deep Learning (DL) techniques offer promising alternatives, however, they are often constrained by challenges such as data scarcity and high-dimensional output requirements. Leveraging a recently proposed vision-based, low-resource DL framework, we develop a novel, lightweight Convolutional Neural Network (CNN)-based model designed to predict coastal flooding under variable SLR projections and shoreline adaptation scenarios. Furthermore, we demonstrate the ability of the model to generalize across diverse geographical contexts by utilizing datasets from two distinct regions: Abu Dhabi and San Francisco. Our findings demonstrate that the proposed model significantly outperforms state-of-the-art methods, reducing the mean absolute error (MAE) in predicted flood depth maps on average by nearly 20%. These results highlight the potential of our approach to serve as a scalable and practical tool for coastal flood management, empowering decision-makers to develop effective mitigation strategies in response to the growing impacts of climate change. Project Page: https://caspiannet.github.io/
>
---
#### [new 008] SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing
- **分类: cs.CV**

- **简介: 该论文针对文本到图像编辑中因反演不准确和梯度纠缠导致的语义失真问题，提出SplitFlow框架。通过语义分解目标提示、独立计算流并软聚合，实现无反演编辑。设计投影与加权机制，平衡多样性与一致性，提升编辑质量与属性解耦能力。**

- **链接: [http://arxiv.org/pdf/2510.25970v1](http://arxiv.org/pdf/2510.25970v1)**

> **作者:** Sung-Hoon Yoon; Minghan Li; Gaspard Beaudouin; Congcong Wen; Muhammad Rafay Azhar; Mengyu Wang
>
> **备注:** Camera-ready version for NeurIPS 2025, 10 pages (main paper)
>
> **摘要:** Rectified flow models have become a de facto standard in image generation due to their stable sampling trajectories and high-fidelity outputs. Despite their strong generative capabilities, they face critical limitations in image editing tasks: inaccurate inversion processes for mapping real images back into the latent space, and gradient entanglement issues during editing often result in outputs that do not faithfully reflect the target prompt. Recent efforts have attempted to directly map source and target distributions via ODE-based approaches without inversion; however,these methods still yield suboptimal editing quality. In this work, we propose a flow decomposition-and-aggregation framework built upon an inversion-free formulation to address these limitations. Specifically, we semantically decompose the target prompt into multiple sub-prompts, compute an independent flow for each, and aggregate them to form a unified editing trajectory. While we empirically observe that decomposing the original flow enhances diversity in the target space, generating semantically aligned outputs still requires consistent guidance toward the full target prompt. To this end, we design a projection and soft-aggregation mechanism for flow, inspired by gradient conflict resolution in multi-task learning. This approach adaptively weights the sub-target velocity fields, suppressing semantic redundancy while emphasizing distinct directions, thereby preserving both diversity and consistency in the final edited output. Experimental results demonstrate that our method outperforms existing zero-shot editing approaches in terms of semantic fidelity and attribute disentanglement. The code is available at https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.
>
---
#### [new 009] BikeScenes: Online LiDAR Semantic Segmentation for Bicycles
- **分类: cs.CV; cs.RO**

- **简介: 该论文针对自行车骑行者安全，提出基于LiDAR的在线语义分割方法BikeScenes。为解决汽车感知技术向自行车场景迁移的域差距问题，构建了包含3021帧标注数据的BikeScenes-lidarseg数据集，并验证了领域特定微调的有效性，显著提升分割精度。**

- **链接: [http://arxiv.org/pdf/2510.25901v1](http://arxiv.org/pdf/2510.25901v1)**

> **作者:** Denniz Goren; Holger Caesar
>
> **摘要:** The vulnerability of cyclists, exacerbated by the rising popularity of faster e-bikes, motivates adapting automotive perception technologies for bicycle safety. We use our multi-sensor 'SenseBike' research platform to develop and evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg Dataset, comprising 3021 consecutive LiDAR scans around the university campus of the TU Delft, semantically annotated for 29 dynamic and static classes. By evaluating model performance, we demonstrate that fine-tuning on our BikeScenes dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This result underscores the necessity and effectiveness of domain-specific training. We highlight key challenges specific to bicycle-mounted, hardware-constrained perception systems and contribute the BikeScenes dataset as a resource for advancing research in cyclist-centric LiDAR segmentation.
>
---
#### [new 010] Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill
- **分类: cs.CV; cs.AI**

- **简介: 该论文提出一种集成计算机视觉的实时故障预测系统，用于钢铁轧制生产线。通过工业相机与深度学习模型结合，实时分析设备运行与钢坯运动状态，融合传感器数据实现故障早期预警，降低非计划停机成本，提升生产可靠性与效率。**

- **链接: [http://arxiv.org/pdf/2510.26684v1](http://arxiv.org/pdf/2510.26684v1)**

> **作者:** Vaibhav Kurrey; Sivakalyan Pujari; Gagan Raj Gupta
>
> **摘要:** We present a long-term deployment study of a machine vision-based anomaly detection system for failure prediction in a steel rolling mill. The system integrates industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time along the process line. Live video streams are processed on a centralized video server using deep learning models, enabling early prediction of equipment failures and process interruptions, thereby reducing unplanned breakdown costs. Server-based inference minimizes the computational load on industrial process control systems (PLCs), supporting scalable deployment across production lines with minimal additional resources. By jointly analyzing sensor data from data acquisition systems and visual inputs, the system identifies the location and probable root causes of failures, providing actionable insights for proactive maintenance. This integrated approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments.
>
---
#### [new 011] Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks
- **分类: cs.CV; cs.CL; cs.RO**

- **简介: 该论文针对水下目标检测任务，解决动态环境中物体识别精度低的问题。通过引入时空建模与空间注意力机制，改进YOLOv5，提出T-YOLOv5及融合CBAM的版本，显著提升复杂场景下的检测准确率，尤其在运动、遮挡条件下表现优异。**

- **链接: [http://arxiv.org/pdf/2510.25797v1](http://arxiv.org/pdf/2510.25797v1)**

> **作者:** Sai Likhith Karri; Ansh Saxena
>
> **摘要:** This study examines the effectiveness of spatio-temporal modeling and the integration of spatial attention mechanisms in deep learning models for underwater object detection. Specifically, in the first phase, the performance of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is developed, through the addition of a Convolutional Block Attention Module (CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the research highlights how temporal modeling improves detection accuracy in dynamic marine environments, particularly under conditions of sudden movements, partial occlusions, and gradual motion. The testing results showed that YOLOv5 achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively, highlighting their superior accuracy and generalization in detecting complex objects. The findings demonstrate that T-YOLOv5 significantly enhances detection reliability compared to the standard model, while T-YOLOv5 with CBAM further improves performance in challenging scenarios, although there is a loss of accuracy when it comes to simpler scenarios.
>
---
#### [new 012] PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus
- **分类: cs.CV**

- **简介: 该论文针对无人机图像中小目标检测难题，提出PT-DETR模型。通过引入PADF模块增强小目标特征提取，设计MFFF模块融合多尺度信息，并采用Focaler-SIoU提升定位精度。相比RT-DETR，显著提升检测性能且参数更少。**

- **链接: [http://arxiv.org/pdf/2510.26630v1](http://arxiv.org/pdf/2510.26630v1)**

> **作者:** Bingcong Huo; Zhiming Wang
>
> **摘要:** To address the challenges in UAV object detection, such as complex backgrounds, severe occlusion, dense small objects, and varying lighting conditions,this paper proposes PT-DETR based on RT-DETR, a novel detection algorithm specifically designed for small objects in UAV imagery. In the backbone network, we introduce the Partially-Aware Detail Focus (PADF) Module to enhance feature extraction for small objects. Additionally,we design the Median-Frequency Feature Fusion (MFFF) module,which effectively improves the model's ability to capture small-object details and contextual information. Furthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box matching capability and increase its sensitivity to small-object features, thereby further enhancing detection accuracy and robustness. Compared with RT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the VisDrone2019 dataset with lower computational complexity and fewer parameters, demonstrating its robustness and feasibility for small-object detection tasks.
>
---
#### [new 013] MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models
- **分类: cs.CV**

- **简介: 该论文提出MoTDiff，首个基于扩散模型的高分辨率运动轨迹估计方法。针对单张模糊图像中运动信息低质、粗粒度的问题，利用多尺度特征条件与新型训练策略，精准恢复细粒度、连贯的高分辨率运动轨迹，在去模糊与编码曝光任务中优于现有方法。**

- **链接: [http://arxiv.org/pdf/2510.26173v1](http://arxiv.org/pdf/2510.26173v1)**

> **作者:** Wontae Choi; Jaelin Lee; Hyung Sup Yun; Byeungwoo Jeon; Il Yong Chun
>
> **备注:** 10 pages, 6 figures
>
> **摘要:** Accurate estimation of motion information is crucial in diverse computational imaging and computer vision applications. Researchers have investigated various methods to extract motion information from a single blurred image, including blur kernels and optical flow. However, existing motion representations are often of low quality, i.e., coarse-grained and inaccurate. In this paper, we propose the first high-resolution (HR) Motion Trajectory estimation framework using Diffusion models (MoTDiff). Different from existing motion representations, we aim to estimate an HR motion trajectory with high-quality from a single motion-blurred image. The proposed MoTDiff consists of two key components: 1) a new conditional diffusion framework that uses multi-scale feature maps extracted from a single blurred image as a condition, and 2) a new training method that can promote precise identification of a fine-grained motion trajectory, consistent estimation of overall shape and position of a motion path, and pixel connectivity along a motion trajectory. Our experiments demonstrate that the proposed MoTDiff can outperform state-of-the-art methods in both blind image deblurring and coded exposure photography applications.
>
---
#### [new 014] OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation
- **分类: cs.CV**

- **简介: 该论文聚焦文档布局生成任务，针对现有数据集单一、模型难以处理复杂长序列的问题，构建了百万级多类型文档布局数据集OmniLayout-1M，并提出两阶段粗到细学习的0.5B模型OmniLayout-LLM，显著提升跨领域布局生成性能。**

- **链接: [http://arxiv.org/pdf/2510.26213v1](http://arxiv.org/pdf/2510.26213v1)**

> **作者:** Hengrui Kang; Zhuangcheng Gu; Zhiyuan Zhao; Zichen Wen; Bin Wang; Weijia Li; Conghui He
>
> **备注:** TL;DR: With OmniLayout-1M dataset and LLM-based coarse-to-fine learning, we enable universal and diverse document layout generation
>
> **摘要:** Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released.
>
---
#### [new 015] FullPart: Generating each 3D Part at Full Resolution
- **分类: cs.CV**

- **简介: 该论文提出FullPart，用于3D部件生成任务。针对现有方法在细节表达和小部件质量上的不足，提出结合隐式与显式表示的框架：先用隐式扩散生成部件边界框，再为每个部件独立使用全分辨率体素网格生成细节，并引入中心点编码保证全局一致性。同时构建了大规模标注数据集PartVerse-XL。**

- **链接: [http://arxiv.org/pdf/2510.26140v1](http://arxiv.org/pdf/2510.26140v1)**

> **作者:** Lihe Ding; Shaocong Dong; Yaokun Li; Chenjian Gao; Xiao Chen; Rui Han; Yihao Kuang; Hong Zhang; Bo Huang; Zhanpeng Huang; Zibin Wang; Dan Xu; Tianfan Xue
>
> **备注:** Project page: https://fullpart3d.github.io
>
> **摘要:** Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among all parts; this often causes small parts to occupy too few voxels, leading to degraded quality. In this paper, we propose FullPart, a novel framework that combines both implicit and explicit paradigms. It first derives the bounding box layout through an implicit box vector-set diffusion process, a task that implicit diffusion handles effectively since box tokens contain little geometric detail. Then, it generates detailed parts, each within its own fixed full-resolution voxel grid. Instead of sharing a global low-resolution space, each part in our method - even small ones - is generated at full resolution, enabling the synthesis of intricate details. We further introduce a center-point encoding strategy to address the misalignment issue when exchanging information between parts of different actual sizes, thereby maintaining global coherence. Moreover, to tackle the scarcity of reliable part data, we present PartVerse-XL, the largest human-annotated 3D part dataset to date with 40K objects and 320K parts. Extensive experiments demonstrate that FullPart achieves state-of-the-art results in 3D part generation. We will release all code, data, and model to benefit future research in 3D part generation.
>
---
#### [new 016] All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles
- **分类: cs.CV**

- **简介: 该论文聚焦自动驾驶中物体检测任务，针对多模态感知与智能融合的碎片化问题，系统梳理传感器与数据集，综述基于Transformer、VLM/LLM的前沿检测方法，提出融合未来技术的发展路线。**

- **链接: [http://arxiv.org/pdf/2510.26641v1](http://arxiv.org/pdf/2510.26641v1)**

> **作者:** Sayed Pedram Haeri Boroujeni; Niloufar Mehrabi; Hazim Alzorgan; Ahmad Sarlak; Mahlagha Fazeli; Abolfazl Razi
>
> **摘要:** Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.
>
---
#### [new 017] Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2
- **分类: cs.CV; physics.geo-ph**

- **简介: 该论文聚焦于北极海冰漂移估算任务，旨在提升基于雷达卫星影像的运动估计精度。针对传统光学流方法局限，首次在RADARSAT-2数据上评估48种深度学习光学流模型，利用浮标数据验证，实现亚千米级精度，生成连续漂移场，推动了深度学习在极地遥感中的应用。**

- **链接: [http://arxiv.org/pdf/2510.26653v1](http://arxiv.org/pdf/2510.26653v1)**

> **作者:** Daniela Martin; Joseph Gallego
>
> **摘要:** Accurate estimation of sea ice drift is critical for Arctic navigation, climate research, and operational forecasting. While optical flow, a computer vision technique for estimating pixel wise motion between consecutive images, has advanced rapidly in computer vision, its applicability to geophysical problems and to satellite SAR imagery remains underexplored. Classical optical flow methods rely on mathematical models and strong assumptions about motion, which limit their accuracy in complex scenarios. Recent deep learning based approaches have substantially improved performance and are now the standard in computer vision, motivating their application to sea ice drift estimation. We present the first large scale benchmark of 48 deep learning optical flow models on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the spatial scales of sea ice motion and typical navigation requirements in the Arctic. Our results demonstrate that the models are capable of capturing consistent regional drift patterns and that recent deep learning based optical flow methods, which have substantially improved motion estimation accuracy compared to classical methods, can be effectively transferred to polar remote sensing. Optical flow produces spatially continuous drift fields, providing motion estimates for every image pixel rather than at sparse buoy locations, offering new opportunities for navigation and climate modeling.
>
---
#### [new 018] EgoExo-Con: Exploring View-Invariant Video Temporal Understanding
- **分类: cs.CV; cs.AI**

- **简介: 该论文聚焦视频大模型在多视角下的时间理解一致性问题。针对现有模型跨视角一致性差、联合训练性能下降的问题，提出EgoExo-Con基准与View-GRPO强化学习框架，通过同步第一/第三人称视频对提升模型跨视角时序理解的一致性。**

- **链接: [http://arxiv.org/pdf/2510.26113v1](http://arxiv.org/pdf/2510.26113v1)**

> **作者:** Minjoon Jung; Junbin Xiao; Junghyun Kim; Byoung-Tak Zhang; Angela Yao
>
> **备注:** project page: \url{https://minjoong507.github.io/projects/EgoExo-Con/}
>
> **摘要:** Can Video-LLMs achieve consistent temporal understanding when videos capture the same event from different viewpoints? To study this, we introduce EgoExo-Con (Consistency), a benchmark of comprehensively synchronized egocentric and exocentric video pairs with human-refined queries in natural language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal Verification and Temporal Grounding. It evaluates not only correctness but consistency across viewpoints. Our analysis reveals two critical limitations of existing Video-LLMs: (1) models often fail to maintain consistency, with results far worse than their single-view performances. (2) When naively finetuned with synchronized videos of both viewpoints, the models show improved consistency but often underperform those trained on a single view. For improvements, we propose View-GRPO, a novel reinforcement learning framework that effectively strengthens view-specific temporal reasoning while encouraging consistent comprehension across viewpoints. Our method demonstrates its superiority over naive SFT and GRPO, especially for improving cross-view consistency. All resources will be made publicly available.
>
---
#### [new 019] AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping
- **分类: cs.CV; cs.IR; cs.MM; 68T05; I.4.0; H.3.1; I.2.10; K.4.4**

- **简介: 该论文提出AdSum框架，解决广告视频自动剪辑任务。针对广告中音频关键性，设计双流音视频融合模型，预测帧重要性以生成短版广告。构建了首个广告专用数据集AdSum204，实验证明其优于现有方法。**

- **链接: [http://arxiv.org/pdf/2510.26569v1](http://arxiv.org/pdf/2510.26569v1)**

> **作者:** Wen Xie; Yanjun Zhu; Gijs Overgoor; Yakov Bart; Agata Lapedriza Garcia; Sarah Ostadabbas
>
> **备注:** Accepted at 32nd International Conference on MultiMedia Modeling
>
> **摘要:** Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall.
>
---
#### [new 020] WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios
- **分类: cs.CV; cs.AI**

- **简介: 该论文针对端到端自动驾驶在长尾罕见场景下的评估难题，提出WOD-E2E数据集与Rater Feedback Score（RFS）评价指标。数据集包含4021段挑战性长尾驾驶片段，结合多视角图像与轨迹信息；RFS通过人工偏好标签衡量预测轨迹质量，更契合真实驾驶的多模态决策需求，推动鲁棒、安全的自动驾驶系统研究。**

- **链接: [http://arxiv.org/pdf/2510.26125v1](http://arxiv.org/pdf/2510.26125v1)**

> **作者:** Runsheng Xu; Hubert Lin; Wonseok Jeon; Hao Feng; Yuliang Zou; Liting Sun; John Gorman; Kate Tolstaya; Sarah Tang; Brandyn White; Ben Sapp; Mingxing Tan; Jyh-Jing Hwang; Drago Anguelov
>
> **摘要:** Vision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.
>
---
#### [new 021] OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research
- **分类: cs.CV**

- **简介: 该论文提出OracleAgent，一个用于甲骨文研究的多模态推理智能体。针对甲骨文研究中信息处理流程复杂、检索效率低的问题，构建了包含140万字形拓片与8万条释文的领域知识库，并集成多工具协同系统，实现高效检索与分析，显著提升研究效率。**

- **链接: [http://arxiv.org/pdf/2510.26114v1](http://arxiv.org/pdf/2510.26114v1)**

> **作者:** Caoshuo Li; Zengmao Ding; Xiaobin Hu; Bang Li; Donghao Luo; Xu Peng; Taisong Jin; Yongge Liu; Shengwei Han; Jing Yang; Xiaoping He; Feng Gao; AndyPian Wu; SevenShu; Chaoyang Wang; Chengjie Wang
>
> **摘要:** As one of the earliest writing systems, Oracle Bone Script (OBS) preserves the cultural and intellectual heritage of ancient civilizations. However, current OBS research faces two major challenges: (1) the interpretation of OBS involves a complex workflow comprising multiple serial and parallel sub-tasks, and (2) the efficiency of OBS information organization and retrieval remains a critical bottleneck, as scholars often spend substantial effort searching for, compiling, and managing relevant resources. To address these challenges, we present OracleAgent, the first agent system designed for the structured management and retrieval of OBS-related information. OracleAgent seamlessly integrates multiple OBS analysis tools, empowered by large language models (LLMs), and can flexibly orchestrate these components. Additionally, we construct a comprehensive domain-specific multimodal knowledge base for OBS, which is built through a rigorous multi-year process of data collection, cleaning, and expert annotation. The knowledge base comprises over 1.4M single-character rubbing images and 80K interpretation texts. OracleAgent leverages this resource through its multimodal tools to assist experts in retrieval tasks of character, document, interpretation text, and rubbing image. Extensive experiments demonstrate that OracleAgent achieves superior performance across a range of multimodal reasoning and generation tasks, surpassing leading mainstream multimodal large language models (MLLMs) (e.g., GPT-4o). Furthermore, our case study illustrates that OracleAgent can effectively assist domain experts, significantly reducing the time cost of OBS research. These results highlight OracleAgent as a significant step toward the practical deployment of OBS-assisted research and automated interpretation systems.
>
---
#### [new 022] Scaling Image Geo-Localization to Continent Level
- **分类: cs.CV; cs.LG**

- **简介: 该论文聚焦于跨大陆尺度的图像地理定位任务，旨在解决全球范围内精确定位图像的难题。针对传统方法在大规模数据下效率低、覆盖不全的问题，提出一种融合代理分类与航空影像嵌入的混合模型，利用隐式位置特征实现亚公里级定位，在欧洲多国区域上超过68%的查询可精确定位至200米内。**

- **链接: [http://arxiv.org/pdf/2510.26795v1](http://arxiv.org/pdf/2510.26795v1)**

> **作者:** Philipp Lindenberger; Paul-Edouard Sarlin; Jan Hosang; Matteo Balice; Marc Pollefeys; Simon Lynen; Eduard Trulls
>
> **备注:** NeurIPS 2025
>
> **摘要:** Determining the precise geographic location of an image at a global scale remains an unsolved challenge. Standard image retrieval techniques are inefficient due to the sheer volume of images (>100M) and fail when coverage is insufficient. Scalable solutions, however, involve a trade-off: global classification typically yields coarse results (10+ kilometers), while cross-view retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions. This paper introduces a hybrid approach that achieves fine-grained geo-localization across a large geographic expanse the size of a continent. We leverage a proxy classification task during training to learn rich feature representations that implicitly encode precise location information. We combine these learned prototypes with embeddings of aerial imagery to increase robustness to the sparsity of ground-level data. This enables direct, fine-grained retrieval over areas spanning multiple countries. Our extensive evaluation demonstrates that our approach can localize within 200m more than 68\% of queries of a dataset covering a large part of Europe. The code is publicly available at https://scaling-geoloc.github.io.
>
---
#### [new 023] On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations
- **分类: cs.CV; cs.AI; cs.RO**

- **简介: 该论文研究视觉-语言-动作（VLA）模型在多模态扰动下的鲁棒性问题，针对真实场景中动作、指令、环境、观测等多模态干扰，提出RobustVLA框架。通过对抗优化与多臂老虎机策略，提升模型对输入输出扰动的鲁棒性，在多种扰动下显著优于基线，尤其在真实机器人上表现优异。**

- **链接: [http://arxiv.org/pdf/2510.00037v3](http://arxiv.org/pdf/2510.00037v3)**

> **作者:** Jianing Guo; Zhenhong Wu; Chang Tu; Yiyao Ma; Xiangqi Kong; Zhiqian Liu; Jiaming Ji; Shuning Zhang; Yuanpei Chen; Kai Chen; Qi Dou; Yaodong Yang; Xianglong Liu; Huijie Zhao; Weifeng Lv; Simin Li
>
> **摘要:** In Vision-Language-Action (VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) pi0 demonstrates superior robustness with a diffusion-based action head. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations. Our RobustVLA is particularly effective on real-world FR5 robot with limited demonstrations, showing absolute gains by 65.6% under perturbations of four modalities.
>
---
#### [new 024] Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition
- **分类: cs.CV; cs.LG**

- **简介: 该论文针对视觉语言模型在零样本识别中因物体-背景关联导致的偏差问题，提出一种表示层反事实校准方法。通过在CLIP特征空间重构反事实嵌入，分离对象与背景影响，提升模型对陌生场景的鲁棒性，无需重训练或提示设计，显著改善最差组与平均准确率，实现更可靠的多模态推理。**

- **链接: [http://arxiv.org/pdf/2510.26466v1](http://arxiv.org/pdf/2510.26466v1)**

> **作者:** Pei Peng; MingKun Xie; Hang Hao; Tong Jin; ShengJun Huang
>
> **摘要:** Object-context shortcuts remain a persistent challenge in vision-language models, undermining zero-shot reliability when test-time scenes differ from familiar training co-occurrences. We recast this issue as a causal inference problem and ask: Would the prediction remain if the object appeared in a different environment? To answer this at inference time, we estimate object and background expectations within CLIP's representation space, and synthesize counterfactual embeddings by recombining object features with diverse alternative contexts sampled from external datasets, batch neighbors, or text-derived descriptions. By estimating the Total Direct Effect and simulating intervention, we further subtract background-only activation, preserving beneficial object-context interactions while mitigating hallucinated scores. Without retraining or prompt design, our method substantially improves both worst-group and average accuracy on context-sensitive benchmarks, establishing a new zero-shot state of the art. Beyond performance, our framework provides a lightweight representation-level counterfactual approach, offering a practical causal avenue for debiased and reliable multimodal reasoning.
>
---
#### [new 025] A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models
- **分类: cs.CV**

- **简介: 该论文针对视觉语言模型测试时提示调优（TPT）中的校准问题，提出A-TPT框架，通过最大化文本特征间的最小夹角距离来增强角度多样性，提升特征分布均匀性。解决了现有方法在类间特征分离度不足导致校准性能差的问题，显著降低校准误差，且在多种数据集上表现优异。**

- **链接: [http://arxiv.org/pdf/2510.26441v1](http://arxiv.org/pdf/2510.26441v1)**

> **作者:** Shihab Aaqil Ahamed; Udaya S. K. P. Miriya Thanthrige; Ranga Rodrigo; Muhammad Haris Khan
>
> **备注:** 23 pages, 14 figures
>
> **摘要:** Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs' reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.
>
---
#### [new 026] Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving
- **分类: cs.CV**

- **简介: 该论文针对端到端自动驾驶中的轨迹生成任务，解决模仿学习模式崩溃及生成轨迹难以满足安全与运动约束的问题。提出基于约束流匹配的CATG框架，直接在生成过程中嵌入安全与动力学约束，并通过控制信号调节驾驶风格，实现多样化、合规的轨迹生成，在NavSim v2挑战中获第二名并获创新奖。**

- **链接: [http://arxiv.org/pdf/2510.26292v1](http://arxiv.org/pdf/2510.26292v1)**

> **作者:** Lin Liu; Guanyi Yu; Ziying Song; Junqiao Li; Caiyan Jia; Feiyang Jia; Peiliang Wu; Yandan Luo
>
> **摘要:** Planning is a critical component of end-to-end autonomous driving. However, prevailing imitation learning methods often suffer from mode collapse, failing to produce diverse trajectory hypotheses. Meanwhile, existing generative approaches struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. To address these limitations, we propose CATG, a novel planning framework that leverages Constrained Flow Matching. Concretely, CATG explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our primary contribution is the novel imposition of explicit constraints directly within the flow matching process, ensuring that the generated trajectories adhere to vital safety and kinematic rules. Secondly, CATG parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Notably, on the NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and was honored with the Innovation Award.
>
---
#### [new 027] SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting
- **分类: cs.CV; cs.GR**

- **简介: 该论文提出SEE4D，解决从无相机位姿的自然视频生成4D时空内容的问题。通过固定虚拟相机视角与自回归视频修复，分离相机控制与场景建模，无需3D标注即可实现多视角一致的4D生成，显著提升泛化性与效率。**

- **链接: [http://arxiv.org/pdf/2510.26796v1](http://arxiv.org/pdf/2510.26796v1)**

> **作者:** Dongyue Lu; Ao Liang; Tianxin Huang; Xiao Fu; Yuyang Zhao; Baorui Ma; Liang Pan; Wei Yin; Lingdong Kong; Wei Tsang Ooi; Ziwei Liu
>
> **备注:** 26 pages; 21 figures; 3 tables; project page: https://see-4d.github.io/
>
> **摘要:** Immersive applications call for synthesizing spatiotemporal 4D content from casual videos without costly 3D supervision. Existing video-to-4D methods typically rely on manually annotated camera poses, which are labor-intensive and brittle for in-the-wild footage. Recent warp-then-inpaint approaches mitigate the need for pose labels by warping input frames along a novel camera trajectory and using an inpainting model to fill missing regions, thereby depicting the 4D scene from diverse viewpoints. However, this trajectory-to-trajectory formulation often entangles camera motion with scene dynamics and complicates both modeling and inference. We introduce SEE4D, a pose-free, trajectory-to-camera framework that replaces explicit trajectory prediction with rendering to a bank of fixed virtual cameras, thereby separating camera control from scene modeling. A view-conditional video inpainting model is trained to learn a robust geometry prior by denoising realistically synthesized warped images and to inpaint occluded or missing regions across virtual viewpoints, eliminating the need for explicit 3D annotations. Building on this inpainting core, we design a spatiotemporal autoregressive inference pipeline that traverses virtual-camera splines and extends videos with overlapping windows, enabling coherent generation at bounded per-step complexity. We validate See4D on cross-view video generation and sparse reconstruction benchmarks. Across quantitative metrics and qualitative assessments, our method achieves superior generalization and improved performance relative to pose- or trajectory-conditioned baselines, advancing practical 4D world modeling from casual videos.
>
---
#### [new 028] Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras
- **分类: cs.CV; cs.RO**

- **简介: 该论文针对事件相机数据的高效处理，提出异步稀疏的Spiking Patches令牌化方法。旨在保留事件相机固有的异步与稀疏特性，同时提升计算效率。实验表明，该方法在手势识别和目标检测任务中显著加速（最高10.4倍），且精度不降反升，为事件视觉提供了新范式。**

- **链接: [http://arxiv.org/pdf/2510.26614v1](http://arxiv.org/pdf/2510.26614v1)**

> **作者:** Christoffer Koo Øhrstrøm; Ronja Güldenring; Lazaros Nalpantidis
>
> **摘要:** We propose tokenization of events and present a tokenizer, Spiking Patches, specifically designed for event cameras. Given a stream of asynchronous and spatially sparse events, our goal is to discover an event representation that preserves these properties. Prior works have represented events as frames or as voxels. However, while these representations yield high accuracy, both frames and voxels are synchronous and decrease the spatial sparsity. Spiking Patches gives the means to preserve the unique properties of event cameras and we show in our experiments that this comes without sacrificing accuracy. We evaluate our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and object detection. Tokens from Spiking Patches yield inference times that are up to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We achieve this while matching their accuracy and even surpassing in some cases with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for object detection. Thus, tokenization constitutes a novel direction in event-based vision and marks a step towards methods that preserve the properties of event cameras.
>
---
#### [new 029] Masked Diffusion Captioning for Visual Feature Learning
- **分类: cs.CV**

- **简介: 该论文提出掩码扩散图像描述（MDC），通过条件于视觉特征的掩码扩散语言模型进行图像描述重建来学习视觉特征。旨在解决传统自回归描述中依赖文本位置、需辅助目标的问题。实验表明，所学特征在下游任务中性能媲美自回归与对比方法。**

- **链接: [http://arxiv.org/pdf/2510.26799v1](http://arxiv.org/pdf/2510.26799v1)**

> **作者:** Chao Feng; Zihao Wei; Andrew Owens
>
> **备注:** EMNLP 2025 (Findings). Project page: https://cfeng16.github.io/mdlm4vfl/
>
> **摘要:** We learn visual features by captioning images with an image-conditioned masked diffusion language model, a formulation we call masked diffusion captioning (MDC). During training, text tokens in each image-caption pair are masked at a randomly chosen ratio, and a decoder conditioned on visual features is trained to reconstruct the original text. After training, the learned visual features can be applied to downstream vision tasks. Unlike autoregressive captioning, the strength of the visual learning signal in MDC does not depend on each token's position in the sequence, reducing the need for auxiliary objectives. Linear probing experiments across a variety of academic-scale models and datasets show that the learned visual features are competitive with those produced by autoregressive and contrastive approaches.
>
---
#### [new 030] HEIR: Learning Graph-Based Motion Hierarchies
- **分类: cs.CV; cs.GR; cs.LG**

- **简介: 该论文提出HEIR，一种基于图神经网络的自监督运动层次建模方法，旨在从数据中自动学习可解释的运动层级结构。针对现有方法依赖人工设计层次、泛化性差的问题，该方法将运动分解为父级继承模式与局部残差，并通过可微分图学习推断层级关系，在1D/2D运动及3D动态场景重建中均取得更真实、可解释的结果。**

- **链接: [http://arxiv.org/pdf/2510.26786v1](http://arxiv.org/pdf/2510.26786v1)**

> **作者:** Cheng Zheng; William Koch; Baiang Li; Felix Heide
>
> **备注:** Code link: https://github.com/princeton-computational-imaging/HEIR
>
> **摘要:** Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/
>
---
#### [new 031] CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments
- **分类: cs.CV; cs.CL**

- **简介: 该论文提出CAVE基准，针对真实世界视觉异常检测与理解任务。解决现有方法局限于工业缺陷或合成异常的问题，通过引入人类认知启发的细粒度标注，支持异常描述、解释与论证，评估视觉语言模型在常识推理与异常感知上的能力。**

- **链接: [http://arxiv.org/pdf/2510.26006v1](http://arxiv.org/pdf/2510.26006v1)**

> **作者:** Rishika Bhagwatkar; Syrielle Montariol; Angelika Romanou; Beatriz Borges; Irina Rish; Antoine Bosselut
>
> **摘要:** Humans can naturally identify, reason about, and explain anomalies in their environment. In computer vision, this long-standing challenge remains limited to industrial defects or unrealistic, synthetically generated anomalies, failing to capture the richness and unpredictability of real-world anomalies. In this work, we introduce CAVE, the first benchmark of real-world visual anomalies. CAVE supports three open-ended tasks: anomaly description, explanation, and justification; with fine-grained annotations for visual grounding and categorizing anomalies based on their visual manifestations, their complexity, severity, and commonness. These annotations draw inspiration from cognitive science research on how humans identify and resolve anomalies, providing a comprehensive framework for evaluating Vision-Language Models (VLMs) in detecting and understanding anomalies. We show that state-of-the-art VLMs struggle with visual anomaly perception and commonsense reasoning, even with advanced prompting strategies. By offering a realistic and cognitively grounded benchmark, CAVE serves as a valuable resource for advancing research in anomaly detection and commonsense reasoning in VLMs.
>
---
#### [new 032] PointSt3R: Point Tracking through 3D Grounded Correspondence
- **分类: cs.CV**

- **简介: 该论文提出PointSt3R，用于3D地面对应下的点跟踪任务。针对静态与动态点跟踪问题，结合重建损失与可见性头，微调MASt3R模型，仅使用少量合成数据，在无时间上下文情况下实现高效点跟踪，显著优于现有方法。**

- **链接: [http://arxiv.org/pdf/2510.26443v1](http://arxiv.org/pdf/2510.26443v1)**

> **作者:** Rhodri Guerrier; Adam W. Harley; Dima Damen
>
> **备注:** http://rhodriguerrier.github.io/PointSt3R
>
> **摘要:** Recent advances in foundational 3D reconstruction models, such as DUSt3R and MASt3R, have shown great potential in 2D and 3D correspondence in static scenes. In this paper, we propose to adapt them for the task of point tracking through 3D grounded correspondence. We first demonstrate that these models are competitive point trackers when focusing on static points, present in current point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose to combine the reconstruction loss with training for dynamic correspondence along with a visibility head, and fine-tuning MASt3R for point tracking using a relatively small amount of synthetic data. Importantly, we only train and evaluate on pairs of frames where one contains the query point, effectively removing any temporal context. Using a mix of dynamic and static point correspondences, we achieve competitive or superior point tracking results on four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\% occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs 82.8). We also present results on 3D point tracking along with several ablations on training datasets and percentage of dynamic correspondences.
>
---
#### [new 033] Improving Classification of Occluded Objects through Scene Context
- **分类: cs.CV**

- **简介: 该论文属于目标检测任务，旨在解决遮挡物体识别难题。通过引入场景上下文信息，提出两种融合策略：一是在预测前根据场景选择网络，二是在检测后融合场景知识优化得分。实验表明，方法提升了召回率与精度，且联合训练有遮挡和无遮挡图像效果更优。**

- **链接: [http://arxiv.org/pdf/2510.26681v1](http://arxiv.org/pdf/2510.26681v1)**

> **作者:** Courtney M. King; Daniel D. Leeds; Damian Lyons; George Kalaitzis
>
> **摘要:** The presence of occlusions has provided substantial challenges to typically-powerful object recognition algorithms. Additional sources of information can be extremely valuable to reduce errors caused by occlusions. Scene context is known to aid in object recognition in biological vision. In this work, we attempt to add robustness into existing Region Proposal Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks through two distinct scene-based information fusion techniques. We present one algorithm under each methodology: the first operates prior to prediction, selecting a custom object network to use based on the identified background scene, and the second operates after detection, fusing scene knowledge into initial object scores output by the RPN. We demonstrate our algorithms on challenging datasets featuring partial occlusions, which show overall improvement in both recall and precision against baseline methods. In addition, our experiments contrast multiple training methodologies for occlusion handling, finding that training on a combination of both occluded and unoccluded images demonstrates an improvement over the others. Our method is interpretable and can easily be adapted to other datasets, offering many future directions for research and practical applications.
>
---
#### [new 034] Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy
- **分类: cs.CV; cond-mat.mtrl-sci**

- **简介: 该论文针对扫描隧道显微镜（STM）成像中因探针退化和采样慢导致的效率问题，提出基于物理信息合成数据的生成模型，实现图像修复与超分辨率。利用36张实验图像训练流匹配与扩散模型，显著提升重建质量并减少采集时间2-4倍，可降低探针调理频率，提升实验通量。**

- **链接: [http://arxiv.org/pdf/2510.25921v1](http://arxiv.org/pdf/2510.25921v1)**

> **作者:** Nikola L. Kolev; Tommaso Rodani; Neil J. Curson; Taylor J. Z. Stock; Alberto Cazzaniga
>
> **摘要:** Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and atom manipulation, but its utility is often limited by tip degradation and slow serial data acquisition. Fabrication adds another layer of complexity since the tip is often subjected to large voltages, which may alter the shape of its apex, requiring it to be conditioned. Here, we propose a machine learning (ML) approach for image repair and super-resolution to alleviate both challenges. Using a dataset of only 36 pristine experimental images of Si(001):H, we demonstrate that a physics-informed synthetic data generation pipeline can be used to train several state-of-the-art flow-matching and diffusion models. Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy (CMMD) score and structural similarity demonstrates that our models are able to effectively restore images and offer a two- to fourfold reduction in image acquisition time by accurately reconstructing images from sparsely sampled data. Our framework has the potential to significantly increase STM experimental throughput by offering a route to reducing the frequency of tip-conditioning procedures and to enhancing frame rates in existing high-speed STM systems.
>
---
#### [new 035] Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios
- **分类: cs.CV**

- **简介: 该论文提出一种基于视觉-语言对齐的动态上下文感知场景推理框架，旨在解决零样本真实场景下的泛化难题。通过融合预训练视觉变压器与大语言模型，利用语言先验增强场景理解，实现无需任务特定训练的上下文自适应推理，在复杂未知环境中显著提升场景理解准确率。**

- **链接: [http://arxiv.org/pdf/2510.26580v1](http://arxiv.org/pdf/2510.26580v1)**

> **作者:** Manjunath Prasad Holenarasipura Rajiv; B. M. Vidyavathi
>
> **备注:** Preprint under review at IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025
>
> **摘要:** In real-world environments, AI systems often face unfamiliar scenarios without labeled data, creating a major challenge for conventional scene understanding models. The inability to generalize across unseen contexts limits the deployment of vision-based applications in dynamic, unstructured settings. This work introduces a Dynamic Context-Aware Scene Reasoning framework that leverages Vision-Language Alignment to address zero-shot real-world scenarios. The goal is to enable intelligent systems to infer and adapt to new environments without prior task-specific training. The proposed approach integrates pre-trained vision transformers and large language models to align visual semantics with natural language descriptions, enhancing contextual comprehension. A dynamic reasoning module refines predictions by combining global scene cues and object-level interactions guided by linguistic priors. Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and Open Images demonstrate up to 18% improvement in scene understanding accuracy over baseline models in complex and unseen environments. Results also show robust performance in ambiguous or cluttered scenes due to the synergistic fusion of vision and language. This framework offers a scalable and interpretable approach for context-aware reasoning, advancing zero-shot generalization in dynamic real-world settings.
>
---
#### [new 036] Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances
- **分类: cs.CV**

- **简介: 该论文研究跨距离虹膜区域验证任务，旨在提升不同距离下识别准确率。通过训练三种复杂度CNN（SqueezeNet、MobileNetv2、ResNet50），分析其互补性，采用融合策略与注意力可视化（LIME、JS散度）揭示网络关注区域差异，显著优于现有方法，达到新最优性能。**

- **链接: [http://arxiv.org/pdf/2510.26282v1](http://arxiv.org/pdf/2510.26282v1)**

> **作者:** Fernando Alonso-Fernandez; Kevin Hernandez Diaz; Jose M. Buades; Kiran Raja; Josef Bigun
>
> **备注:** Accepted at BIOSIG 2025 conference
>
> **摘要:** We study the complementarity of different CNNs for periocular verification at different distances on the UBIPr database. We train three architectures of increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics, compare different network initialisations, and apply score-level fusion via logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon divergence to compare attention patterns of the CNNs. While ResNet50 consistently performs best individually, the fusion provides substantial gains, especially when combining all three networks. Heatmaps show that networks usually focus on distinct regions of a given image, which explains their complementarity. Our method significantly outperforms previous works on UBIPr, achieving a new state-of-the-art.
>
---
#### [new 037] Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG
- **分类: cs.CV**

- **简介: 该论文研究音乐类型与情绪之间的关联，属于情感计算任务。旨在解决不同音乐如何影响人的情绪这一问题。通过问卷调查与EEG脑电监测，采集参与者听音乐时的主观感受与脑活动数据，分析二者关联，揭示音乐引发情绪的神经机制。**

- **链接: [http://arxiv.org/pdf/2510.26304v1](http://arxiv.org/pdf/2510.26304v1)**

> **作者:** Jelizaveta Jankowska; Bożena Kostek; Fernando Alonso-Fernandez; Prayag Tiwari
>
> **备注:** Published at IWAIPR 2025 conference
>
> **摘要:** The subject of this work is to check how different types of music affect human emotions. While listening to music, a subjective survey and brain activity measurements were carried out using an EEG helmet. The aim is to demonstrate the impact of different music genres on emotions. The research involved a diverse group of participants of different gender and musical preferences. This had the effect of capturing a wide range of emotional responses to music. After the experiment, a relationship analysis of the respondents' questionnaires with EEG signals was performed. The analysis revealed connections between emotions and observed brain activity.
>
---
#### [new 038] Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction
- **分类: cs.CV**

- **简介: 该论文聚焦于从草图预测3D人体姿态的任务。针对缺乏真实标注数据导致传统方法依赖低效优化的问题，提出Sketch2PoseNet框架：利用扩散模型合成12万对草图-3D姿态数据，构建SKEP-120K数据集；结合2D姿态检测与生成先验，实现端到端高效、通用的草图到3D姿态估计，显著提升精度与速度。**

- **链接: [http://arxiv.org/pdf/2510.26196v1](http://arxiv.org/pdf/2510.26196v1)**

> **作者:** Li Wang; Yiyu Zhuang; Yanwen Wang; Xun Cao; Chuan Guo; Xinxin Zuo; Hao Zhu
>
> **备注:** SIGGRAPH Asia 2025
>
> **摘要:** 3D human pose estimation from sketches has broad applications in computer animation and film production. Unlike traditional human pose estimation, this task presents unique challenges due to the abstract and disproportionate nature of sketches. Previous sketch-to-pose methods, constrained by the lack of large-scale sketch-3D pose annotations, primarily relied on optimization with heuristic rules-an approach that is both time-consuming and limited in generalizability. To address these challenges, we propose a novel approach leveraging a "learn from synthesis" strategy. First, a diffusion model is trained to synthesize sketch images from 2D poses projected from 3D human poses, mimicking disproportionate human structures in sketches. This process enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k accurate sketch-3D pose annotation pairs across various sketch styles. Building on this synthetic dataset, we introduce an end-to-end data-driven framework for estimating human poses and shapes from diverse sketch styles. Our framework combines existing 2D pose detectors and generative diffusion priors for sketch feature extraction with a feed-forward neural network for efficient 2D pose estimation. Multiple heuristic loss functions are incorporated to guarantee geometric coherence between the derived 3D poses and the detected 2D poses while preserving accurate self-contacts. Qualitative, quantitative, and subjective evaluations collectively show that our model substantially surpasses previous ones in both estimation accuracy and speed for sketch-to-pose tasks.
>
---
#### [new 039] Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI
- **分类: cs.CV**

- **简介: 该论文针对放射治疗中实时肿瘤追踪任务，解决胸腹腔动态MRI下数据稀缺难题。采用基于SAM2.1的微调方法，利用首帧标注生成掩码提示，通过小样本训练实现快速精准分割，在保证实时性（<1秒/帧）的同时取得0.8794的Dice分数，显著提升肿瘤追踪精度。**

- **链接: [http://arxiv.org/pdf/2510.25990v1](http://arxiv.org/pdf/2510.25990v1)**

> **作者:** Valentin Boussot; Cédric Hémon; Jean-Claude Nunes; Jean-Louis Dillenseger
>
> **备注:** Paper for the Trackrad2025 challenge, Team BreizhTrack
>
> **摘要:** In this work, we address the TrackRAD2025 challenge of real-time tumor tracking in cine-MRI sequences of the thoracic and abdominal regions under strong data scarcity constraints. Two complementary strategies were explored: (i) unsupervised registration with the IMPACT similarity metric and (ii) foundation model-based segmentation leveraging SAM 2.1 and its recent variants through prompt-based interaction. Due to the one-second runtime constraint, the SAM-based method was ultimately selected. The final configuration used SAM2.1 b+ with mask-based prompts from the first annotated slice, fine-tuned solely on the small labeled subset from TrackRAD2025. Training was configured to minimize overfitting, using 1024x1024 patches (batch size 1), standard augmentations, and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve generalization while adapting to annotator-specific styles. Training lasted 300 epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently applied across all anatomical sites and MRI field strengths. Test-time augmentation was considered but ultimately discarded due to negligible performance gains. The final model was selected based on the highest Dice Similarity Coefficient achieved on the validation set after fine-tuning. On the hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall in the TrackRAD2025 challenge. These results highlight the strong potential of foundation models for accurate and real-time tumor tracking in MRI-guided radiotherapy.
>
---
#### [new 040] Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing
- **分类: cs.CV; cs.AI; cs.CL; cs.LG**

- **简介: 该论文聚焦于大视觉语言模型（LVLMs）的自提升任务，针对自提升过程中模型过度偏向简单问题（头数据）而忽视复杂问题（尾数据）的“马太效应”问题，提出通过分布重塑与轨迹重采样两种策略实现头尾平衡，显著提升模型在复杂视觉推理任务上的表现。**

- **链接: [http://arxiv.org/pdf/2510.26474v1](http://arxiv.org/pdf/2510.26474v1)**

> **作者:** Xin Guo; Zhiheng Xi; Yiwen Ding; Yitao Zhai; Xiaowei Shi; Xunliang Cai; Tao Gui; Qi Zhang; Xuanjing Huang
>
> **备注:** Preprint
>
> **摘要:** Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew effect"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.
>
---
#### [new 041] CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing
- **分类: cs.CV; cs.LG; eess.IV**

- **简介: 该论文提出CYPRESS模型，用于高分辨率油菜产量预测。针对传统方法精度低、粒度粗的问题，利用预训练遥感基础模型Prithvi-EO-2.0-600M，通过微调实现多时相卫星影像到像素级产量图的回归，显著提升预测精度，推动精准农业发展。**

- **链接: [http://arxiv.org/pdf/2510.26609v1](http://arxiv.org/pdf/2510.26609v1)**

> **作者:** Shayan Nejadshamsi; Yuanyuan Zhang; Shadi Zaki; Brock Porth; Lysa Porth; Vahab Khoshdel
>
> **摘要:** Accurate and timely crop yield prediction is crucial for global food security and modern agricultural management. Traditional methods often lack the scalability and granularity required for precision farming. This paper introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing), a deep learning model designed for high-resolution, intra-field canola yield prediction. CYPRESS leverages a pre-trained, large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for a continuous regression task, transforming multi-temporal satellite imagery into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from the Canadian Prairies, CYPRESS demonstrates superior performance over existing deep learning-based yield prediction models, highlighting the effectiveness of fine-tuning foundation models for specialized agricultural applications. By providing a continuous, high-resolution output, CYPRESS offers a more actionable tool for precision agriculture than conventional classification or county-level aggregation methods. This work validates a novel approach that bridges the gap between large-scale Earth observation and on-farm decision-making, offering a scalable solution for detailed agricultural monitoring.
>
---
#### [new 042] MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency
- **分类: cs.CV; cs.LG**

- **简介: 该论文针对文本生成图像（T2I）模型训练中用户偏好对齐不足、效率低的问题，提出MIRO方法。通过在训练阶段同时条件化多个奖励模型，使模型直接学习用户偏好，显著提升图像质量与生成效率，并在多个基准上达到最优性能。**

- **链接: [http://arxiv.org/pdf/2510.25897v1](http://arxiv.org/pdf/2510.25897v1)**

> **作者:** Nicolas Dufour; Lucas Degeorge; Arijit Ghosh; Vicky Kalogeiton; David Picard
>
> **备注:** Project page: https://nicolas-dufour.github.io/miro
>
> **摘要:** Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them to a reward, typically user preference. This discarding of informative data together with the optimizing for a single reward tend to harm diversity, semantic fidelity and efficiency. Instead of this post-processing, we propose to condition the model on multiple reward models during training to let the model learn user preferences directly. We show that this not only dramatically improves the visual quality of the generated images but it also significantly speeds up the training. Our proposed method, called MIRO, achieves state-of-the-art performances on the GenEval compositional benchmark and user-preference scores (PickAScore, ImageReward, HPSv2).
>
---
#### [new 043] SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models
- **分类: cs.CV; cs.LG**

- **简介: 该论文提出SteerVLM，一种轻量级激活调制模块，用于在不修改模型权重的情况下，动态控制视觉语言模型的输出语义。通过学习提示嵌入中的目标与反向行为，实现推理时的细粒度控制，解决指令遵循与幻觉问题。模块仅需原模型0.14%参数，且引入新数据集VNIA促进评估。**

- **链接: [http://arxiv.org/pdf/2510.26769v1](http://arxiv.org/pdf/2510.26769v1)**

> **作者:** Anushka Sivakumar; Andrew Zhang; Zaber Hakim; Chris Thomas
>
> **摘要:** This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLM's size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering.
>
---
#### [new 044] EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models
- **分类: cs.CV**

- **简介: 该论文属于脑电驱动图像重建任务，旨在解决现有方法忽视空间注意力导致图像质量与语义一致性差的问题。提出双条件框架，结合EEG嵌入与视觉显著图，通过ATM提取脑电特征，微调Stable Diffusion并引入ControlNet实现空间控制，显著提升重建质量与人眼注意力对齐度。**

- **链接: [http://arxiv.org/pdf/2510.26391v1](http://arxiv.org/pdf/2510.26391v1)**

> **作者:** Igor Abramov; Ilya Makarov
>
> **备注:** Demo paper
>
> **摘要:** Existing EEG-driven image reconstruction methods often overlook spatial attention mechanisms, limiting fidelity and semantic coherence. To address this, we propose a dual-conditioning framework that combines EEG embeddings with spatial saliency maps to enhance image generation. Our approach leverages the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals with visual semantics, while a ControlNet branch conditions generation on saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves a significant improvement in the quality of low- and high-level image features over existing approaches. Simultaneously, strongly aligning with human visual attention. The results demonstrate that attentional priors resolve EEG ambiguities, enabling high-fidelity reconstructions with applications in medical diagnostics and neuroadaptive interfaces, advancing neural decoding through efficient adaptation of pre-trained diffusion models.
>
---
#### [new 045] Emu3.5: Native Multimodal Models are World Learners
- **分类: cs.CV**

- **简介: 该论文提出Emu3.5，一个原生多模态世界模型，旨在通过统一视觉与语言的下一标记预测，实现跨模态状态预测。针对多模态生成效率与一致性问题，提出DiDA加速推理，并在长时序生成、任意到图像及复杂图像生成任务中表现优异，具备强泛化世界建模能力。**

- **链接: [http://arxiv.org/pdf/2510.26583v1](http://arxiv.org/pdf/2510.26583v1)**

> **作者:** Yufeng Cui; Honghao Chen; Haoge Deng; Xu Huang; Xinghang Li; Jirong Liu; Yang Liu; Zhuoyan Luo; Jinsheng Wang; Wenxuan Wang; Yueze Wang; Chengyuan Wang; Fan Zhang; Yingli Zhao; Ting Pan; Xianduo Li; Zecheng Hao; Wenxuan Ma; Zhuo Chen; Yulong Ao; Tiejun Huang; Zhongyuan Wang; Xinlong Wang
>
> **备注:** project page: https://emu.world
>
> **摘要:** We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.
>
---
#### [new 046] Dynamic VLM-Guided Negative Prompting for Diffusion Models
- **分类: cs.CV; cs.AI**

- **简介: 该论文针对扩散模型中固定负提示导致生成质量受限的问题，提出动态负提示方法。利用视觉语言模型（VLM）在去噪过程中自动生成上下文相关的负提示，提升文本图像对齐效果。实验验证了负引导强度与对齐度的权衡。**

- **链接: [http://arxiv.org/pdf/2510.26052v1](http://arxiv.org/pdf/2510.26052v1)**

> **作者:** Hoyeon Chang; Seungjin Kim; Yoonseok Choi
>
> **备注:** 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: The First Workshop on Generative and Protective AI for Content Creation
>
> **摘要:** We propose a novel approach for dynamic negative prompting in diffusion models that leverages Vision-Language Models (VLMs) to adaptively generate negative prompts during the denoising process. Unlike traditional Negative Prompting methods that use fixed negative prompts, our method generates intermediate image predictions at specific denoising steps and queries a VLM to produce contextually appropriate negative prompts. We evaluate our approach on various benchmark datasets and demonstrate the trade-offs between negative guidance strength and text-image alignment.
>
---
#### [new 047] Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM
- **分类: cs.CV; cs.RO**

- **简介: 该论文针对RGB-D SLAM中帧关联性能不足的问题，提出利用网络梯度生成的层间注意力信息，增强CNN特征对语义物体的空间感知能力。通过融合注意力引导的特征表示，提升了大环境下的帧关联准确率，推动了视觉任务中语义理解与定位的结合。**

- **链接: [http://arxiv.org/pdf/2510.26131v1](http://arxiv.org/pdf/2510.26131v1)**

> **作者:** Ali Caglayan; Nevrez Imamoglu; Oguzhan Guclu; Ali Osman Serhatoglu; Ahmet Burak Can; Ryosuke Nakamura
>
> **备注:** double-column 5 pages, 3 figures
>
> **摘要:** Attention models have recently emerged as a powerful approach, demonstrating significant progress in various fields. Visualization techniques, such as class activation mapping, provide visual insights into the reasoning of convolutional neural networks (CNNs). Using network gradients, it is possible to identify regions where the network pays attention during image recognition tasks. Furthermore, these gradients can be combined with CNN features to localize more generalizable, task-specific attentive (salient) regions within scenes. However, explicit use of this gradient-based attention information integrated directly into CNN representations for semantic object understanding remains limited. Such integration is particularly beneficial for visual tasks like simultaneous localization and mapping (SLAM), where CNN representations enriched with spatially attentive object locations can enhance performance. In this work, we propose utilizing task-specific network attention for RGB-D indoor SLAM. Specifically, we integrate layer-wise attention information derived from network gradients with CNN feature representations to improve frame association performance. Experimental results indicate improved performance compared to baseline methods, particularly for large environments.
>
---
#### [new 048] LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation
- **分类: cs.CV; cs.AI**

- **简介: 该论文针对长视频文本生成任务，解决现有评估基准在复杂提示下缺乏细粒度与抽象维度评价的问题。提出LoCoT2V-Bench基准，包含真实复杂提示与多维评估框架，涵盖事件对齐、时间一致性及叙事情感等指标，系统评测九个模型，揭示其在跨事件一致性和主题表达上的不足。**

- **链接: [http://arxiv.org/pdf/2510.26412v1](http://arxiv.org/pdf/2510.26412v1)**

> **作者:** Xiangqing Zheng; Chengyue Wu; Kehai Chen; Min Zhang
>
> **摘要:** Recently text-to-video generation has made impressive progress in producing short, high-quality clips, but evaluating long-form outputs remains a major challenge especially when processing complex prompts. Existing benchmarks mostly rely on simplified prompts and focus on low-level metrics, overlooking fine-grained alignment with prompts and abstract dimensions such as narrative coherence and thematic expression. To address these gaps, we propose LoCoT2V-Bench, a benchmark specifically designed for long video generation (LVG) under complex input conditions. Based on various real-world videos, LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating elements like scene transitions and event dynamics. Moreover, it constructs a multi-dimensional evaluation framework that includes our newly proposed metrics such as event-level alignment, fine-grained temporal consistency, content clarity, and the Human Expectation Realization Degree (HERD) that focuses on more abstract attributes like narrative flow, emotional response, and character development. Using this framework, we conduct a comprehensive evaluation of nine representative LVG models, finding that while current methods perform well on basic visual and temporal aspects, they struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence, etc. Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for evaluating long-form complex text-to-video generation and highlights critical directions for future method improvement.
>
---
#### [new 049] Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws
- **分类: cs.CV**

- **简介: 该论文针对红外与可见光图像融合任务，解决现有方法在信息平衡与可解释性方面的不足。提出基于人类认知规律的HCLFuse方法，通过多尺度掩码调节变分瓶颈编码器与时变物理引导扩散模型，提升结构一致性与细节质量，实现更可靠、高质量的融合结果。**

- **链接: [http://arxiv.org/pdf/2510.26268v1](http://arxiv.org/pdf/2510.26268v1)**

> **作者:** Lin Guo; Xiaoqing Luo; Wei Xie; Zhancheng Zhang; Hui Li; Rui Wang; Zhenhua Feng; Xiaoning Song
>
> **备注:** NeurIPS 2025 spotlight
>
> **摘要:** Existing infrared and visible image fusion methods often face the dilemma of balancing modal information. Generative fusion methods reconstruct fused images by learning from data distributions, but their generative capabilities remain limited. Moreover, the lack of interpretability in modal information selection further affects the reliability and consistency of fusion results in complex scenarios. This manuscript revisits the essence of generative image fusion under the inspiration of human cognitive laws and proposes a novel infrared and visible image fusion method, termed HCLFuse. First, HCLFuse investigates the quantification theory of information mapping in unsupervised fusion networks, which leads to the design of a multi-scale mask-regulated variational bottleneck encoder. This encoder applies posterior probability modeling and information decomposition to extract accurate and concise low-level modal information, thereby supporting the generation of high-fidelity structural details. Furthermore, the probabilistic generative capability of the diffusion model is integrated with physical laws, forming a time-varying physical guidance mechanism that adaptively regulates the generation process at different stages, thereby enhancing the ability of the model to perceive the intrinsic structure of data and reducing dependence on data quality. Experimental results show that the proposed method achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets and significantly improves semantic segmentation metrics. This fully demonstrates the advantages of this generative image fusion method, drawing inspiration from human cognition, in enhancing structural consistency and detail quality.
>
---
#### [new 050] MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction
- **分类: cs.CV; cs.AI**

- **简介: 该论文提出MV-MLM模型，用于乳腺癌诊断与风险预测。针对医学影像标注成本高问题，利用合成报告与多视图自监督学习，提升模型在少量标注数据下的性能，实现对良恶性、亚型及风险的精准分类。**

- **链接: [http://arxiv.org/pdf/2510.26151v1](http://arxiv.org/pdf/2510.26151v1)**

> **作者:** Shunjie-Fabian Zheng; Hyeonjun Lee; Thijs Kooi; Ali Diba
>
> **备注:** Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD) Workshop at ICCV 2025
>
> **摘要:** Large annotated datasets are essential for training robust Computer-Aided Diagnosis (CAD) models for breast cancer detection or risk prediction. However, acquiring such datasets with fine-detailed annotation is both costly and time-consuming. Vision-Language Models (VLMs), such as CLIP, which are pre-trained on large image-text pairs, offer a promising solution by enhancing robustness and data efficiency in medical imaging tasks. This paper introduces a novel Multi-View Mammography and Language Model for breast cancer classification and risk prediction, trained on a dataset of paired mammogram images and synthetic radiology reports. Our MV-MLM leverages multi-view supervision to learn rich representations from extensive radiology data by employing cross-modal self-supervision across image-text pairs. This includes multiple views and the corresponding pseudo-radiology reports. We propose a novel joint visual-textual learning strategy to enhance generalization and accuracy performance over different data types and tasks to distinguish breast tissues or cancer characteristics(calcification, mass) and utilize these patterns to understand mammography images and predict cancer risk. We evaluated our method on both private and publicly available datasets, demonstrating that the proposed model achieves state-of-the-art performance in three classification tasks: (1) malignancy classification, (2) subtype classification, and (3) image-based cancer risk prediction. Furthermore, the model exhibits strong data efficiency, outperforming existing fully supervised or VLM baselines while trained on synthetic text reports and without the need for actual radiology reports.
>
---
#### [new 051] Surpassing state of the art on AMD area estimation from RGB fundus images through careful selection of U-Net architectures and loss functions for class imbalance
- **分类: cs.CV; cs.LG; eess.IV; 68T07, 68T05, 68T45, 92C55; I.2.6; J.3**

- **简介: 该论文聚焦于基于RGB眼底图像的AMD病变语义分割任务，旨在提升病变区域面积估计精度。针对类别不平衡问题，通过优化U-Net架构、选择不同编码器及专用损失函数，显著超越了ADAM挑战赛现有成果。**

- **链接: [http://arxiv.org/pdf/2510.26778v1](http://arxiv.org/pdf/2510.26778v1)**

> **作者:** Valentyna Starodub; Mantas Lukoševičius
>
> **摘要:** Age-related macular degeneration (AMD) is one of the leading causes of irreversible vision impairment in people over the age of 60. This research focuses on semantic segmentation for AMD lesion detection in RGB fundus images, a non-invasive and cost-effective imaging technique. The results of the ADAM challenge - the most comprehensive AMD detection from RGB fundus images research competition and open dataset to date - serve as a benchmark for our evaluation. Taking the U-Net connectivity as a base of our framework, we evaluate and compare several approaches to improve the segmentation model's architecture and training pipeline, including pre-processing techniques, encoder (backbone) deep network types of varying complexity, and specialized loss functions to mitigate class imbalances on image and pixel levels. The main outcome of this research is the final configuration of the AMD detection framework, which outperforms all the prior ADAM challenge submissions on the multi-class segmentation of different AMD lesion types in non-invasive RGB fundus images. The source code used to conduct the experiments presented in this paper is made freely available.
>
---
#### [new 052] Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer
- **分类: cs.CV; cs.AI; q-bio.NC**

- **简介: 该论文聚焦于从fMRI脑信号重建视觉图像的任务，旨在提升重建图像的忠实度。针对现有方法语义不准确、依赖大量数据的问题，提出Brain-IT模型，通过共享的脑交互变换器（BIT）实现跨脑功能簇的信息整合，预测高阶语义与低阶结构特征，引导扩散模型生成更精准图像。仅需1小时数据即可达到媲美40小时训练的效果。**

- **链接: [http://arxiv.org/pdf/2510.25976v1](http://arxiv.org/pdf/2510.25976v1)**

> **作者:** Roman Beliy; Amit Zalcher; Jonathan Kogman; Navve Wasserman; Michal Irani
>
> **摘要:** Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present "Brain-IT", a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional-clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters & subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i)high-level semantic features which steer the diffusion model toward the correct semantic content of the image; and (ii)low-level structural features which help to initialize the diffusion process with the correct coarse layout of the image. BIT's design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current SotA approaches both visually and by standard objective metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40-hour recordings.
>
---
#### [new 053] Security Risk of Misalignment between Text and Image in Multi-modal Model
- **分类: cs.CV; cs.AI; cs.CR**

- **简介: 该论文研究多模态扩散模型中文本与图像的对齐安全风险。针对现有模型在固定提示下仍易生成不当内容的问题，提出仅通过修改输入图像即可操控输出的新型攻击PReMA，验证了其在图像修复与风格迁移任务中的有效性，揭示了模型潜在的安全威胁。**

- **链接: [http://arxiv.org/pdf/2510.26105v1](http://arxiv.org/pdf/2510.26105v1)**

> **作者:** Xiaosen Wang; Zhijin Ge; Shaokang Wang
>
> **摘要:** Despite the notable advancements and versatility of multi-modal diffusion models, such as text-to-image models, their susceptibility to adversarial inputs remains underexplored. Contrary to expectations, our investigations reveal that the alignment between textual and Image modalities in existing diffusion models is inadequate. This misalignment presents significant risks, especially in the generation of inappropriate or Not-Safe-For-Work (NSFW) content. To this end, we propose a novel attack called Prompt-Restricted Multi-modal Attack (PReMA) to manipulate the generated content by modifying the input image in conjunction with any specified prompt, without altering the prompt itself. PReMA is the first attack that manipulates model outputs by solely creating adversarial images, distinguishing itself from prior methods that primarily generate adversarial prompts to produce NSFW content. Consequently, PReMA poses a novel threat to the integrity of multi-modal diffusion models, particularly in image-editing applications that operate with fixed prompts. Comprehensive evaluations conducted on image inpainting and style transfer tasks across various models confirm the potent efficacy of PReMA.
>
---
#### [new 054] Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection
- **分类: cs.CV**

- **简介: 该论文针对少样本异常检测任务，解决因图像级文本描述导致视觉与语义错位的问题。提出多层级细粒度语义描述（MFSC）与FineGrainedAD框架，通过可学习提示与多层级对齐策略，提升异常定位精度。**

- **链接: [http://arxiv.org/pdf/2510.26464v1](http://arxiv.org/pdf/2510.26464v1)**

> **作者:** Yuanting Fan; Jun Liu; Xiaochen Chen; Bin-Bin Gao; Jian Li; Yong Liu; Jinlong Peng; Chengjie Wang
>
> **备注:** 12 pages, 7 figures
>
> **摘要:** Few-shot anomaly detection (FSAD) methods identify anomalous regions with few known normal samples. Most existing methods rely on the generalization ability of pre-trained vision-language models (VLMs) to recognize potentially anomalous regions through feature similarity between text descriptions and images. However, due to the lack of detailed textual descriptions, these methods can only pre-define image-level descriptions to match each visual patch token to identify potential anomalous regions, which leads to the semantic misalignment between image descriptions and patch-level visual anomalies, achieving sub-optimal localization performance. To address the above issues, we propose the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and fine-grained textual descriptions for existing anomaly detection datasets with automatic construction pipeline. Based on the MFSC, we propose a novel framework named FineGrainedAD to improve anomaly localization performance, which consists of two components: Multi-Level Learnable Prompt (MLLP) and Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics into multi-level learnable prompts through automatic replacement and concatenation mechanism, while MLSA designs region aggregation strategy and multi-level alignment training to facilitate learnable prompts better align with corresponding visual regions. Experiments demonstrate that the proposed FineGrainedAD achieves superior overall performance in few-shot settings on MVTec-AD and VisA datasets.
>
---
#### [new 055] CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark
- **分类: cs.CV**

- **简介: 该论文提出CRAG-MM基准，针对可穿戴设备场景下的多模态多轮问答任务，解决现有MM-RAG缺乏全面评估的问题。构建了6.5K图像-问题-答案三元组与2K多轮对话数据，涵盖多种图像质量与问题类型，设计三类任务并提供检索接口。实验表明当前方法性能有限，基准已推动领域进步。**

- **链接: [http://arxiv.org/pdf/2510.26160v1](http://arxiv.org/pdf/2510.26160v1)**

> **作者:** Jiaqi Wang; Xiao Yang; Kai Sun; Parth Suresh; Sanat Sharma; Adam Czyzewski; Derek Andersen; Surya Appini; Arkav Banerjee; Sajal Choudhary; Shervin Ghasemlou; Ziqiang Guan; Akil Iyer; Haidar Khan; Lingkun Kong; Roy Luo; Tiffany Ma; Zhen Qiao; David Tran; Wenfang Xu; Skyler Yeatman; Chen Zhou; Gunveer Gujral; Yinglong Xia; Shane Moon; Nicolas Scheffer; Nirav Shah; Eun Chang; Yue Liu; Florian Metze; Tammy Stark; Zhaleh Feizollahi; Andrea Jessee; Mangesh Pujari; Ahmed Aly; Babak Damavandi; Rakesh Wanga; Anuj Kumar; Rohit Patel; Wen-tau Yih; Xin Luna Dong
>
> **摘要:** Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations -- each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions have similar quality (32%/45%), underscoring ample room for improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K participants and 5K submissions, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field.
>
---
#### [new 056] SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine Segmentation from Ultrasound Volume Projection Imaging
- **分类: cs.CV**

- **简介: 该论文针对超声体积投影成像中的脊柱分割任务，解决因忽略骨骼结构特征与空间相关性导致的分割不准确问题。提出SA²Net模型，通过尺度自适应互补策略捕捉长距离关联，并引入结构-亲和变换融合类特定亲和信息，结合Transformer解码器实现结构感知推理，显著提升分割精度与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2510.26568v1](http://arxiv.org/pdf/2510.26568v1)**

> **作者:** Hao Xie; Zixun Huang; Yushen Zuo; Yakun Ju; Frank H. F. Leung; N. F. Law; Kin-Man Lam; Yong-Ping Zheng; Sai Ho Ling
>
> **备注:** Accepted by Computerized Medical Imaging and Graphics (CMIG)
>
> **摘要:** Spine segmentation, based on ultrasound volume projection imaging (VPI), plays a vital role for intelligent scoliosis diagnosis in clinical applications. However, this task faces several significant challenges. Firstly, the global contextual knowledge of spines may not be well-learned if we neglect the high spatial correlation of different bone features. Secondly, the spine bones contain rich structural knowledge regarding their shapes and positions, which deserves to be encoded into the segmentation process. To address these challenges, we propose a novel scale-adaptive structure-aware network (SA$^{2}$Net) for effective spine segmentation. First, we propose a scale-adaptive complementary strategy to learn the cross-dimensional long-distance correlation features for spinal images. Second, motivated by the consistency between multi-head self-attention in Transformers and semantic level affinity, we propose structure-affinity transformation to transform semantic features with class-specific affinity and combine it with a Transformer decoder for structure-aware reasoning. In addition, we adopt a feature mixing loss aggregation method to enhance model training. This method improves the robustness and accuracy of the segmentation process. The experimental results demonstrate that our SA$^{2}$Net achieves superior segmentation performance compared to other state-of-the-art methods. Moreover, the adaptability of SA$^{2}$Net to various backbones enhances its potential as a promising tool for advanced scoliosis diagnosis using intelligent spinal image analysis. The code and experimental demo are available at https://github.com/taetiseo09/SA2Net.
>
---
#### [new 057] ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts
- **分类: cs.CV; cs.AI**

- **简介: 该论文提出ConceptScope框架，用于自动识别视觉数据集中的偏差。针对缺乏细粒度标注导致的偏差检测困难问题，利用稀疏自编码器从视觉基础模型中提取可解释概念，分类并量化目标、上下文与偏差概念，实现数据集表征、偏见发现与模型鲁棒性评估。**

- **链接: [http://arxiv.org/pdf/2510.26186v1](http://arxiv.org/pdf/2510.26186v1)**

> **作者:** Jinho Choi; Hyesu Lim; Steffen Schneider; Jaegul Choo
>
> **备注:** Published in the Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025)
>
> **摘要:** Dataset bias, where data points are skewed to certain concepts, is ubiquitous in machine learning datasets. Yet, systematically identifying these biases is challenging without costly, fine-grained attribute annotations. We present ConceptScope, a scalable and automated framework for analyzing visual datasets by discovering and quantifying human-interpretable concepts using Sparse Autoencoders trained on representations from vision foundation models. ConceptScope categorizes concepts into target, context, and bias types based on their semantic relevance and statistical correlation to class labels, enabling class-level dataset characterization, bias identification, and robustness evaluation through concept-based subgrouping. We validate that ConceptScope captures a wide range of visual concepts, including objects, textures, backgrounds, facial attributes, emotions, and actions, through comparisons with annotated datasets. Furthermore, we show that concept activations produce spatial attributions that align with semantically meaningful image regions. ConceptScope reliably detects known biases (e.g., background bias in Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects in ImageNet), offering a practical tool for dataset auditing and model diagnostics.
>
---
#### [new 058] The Impact and Outlook of 3D Gaussian Splatting
- **分类: cs.CV; cs.GR**

- **简介: 该论文综述3D高斯点阵（3DGS）技术的发展，聚焦其在高效训练、动态场景建模、数学基础深化及移动端/VR应用等方面的进展。旨在解决3D场景表示的效率与实用性问题，推动其向大规模、实时、可部署方向演进。**

- **链接: [http://arxiv.org/pdf/2510.26694v1](http://arxiv.org/pdf/2510.26694v1)**

> **作者:** Bernhard Kerbl
>
> **备注:** Article written for Frontiers of Science Award, International Congress on Basic Science, 2025
>
> **摘要:** Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed the landscape of 3D scene representations, inspiring an extensive body of associated research. Follow-up work includes analyses and contributions that enhance the efficiency, scalability, and real-world applicability of 3DGS. In this summary, we present an overview of several key directions that have emerged in the wake of 3DGS. We highlight advances enabling resource-efficient training and rendering, the evolution toward dynamic (or four-dimensional, 4DGS) representations, and deeper exploration of the mathematical foundations underlying its appearance modeling and rendering process. Furthermore, we examine efforts to bring 3DGS to mobile and virtual reality platforms, its extension to massive-scale environments, and recent progress toward near-instant radiance field reconstruction via feed-forward or distributed computation. Collectively, these developments illustrate how 3DGS has evolved from a breakthrough representation into a versatile and foundational tool for 3D vision and graphics.
>
---
#### [new 059] Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping
- **分类: cs.CV**

- **简介: 该论文聚焦于基于眼周区域的深度人脸识别任务，旨在提升小样本条件下的识别性能。通过在大规模VGGFace2数据集上训练CNN模型，并利用眼周裁剪图像进行实验，显著提升了识别精度，尤其在高质量数据集UFPR-Periocular上实现了当前最低的EER（1-2%）。**

- **链接: [http://arxiv.org/pdf/2510.26294v1](http://arxiv.org/pdf/2510.26294v1)**

> **作者:** Fernando Alonso-Fernandez; Kevin Hernandez-Diaz; Jose Maria Buades Rubio; Josef Bigun
>
> **备注:** Published at IWAIPR 2025 conference
>
> **摘要:** We focus on ocular biometrics, specifically the periocular region (the area around the eye), which offers high discrimination and minimal acquisition constraints. We evaluate three Convolutional Neural Network architectures of varying depth and complexity to assess their effectiveness for periocular recognition. The networks are trained on 1,907,572 ocular crops extracted from the large-scale VGGFace2 database. This significantly contrasts with existing works, which typically rely on small-scale periocular datasets for training having only a few thousand images. Experiments are conducted with ocular images from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images, and the UFPR-Periocular database, which consists of selfies captured via mobile devices with user guidance on the screen. Due to the uncontrolled conditions of VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from 9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In contrast, UFPR-Periocular yields significantly better performance (EERs of 1-2%), thanks to higher image quality and more consistent acquisition protocols. To the best of our knowledge, these are the lowest reported EERs on the UFPR dataset to date.
>
---
#### [new 060] OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes
- **分类: cs.CV; cs.GR; cs.LG**

- **简介: 该论文提出OmniX框架，解决2D全景生成与3D场景重建中感知与生成脱节的问题。通过复用2D生成模型实现全景几何、纹理及PBR材质的统一感知，构建图形级3D场景，支持物理渲染与模拟。工作包括设计跨模态适配器结构和建立大规模合成全景数据集。**

- **链接: [http://arxiv.org/pdf/2510.26800v1](http://arxiv.org/pdf/2510.26800v1)**

> **作者:** Yukun Huang; Jiwen Yu; Yanning Zhou; Jianan Wang; Xintao Wang; Pengfei Wan; Xihui Liu
>
> **备注:** Project page: https://yukun-huang.github.io/OmniX/
>
> **摘要:** There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.
>
---
#### [new 061] Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models
- **分类: cs.CV; cs.CL**

- **简介: 该论文聚焦视觉语言模型的时间推理能力，提出AoT-PsyPhyBENCH基准，评估模型判断视频时间方向（正向/反向）的能力。针对现有模型在物理不可逆过程与因果动作理解上的薄弱表现，通过心理物理学验证的自然视频数据集，揭示其在时间连续性与因果推理上的根本缺陷，并开源数据与代码以推动研究。**

- **链接: [http://arxiv.org/pdf/2510.26241v1](http://arxiv.org/pdf/2510.26241v1)**

> **作者:** Shiho Matta; Lis Kanashiro Pereira; Peitao Han; Fei Cheng; Shigeru Kitazawa
>
> **备注:** 10 pages
>
> **摘要:** Modern vision-language models (VLMs) excel at many multimodal tasks, yet their grasp of temporal information in video remains weak and, crucially, under-evaluated. We probe this gap with a deceptively simple but revealing challenge: judging the arrow of time (AoT)-whether a short clip is played forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated benchmark that tests whether VLMs can infer temporal direction in natural videos using the same stimuli and behavioral baselines established for humans. Our comprehensive evaluation of open-weight and proprietary, reasoning and non-reasoning VLMs reveals that most models perform near chance, and even the best lag far behind human accuracy on physically irreversible processes (e.g., free fall, diffusion/explosion) and causal manual actions (division/addition) that humans recognize almost instantly. These results highlight a fundamental gap in current multimodal systems: while they capture rich visual-semantic correlations, they lack the inductive biases required for temporal continuity and causal understanding. We release the code and data for AoT-PsyPhyBENCH to encourage further progress in the physical and temporal reasoning capabilities of VLMs.
>
---
#### [new 062] Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh
- **分类: cs.CV**

- **简介: 该论文属于目标检测任务，旨在解决自动识别城市交通中难以区分的电动三轮车问题。针对孟加拉国交通监控中电动三轮车识别困难的问题，提出基于YOLOv8的实时检测方法，构建并公开了1730张标注数据集，实现高精度检测（mAP50=83.447%），有效应对密集与稀疏交通场景。**

- **链接: [http://arxiv.org/pdf/2510.26154v1](http://arxiv.org/pdf/2510.26154v1)**

> **作者:** Sudipto Das Sukanto; Diponker Roy; Fahim Shakil; Nirjhar Singha; Abdullah Asik; Aniket Joarder; Mridha Md Nafis Fuad; Muhammad Ibrahim
>
> **备注:** 16 pages
>
> **摘要:** Modes of transportation vary across countries depending on geographical location and cultural context. In South Asian countries rickshaws are among the most common means of local transport. Based on their mode of operation, rickshaws in cities across Bangladesh can be broadly classified into non-auto (pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from accessing certain routes. However, existing surveillance systems make it quite difficult to monitor them due to their similarity to other vehicles, especially non-auto rickshaws whereas manual video analysis is too time-consuming. This paper presents a machine learning-based approach to automatically detect auto-rickshaws in traffic images. In this system, we used real-time object detection using the YOLOv8 model. For training purposes, we prepared a set of 1,730 annotated images that were captured under various traffic conditions. The results show that our proposed model performs well in real-time auto-rickshaw detection and offers an mAP50 of 83.447% and binary precision and recall values above 78%, demonstrating its effectiveness in handling both dense and sparse traffic scenarios. The dataset has been publicly released for further research.
>
---
#### [new 063] JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting
- **分类: cs.CV**

- **简介: 该论文提出JOGS框架，解决传统新视角合成中依赖外部相机位姿估计（如COLMAP）导致的误差传播与计算瓶颈问题。通过联合优化3D高斯点与相机位姿，采用交替优化策略，提升重建精度与位姿准确性，在复杂场景下表现更优。**

- **链接: [http://arxiv.org/pdf/2510.26117v1](http://arxiv.org/pdf/2510.26117v1)**

> **作者:** Yuxuan Li; Tao Wang; Xianben Yang
>
> **摘要:** Traditional novel view synthesis methods heavily rely on external camera pose estimation tools such as COLMAP, which often introduce computational bottlenecks and propagate errors. To address these challenges, we propose a unified framework that jointly optimizes 3D Gaussian points and camera poses without requiring pre-calibrated inputs. Our approach iteratively refines 3D Gaussian parameters and updates camera poses through a novel co-optimization strategy, ensuring simultaneous improvements in scene reconstruction fidelity and pose accuracy. The key innovation lies in decoupling the joint optimization into two interleaved phases: first, updating 3D Gaussian parameters via differentiable rendering with fixed poses, and second, refining camera poses using a customized 3D optical flow algorithm that incorporates geometric and photometric constraints. This formulation progressively reduces projection errors, particularly in challenging scenarios with large viewpoint variations and sparse feature distributions, where traditional methods struggle. Extensive evaluations on multiple datasets demonstrate that our approach significantly outperforms existing COLMAP-free techniques in reconstruction quality, and also surpasses the standard COLMAP-based baseline in general.
>
---
#### [new 064] A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading
- **分类: cs.CV**

- **简介: 该论文针对糖尿病视网膜病变（DR）分级任务，旨在突破单一CNN或ViT模型的性能瓶颈。提出基于证据理论的混合框架，融合CNN局部特征与ViT全局特征，通过深度证据网络生成支持证据，自适应优化特征融合，提升分类准确率与决策可解释性。**

- **链接: [http://arxiv.org/pdf/2510.26315v1](http://arxiv.org/pdf/2510.26315v1)**

> **作者:** Junlai Qiu; Yunzhu Chen; Hao Zheng; Yawen Huang; Yuexiang Li
>
> **摘要:** Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged and elderly people, which significantly impacts their daily lives and mental health. To improve the efficiency of clinical screening and enable the early detection of DR, a variety of automated DR diagnosis systems have been recently established based on convolutional neural network (CNN) or vision Transformer (ViT). However, due to the own shortages of CNN / ViT, the performance of existing methods using single-type backbone has reached a bottleneck. One potential way for the further improvements is integrating different kinds of backbones, which can fully leverage the respective strengths of them (\emph{i.e.,} the local feature extraction capability of CNN and the global feature capturing ability of ViT). To this end, we propose a novel paradigm to effectively fuse the features extracted by different backbones based on the theory of evidence. Specifically, the proposed evidential fusion paradigm transforms the features from different backbones into supporting evidences via a set of deep evidential networks. With the supporting evidences, the aggregated opinion can be accordingly formed, which can be used to adaptively tune the fusion pattern between different backbones and accordingly boost the performance of our hybrid model. We evaluated our method on two publicly available DR grading datasets. The experimental results demonstrate that our hybrid model not only improves the accuracy of DR grading, compared to the state-of-the-art frameworks, but also provides the excellent interpretability for feature fusion and decision-making.
>
---
#### [new 065] BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation
- **分类: cs.CV; I.4.3**

- **简介: 该论文提出BasicAVSR，解决任意尺度视频超分辨率任务中的细节恢复、时序一致性和计算效率问题。通过引入频域先验、光流引导传播、二阶运动补偿和超采样单元，构建可适应在线/离线场景的强基线模型，显著提升重建质量与速度。**

- **链接: [http://arxiv.org/pdf/2510.26149v1](http://arxiv.org/pdf/2510.26149v1)**

> **作者:** Wei Shang; Wanying Zhang; Shuhang Gu; Pengfei Zhu; Qinghua Hu; Dongwei Ren
>
> **备注:** 13 pages, 10 figures, 5 tables
>
> **摘要:** Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution of video frames, potentially at various scaling factors, which presents several challenges regarding spatial detail reproduction, temporal consistency, and computational complexity. In this paper, we propose a strong baseline BasicAVSR for AVSR by integrating four key components: 1) adaptive multi-scale frequency priors generated from image Laplacian pyramids, 2) a flow-guided propagation unit to aggregate spatiotemporal information from adjacent frames, 3) a second-order motion compensation unit for more accurate spatial alignment of adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and content-independent upsampling kernels. To meet diverse application demands, we instantiate three propagation variants: (i) a unidirectional RNN unit for strictly online inference, (ii) a unidirectional RNN unit empowered with a limited lookahead that tolerates a small output delay, and (iii) a bidirectional RNN unit designed for offline tasks where computational resources are less constrained. Experimental results demonstrate the effectiveness and adaptability of our model across these different scenarios. Through extensive experiments, we show that BasicAVSR significantly outperforms existing methods in terms of super-resolution quality, generalization ability, and inference speed. Our work not only advances the state-of-the-art in AVSR but also extends its core components to multiple frameworks for diverse scenarios. The code is available at https://github.com/shangwei5/BasicAVSR.
>
---
#### [new 066] FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation
- **分类: cs.CV**

- **简介: 该论文提出FlexICL框架，解决儿科肘腕超声图像中骨骼结构自动分割因标注数据稀缺导致的难题。通过少样本帧标注与视觉上下文学习，结合创新图像拼接与增强策略，在仅5%标注数据下实现高效精准分割，显著优于现有模型。**

- **链接: [http://arxiv.org/pdf/2510.26049v1](http://arxiv.org/pdf/2510.26049v1)**

> **作者:** Yuyue Zhou; Jessica Knight; Shrimanti Ghosh; Banafshe Felfeliyan; Jacob L. Jaremko; Abhilash R. Hareendranathan
>
> **摘要:** Elbow and wrist fractures are the most common fractures in pediatric populations. Automatic segmentation of musculoskeletal structures in ultrasound (US) can improve diagnostic accuracy and treatment planning. Fractures appear as cortical defects but require expert interpretation. Deep learning (DL) can provide real-time feedback and highlight key structures, helping lightly trained users perform exams more confidently. However, pixel-wise expert annotations for training remain time-consuming and costly. To address this challenge, we propose FlexICL, a novel and flexible in-context learning (ICL) framework for segmenting bony regions in US images. We apply it to an intra-video segmentation setting, where experts annotate only a small subset of frames, and the model segments unseen frames. We systematically investigate various image concatenation techniques and training strategies for visual ICL and introduce novel concatenation methods that significantly enhance model performance with limited labeled data. By integrating multiple augmentation strategies, FlexICL achieves robust segmentation performance across four wrist and elbow US datasets while requiring only 5% of the training images. It outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and conventional segmentation models like U-Net and TransUNet by 1-27% Dice coefficient on 1,252 US sweeps. These initial results highlight the potential of FlexICL as an efficient and scalable solution for US image segmentation well suited for medical imaging use cases where labeled data is scarce.
>
---
#### [new 067] Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文研究视频模型作为零样本推理器的可行性，聚焦Veo-3模型在12个视觉推理维度的表现。通过构建MME-CoF基准，系统评估其链式帧（CoF）推理能力，发现模型在短时空间一致性等方面表现良好，但在长时因果推理和抽象逻辑上仍有限。结论为当前模型尚不能独立胜任复杂推理任务，但可作为辅助视觉引擎。**

- **链接: [http://arxiv.org/pdf/2510.26802v1](http://arxiv.org/pdf/2510.26802v1)**

> **作者:** Ziyu Guo; Xinyan Chen; Renrui Zhang; Ruichuan An; Yu Qi; Dongzhi Jiang; Xiangtai Li; Manyuan Zhang; Hongsheng Li; Pheng-Ann Heng
>
> **备注:** Project Page: https://video-cof.github.io
>
> **摘要:** Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io
>
---
#### [new 068] Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders
- **分类: cs.CV**

- **简介: 该论文针对视频大模型在时间理解上的不足，提出在视觉编码器中引入堆叠时序注意力模块，增强对动作序列和帧间关系的捕捉能力。实验表明，该方法显著提升视频问答与动作识别性能，在多个基准上最高提升5.5%。**

- **链接: [http://arxiv.org/pdf/2510.26027v1](http://arxiv.org/pdf/2510.26027v1)**

> **作者:** Ali Rasekh; Erfan Bagheri Soula; Omid Daliran; Simon Gottschalk; Mohsen Fayyaz
>
> **备注:** Accepted to NeurIPS 2025
>
> **摘要:** Despite significant advances in Multimodal Large Language Models (MLLMs), understanding complex temporal dynamics in videos remains a major challenge. Our experiments show that current Video Large Language Model (Video-LLM) architectures have critical limitations in temporal understanding, struggling with tasks that require detailed comprehension of action sequences and temporal progression. In this work, we propose a Video-LLM architecture that introduces stacked temporal attention modules directly within the vision encoder. This design incorporates a temporal attention in vision encoder, enabling the model to better capture the progression of actions and the relationships between frames before passing visual tokens to the LLM. Our results show that this approach significantly improves temporal reasoning and outperforms existing models in video question answering tasks, specifically in action recognition. We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to +5.5%. By enhancing the vision encoder with temporal structure, we address a critical gap in video understanding for Video-LLMs. Project page and code are available at: https://alirasekh.github.io/STAVEQ2/.
>
---
#### [new 069] Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology
- **分类: cs.CV**

- **简介: 该论文针对真实场景下敏捷遥感卫星星座调度难题，提出首个大规模基准集AEOS-Bench与基于Transformer的调度模型AEOS-Former。通过高保真仿真生成带真实约束的多规模场景，结合约束感知注意力机制与迭代学习，显著提升任务完成率与能效。**

- **链接: [http://arxiv.org/pdf/2510.26297v1](http://arxiv.org/pdf/2510.26297v1)**

> **作者:** Luting Wang; Yinghao Xiang; Hongliang Huang; Dongjun Li; Chen Gao; Si Liu
>
> **摘要:** Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented flexibility for monitoring the Earth's surface, but their scheduling remains challenging under large-scale scenarios, dynamic environments, and stringent constraints. Existing methods often simplify these complexities, limiting their real-world performance. We address this gap with a unified framework integrating a standardized benchmark suite and a novel scheduling model. Our benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and $16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to $300$ imaging tasks. These scenarios are generated via a high-fidelity simulation platform, ensuring realistic satellite behavior such as orbital dynamics and resource constraints. Ground truth scheduling annotations are provided for each scenario. To our knowledge, AEOS-Bench is the first large-scale benchmark suite tailored for realistic constellation scheduling. Building upon this benchmark, we introduce AEOS-Former, a Transformer-based scheduling model that incorporates a constraint-aware attention mechanism. A dedicated internal constraint module explicitly models the physical and operational limits of each satellite. Through simulation-based iterative learning, AEOS-Former adapts to diverse scenarios, offering a robust solution for AEOS constellation scheduling. Experimental results demonstrate that AEOS-Former outperforms baseline models in task completion and energy efficiency, with ablation studies highlighting the contribution of each component. Code and data are provided in https://github.com/buaa-colalab/AEOSBench.
>
---
#### [new 070] ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching
- **分类: cs.CV; cs.AI**

- **简介: 该论文提出ResMatching，一种基于引导条件流匹配的噪声鲁棒计算超分辨率方法，用于荧光显微成像。针对低分辨率图像中高频信息缺失与噪声干扰问题，通过学习强数据先验提升重建质量，实现高保真度与感知真实性的最佳平衡，并可输出像素级不确定性估计。**

- **链接: [http://arxiv.org/pdf/2510.26601v1](http://arxiv.org/pdf/2510.26601v1)**

> **作者:** Anirban Ray; Vera Galinova; Florian Jug
>
> **备注:** 5 pages, 4 figures
>
> **摘要:** Computational Super-Resolution (CSR) in fluorescence microscopy has, despite being an ill-posed problem, a long history. At its very core, CSR is about finding a prior that can be used to extrapolate frequencies in a micrograph that have never been imaged by the image-generating microscope. It stands to reason that, with the advent of better data-driven machine learning techniques, stronger prior can be learned and hence CSR can lead to better results. Here, we present ResMatching, a novel CSR method that uses guided conditional flow matching to learn such improved data-priors. We evaluate ResMatching on 4 diverse biological structures from the BioSR dataset and compare its results against 7 baselines. ResMatching consistently achieves competitive results, demonstrating in all cases the best trade-off between data fidelity and perceptual realism. We observe that CSR using ResMatching is particularly effective in cases where a strong prior is hard to learn, e.g. when the given low-resolution images contain a lot of noise. Additionally, we show that ResMatching can be used to sample from an implicitly learned posterior distribution and that this distribution is calibrated for all tested use-cases, enabling our method to deliver a pixel-wise data-uncertainty term that can guide future users to reject uncertain predictions.
>
---
#### [new 071] ChartAB: A Benchmark for Chart Grounding & Dense Alignment
- **分类: cs.CV**

- **简介: 该论文提出ChartAB基准，用于评估视觉语言模型在图表理解中的细粒度对齐能力。针对现有模型在图表细节感知与跨图比较上的不足，构建了包含数据提取、元素定位与属性识别的评测体系，并设计双阶段推理流程，揭示模型在图表理解中的偏差与弱点，推动模型向更精准的图表分析发展。**

- **链接: [http://arxiv.org/pdf/2510.26781v1](http://arxiv.org/pdf/2510.26781v1)**

> **作者:** Aniruddh Bansal; Davit Soselia; Dang Nguyen; Tianyi Zhou
>
> **摘要:** Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.
>
---
#### [new 072] Comparative Analysis of Deep Learning Models for Olive Tree Crown and Shadow Segmentation Towards Biovolume Estimation
- **分类: eess.IV; cs.CV**

- **简介: 该论文针对橄榄树生物量估算任务，解决高分辨率无人机影像中树冠与阴影分割难题。通过对比U-Net、YOLOv11m-seg和Mask R-CNN模型，基于手动标注数据提取空间特征，结合太阳几何计算树高，实现每棵树生物量估计。结果表明，Mask R-CNN精度最高，YOLOv11m-seg速度最快，适用于不同应用场景。**

- **链接: [http://arxiv.org/pdf/2510.26573v1](http://arxiv.org/pdf/2510.26573v1)**

> **作者:** Wondimagegn Abebe Demissie; Stefano Roccella; Rudy Rossetto; Antonio Minnocci; Andrea Vannini; Luca Sebastiani
>
> **备注:** 6 pages, 2025 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor)
>
> **摘要:** Olive tree biovolume estimation is a key task in precision agriculture, supporting yield prediction and resource management, especially in Mediterranean regions severely impacted by climate-induced stress. This study presents a comparative analysis of three deep learning models U-Net, YOLOv11m-seg, and Mask RCNN for segmenting olive tree crowns and their shadows in ultra-high resolution UAV imagery. The UAV dataset, acquired over Vicopisano, Italy, includes manually annotated crown and shadow masks. Building on these annotations, the methodology emphasizes spatial feature extraction and robust segmentation; per-tree biovolume is then estimated by combining crown projected area with shadow-derived height using solar geometry. In testing, Mask R-CNN achieved the best overall accuracy (F1 = 0.86; mIoU = 0.72), while YOLOv11m-seg provided the fastest throughput (0.12 second per image). The estimated biovolumes spanned from approximately 4 to 24 cubic meters, reflecting clear structural differences among trees. These results indicate Mask R-CNN is preferable when biovolume accuracy is paramount, whereas YOLOv11m-seg suits large-area deployments where speed is critical; U-Net remains a lightweight, high-sensitivity option. The framework enables accurate, scalable orchard monitoring and can be further strengthened with DEM or DSM integration and field calibration for operational decision support.
>
---
#### [new 073] DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection System
- **分类: cs.RO; cs.AI; cs.CV**

- **简介: 该论文提出DARTS系统，解决传统交通事件检测依赖固定设施、响应慢、覆盖有限的问题。通过无人机搭载AI算法，实现高机动性实时监控，融合热成像与轻量化模型，提升检测精度与隐私保护，支持事件快速识别、验证及拥堵追踪，显著提升应急响应效率与交通管理灵活性。**

- **链接: [http://arxiv.org/pdf/2510.26004v1](http://arxiv.org/pdf/2510.26004v1)**

> **作者:** Bai Li; Achilleas Kourtellis; Rong Cao; Joseph Post; Brian Porter; Yu Zhang
>
> **备注:** Preprint version. This manuscript is currently under review at Transportation Research Part C: Emerging Technologies. The PDF corresponds to the version submitted in June 2025. The main findings of this work were recognized with the Best Intelligent Transportation Systems Paper Award at the 2025 TRB Annual Meeting
>
> **摘要:** Rapid and reliable incident detection is critical for reducing crash-related fatalities, injuries, and congestion. However, conventional methods, such as closed-circuit television, dashcam footage, and sensor-based detection, separate detection from verification, suffer from limited flexibility, and require dense infrastructure or high penetration rates, restricting adaptability and scalability to shifting incident hotspots. To overcome these challenges, we developed DARTS, a drone-based, AI-powered real-time traffic incident detection system. DARTS integrates drones' high mobility and aerial perspective for adaptive surveillance, thermal imaging for better low-visibility performance and privacy protection, and a lightweight deep learning framework for real-time vehicle trajectory extraction and incident detection. The system achieved 99% detection accuracy on a self-collected dataset and supports simultaneous online visual verification, severity assessment, and incident-induced congestion propagation monitoring via a web-based interface. In a field test on Interstate 75 in Florida, DARTS detected and verified a rear-end collision 12 minutes earlier than the local transportation management center and monitored incident-induced congestion propagation, suggesting potential to support faster emergency response and enable proactive traffic control to reduce congestion and secondary crash risk. Crucially, DARTS's flexible deployment architecture reduces dependence on frequent physical patrols, indicating potential scalability and cost-effectiveness for use in remote areas and resource-constrained settings. This study presents a promising step toward a more flexible and integrated real-time traffic incident detection system, with significant implications for the operational efficiency and responsiveness of modern transportation management.
>
---
#### [new 074] Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods
- **分类: cs.LG; cs.AI; cs.CL; cs.CV**

- **简介: 该论文研究知识蒸馏（KD）对模型去偏能力的迁移影响，针对自然语言推理与图像分类任务，发现KD会削弱去偏效果。通过实验揭示其内在机制，并提出数据增强、迭代蒸馏与权重初始化三种改进方案，首次系统探讨了去偏方法在KD中的可迁移性。**

- **链接: [http://arxiv.org/pdf/2510.26038v1](http://arxiv.org/pdf/2510.26038v1)**

> **作者:** Jiali Cheng; Chirag Agarwal; Hadi Amiri
>
> **摘要:** Knowledge distillation (KD) is an effective method for model compression and transferring knowledge between models. However, its effect on model's robustness against spurious correlations that degrade performance on out-of-distribution data remains underexplored. This study investigates the effect of knowledge distillation on the transferability of ``debiasing'' capabilities from teacher models to student models on natural language inference (NLI) and image classification tasks. Through extensive experiments, we illustrate several key findings: (i) overall the debiasing capability of a model is undermined post-KD; (ii) training a debiased model does not benefit from injecting teacher knowledge; (iii) although the overall robustness of a model may remain stable post-distillation, significant variations can occur across different types of biases; and (iv) we pin-point the internal attention pattern and circuit that causes the distinct behavior post-KD. Given the above findings, we propose three effective solutions to improve the distillability of debiasing methods: developing high quality data for augmentation, implementing iterative knowledge distillation, and initializing student models with weights obtained from teacher models. To the best of our knowledge, this is the first study on the effect of KD on debiasing and its interenal mechanism at scale. Our findings provide understandings on how KD works and how to design better debiasing methods.
>
---
#### [new 075] Self-localization on a 3D map by fusing global and local features from a monocular camera
- **分类: cs.RO; cs.CV**

- **简介: 该论文聚焦于单目相机下的3D地图自定位任务，针对动态障碍物导致传统CNN方法失效的问题，提出融合CNN与视觉变压器（ViT）的新方法，同时利用局部与全局特征，显著提升定位精度与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2510.26170v1](http://arxiv.org/pdf/2510.26170v1)**

> **作者:** Satoshi Kikuch; Masaya Kato; Tsuyoshi Tasaki
>
> **摘要:** Self-localization on a 3D map by using an inexpensive monocular camera is required to realize autonomous driving. Self-localization based on a camera often uses a convolutional neural network (CNN) that can extract local features that are calculated by nearby pixels. However, when dynamic obstacles, such as people, are present, CNN does not work well. This study proposes a new method combining CNN with Vision Transformer, which excels at extracting global features that show the relationship of patches on whole image. Experimental results showed that, compared to the state-of-the-art method (SOTA), the accuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times higher than that without dynamic obstacles. Moreover, the self-localization error of our method is 20.1% smaller than that of SOTA on public datasets. Additionally, our robot using our method can localize itself with 7.51cm error on average, which is more accurate than SOTA.
>
---
#### [new 076] SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for Multi-Organ Segmentation
- **分类: eess.IV; cs.AI; cs.CV**

- **简介: 该论文针对多器官分割任务，解决器官大小形状差异大导致的分割困难问题。提出SPG-CDENet，通过空间先验网络生成粗略定位，结合跨双编码器网络，利用对称交叉注意力与流式解码器增强特征融合与传递，显著提升分割精度。**

- **链接: [http://arxiv.org/pdf/2510.26390v1](http://arxiv.org/pdf/2510.26390v1)**

> **作者:** Xizhi Tian; Changjun Zhou; Yulin. Yang
>
> **摘要:** Multi-organ segmentation is a critical task in computer-aided diagnosis. While recent deep learning methods have achieved remarkable success in image segmentation, huge variations in organ size and shape challenge their effectiveness in multi-organ segmentation. To address these challenges, we propose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel two-stage segmentation paradigm designed to improve multi-organ segmentation accuracy. Our SPG-CDENet consists of two key components: a spatial prior network and a cross dual encoder network. The prior network generates coarse localization maps that delineate the approximate ROI, serving as spatial guidance for the dual encoder network. The cross dual encoder network comprises four essential components: a global encoder, a local encoder, a symmetric cross-attention module, and a flow-based decoder. The global encoder captures global semantic features from the entire image, while the local encoder focuses on features from the prior network. To enhance the interaction between the global and local encoders, a symmetric cross-attention module is proposed across all layers of the encoders to fuse and refine features. Furthermore, the flow-based decoder directly propagates high-level semantic features from the final encoder layer to all decoder layers, maximizing feature preservation and utilization. Extensive qualitative and quantitative experiments on two public datasets demonstrate the superior performance of SPG-CDENet compared to existing segmentation methods. Furthermore, ablation studies further validate the effectiveness of the proposed modules in improving segmentation accuracy.
>
---
#### [new 077] SAMRI: Segment Anything Model for MRI
- **分类: eess.IV; cs.CV**

- **简介: 该论文提出SAMRI，针对MRI图像分割任务，解决传统方法在多变成像条件下的泛化性差问题。通过仅微调掩码解码器的两阶段策略，显著降低训练成本，实现高精度（平均Dice=0.87）与强泛化能力，尤其擅长小且重要的结构分割。**

- **链接: [http://arxiv.org/pdf/2510.26635v1](http://arxiv.org/pdf/2510.26635v1)**

> **作者:** Zhao Wang; Wei Dai; Thuy Thanh Dao; Steffen Bollmann; Hongfu Sun; Craig Engstrom; Shekhar S. Chandra
>
> **摘要:** Accurate magnetic resonance imaging (MRI) segmentation is crucial for clinical decision-making, but remains labor-intensive when performed manually. Convolutional neural network (CNN)-based methods can be accurate and efficient, but often generalize poorly to MRI's variable contrast, intensity inhomogeneity, and protocols. Although the transformer-based Segment Anything Model (SAM) has demonstrated remarkable generalizability in natural images, existing adaptations often treat MRI as another imaging modality, overlooking these modality-specific challenges. We present SAMRI, an MRI-specialized SAM trained and validated on 1.1 million labeled MR slices spanning whole-body organs and pathologies. We demonstrate that SAM can be effectively adapted to MRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing training time by 94% and trainable parameters by 96% versus full-model retraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice of 0.87, delivering state-of-the-art accuracy across anatomical regions and robust generalization on unseen structures, particularly small and clinically important structures.
>
---
#### [new 078] CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse
- **分类: cs.LG; cs.CV; cs.RO**

- **简介: 该论文提出CorVS，一种基于视觉轨迹与传感器数据对应关系的人员识别方法。针对工业仓库中仅靠视觉识别人员不现实的问题，利用深度学习预测轨迹与传感器数据的匹配概率与可靠性，实现高鲁棒性人员识别。通过真实仓库数据验证了方法的有效性。**

- **链接: [http://arxiv.org/pdf/2510.26369v1](http://arxiv.org/pdf/2510.26369v1)**

> **作者:** Kazuma Kano; Yuki Mori; Shin Katayama; Kenta Urano; Takuro Yonezawa; Nobuo Kawaguchi
>
> **备注:** 7 pages, 3 figures, accepted to IPIN 2025
>
> **摘要:** Worker location data is key to higher productivity in industrial sites. Cameras are a promising tool for localization in logistics warehouses since they also offer valuable environmental contexts such as package status. However, identifying individuals with only visual data is often impractical. Accordingly, several prior studies identified people in videos by comparing their trajectories and wearable sensor measurements. While this approach has advantages such as independence from appearance, the existing methods may break down under real-world conditions. To overcome this challenge, we propose CorVS, a novel data-driven person identification method based on correspondence between visual tracking trajectories and sensor measurements. Firstly, our deep learning model predicts correspondence probabilities and reliabilities for every pair of a trajectory and sensor measurements. Secondly, our algorithm matches the trajectories and sensor measurements over time using the predicted probabilities and reliabilities. We developed a dataset with actual warehouse operations and demonstrated the method's effectiveness for real-world applications.
>
---
#### [new 079] Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start
- **分类: cs.LG; cs.AI; cs.CL; cs.CV**

- **简介: 该论文针对多模态大模型冷启动阶段的泛化能力不足问题，提出SPECS框架。通过自蒸馏生成偏好数据，采用基于偏好训练的冷启动策略，解耦任务推理与输出格式，提升泛化性。实验表明，该方法显著优于基线，在多个基准上性能提升显著。**

- **链接: [http://arxiv.org/pdf/2510.25801v1](http://arxiv.org/pdf/2510.25801v1)**

> **作者:** Kun Chen; Peng Shi; Haibo Qiu; Zhixiong Zeng; Siqi Yang; Wenji Mao; Lin Ma
>
> **备注:** Project Page: https://github.com/Kwen-Chen/SPECS-VL
>
> **摘要:** Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of "MLLM-r1" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution "stuckness," improving exploration, stabilizing training, and raising the performance ceiling.
>
---
#### [new 080] Groupwise Registration with Physics-Informed Test-Time Adaptation on Multi-parametric Cardiac MRI
- **分类: eess.IV; cs.CV**

- **简介: 该论文针对多参数心脏MRI图像配准问题，提出一种基于物理信息测试时自适应的分组配准方法。通过利用物理模型生成的合成图像作为参考，实现跨对比度图像的像素级对齐，提升多模态图像配准精度与泛化能力。**

- **链接: [http://arxiv.org/pdf/2510.26022v1](http://arxiv.org/pdf/2510.26022v1)**

> **作者:** Xinqi Li; Yi Zhang; Li-Ting Huang; Hsiao-Huang Chang; Thoralf Niendorf; Min-Chi Ku; Qian Tao; Hsin-Jung Yang
>
> **摘要:** Multiparametric mapping MRI has become a viable tool for myocardial tissue characterization. However, misalignment between multiparametric maps makes pixel-wise analysis challenging. To address this challenge, we developed a generalizable physics-informed deep-learning model using test-time adaptation to enable group image registration across contrast weighted images acquired from multiple physical models (e.g., a T1 mapping model and T2 mapping model). The physics-informed adaptation utilized the synthetic images from specific physics model as registration reference, allows for transductive learning for various tissue contrast. We validated the model in healthy volunteers with various MRI sequences, demonstrating its improvement for multi-modal registration with a wide range of image contrast variability.
>
---
#### [new 081] ProstNFound+: A Prospective Study using Medical Foundation Models for Prostate Cancer Detection
- **分类: eess.IV; cs.CV**

- **简介: 该论文提出ProstNFound+，一种用于微超声图像前列腺癌检测的医疗基础模型。针对现有模型在临床应用中泛化性差的问题，通过适配器微调与临床生物标志物提示编码，实现对新数据的强泛化能力，并在前瞻性验证中表现稳定，生成可解释热图与风险评分，为临床提供可扩展、透明的辅助诊断方案。**

- **链接: [http://arxiv.org/pdf/2510.26703v1](http://arxiv.org/pdf/2510.26703v1)**

> **作者:** Paul F. R. Wilson; Mohamed Harmanani; Minh Nguyen Nhat To; Amoon Jamzad; Tarek Elghareb; Zhuoxin Guo; Adam Kinnaird; Brian Wodlinger; Purang Abolmaesumi; Parvin Mousavi
>
> **摘要:** Purpose: Medical foundation models (FMs) offer a path to build high-performance diagnostic systems. However, their application to prostate cancer (PCa) detection from micro-ultrasound ({\mu}US) remains untested in clinical settings. We present ProstNFound+, an adaptation of FMs for PCa detection from {\mu}US, along with its first prospective validation. Methods: ProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt encoder that embeds PCa-specific clinical biomarkers. The model generates a cancer heatmap and a risk score for clinically significant PCa. Following training on multi-center retrospective data, the model is prospectively evaluated on data acquired five years later from a new clinical site. Model predictions are benchmarked against standard clinical scoring protocols (PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the prospective data, with no performance degradation compared to retrospective evaluation. It aligns closely with clinical scores and produces interpretable heatmaps consistent with biopsy-confirmed lesions. Conclusion: The results highlight its potential for clinical deployment, offering a scalable and interpretable alternative to expert-driven protocols.
>
---
#### [new 082] Clone Deterministic 3D Worlds with Geometrically-Regularized World Models
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文针对世界模型在长时序预测中性能退化的问题，提出几何正则化世界模型（GRWM），通过约束感官轨迹在潜在空间的连续性，提升表示质量。工作聚焦于构建可完全复制确定性3D环境的世界模型，显著增强长期预测的保真度与稳定性，证明了改进表示学习对构建鲁棒世界模型的关键作用。**

- **链接: [http://arxiv.org/pdf/2510.26782v1](http://arxiv.org/pdf/2510.26782v1)**

> **作者:** Zaishuo Xia; Yukuan Lu; Xinyi Li; Yifan Xu; Yubei Chen
>
> **摘要:** A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. Despite rapid progress, current world models remain brittle and degrade over long horizons. We argue that a central cause is representation quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or entangled latents make dynamics learning unnecessarily hard. We therefore ask whether improving representation learning alone can substantially improve world-model performance. In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone and overfit to a deterministic 3D world. We propose Geometrically-Regularized World Models (GRWM), which enforces that consecutive points along a natural sensory trajectory remain close in latent representation space. This approach yields significantly improved latent representations that align closely with the true topology of the environment. GRWM is plug-and-play, requires only minimal architectural modification, scales with trajectory length, and is compatible with diverse latent generative backbones. Across deterministic 3D settings and long-horizon prediction tasks, GRWM significantly increases rollout fidelity and stability. Analyses show that its benefits stem from learning a latent manifold with superior geometric structure. These findings support a clear takeaway: improving representation learning is a direct and useful path to robust world models, delivering reliable long-horizon predictions without enlarging the dynamics module.
>
---
#### [new 083] BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric Brain MRI
- **分类: eess.IV; cs.CV**

- **简介: 该论文针对儿科脑部MRI图像中伪影严重程度评估任务，解决因类别不平衡导致的模型性能下降问题。提出BRIQA方法，通过梯度损失重加权与旋转批处理策略，提升对少数类伪影（如噪声、条带）的识别能力，显著改善宏平均F1分数。**

- **链接: [http://arxiv.org/pdf/2510.26661v1](http://arxiv.org/pdf/2510.26661v1)**

> **作者:** Alya Almsouti; Ainur Khamitova; Darya Taratynova; Mohammad Yaqub
>
> **摘要:** Assessing the severity of artifacts in pediatric brain Magnetic Resonance Imaging (MRI) is critical for diagnostic accuracy, especially in low-field systems where the signal-to-noise ratio is reduced. Manual quality assessment is time-consuming and subjective, motivating the need for robust automated solutions. In this work, we propose BRIQA (Balanced Reweighting in Image Quality Assessment), which addresses class imbalance in artifact severity levels. BRIQA uses gradient-based loss reweighting to dynamically adjust per-class contributions and employs a rotating batching scheme to ensure consistent exposure to underrepresented classes. Through experiments, no single architecture performs best across all artifact types, emphasizing the importance of architectural diversity. The rotating batching configuration improves performance across metrics by promoting balanced learning when combined with cross-entropy loss. BRIQA improves average macro F1 score from 0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098), Positioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012) artifact severity classification. The code is available at https://github.com/BioMedIA-MBZUAI/BRIQA.
>
---
#### [new 084] AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM
- **分类: cs.RO; cs.CV**

- **简介: 该论文提出AgriGS-SLAM，面向果园季节性变化下的实时3D场景重建任务。针对重复结构、外观变化与叶动导致的定位与建图挑战，融合多视角3D高斯点云渲染与激光雷达直接里程计，通过批处理光栅化与梯度驱动地图管理，在保证实时性的同时提升重建精度与轨迹稳定性。**

- **链接: [http://arxiv.org/pdf/2510.26358v1](http://arxiv.org/pdf/2510.26358v1)**

> **作者:** Mirko Usuelli; David Rapado-Rincon; Gert Kootstra; Matteo Matteucci
>
> **摘要:** Autonomous robots in orchards require real-time 3D scene understanding despite repetitive row geometry, seasonal appearance changes, and wind-driven foliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian Splatting (3DGS) rendering. Batch rasterization across complementary viewpoints recovers orchard structure under occlusions, while a unified gradient-driven map lifecycle executed between keyframes preserves fine details and bounds memory. Pose refinement is guided by a probabilistic LiDAR-based depth consistency term, back-propagated through the camera projection to tighten geometry-appearance coupling. We deploy the system on a field platform in apple and pear orchards across dormancy, flowering, and harvesting, using a standardized trajectory protocol that evaluates both training-view and novel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons and sites, AgriGS-SLAM delivers sharper, more stable reconstructions and steadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while maintaining real-time performance on-tractor. While demonstrated in orchard monitoring, the approach can be applied to other outdoor domains requiring robust multimodal perception.
>
---
#### [new 085] MORE: Multi-Organ Medical Image REconstruction Dataset
- **分类: eess.IV; cs.CV; cs.MM**

- **简介: 该论文针对医学CT图像重建中模型泛化能力差的问题，提出MORE数据集，涵盖9个器官、15类病灶的多样化数据。旨在提升深度学习模型在未见解剖结构和病灶上的泛化性能，并建立强基线方法，验证了大规模异构数据与优化方法对提升重建鲁棒性的有效性。**

- **链接: [http://arxiv.org/pdf/2510.26759v1](http://arxiv.org/pdf/2510.26759v1)**

> **作者:** Shaokai Wu; Yapan Guo; Yanbiao Ji; Jing Tong; Yuxiang Lu; Mei Li; Suizhi Huang; Yue Ding; Hongtao Lu
>
> **备注:** Accepted to ACMMM 2025
>
> **摘要:** CT reconstruction provides radiologists with images for diagnosis and treatment, yet current deep learning methods are typically limited to specific anatomies and datasets, hindering generalization ability to unseen anatomies and lesions. To address this, we introduce the Multi-Organ medical image REconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies with 15 lesion types. This dataset serves two key purposes: (1) enabling robust training of deep learning models on extensive, heterogeneous data, and (2) facilitating rigorous evaluation of model generalization for CT reconstruction. We further establish a strong baseline solution that outperforms prior approaches under these challenging conditions. Our results demonstrate that: (1) a comprehensive dataset helps improve the generalization capability of models, and (2) optimization-based methods offer enhanced robustness for unseen anatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our project page https://more-med.github.io/
>
---
#### [new 086] StructLayoutFormer:Conditional Structured Layout Generation via Structure Serialization and Disentanglement
- **分类: cs.GR; cs.CV**

- **简介: 该论文提出StructLayoutFormer，一种基于Transformer的条件化结构化布局生成方法。针对现有数据驱动方法无法生成可编辑布局结构的问题，通过结构序列化与结构-位置解耦，实现可控的结构化布局生成，支持结构提取与迁移，显著优于基线方法。**

- **链接: [http://arxiv.org/pdf/2510.26141v1](http://arxiv.org/pdf/2510.26141v1)**

> **作者:** Xin Hu; Pengfei Xu; Jin Zhou; Hongbo Fu; Hui Huang
>
> **摘要:** Structured layouts are preferable in many 2D visual contents (\eg, GUIs, webpages) since the structural information allows convenient layout editing. Computational frameworks can help create structured layouts but require heavy labor input. Existing data-driven approaches are effective in automatically generating fixed layouts but fail to produce layout structures. We present StructLayoutFormer, a novel Transformer-based approach for conditional structured layout generation. We use a structure serialization scheme to represent structured layouts as sequences. To better control the structures of generated layouts, we disentangle the structural information from the element placements. Our approach is the first data-driven approach that achieves conditional structured layout generation and produces realistic layout structures explicitly. We compare our approach with existing data-driven layout generation approaches by including post-processing for structure extraction. Extensive experiments have shown that our approach exceeds these baselines in conditional structured layout generation. We also demonstrate that our approach is effective in extracting and transferring layout structures. The code is publicly available at %\href{https://github.com/Teagrus/StructLayoutFormer} {https://github.com/Teagrus/StructLayoutFormer}.
>
---
## 更新

#### [replaced 001] DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment
- **分类: cs.RO; cs.CV**

- **链接: [http://arxiv.org/pdf/2510.17148v3](http://arxiv.org/pdf/2510.17148v3)**

> **作者:** Yu Gao; Anqing Jiang; Yiru Wang; Wang Jijun; Hao Jiang; Zhigang Sun; Heng Yuwen; Wang Shuo; Hao Zhao; Sun Hao
>
> **摘要:** Conventional end-to-end (E2E) driving models are effective at generating physically plausible trajectories, but often fail to generalize to long-tail scenarios due to the lack of essential world knowledge to understand and reason about surrounding environments. In contrast, Vision-Language-Action (VLA) models leverage world knowledge to handle challenging cases, but their limited 3D reasoning capability can lead to physically infeasible actions. In this work we introduce DiffVLA++, an enhanced autonomous driving framework that explicitly bridges cognitive reasoning and E2E planning through metric-guided alignment. First, we build a VLA module directly generating semantically grounded driving trajectories. Second, we design an E2E module with a dense trajectory vocabulary that ensures physical feasibility. Third, and most critically, we introduce a metric-guided trajectory scorer that guides and aligns the outputs of the VLA and E2E modules, thereby integrating their complementary strengths. The experiment on the ICCV 2025 Autonomous Grand Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.
>
---
#### [replaced 002] Neighborhood Feature Pooling for Remote Sensing Image Classification
- **分类: cs.CV; eess.IV; 68T07; I.4.8; I.2.10**

- **链接: [http://arxiv.org/pdf/2510.25077v2](http://arxiv.org/pdf/2510.25077v2)**

> **作者:** Fahimeh Orvati Nia; Amirmohammad Mohammadi; Salim Al Kharsa; Pragati Naikare; Zigfried Hampel-Arias; Joshua Peeples
>
> **备注:** 9 pages, 5 figures
>
> **摘要:** In this work, we propose neighborhood feature pooling (NFP) as a novel texture feature extraction method for remote sensing image classification. The NFP layer captures relationships between neighboring inputs and efficiently aggregates local similarities across feature dimensions. Implemented using convolutional layers, NFP can be seamlessly integrated into any network. Results comparing the baseline models and the NFP method indicate that NFP consistently improves performance across diverse datasets and architectures while maintaining minimal parameter overhead.
>
---
#### [replaced 003] OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models
- **分类: cs.CV; cs.LG; I.2.10; I.4.8; I.2.6**

- **链接: [http://arxiv.org/pdf/2411.10501v2](http://arxiv.org/pdf/2411.10501v2)**

> **作者:** Mathis Koroglu; Hugo Caselles-Dupré; Guillaume Jeanneret Sanmiguel; Matthieu Cord
>
> **备注:** 8 pages, 1 supplementary page, 9 figures
>
> **摘要:** We consider the problem of text-to-video generation tasks with precise control for various applications such as camera movement control and video-to-video editing. Most methods tacking this problem rely on providing user-defined controls, such as binary masks or camera movement embeddings. In our approach we propose OnlyFlow, an approach leveraging the optical flow firstly extracted from an input video to condition the motion of generated videos. Using a text prompt and an input video, OnlyFlow allows the user to generate videos that respect the motion of the input video as well as the text prompt. This is implemented through an optical flow estimation model applied on the input video, which is then fed to a trainable optical flow encoder. The output feature maps are then injected into the text-to-video backbone model. We perform quantitative, qualitative and user preference studies to show that OnlyFlow positively compares to state-of-the-art methods on a wide range of tasks, even though OnlyFlow was not specifically trained for such tasks. OnlyFlow thus constitutes a versatile, lightweight yet efficient method for controlling motion in text-to-video generation. Models and code will be made available on GitHub and HuggingFace.
>
---
#### [replaced 004] Unleashing Diffusion Transformers for Visual Correspondence by Modulating Massive Activations
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.18584v2](http://arxiv.org/pdf/2505.18584v2)**

> **作者:** Chaofan Gan; Yuanpeng Tu; Xi Chen; Tieyuan Chen; Yuxi Li; Mehrtash Harandi; Weiyao Lin
>
> **备注:** NeurIPS 2025
>
> **摘要:** Pre-trained stable diffusion models (SD) have shown great advances in visual correspondence. In this paper, we investigate the capabilities of Diffusion Transformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs exhibit a critical phenomenon in which very few feature activations exhibit significantly larger values than others, known as \textit{massive activations}, leading to uninformative representations and significant performance degradation for DiTs. The massive activations consistently concentrate at very few fixed dimensions across all image patch tokens, holding little local information. We trace these dimension-concentrated massive activations and find that such concentration can be effectively localized by the zero-initialized Adaptive Layer Norm (AdaLN-zero). Building on these findings, we propose Diffusion Transformer Feature (DiTF), a training-free framework designed to extract semantic-discriminative features from DiTs. Specifically, DiTF employs AdaLN to adaptively localize and normalize massive activations with channel-wise modulation. In addition, we develop a channel discard strategy to further eliminate the negative impacts from massive activations. Experimental results demonstrate that our DiTF outperforms both DINO and SD-based models and establishes a new state-of-the-art performance for DiTs in different visual correspondence tasks (\eg, with +9.4\% on Spair-71k and +4.4\% on AP-10K-C.S.).
>
---
#### [replaced 005] NerfBaselines: Consistent and Reproducible Evaluation of Novel View Synthesis Methods
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2406.17345v2](http://arxiv.org/pdf/2406.17345v2)**

> **作者:** Jonas Kulhanek; Torsten Sattler
>
> **备注:** NeurIPS 2025 D&B; Web: https://jkulhanek.com/nerfbaselines
>
> **摘要:** Novel view synthesis is an important problem with many applications, including AR/VR, gaming, and robotic simulations. With the recent rapid development of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) methods, it is becoming difficult to keep track of the current state of the art (SoTA) due to methods using different evaluation protocols, codebases being difficult to install and use, and methods not generalizing well to novel 3D scenes. In our experiments, we show that even tiny differences in the evaluation protocols of various methods can artificially boost the performance of these methods. This raises questions about the validity of quantitative comparisons performed in the literature. To address these questions, we propose NerfBaselines, an evaluation framework which provides consistent benchmarking tools, ensures reproducibility, and simplifies the installation and use of various methods. We validate our implementation experimentally by reproducing the numbers reported in the original papers. For improved accessibility, we release a web platform that compares commonly used methods on standard benchmarks. We strongly believe NerfBaselines is a valuable contribution to the community as it ensures that quantitative results are comparable and thus truly measure progress in the field of novel view synthesis.
>
---
#### [replaced 006] A Continuous and Interpretable Morphometric for Robust Quantification of Dynamic Biological Shapes
- **分类: cs.CV; cs.CG; q-bio.QM**

- **链接: [http://arxiv.org/pdf/2410.21004v2](http://arxiv.org/pdf/2410.21004v2)**

> **作者:** Roua Rouatbi; Juan-Esteban Suarez Cardona; Alba Villaronga-Luque; Jesse V. Veenvliet; Ivo F. Sbalzarini
>
> **摘要:** We introduce the Push-Forward Signed Distance Morphometric (PF-SDM) for shape quantification in biomedical imaging. The PF-SDM compactly encodes geometric and topological properties of closed shapes, including their skeleton and symmetries. This provides robust and interpretable features for shape comparison and machine learning. The PF-SDM is mathematically smooth, providing access to gradients and differential-geometric quantities. It also extends to temporal dynamics and allows fusing spatial intensity distributions, such as genetic markers, with shape dynamics. We present the PF-SDM theory, benchmark it on synthetic data, and apply it to predicting body-axis formation in mouse gastruloids, outperforming a CNN baseline in both accuracy and speed.
>
---
#### [replaced 007] Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features
- **分类: cs.CV; cs.CR**

- **链接: [http://arxiv.org/pdf/2506.21046v2](http://arxiv.org/pdf/2506.21046v2)**

> **作者:** Shangbo Wu; Yu-an Tan; Ruinan Ma; Wencong Ma; Dehua Zhu; Yuanzhang Li
>
> **备注:** 14 pages, 9 figures, accepted at ICCV 2025
>
> **摘要:** The ability of deep neural networks (DNNs) come from extracting and interpreting features from the data provided. By exploiting intermediate features in DNNs instead of relying on hard labels, we craft adversarial perturbation that generalize more effectively, boosting black-box transferability. These features ubiquitously come from supervised learning in previous work. Inspired by the exceptional synergy between self-supervised learning and the Transformer architecture, this paper explores whether exploiting self-supervised Vision Transformer (ViT) representations can improve adversarial transferability. We present dSVA -- a generative dual self-supervised ViT features attack, that exploits both global structural features from contrastive learning (CL) and local textural features from masked image modeling (MIM), the self-supervised learning paradigm duo for ViTs. We design a novel generative training framework that incorporates a generator to create black-box adversarial examples, and strategies to train the generator by exploiting joint features and the attention mechanism of self-supervised ViTs. Our findings show that CL and MIM enable ViTs to attend to distinct feature tendencies, which, when exploited in tandem, boast great adversarial generalizability. By disrupting dual deep features distilled by self-supervised ViTs, we are rewarded with remarkable black-box transferability to models of various architectures that outperform state-of-the-arts. Code available at https://github.com/spencerwooo/dSVA.
>
---
#### [replaced 008] Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.23885v2](http://arxiv.org/pdf/2509.23885v2)**

> **作者:** Guoquan Wei; Liu Shi; Zekun Zhou; Wenzhe Shan; Qiegen Liu
>
> **摘要:** Current models based on deep learning for low-dose CT denoising rely heavily on paired data and generalize poorly. Even the more concerned diffusion models need to learn the distribution of clean data for reconstruction, which is difficult to satisfy in medical clinical applications. At the same time, self-supervised-based methods face the challenge of significant degradation of generalizability of models pre-trained for the current dose to expand to other doses. To address these issues, this work proposes a novel method of TUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised contextual sub-data for low-dose CT reconstruction. Firstly, a contextual subdata self-enhancing similarity strategy is designed for denoising centered on the LDCT projection domain, which provides an initial prior for the subsequent progress. Subsequently, the initial prior is used to combine knowledge distillation with a deep combination of latent diffusion models for optimizing image details. The pre-trained model is used for inference reconstruction, and the pixel-level self-correcting fusion technique is proposed for fine-grained reconstruction of the image domain to enhance the image fidelity, using the initial prior and the LDCT image as a guide. In addition, the technique is flexibly applied to the generalization of upper and lower doses or even unseen doses. Dual-domain strategy cascade for self-supervised LDCT denoising, TurnDiff requires only LDCT projection domain data for training and testing. Comprehensive evaluation on both benchmark datasets and real-world data demonstrates that TurnDiff consistently outperforms state-of-the-art methods in both reconstruction and generalization.
>
---
#### [replaced 009] FARMER: Flow AutoRegressive Transformer over Pixels
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2510.23588v2](http://arxiv.org/pdf/2510.23588v2)**

> **作者:** Guangting Zheng; Qinyu Zhao; Tao Yang; Fei Xiao; Zhijie Lin; Jie Wu; Jiajun Deng; Yanyong Zhang; Rui Zhu
>
> **备注:** Bytedance Seed Technical Report
>
> **摘要:** Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.
>
---
#### [replaced 010] Cycle Diffusion Model for Counterfactual Image Generation
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.24267v2](http://arxiv.org/pdf/2509.24267v2)**

> **作者:** Fangrui Huang; Alan Wang; Binxu Li; Bailey Trang; Ridvan Yesiloglu; Tianyu Hua; Wei Peng; Ehsan Adeli
>
> **摘要:** Deep generative models have demonstrated remarkable success in medical image synthesis. However, ensuring conditioning faithfulness and high-quality synthetic images for direct or counterfactual generation remains a challenge. In this work, we introduce a cycle training framework to fine-tune diffusion models for improved conditioning adherence and enhanced synthetic image realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency between generated and original images by incorporating cycle constraints, enabling more reliable direct and counterfactual generation. Experiments on a combined 3D brain MRI dataset (from ABCD, HCP aging & young adults, ADNI, and PPMI) show that our method improves conditioning accuracy and enhances image quality as measured by FID and SSIM. The results suggest that the cycle strategy used in CDM can be an effective method for refining diffusion-based medical image generation, with applications in data augmentation, counterfactual, and disease progression modeling.
>
---
#### [replaced 011] Learning World Models for Interactive Video Generation
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.21996v2](http://arxiv.org/pdf/2505.21996v2)**

> **作者:** Taiye Chen; Xun Hu; Zihan Ding; Chi Jin
>
> **备注:** Project page: https://sites.google.com/view/vrag
>
> **摘要:** Foundational world models must be both interactive and preserve spatiotemporal coherence for effective future planning with action choices. However, present models for long video generation have limited inherent world modeling capabilities due to two main challenges: compounding errors and insufficient memory mechanisms. We enhance image-to-video models with interactive capabilities through additional action conditioning and autoregressive framework, and reveal that compounding error is inherently irreducible in autoregressive video generation, while insufficient memory mechanism leads to incoherence of world models. We propose video retrieval augmented generation (VRAG) with explicit global state conditioning, which significantly reduces long-term compounding errors and increases spatiotemporal consistency of world models. In contrast, naive autoregressive generation with extended context windows and retrieval-augmented generation prove less effective for video generation, primarily due to the limited in-context learning capabilities of current video models. Our work illuminates the fundamental challenges in video world models and establishes a comprehensive benchmark for improving video generation models with internal world modeling capabilities.
>
---
#### [replaced 012] Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers
- **分类: cs.CV; cs.AI; cs.CL; cs.MA**

- **链接: [http://arxiv.org/pdf/2505.21497v2](http://arxiv.org/pdf/2505.21497v2)**

> **作者:** Wei Pang; Kevin Qinghong Lin; Xiangru Jian; Xi He; Philip Torr
>
> **备注:** Project Page: https://github.com/Paper2Poster/Paper2Poster
>
> **摘要:** Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.
>
---
#### [replaced 013] RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.02393v3](http://arxiv.org/pdf/2506.02393v3)**

> **作者:** Yongxian Liu; Boyang Li; Ting Liu; Zaiping Lin; Wei An
>
> **备注:** We have updated the journal reference and DOI
>
> **摘要:** Infrared small target detection is a challenging task due to its unique characteristics (e.g., small, dim, shapeless and changeable). Recently published CNN-based methods have achieved promising performance with heavy feature extraction and fusion modules. To achieve efficient and effective detection, we propose a recurrent reusable-convolution attention network (RRCA-Net) for infrared small target detection. Specifically, RRCA-Net incorporates reusable-convolution block (RuCB) in a recurrent manner without introducing extra parameters. With the help of the repetitive iteration in RuCB, the high-level information of small targets in the deep layers can be well maintained and further refined. Then, a dual interactive attention aggregation module (DIAAM) is proposed to promote the mutual enhancement and fusion of refined information. In this way, RRCA-Net can both achieve high-level feature refinement and enhance the correlation of contextual information between adjacent layers. Moreover, to achieve steady convergence, we design a target characteristic inspired loss function (DpT-k loss) by integrating physical and mathematical constraints. Experimental results on three benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate that our RRCA-Net can achieve comparable performance to the state-of-the-art methods while maintaining a small number of parameters, and act as a plug and play module to introduce consistent performance improvement for several popular IRSTD methods.
>
---
#### [replaced 014] Empowering Agentic Video Analytics Systems with Video Language Models
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.00254v4](http://arxiv.org/pdf/2505.00254v4)**

> **作者:** Yuxuan Yan; Shiqi Jiang; Ting Cao; Yifan Yang; Qianqian Yang; Yuanchao Shu; Yuqing Yang; Lili Qiu
>
> **备注:** Accepted to NDSI 2026, 19pages, 12 figures, complementary evaluations and appendix
>
> **摘要:** AI-driven video analytics has become increasingly important across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Vision Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVA, a VLM-powered system designed for open-ended, advanced video analytics. AVA incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively-significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVA-100, AVA achieves top-tier performance with an accuracy of 75.8%. The source code of AVA is available at https://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at https://huggingface.co/datasets/iesc/Ava-100.
>
---
#### [replaced 015] StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.18766v2](http://arxiv.org/pdf/2505.18766v2)**

> **作者:** Yanjie Li; Wenxuan Zhang; Xinqi Lyu; Yihao Liu; Bin Xiao
>
> **备注:** Accepted by NIPS2025
>
> **摘要:** Recently, text-to-image diffusion models have been widely used for style mimicry and personalized customization through methods such as DreamBooth and Textual Inversion. This has raised concerns about intellectual property protection and the generation of deceptive content. Recent studies, such as Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect images from these attacks. However, recent purification-based methods, such as DiffPure and Noise Upscaling, have successfully attacked these latest defenses, showing the vulnerabilities of these methods. Moreover, present methods show limited transferability across models, making them less effective against unknown text-to-image models. To address these issues, we propose a novel anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes the style-related features in the latent space to make it deviate from the original image, which improves model-agnostic transferability. Additionally, to enhance the perturbation's ability to bypass diffusion-based purification, we designed a novel upscale loss that involves ensemble purifiers and upscalers during training. Extensive experiments on the WikiArt and CelebA datasets demonstrate that StyleGuard outperforms existing methods in robustness against various transformations and purifications, effectively countering style mimicry in various models. Moreover, StyleGuard is effective on different style mimicry methods, including DreamBooth and Textual Inversion. The code is available at https://github.com/PolyLiYJ/StyleGuard.
>
---
#### [replaced 016] TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection
- **分类: cs.CV; cs.MM**

- **链接: [http://arxiv.org/pdf/2509.04448v2](http://arxiv.org/pdf/2509.04448v2)**

> **作者:** Zehong Yan; Peng Qi; Wynne Hsu; Mong Li Lee
>
> **备注:** EMNLP 2025 Oral; Project Homepage: https://yanzehong.github.io/trust-vl/
>
> **摘要:** Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.
>
---
#### [replaced 017] MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?
- **分类: cs.CV; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2503.09499v3](http://arxiv.org/pdf/2503.09499v3)**

> **作者:** Zhe Xu; Daoyuan Chen; Zhenqing Ling; Yaliang Li; Ying Shen
>
> **备注:** Accepted by NeurIPS'25. 30 pages, 2 figures, 13 tables
>
> **摘要:** Large foundation models face challenges in acquiring transferable, structured thinking abilities, especially when supervised with rigid templates or crowd-annotated instruction datasets. Unlike prior approaches, we focus on a thinking-centric data synthesis paradigm that enables models to evolve through self-generated, cognitively guided data. We propose MindGYM, a structured and scalable framework for question synthesis, composed of: (1) Cognitive Thinking Process Injection, which infuses high-level reasoning objectives to shape the model's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating atomic questions from diverse semantic types to encourage broader thinking; and (3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop questions based on QA seeds for deeper reasoning. Detailed analysis shows that synthetic data generated by our method achieves 16.7% higher average quality and 67.91% lower quality variance compared to baseline sources, highlighting that both high-quality and self-contained data are essential for effective, thinking-oriented fine-tuning. MindGYM improves performance on six reasoning benchmarks, achieving gains of up to 16% on MathVision using only 400 data samples, and generalizable improvements across different model sizes and architectures. MindGYM underscores the viability of self-challenging mechanisms in refining large model capabilities while minimizing human intervention and resource demands. Code and data are released to promote data-centric research into self-evolving foundation models driven by their internal reasoning capabilities.
>
---
#### [replaced 018] Quality-Aware Prototype Memory for Face Representation Learning
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2311.07734v2](http://arxiv.org/pdf/2311.07734v2)**

> **作者:** Evgeny Smirnov; Vasiliy Galyuk; Evgeny Lukyanets
>
> **备注:** Preprint
>
> **摘要:** Prototype Memory is a powerful model for face representation learning. It enables training face recognition models on datasets of any size by generating prototypes (classifier weights) on the fly and efficiently utilizing them. Prototype Memory demonstrated strong results in many face recognition benchmarks. However, the algorithm of prototype generation, used in it, is prone to the problems of imperfectly calculated prototypes in case of low-quality or poorly recognizable faces in the images, selected for the prototype creation. All images of the same person presented in the mini-batch are used with equal weights, and the resulting averaged prototype can be contaminated by imperfect embeddings of low-quality face images. This may lead to misleading training signals and degrade the performance of the trained models. In this paper, we propose a simple and effective way to improve Prototype Memory with quality-aware prototype generation. Quality-Aware Prototype Memory uses different weights for images of different quality in the process of prototype generation. With this improvement, prototypes receive more informative signals from high-quality images and are less affected by low-quality ones. We propose and compare several methods of quality estimation and usage, perform extensive experiments on the different face recognition benchmarks and demonstrate the advantages of the proposed model compared to the basic version of Prototype Memory.
>
---
#### [replaced 019] DDL: A Large-Scale Datasets for Deepfake Detection and Localization in Diversified Real-World Scenarios
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.23292v2](http://arxiv.org/pdf/2506.23292v2)**

> **作者:** Changtao Miao; Yi Zhang; Weize Gao; Zhiya Tan; Weiwei Feng; Man Luo; Jianshu Li; Ajian Liu; Yunfeng Diao; Qi Chu; Tao Gong; Zhe Li; Weibin Yao; Joey Tianyi Zhou
>
> **备注:** This paper is a preliminary version, with an extended and comprehensive version currently under development
>
> **摘要:** Recent advances in AIGC have exacerbated the misuse of malicious deepfake content, making the development of reliable deepfake detection methods an essential means to address this challenge. Although existing deepfake detection models demonstrate outstanding performance in detection metrics, most methods only provide simple binary classification results, lacking interpretability. Recent studies have attempted to enhance the interpretability of classification results by providing spatial manipulation masks or temporal forgery segments. However, due to the limitations of forgery datasets, the practical effectiveness of these methods remains suboptimal. The primary reason lies in the fact that most existing deepfake datasets contain only binary labels, with limited variety in forgery scenarios, insufficient diversity in deepfake types, and relatively small data scales, making them inadequate for complex real-world scenarios.To address this predicament, we construct a novel large-scale deepfake detection and localization (\textbf{DDL}) dataset containing over $\textbf{1.4M+}$ forged samples and encompassing up to $\textbf{80}$ distinct deepfake methods. The DDL design incorporates four key innovations: (1) \textbf{Comprehensive Deepfake Methods} (covering 7 different generation architectures and a total of 80 methods), (2) \textbf{Varied Manipulation Modes} (incorporating 7 classic and 3 novel forgery modes), (3) \textbf{Diverse Forgery Scenarios and Modalities} (including 3 scenarios and 3 modalities), and (4) \textbf{Fine-grained Forgery Annotations} (providing 1.18M+ precise spatial masks and 0.23M+ precise temporal segments).Through these improvements, our DDL not only provides a more challenging benchmark for complex real-world forgeries but also offers crucial support for building next-generation deepfake detection, localization, and interpretability methods.
>
---
#### [replaced 020] Two Heads are Better than One: Robust Learning Meets Multi-branch Models
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2208.08083v3](http://arxiv.org/pdf/2208.08083v3)**

> **作者:** Zongyuan Zhang; Qingwen Bu; Tianyang Duan; Zheng Lin; Yuhao Qing; Zihan Fang; Heming Cui; Dong Huang
>
> **备注:** Camera-ready version for ICPADS 2025
>
> **摘要:** Deep neural networks (DNNs) are vulnerable to adversarial examples, in which DNNs are misled to false outputs due to inputs containing imperceptible perturbations. Adversarial training, a reliable and effective method of defense, may significantly reduce the vulnerability of neural networks and becomes the de facto standard for robust learning. While many recent works practice the data-centric philosophy, such as how to generate better adversarial examples or use generative models to produce additional training data, we look back to the models themselves and revisit the adversarial robustness from the perspective of deep feature distribution as an insightful complementarity. In this paper, we propose \textit{Branch Orthogonality adveRsarial Training} (BORT) to obtain state-of-the-art performance with solely the original dataset for adversarial training. To practice our design idea of integrating multiple orthogonal solution spaces, we leverage a simple and straightforward multi-branch neural network that eclipses adversarial attacks with no increase in inference time. We heuristically propose a corresponding loss function, branch-orthogonal loss, to make each solution space of the multi-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100 and SVHN against $\ell_{\infty}$ norm-bounded perturbations of size $\epsilon = 8/255$, respectively. Exhaustive experiments are conducted to show that our method goes beyond all state-of-the-art methods without any tricks. Compared to all methods that do not use additional data for training, our models achieve 67.3\% and 41.5\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the state-of-the-art by +7.23\% and +9.07\%). We also outperform methods using a training set with a far larger scale than ours.
>
---
#### [replaced 021] Resource Efficient Multi-stain Kidney Glomeruli Segmentation via Self-supervision
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.15389v3](http://arxiv.org/pdf/2412.15389v3)**

> **作者:** Zeeshan Nisar; Friedrich Feuerhake; Thomas Lampert
>
> **备注:** 39 pages, 10 figures, 4 Tables
>
> **摘要:** Semantic segmentation under domain shift remains a fundamental challenge in computer vision, particularly when labelled training data is scarce. This challenge is particularly exemplified in histopathology image analysis, where the same tissue structures must be segmented across images captured under different imaging conditions (stains), each representing a distinct visual domain. Traditional deep learning methods like UNet require extensive labels, which is both costly and time-consuming, particularly when dealing with multiple domains (or stains). To mitigate this, various unsupervised domain adaptation based methods such as UDAGAN have been proposed, which reduce the need for labels by requiring only one (source) stain to be labelled. Nonetheless, obtaining source stain labels can still be challenging. This article shows that through self-supervised pre-training -- including SimCLR, BYOL, and a novel approach, HR-CS-CO -- the performance of these segmentation methods (UNet, and UDAGAN) can be retained even with 95% fewer labels. Notably, with self-supervised pre-training and using only 5% labels, the performance drops are minimal: 5.9% for UNet and 6.2% for UDAGAN, averaged over all stains, compared to their respective fully supervised counterparts (without pre-training, using 100% labels). Furthermore, these findings are shown to generalise beyond their training distribution to public benchmark datasets. Implementations and pre-trained models are publicly available \href{https://github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git}{online}.
>
---
#### [replaced 022] Fit for Purpose? Deepfake Detection in the Real World
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2510.16556v2](http://arxiv.org/pdf/2510.16556v2)**

> **作者:** Guangyu Lin; Li Lin; Christina P. Walker; Daniel S. Schiff; Shu Hu
>
> **摘要:** The rapid proliferation of AI-generated content, driven by advances in generative adversarial networks, diffusion models, and multimodal large language models, has made the creation and dissemination of synthetic media effortless, heightening the risks of misinformation, particularly political deepfakes that distort truth and undermine trust in political institutions. In turn, governments, research institutions, and industry have strongly promoted deepfake detection initiatives as solutions. Yet, most existing models are trained and validated on synthetic, laboratory-controlled datasets, limiting their generalizability to the kinds of real-world political deepfakes circulating on social platforms that affect the public. In this work, we introduce the first systematic benchmark based on the Political Deepfakes Incident Database, a curated collection of real-world political deepfakes shared on social media since 2018. Our study includes a systematic evaluation of state-of-the-art deepfake detectors across academia, government, and industry. We find that the detectors from academia and government perform relatively poorly. While paid detection tools achieve relatively higher performance than free-access models, all evaluated detectors struggle to generalize effectively to authentic political deepfakes, and are vulnerable to simple manipulations, especially in the video domain. Results urge the need for politically contextualized deepfake detection frameworks to better safeguard the public in real-world settings.
>
---
#### [replaced 023] GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular Degeneration Risk Factor Detection and Prediction
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2510.02781v2](http://arxiv.org/pdf/2510.02781v2)**

> **作者:** Daeyoung Kim
>
> **摘要:** Age Related Macular Degeneration(AMD) has been one of the most leading causes of permanent vision impairment in ophthalmology. Though treatments, such as anti VEGF drugs or photodynamic therapies, were developed to slow down the degenerative process of AMD, there is still no specific cure to reverse vision loss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD or AMD itself within the patient retina in early stages is a crucial task to reduce the possibility of vision impairment. Apart from traditional approaches, deep learning based methods, especially attention mechanism based CNNs and GradCAM based XAI analysis on OCT scans, exhibited successful performance in distinguishing AMD retina from normal retinas, making it possible to use AI driven models to aid medical diagnosis and analysis by ophthalmologists regarding AMD. However, though having significant success, previous works mostly focused on prediction performance itself, not pathologies or underlying causal mechanisms of AMD, which can prohibit intervention analysis on specific factors or even lead to less reliable decisions. Thus, this paper introduces a novel causal AMD analysis model: GCVAMD, which incorporates a modified CausalVAE approach that can extract latent causal factors from only raw OCT images. By considering causality in AMD detection, GCVAMD enables causal inference such as treatment simulation or intervention analysis regarding major risk factors: drusen and neovascularization, while returning informative latent causal features that can enhance downstream tasks. Results show that through GCVAMD, drusen status and neovascularization status can be identified with AMD causal mechanisms in GCVAMD latent spaces, which can in turn be used for various tasks from AMD detection(classification) to intervention analysis.
>
---
#### [replaced 024] D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning - A Benchmark Dataset and Method
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2509.06771v2](http://arxiv.org/pdf/2509.06771v2)**

> **作者:** Sai Kartheek Reddy Kasu; Mohammad Zia Ur Rehman; Shahid Shafi Dar; Rishi Bharat Junghare; Dhanvin Sanjay Namboodiri; Nagendra Kumar
>
> **备注:** Accepted at IEEE International Conference on Data Mining (ICDM) 2025
>
> **摘要:** Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning
>
---
#### [replaced 025] Smoothing Slot Attention Iterations and Recurrences
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2508.05417v2](http://arxiv.org/pdf/2508.05417v2)**

> **作者:** Rongzhen Zhao; Wenyan Yang; Juho Kannala; Joni Pajarinen
>
> **摘要:** Slot Attention (SA) and its variants lie at the heart of mainstream Object-Centric Learning (OCL). Objects in an image can be aggregated into respective slot vectors, by \textit{iteratively} refining cold-start query vectors, typically three times, via SA on image features. For video, such aggregation is \textit{recurrently} shared across frames, with queries cold-started on the first frame while transitioned from the previous frame's slots on non-first frames. However, the cold-start queries lack sample-specific cues thus hinder precise aggregation on the image or video's first frame; Also, non-first frames' queries are already sample-specific thus require transforms different from the first frame's aggregation. We address these issues for the first time with our \textit{SmoothSA}: (1) To smooth SA iterations on the image or video's first frame, we \textit{preheat} the cold-start queries with rich information of input features, via a tiny module self-distilled inside OCL; (2) To smooth SA recurrences across all video frames, we \textit{differentiate} the homogeneous transforms on the first and non-first frames, by using full and single iterations respectively. Comprehensive experiments on object discovery, recognition and downstream benchmarks validate our method's effectiveness. Further analyses intuitively illuminate how our method smooths SA iterations and recurrences. Our source code, model checkpoints and training logs are available on https://github.com/Genera1Z/SmoothSA.
>
---
#### [replaced 026] EmoAttack: Emotion-to-Image Diffusion Models for Emotional Backdoor Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2406.15863v3](http://arxiv.org/pdf/2406.15863v3)**

> **作者:** Tianyu Wei; Shanmin Pang; Qi Guo; Yizhuo Ma; Xiaofeng Cao; Qing Guo
>
> **摘要:** Text-to-image diffusion models can generate realistic images based on textual inputs, enabling users to convey their opinions visually through language. Meanwhile, within language, emotion plays a crucial role in expressing personal opinions in our daily lives and the inclusion of maliciously negative content can lead users astray, exacerbating negative emotions. Recognizing the success of diffusion models and the significance of emotion, we investigate a previously overlooked risk associated with text-to-image diffusion models, that is, utilizing emotion in the input texts to introduce negative content and provoke unfavorable emotions in users. Specifically, we identify a new backdoor attack, i.e., emotion-aware backdoor attack (EmoAttack), which introduces malicious negative content triggered by emotional texts during image generation. We formulate such an attack as a diffusion personalization problem to avoid extensive model retraining and propose the EmoBooth. Unlike existing personalization methods, our approach fine-tunes a pre-trained diffusion model by establishing a mapping between a cluster of emotional words and a given reference image containing malicious negative content. To validate the effectiveness of our method, we built a dataset and conducted extensive analysis and discussion about its effectiveness. Given consumers' widespread use of diffusion models, uncovering this threat is critical for society.
>
---
#### [replaced 027] Towards Predicting Any Human Trajectory In Context
- **分类: cs.CV; cs.AI; cs.CL; cs.RO**

- **链接: [http://arxiv.org/pdf/2506.00871v2](http://arxiv.org/pdf/2506.00871v2)**

> **作者:** Ryo Fujii; Hideo Saito; Ryo Hachiuma
>
> **备注:** NeurIPS 2025
>
> **摘要:** Predicting accurate future trajectories of pedestrians is essential for autonomous systems but remains a challenging task due to the need for adaptability in different environments and domains. A common approach involves collecting scenario-specific data and performing fine-tuning via backpropagation. However, the need to fine-tune for each new scenario is often impractical for deployment on edge devices. To address this challenge, we introduce \paper, an In-Context Learning (ICL) framework for pedestrian trajectory prediction that enables adaptation without fine-tuning on the scenario-specific data at inference time without requiring weight updates. We propose a spatio-temporal similarity-based example selection (STES) method that selects relevant examples from previously observed trajectories within the same scene by identifying similar motion patterns at corresponding locations. To further refine this selection, we introduce prediction-guided example selection (PG-ES), which selects examples based on both the past trajectory and the predicted future trajectory, rather than relying solely on the past trajectory. This approach allows the model to account for long-term dynamics when selecting examples. Finally, instead of relying on small real-world datasets with limited scenario diversity, we train our model on a large-scale synthetic dataset to enhance its prediction ability by leveraging in-context examples. Extensive experiments demonstrate that TrajICL achieves remarkable adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches across multiple public benchmarks. Project Page: https://fujiry0.github.io/TrajICL-project-page/.
>
---
#### [replaced 028] UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2501.05783v2](http://arxiv.org/pdf/2501.05783v2)**

> **作者:** Yanjie Li; Kaisheng Liang; Bin Xiao
>
> **备注:** 23 pages, 22 figures, accepted by ICLR2025
>
> **摘要:** In recent research, adversarial attacks on person detectors using patches or static 3D model-based texture modifications have struggled with low success rates due to the flexible nature of human movement. Modeling the 3D deformations caused by various actions has been a major challenge. Fortunately, advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer new possibilities. In this paper, we introduce UV-Attack, a groundbreaking approach that achieves high success rates even with extensive and unseen human actions. We address the challenge above by leveraging dynamic-NeRF-based UV mapping. UV-Attack can generate human images across diverse actions and viewpoints, and even create novel actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying clothing textures is challenging because they are embedded in neural network parameters. To tackle this, UV-Attack generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes the attack more practical. We also propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views. Our experiments show that UV-Attack achieves a 92.7% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly outperforming the state-of-the-art AdvCamou attack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the latest YOLOv8 detector in black-box settings. This work highlights the potential of dynamic NeRF-based UV mapping for creating more effective adversarial attacks on person detectors, addressing key challenges in modeling human movement and texture modification. The code is available at https://github.com/PolyLiYJ/UV-Attack.
>
---
#### [replaced 029] From One to More: Contextual Part Latents for 3D Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.08772v2](http://arxiv.org/pdf/2507.08772v2)**

> **作者:** Shaocong Dong; Lihe Ding; Xiao Chen; Yaokun Li; Yuxin Wang; Yucheng Wang; Qi Wang; Jaehyeok Kim; Chenjian Gao; Zhanpeng Huang; Zibin Wang; Tianfan Xue; Dan Xu
>
> **备注:** Project page: https://copart3d.github.io/
>
> **摘要:** Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.
>
---
#### [replaced 030] Dynamic Traceback Learning for Medical Report Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2401.13267v4](http://arxiv.org/pdf/2401.13267v4)**

> **作者:** Shuchang Ye; Mingyuan Meng; Mingjian Li; Dagan Feng; Usman Naseem; Jinman Kim
>
> **备注:** Accepted to IEEE Transactions on Multimedia (TMM)
>
> **摘要:** Automated medical report generation has demonstrated the potential to significantly reduce the workload associated with time-consuming medical reporting. Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation. However, when trained end-to-end and applied directly to medical image-to-text generation, they face two significant challenges: i) difficulty in accurately capturing subtle yet crucial pathological details, and ii) reliance on both visual and textual inputs during inference, leading to performance degradation in zero-shot inference when only images are available. To address these challenges, this study proposes a novel multimodal dynamic traceback learning framework (DTrace). Specifically, we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of image and text input, enabling text generation without strong reliance on the input from both modalities during inference. The learning of cross-modal knowledge is enhanced by supervising the model to recover masked semantic information from a complementary counterpart. Extensive experiments conducted on two benchmark datasets, IU-Xray and MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms state-of-the-art methods for medical report generation.
>
---
#### [replaced 031] LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.23158v2](http://arxiv.org/pdf/2505.23158v2)**

> **作者:** Jonas Kulhanek; Marie-Julie Rakotosaona; Fabian Manhardt; Christina Tsalicoglou; Michael Niemeyer; Torsten Sattler; Songyou Peng; Federico Tombari
>
> **备注:** NeurIPS 2025; Web: https://lodge-gs.github.io/
>
> **摘要:** In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements.
>
---
#### [replaced 032] HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2508.10566v2](http://arxiv.org/pdf/2508.10566v2)**

> **作者:** Shiyu Liu; Kui Jiang; Xianming Liu; Hongxun Yao; Xiaocheng Feng
>
> **摘要:** Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.
>
---
#### [replaced 033] Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2508.07981v3](http://arxiv.org/pdf/2508.07981v3)**

> **作者:** Fangyuan Mao; Aiming Hao; Jintao Chen; Dongxia Liu; Xiaokun Feng; Jiashu Zhu; Meiqi Wu; Chubin Chen; Jiahong Wu; Xiangxiang Chu
>
> **摘要:** Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.
>
---
#### [replaced 034] KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2508.08186v2](http://arxiv.org/pdf/2508.08186v2)**

> **作者:** Md Meftahul Ferdaus; Mahdi Abdelguerfi; Elias Ioup; Steven Sloan; Kendall N. Niles; Ken Pathak
>
> **备注:** This work has been submitted to the IEEE for possible publication
>
> **摘要:** Semantic segmentation of structural defects in civil infrastructure remains challenging due to variable defect appearances, harsh imaging conditions, and significant class imbalance. Current deep learning methods, despite their effectiveness, typically require millions of parameters, rendering them impractical for real-time inspection systems. We introduce KARMA (Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient semantic segmentation framework that models complex defect patterns through compositions of one-dimensional functions rather than conventional convolutions. KARMA features three technical innovations: (1) a parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging low-rank factorization for KAN-based feature transformation; (2) an optimized feature pyramid structure with separable convolutions for multi-scale defect analysis; and (3) a static-dynamic prototype mechanism that enhances feature representation for imbalanced classes. Extensive experiments on benchmark infrastructure inspection datasets demonstrate that KARMA achieves competitive or superior mean IoU performance compared to state-of-the-art approaches, while using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction). Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for real-time deployment, enabling practical automated infrastructure inspection systems without compromising accuracy. The source code can be accessed at the following URL: https://github.com/faeyelab/karma.
>
---
#### [replaced 035] GameFactory: Creating New Games with Generative Interactive Videos
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2501.08325v4](http://arxiv.org/pdf/2501.08325v4)**

> **作者:** Jiwen Yu; Yiran Qin; Xintao Wang; Pengfei Wan; Di Zhang; Xihui Liu
>
> **备注:** ICCV 2025 Highlight, Project Page: https://yujiwen.github.io/gamefactory
>
> **摘要:** Generative videos have the potential to revolutionize game development by autonomously creating new content. In this paper, we present GameFactory, a framework for action-controlled scene-generalizable game video generation. We first address the fundamental challenge of action controllability by introducing GF-Minecraft, an action-annotated game video dataset without human bias, and developing an action control module that enables precise control over both keyboard and mouse inputs. We further extend to support autoregressive generation for unlimited-length interactive videos. More importantly, GameFactory tackles the critical challenge of scene-generalizable action control, which most existing methods fail to address. To enable the creation of entirely new and diverse games beyond fixed styles and scenes, we leverage the open-domain generative priors from pre-trained video diffusion models. To bridge the domain gap between open-domain priors and small-scale game datasets, we propose a multi-phase training strategy with a domain adapter that decouples game style learning from action control. This decoupling ensures that action control learning is no longer bound to specific game styles, thereby achieving scene-generalizable action control. Experimental results demonstrate that GameFactory effectively generates open-domain action-controllable game videos, representing a significant step forward in AI-driven game generation.
>
---
#### [replaced 036] GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2510.22319v2](http://arxiv.org/pdf/2510.22319v2)**

> **作者:** Jing Wang; Jiajun Liang; Jie Liu; Henglin Liu; Gongye Liu; Jun Zheng; Wanyuan Pang; Ao Ma; Zhenyu Xie; Xintao Wang; Meng Wang; Pengfei Wan; Xiaodan Liang
>
> **备注:** Project Page: https://jingw193.github.io/GRPO-Guard/
>
> **摘要:** Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality.
>
---
#### [replaced 037] ReCon-GS: Continuum-Preserved Gaussian Streaming for Fast and Compact Reconstruction of Dynamic Scenes
- **分类: eess.IV; cs.CV; cs.MM**

- **链接: [http://arxiv.org/pdf/2509.24325v2](http://arxiv.org/pdf/2509.24325v2)**

> **作者:** Jiaye Fu; Qiankun Gao; Chengxiang Wen; Yanmin Wu; Siwei Ma; Jiaqi Zhang; Jian Zhang
>
> **备注:** Published in NeurIPS 2025
>
> **摘要:** Online free-viewpoint video (FVV) reconstruction is challenged by slow per-frame optimization, inconsistent motion estimation, and unsustainable storage demands. To address these challenges, we propose the Reconfigurable Continuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework that enables high fidelity online dynamic scene reconstruction and real-time rendering. Specifically, we dynamically allocate multi-level Anchor Gaussians in a density-adaptive fashion to capture inter-frame geometric deformations, thereby decomposing scene motion into compact coarse-to-fine representations. Then, we design a dynamic hierarchy reconfiguration strategy that preserves localized motion expressiveness through on-demand anchor re-hierarchization, while ensuring temporal consistency through intra-hierarchical deformation inheritance that confines transformation priors to their respective hierarchy levels. Furthermore, we introduce a storage-aware optimization mechanism that flexibly adjusts the density of Anchor Gaussians at different hierarchy levels, enabling a controllable trade-off between reconstruction fidelity and memory usage. Extensive experiments on three widely used datasets demonstrate that, compared to state-of-the-art methods, ReCon-GS improves training efficiency by approximately 15% and achieves superior FVV synthesis quality with enhanced robustness and stability. Moreover, at equivalent rendering quality, ReCon-GS slashes memory requirements by over 50% compared to leading state-of-the-art methods.
>
---
#### [replaced 038] VerifIoU - Robustness of Object Detection to Perturbations
- **分类: cs.CV; cs.AI; cs.NE**

- **链接: [http://arxiv.org/pdf/2403.08788v2](http://arxiv.org/pdf/2403.08788v2)**

> **作者:** Noémie Cohen; Mélanie Ducoffe; Ryma Boumazouza; Christophe Gabreau; Claire Pagetti; Xavier Pucel; Audrey Galametz
>
> **摘要:** We introduce a novel Interval Bound Propagation (IBP) approach for the formal verification of object detection models, specifically targeting the Intersection over Union (IoU) metric. The approach has been implemented in an open source code, named IBP IoU, compatible with popular abstract interpretation based verification tools. The resulting verifier is evaluated on landing approach runway detection and handwritten digit recognition case studies. Comparisons against a baseline (Vanilla IBP IoU) highlight the superior performance of IBP IoU in ensuring accuracy and stability, contributing to more secure and robust machine learning applications.
>
---
#### [replaced 039] ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models
- **分类: cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.13444v2](http://arxiv.org/pdf/2505.13444v2)**

> **作者:** Liyan Tang; Grace Kim; Xinyu Zhao; Thom Lake; Wenxuan Ding; Fangcong Yin; Prasann Singhal; Manya Wadhwa; Zeyu Leo Liu; Zayne Sprague; Ramya Namuduri; Bodun Hu; Juan Diego Rodriguez; Puyuan Peng; Greg Durrett
>
> **备注:** NeurIPS 2025 Datasets & Benchmarks
>
> **摘要:** Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.
>
---
#### [replaced 040] Locality in Image Diffusion Models Emerges from Data Statistics
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2509.09672v2](http://arxiv.org/pdf/2509.09672v2)**

> **作者:** Artem Lukoianov; Chenyang Yuan; Justin Solomon; Vincent Sitzmann
>
> **备注:** 31 pages, 20 figures, 7 tables
>
> **摘要:** Recent work has shown that the generalization ability of image diffusion models arises from the locality properties of the trained neural network. In particular, when denoising a particular pixel, the model relies on a limited neighborhood of the input image around that pixel, which, according to the previous work, is tightly related to the ability of these models to produce novel images. Since locality is central to generalization, it is crucial to understand why diffusion models learn local behavior in the first place, as well as the factors that govern the properties of locality patterns. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset and is not due to the inductive bias of convolutional neural networks, as suggested in previous work. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to deep neural denoisers. We show, both theoretically and experimentally, that this locality arises directly from pixel correlations present in the image datasets. Moreover, locality patterns are drastically different on specialized datasets, approximating principal components of the data's covariance. We use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than prior expert-crafted alternatives. Our key takeaway is that while neural network architectures influence generation quality, their primary role is to capture locality patterns inherent in the data.
>
---
#### [replaced 041] TeleEgo: Benchmarking Egocentric AI Assistants in the Wild
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2510.23981v2](http://arxiv.org/pdf/2510.23981v2)**

> **作者:** Jiaqi Yan; Ruilong Ren; Jingren Liu; Shuning Xu; Ling Wang; Yiheng Wang; Yun Wang; Long Zhang; Xiangyu Chen; Changzhi Sun; Jixiang Luo; Dell Zhang; Hao Sun; Chi Zhang; Xuelong Li
>
> **摘要:** Egocentric AI assistants in real-world settings must process multi-modal inputs (video, audio, text), respond in real time, and retain evolving long-term memory. However, existing benchmarks typically evaluate these abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming, omni-modal benchmark for evaluating egocentric AI assistants in realistic daily contexts. The dataset features over 14 hours per participant of synchronized egocentric video, audio, and text across four domains: work \& study, lifestyle \& routines, social activities, and outings \& culture. All data is aligned on a unified global timeline and includes high-quality visual narrations and speech transcripts, curated through human refinement.TeleEgo defines 12 diagnostic subtasks across three core capabilities: Memory (recalling past events), Understanding (interpreting the current moment), and Cross-Memory Reasoning (linking distant events). It contains 3,291 human-verified QA items spanning multiple question formats (single-choice, binary, multi-choice, and open-ended), evaluated strictly in a streaming setting. We propose two key metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess correctness, temporal responsiveness, and long-term retention. TeleEgo provides a realistic and comprehensive evaluation to advance the development of practical AI assistants.
>
---
#### [replaced 042] Neural Atlas Graphs for Dynamic Scene Decomposition and Editing
- **分类: cs.GR; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2509.16336v2](http://arxiv.org/pdf/2509.16336v2)**

> **作者:** Jan Philipp Schneider; Pratik Singh Bisht; Ilya Chugunov; Andreas Kolb; Michael Moeller; Felix Heide
>
> **摘要:** Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes. Project Page: https://princeton-computational-imaging.github.io/nag/
>
---
#### [replaced 043] Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.20392v4](http://arxiv.org/pdf/2412.20392v4)**

> **作者:** Zhifang Zhang; Shuo He; Haobo Wang; Bingquan Shen; Lei Feng
>
> **摘要:** Multimodal contrastive learning models (e.g., CLIP) can learn high-quality representations from large-scale image-text datasets, while they exhibit significant vulnerabilities to backdoor attacks, raising serious safety concerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem from its tendency to encode features beyond in-dataset predictive patterns, compromising its visual feature resistivity to input perturbations. This makes its encoded features highly susceptible to being reshaped by backdoor triggers. To address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a novel defense approach that employs deep visual prompt tuning with a specially designed feature-repelling loss. Specifically, RVPT adversarially repels the encoded features from deeper layers while optimizing the standard cross-entropy loss, ensuring that only predictive features in downstream tasks are encoded, thereby enhancing CLIP's visual feature resistivity against input perturbations and mitigating its susceptibility to backdoor attacks. Unlike existing multimodal backdoor defense methods that typically require the availability of poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot downstream clean samples and only tunes a small number of parameters. Empirical results demonstrate that RVPT tunes only 0.27\% of the parameters in CLIP, yet it significantly outperforms state-of-the-art defense methods, reducing the attack success rate from 89.70\% to 2.76\% against the most advanced multimodal attacks on ImageNet and effectively generalizes its defensive capabilities across multiple datasets.
>
---
#### [replaced 044] Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.12191v2](http://arxiv.org/pdf/2505.12191v2)**

> **作者:** Wenquan Lu; Jiaqi Zhang; Hugues Van Assel; Randall Balestriero
>
> **备注:** NeurIPS 2025
>
> **摘要:** Self-Supervised Learning (SSL) has become a powerful solution to extract rich representations from unlabeled data. Yet, SSL research is mostly focused on clean, curated and high-quality datasets. As a result, applying SSL on noisy data remains a challenge, despite being crucial to applications such as astrophysics, medical imaging, geophysics or finance. In this work, we present a fully self-supervised framework that enables noise-robust representation learning without requiring a denoiser at inference or downstream fine-tuning. Our method first trains an SSL denoiser on noisy data, then uses it to construct a denoised-to-noisy data curriculum (i.e., training first on denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2), combined with a teacher-guided regularization that anchors noisy embeddings to their denoised counterparts. This process encourages the model to internalize noise robustness. Notably, the denoiser can be discarded after pretraining, simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise ($\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by 4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from noise-aware pretraining. The code is available at https://github.com/wenquanlu/noisy_dinov2.
>
---
#### [replaced 045] Real-Time Neural Video Compression with Unified Intra and Inter Coding
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2510.14431v3](http://arxiv.org/pdf/2510.14431v3)**

> **作者:** Hui Xiang; Yifan Bian; Li Li; Jingran Wu; Xianguo Zhang; Dong Liu
>
> **备注:** 10 pages
>
> **摘要:** Neural video compression (NVC) technologies have advanced rapidly in recent years, yielding state-of-the-art schemes such as DCVC-RT that offer superior compression efficiency to H.266/VVC and real-time encoding/decoding capabilities. Nonetheless, existing NVC schemes have several limitations, including inefficiency in dealing with disocclusion and new content, interframe error propagation and accumulation, among others. To eliminate these limitations, we borrow the idea from classic video coding schemes, which allow intra coding within inter-coded frames. With the intra coding tool enabled, disocclusion and new content are properly handled, and interframe error propagation is naturally intercepted without the need for manual refresh mechanisms. We present an NVC framework with unified intra and inter coding, where every frame is processed by a single model that is trained to perform intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame compression design to exploit interframe redundancy not only forwardly but also backwardly. Experimental results show that our scheme outperforms DCVC-RT by an average of 12.1% BD-rate reduction, delivers more stable bitrate and quality per frame, and retains real-time encoding/decoding performances. Code and models will be released.
>
---
#### [replaced 046] CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling
- **分类: cs.RO; cs.CV**

- **链接: [http://arxiv.org/pdf/2506.19816v2](http://arxiv.org/pdf/2506.19816v2)**

> **作者:** Hao Li; Shuai Yang; Yilun Chen; Xinyi Chen; Xiaoda Yang; Yang Tian; Hanqing Wang; Tai Wang; Dahua Lin; Feng Zhao; Jiangmiao Pang
>
> **备注:** 39 pages, 24 figures
>
> **摘要:** Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong performance in robotic manipulation. However, these models remain constrained by the single-frame image paradigm and fail to fully leverage the temporal information offered by multi-frame histories, as directly feeding multiple frames into VLM backbones incurs substantial computational overhead and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame pretraining on large-scale embodied datasets with autoregressive prediction of action tokens, establishing an effective embodied vision-language foundation; (2) Multi-frame post-training, which adapts the prediction of the vision-language backbone from discrete tokens to learnable features, and aggregates historical information via feature chunking. CronusVLA effectively addresses the existing challenges of multi-frame modeling while enhancing performance and observational robustness. To evaluate the robustness under temporal and spatial disturbances, we introduce SimplerEnv-OR, a novel benchmark featuring 24 types of observational disturbances and 120 severity levels. Experiments across three embodiments in simulated and real-world environments demonstrate that CronusVLA achieves leading performance and superior robustness, with a 70.9% success rate on SimplerEnv, a 26.8% improvement over OpenVLA on LIBERO, and the highest robustness score on SimplerEnv-OR. These results highlight the potential of efficient multi-frame adaptation in VLA models for more powerful and robust real-world deployment.
>
---
#### [replaced 047] CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2503.04852v3](http://arxiv.org/pdf/2503.04852v3)**

> **作者:** Disheng Liu; Yiran Qiao; Wuche Liu; Yiren Lu; Yunlai Zhou; Tuo Liang; Yu Yin; Jing Ma
>
> **备注:** Datasets link: https://huggingface.co/datasets/LLDDSS/Causal3D_Dataset
>
> **摘要:** True intelligence hinges on the ability to uncover and leverage hidden causal relations. Despite significant progress in AI and computer vision (CV), there remains a lack of benchmarks for assessing models' abilities to infer latent causality from complex visual data. In this paper, we introduce \textsc{\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates structured data (tables) with corresponding visual representations (images) to evaluate causal reasoning. Designed within a systematic framework, Causal3D comprises 19 3D-scene datasets capturing diverse causal relations, views, and backgrounds, enabling evaluations across scenes of varying complexity. We assess multiple state-of-the-art methods, including classical causal discovery, causal representation learning, and large/vision-language models (LLMs/VLMs). Our experiments show that as causal structures grow more complex without prior knowledge, performance declines significantly, highlighting the challenges even advanced methods face in complex causal scenarios. Causal3D serves as a vital resource for advancing causal reasoning in CV and fostering trustworthy AI in critical domains.
>
---
#### [replaced 048] GenIR: Generative Visual Feedback for Mental Image Retrieval
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2506.06220v2](http://arxiv.org/pdf/2506.06220v2)**

> **作者:** Diji Yang; Minghao Liu; Chung-Hsiang Lo; Yi Zhang; James Davis
>
> **备注:** NeurIPS 2025
>
> **摘要:** Vision-language models (VLMs) have shown strong performance on text-to-image retrieval benchmarks. However, bridging this success to real-world applications remains a challenge. In practice, human search behavior is rarely a one-shot action. Instead, it is often a multi-round process guided by clues in mind. That is, a mental image ranging from vague recollections to vivid mental representations of the target image. Motivated by this gap, we study the task of Mental Image Retrieval (MIR), which targets the realistic yet underexplored setting where users refine their search for a mentally envisioned image through multi-round interactions with an image search engine. Central to successful interactive retrieval is the capability of machines to provide users with clear, actionable feedback; however, existing methods rely on indirect or abstract verbal feedback, which can be ambiguous, misleading, or ineffective for users to refine the query. To overcome this, we propose GenIR, a generative multi-round retrieval paradigm leveraging diffusion-based image generation to explicitly reify the AI system's understanding at each round. These synthetic visual representations provide clear, interpretable feedback, enabling users to refine their queries intuitively and effectively. We further introduce a fully automated pipeline to generate a high-quality multi-round MIR dataset. Experimental results demonstrate that GenIR significantly outperforms existing interactive methods in the MIR scenario. This work establishes a new task with a dataset and an effective generative retrieval method, providing a foundation for future research in this direction
>
---
#### [replaced 049] LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground Person Re-Identification
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.23722v3](http://arxiv.org/pdf/2503.23722v3)**

> **作者:** Pingping Zhang; Xiang Hu; Yuhao Wang; Huchuan Lu
>
> **备注:** More modifications may be performed
>
> **摘要:** As an important task in intelligent transportation systems, Aerial-Ground person Re-IDentification (AG-ReID) aims to retrieve specific persons across heterogeneous cameras in different viewpoints. Previous methods typically adopt deep learning-based models, focusing on extracting view-invariant features. However, they usually overlook the semantic information in person attributes. In addition, existing training strategies often rely on full fine-tuning large-scale models, which significantly increases training costs. To address these issues, we propose a novel framework named LATex for AG-ReID, which adopts prompt-tuning strategies to leverage attribute-based text knowledge. Specifically, with the Contrastive Language-Image Pre-training (CLIP) model, we first propose an Attribute-aware Image Encoder (AIE) to extract both global semantic features and attribute-aware features from input images. Then, with these features, we propose a Prompted Attribute Classifier Group (PACG) to predict person attributes and obtain attribute representations. Finally, we design a Coupled Prompt Template (CPT) to transform attribute representations and view information into structured sentences. These sentences are processed by the text encoder of CLIP to generate more discriminative features. As a result, our framework can fully leverage attribute-based text knowledge to improve AG-ReID performance. Extensive experiments on three AG-ReID benchmarks demonstrate the effectiveness of our proposed methods. The source code is available at https://github.com/kevinhu314/LATex.
>
---
#### [replaced 050] GSE: Group-wise Sparse and Explainable Adversarial Attacks
- **分类: cs.CV; cs.CR; cs.LG; math.OC**

- **链接: [http://arxiv.org/pdf/2311.17434v5](http://arxiv.org/pdf/2311.17434v5)**

> **作者:** Shpresim Sadiku; Moritz Wagner; Sebastian Pokutta
>
> **摘要:** Sparse adversarial attacks fool deep neural networks (DNNs) through minimal pixel perturbations, often regularized by the $\ell_0$ norm. Recent efforts have replaced this norm with a structural sparsity regularizer, such as the nuclear group norm, to craft group-wise sparse adversarial attacks. The resulting perturbations are thus explainable and hold significant practical relevance, shedding light on an even greater vulnerability of DNNs. However, crafting such attacks poses an optimization challenge, as it involves computing norms for groups of pixels within a non-convex objective. We address this by presenting a two-phase algorithm that generates group-wise sparse attacks within semantically meaningful areas of an image. Initially, we optimize a quasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored for non-convex programming. Subsequently, the algorithm transitions to a projected Nesterov's accelerated gradient descent with $2-$norm regularization applied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and ImageNet datasets demonstrate a remarkable increase in group-wise sparsity, e.g., $50.9\%$ on CIFAR-10 and $38.4\%$ on ImageNet (average case, targeted attack). This performance improvement is accompanied by significantly faster computation times, improved explainability, and a $100\%$ attack success rate.
>
---
#### [replaced 051] Predicting Video Slot Attention Queries from Random Slot-Feature Pairs
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2508.01345v3](http://arxiv.org/pdf/2508.01345v3)**

> **作者:** Rongzhen Zhao; Jian Li; Juho Kannala; Joni Pajarinen
>
> **摘要:** Unsupervised video Object-Centric Learning (OCL) is promising as it enables object-level scene representation and dynamics modeling as we humans do. Mainstream video OCL methods adopt a recurrent architecture: An aggregator aggregates current video frame into object features, termed slots, under some queries; A transitioner transits current slots to queries for the next frame. This is an effective architecture but all existing implementations both (\textit{i1}) neglect to incorporate next frame features, the most informative source for query prediction, and (\textit{i2}) fail to learn transition dynamics, the knowledge essential for query prediction. To address these issues, we propose Random Slot-Feature pair for learning Query prediction (RandSF.Q): (\textit{t1}) We design a new transitioner to incorporate both slots and features, which provides more information for query prediction; (\textit{t2}) We train the transitioner to predict queries from slot-feature pairs randomly sampled from available recurrences, which drives it to learn transition dynamics. Experiments on scene representation demonstrate that our method surpass existing video OCL methods significantly, e.g., up to 10 points on object discovery, setting new state-of-the-art. Such superiority also benefits downstream tasks like dynamics modeling. Our core source code, model checkpoints and training logs are available on https://github.com/Genera1Z/RandSF.Q.
>
---
#### [replaced 052] Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial Expressions Using Static Expression Data
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2409.06154v3](http://arxiv.org/pdf/2409.06154v3)**

> **作者:** Yin Chen; Jia Li; Yu Zhang; Zhenzhen Hu; Shiguang Shan; Meng Wang; Richang Hong
>
> **备注:** The code and model are publicly available here https://github.com/MSA-LMC/S4D
>
> **摘要:** Dynamic facial expression recognition (DFER) infers emotions from the temporal evolution of expressions, unlike static facial expression recognition (SFER), which relies solely on a single snapshot. This temporal analysis provides richer information and promises greater recognition capability. However, current DFER methods often exhibit unsatisfied performance largely due to fewer training samples compared to SFER. Given the inherent correlation between static and dynamic expressions, we hypothesize that leveraging the abundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic (S4D), a unified dual-modal learning framework that integrates SFER data as a complementary resource for DFER. Specifically, S4D employs dual-modal self-supervised pre-training on facial images and videos using a shared Vision Transformer (ViT) encoder-decoder architecture, yielding improved spatiotemporal representations. The pre-trained encoder is then fine-tuned on static and dynamic expression datasets in a multi-task learning setup to facilitate emotional information interaction. Unfortunately, vanilla multi-task learning in our study results in negative transfer. To address this, we propose an innovative Mixture of Adapter Experts (MoAE) module that facilitates task-specific knowledge acquisition while effectively extracting shared knowledge from both static and dynamic expression data. Extensive experiments demonstrate that S4D achieves a deeper understanding of DFER, setting new state-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with weighted average recall (WAR) of 53.65\%, 58.44\%, and 76.68\%, respectively. Additionally, a systematic correlation analysis between SFER and DFER tasks is presented, which further elucidates the potential benefits of leveraging SFER.
>
---
#### [replaced 053] VC4VG: Optimizing Video Captions for Text-to-Video Generation
- **分类: cs.CV; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2510.24134v2](http://arxiv.org/pdf/2510.24134v2)**

> **作者:** Yang Du; Zhuoran Lin; Kaiqiang Song; Biao Wang; Zhicheng Zheng; Tiezheng Ge; Bo Zheng; Qin Jin
>
> **备注:** Accepted by EMNLP 2025
>
> **摘要:** Recent advances in text-to-video (T2V) generation highlight the critical role of high-quality video-text pairs in training models capable of producing coherent and instruction-aligned videos. However, strategies for optimizing video captions specifically for T2V training remain underexplored. In this paper, we introduce VC4VG (Video Captioning for Video Generation), a comprehensive caption optimization framework tailored to the needs of T2V models. We begin by analyzing caption content from a T2V perspective, decomposing the essential elements required for video reconstruction into multiple dimensions, and proposing a principled caption design methodology. To support evaluation, we construct VC4VG-Bench, a new benchmark featuring fine-grained, multi-dimensional, and necessity-graded metrics aligned with T2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a strong correlation between improved caption quality and video generation performance, validating the effectiveness of our approach. We release all benchmark tools and code at https://github.com/alimama-creative/VC4VG to support further research.
>
---
#### [replaced 054] MoralCLIP: Contrastive Alignment of Vision-and-Language Representations with Moral Foundations Theory
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.05696v2](http://arxiv.org/pdf/2506.05696v2)**

> **作者:** Ana Carolina Condez; Diogo Tavares; João Magalhães
>
> **备注:** Updated version: corresponds to the ACM MM '25 published paper and includes full appendix material
>
> **摘要:** Recent advances in vision-language models have enabled rich semantic understanding across modalities. However, these encoding methods lack the ability to interpret or reason about the moral dimensions of content-a crucial aspect of human cognition. In this paper, we address this gap by introducing MoralCLIP, a novel embedding representation method that extends multimodal learning with explicit moral grounding based on Moral Foundations Theory (MFT). Our approach integrates visual and textual moral cues into a unified embedding space, enabling cross-modal moral alignment. MoralCLIP is grounded on the multi-label dataset Social-Moral Image Database to identify co-occurring moral foundations in visual content. For MoralCLIP training, we design a moral data augmentation strategy to scale our annotated dataset to 15,000 image-text pairs labeled with MFT-aligned dimensions. Our results demonstrate that explicit moral supervision improves both unimodal and multimodal understanding of moral content, establishing a foundation for morally-aware AI systems capable of recognizing and aligning with human moral values.
>
---
#### [replaced 055] Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction
- **分类: cs.LG; cs.CV; 65M70 (Primary), 68T07 (Secondary); I.2.6; I.4.8; G.1.8**

- **链接: [http://arxiv.org/pdf/2510.23117v2](http://arxiv.org/pdf/2510.23117v2)**

> **作者:** Omer Jauhar Khan; Sudais Khan; Hafeez Anwar; Shahzeb Khan; Shams Ul Arifeen
>
> **备注:** 12 pages, 17 figures. Preprint
>
> **摘要:** Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs. The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.
>
---
#### [replaced 056] ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.06078v2](http://arxiv.org/pdf/2507.06078v2)**

> **作者:** Chihan Huang; Hao Tang
>
> **摘要:** Despite the success of deep learning across various domains, it remains vulnerable to adversarial attacks. Although many existing adversarial attack methods achieve high success rates, they typically rely on $\ell_{p}$-norm perturbation constraints, which do not align with human perceptual capabilities. Consequently, researchers have shifted their focus toward generating natural, unrestricted adversarial examples (UAEs). GAN-based approaches suffer from inherent limitations, such as poor image quality due to instability and mode collapse. Meanwhile, diffusion models have been employed for UAE generation, but they still rely on iterative PGD perturbation injection, without fully leveraging their central denoising capabilities. In this paper, we introduce a novel approach for generating UAEs based on diffusion models, named ScoreAdv. This method incorporates an interpretable adversarial guidance mechanism to gradually shift the sampling distribution towards the adversarial distribution, while using an interpretable saliency map to inject the visual information of a reference image into the generated samples. Notably, our method is capable of generating an unlimited number of natural adversarial examples and can attack not only classification models but also retrieval models. We conduct extensive experiments on ImageNet and CelebA datasets, validating the performance of ScoreAdv across ten target models in both black-box and white-box settings. Our results demonstrate that ScoreAdv achieves state-of-the-art attack success rates and image quality, while maintaining inference efficiency. Furthermore, the dynamic balance between denoising and adversarial perturbation enables ScoreAdv to remain robust even under defensive measures.
>
---
#### [replaced 057] Language-guided Open-world Video Anomaly Detection under Weak Supervision
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.13160v2](http://arxiv.org/pdf/2503.13160v2)**

> **作者:** Zihao Liu; Xiaoyu Wu; Jianqin Wu; Xuxu Wang; Linlin Yang
>
> **摘要:** Video anomaly detection (VAD) aims to detect anomalies that deviate from what is expected. In open-world scenarios, the expected events may change as requirements change. For example, not wearing a mask may be considered abnormal during a flu outbreak but normal otherwise. However, existing methods assume that the definition of anomalies is invariable, and thus are not applicable to the open world. To address this, we propose a novel open-world VAD paradigm with variable definitions, allowing guided detection through user-provided natural language at inference time. This paradigm necessitates establishing a robust mapping from video and textual definition to anomaly scores. Therefore, we propose LaGoVAD (Language-guided Open-world Video Anomaly Detector), a model that dynamically adapts anomaly definitions under weak supervision with two regularization strategies: diversifying the relative durations of anomalies via dynamic video synthesis, and enhancing feature robustness through contrastive learning with negative mining. Training such adaptable models requires diverse anomaly definitions, but existing datasets typically provide labels without semantic descriptions. To bridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the largest and most diverse video anomaly dataset to date, featuring 35,279 annotated videos with multi-level category labels and descriptions that explicitly define anomalies. Zero-shot experiments on seven datasets demonstrate LaGoVAD's SOTA performance. Our dataset and code will be released at https://github.com/Kamino666/LaGoVAD-PreVAD.
>
---
#### [replaced 058] MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2510.25327v2](http://arxiv.org/pdf/2510.25327v2)**

> **作者:** Runxi Huang; Mingxuan Yu; Mingyu Tsoi; Xiaomin Ouyang
>
> **备注:** Code available at: https://github.com/HKUST-MINSys-Lab/MMEdge. Accepted by SenSys 2026
>
> **摘要:** Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex inter-modality dependencies. In this paper, we propose MMEdge, an new on-device multi-modal inference framework based on pipelined sensing and encoding. Instead of waiting for complete sensor inputs, MMEdge decomposes the entire inference process into a sequence of fine-grained sensing and encoding units, allowing computation to proceed incrementally as data arrive. MMEdge also introduces a lightweight but effective temporal aggregation module that captures rich temporal dynamics across different pipelined units to maintain accuracy performance. Such pipelined design also opens up opportunities for fine-grained cross-modal optimization and early decision-making during inference. To further enhance system performance under resource variability and input data complexity, MMEdge incorporates an adaptive multimodal configuration optimizer that dynamically selects optimal sensing and model configurations for each modality under latency constraints, and a cross-modal speculative skipping mechanism that bypasses future units of slower modalities when early predictions reach sufficient confidence. We evaluate MMEdge using two public multimodal datasets and deploy it on a real-world unmanned aerial vehicle (UAV)-based multimodal testbed. The results show that MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.
>
---
#### [replaced 059] SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in Diffusion Models via RKE Score
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2506.10173v2](http://arxiv.org/pdf/2506.10173v2)**

> **作者:** Mohammad Jalali; Haoyu Lei; Amin Gohari; Farzan Farnia
>
> **摘要:** Diffusion models have demonstrated remarkable success in high-fidelity image synthesis and prompt-guided generative modeling. However, ensuring adequate diversity in generated samples of prompt-guided diffusion models remains a challenge, particularly when the prompts span a broad semantic spectrum and the diversity of generated data needs to be evaluated in a prompt-aware fashion across semantically similar prompts. Recent methods have introduced guidance via diversity measures to encourage more varied generations. In this work, we extend the diversity measure-based approaches by proposing the Scalable Prompt-Aware R\'eny Kernel Entropy Diversity Guidance (SPARKE) method for prompt-aware diversity guidance. SPARKE utilizes conditional entropy for diversity guidance, which dynamically conditions diversity measurement on similar prompts and enables prompt-aware diversity control. While the entropy-based guidance approach enhances prompt-aware diversity, its reliance on the matrix-based entropy scores poses computational challenges in large-scale generation settings. To address this, we focus on the special case of Conditional latent RKE Score Guidance, reducing entropy computation and gradient-based optimization complexity from the $O(n^3)$ of general entropy measures to $O(n)$. The reduced computational complexity allows for diversity-guided sampling over potentially thousands of generation rounds on different prompts. We numerically test the SPARKE method on several text-to-image diffusion models, demonstrating that the proposed method improves the prompt-aware diversity of the generated data without incurring significant computational costs. We release our code on the project page: https://mjalali.github.io/SPARKE
>
---
#### [replaced 060] LinearSR: Unlocking Linear Attention for Stable and Efficient Image Super-Resolution
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2510.08771v2](http://arxiv.org/pdf/2510.08771v2)**

> **作者:** Xiaohui Li; Shaobin Zhuang; Shuo Cao; Yang Yang; Yuandong Pu; Qi Qin; Siqi Luo; Bin Fu; Yihao Liu
>
> **备注:** 19 pages, 9 figures, 6 tables
>
> **摘要:** Generative models for Image Super-Resolution (SR) are increasingly powerful, yet their reliance on self-attention's quadratic complexity (O(N^2)) creates a major computational bottleneck. Linear Attention offers an O(N) solution, but its promise for photorealistic SR has remained largely untapped, historically hindered by a cascade of interrelated and previously unsolved challenges. This paper introduces LinearSR, a holistic framework that, for the first time, systematically overcomes these critical hurdles. Specifically, we resolve a fundamental, training instability that causes catastrophic model divergence using our novel "knee point"-based Early-Stopping Guided Fine-tuning (ESGF) strategy. Furthermore, we mitigate the classic perception-distortion trade-off with a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we establish an effective and lightweight guidance paradigm, TAG, derived from our "precision-over-volume" principle. Our resulting LinearSR model simultaneously delivers state-of-the-art perceptual quality with exceptional efficiency. Its core diffusion forward pass (1-NFE) achieves SOTA-level speed, while its overall multi-step inference time remains highly competitive. This work provides the first robust methodology for applying Linear Attention in the photorealistic SR domain, establishing a foundational paradigm for future research in efficient generative super-resolution.
>
---
#### [replaced 061] Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.11094v4](http://arxiv.org/pdf/2503.11094v4)**

> **作者:** Weichen Zhang; Zile Zhou; Xin Zeng; Xuchen Liu; Jianjie Fang; Chen Gao; Yong Li; Jinqiang Cui; Xinlei Chen; Xiao-Ping Zhang
>
> **摘要:** Spatial reasoning is a fundamental capability of multimodal large language models (MLLMs), yet their performance in open aerial environments remains underexplored. In this work, we present Open3D-VQA, a novel benchmark for evaluating MLLMs' ability to reason about complex spatial relationships from an aerial perspective. The benchmark comprises 73k QA pairs spanning 7 general spatial reasoning tasks, including multiple-choice, true/false, and short-answer formats, and supports both visual and point cloud modalities. The questions are automatically generated from spatial relations extracted from both real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs reveals that: 1) Models are generally better at answering questions about relative spatial relations than absolute distances, 2) 3D LLMs fail to demonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on the simulated dataset can significantly improve the model's spatial reasoning performance in real-world scenarios. We release our benchmark, data generation pipeline, and evaluation toolkit to support further research: https://github.com/EmbodiedCity/Open3D-VQA.code.
>
---
#### [replaced 062] Reasoning Visual Language Model for Chest X-Ray Analysis
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2510.23968v2](http://arxiv.org/pdf/2510.23968v2)**

> **作者:** Andriy Myronenko; Dong Yang; Baris Turkbey; Mariam Aboian; Sena Azamat; Esra Akcicek; Hongxu Yin; Pavlo Molchanov; Marc Edgar; Yufan He; Pengfei Guo; Yucheng Tang; Daguang Xu
>
> **备注:** NV-Reason-CXR-3B
>
> **摘要:** Vision-language models (VLMs) have shown strong promise for medical image analysis, but most remain opaque, offering predictions without the transparent, stepwise reasoning clinicians rely on. We present a framework that brings chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by reasoning-first training paradigms, our approach is designed to learn how experts reason, not just what they conclude, by aligning intermediate steps with observable image evidence and radiology workflow. Beyond accuracy, the explicit reasoning traces support clinical auditability: they reveal why a conclusion was reached, which alternatives were considered, and where uncertainty remains, enabling quality assurance, error analysis, and safer human-AI collaboration. Our model couples high-fidelity visual encoding with a two-stage training recipe: a reasoning-style supervised fine-tuning (SFT) followed by reinforcement learning (RL) that uses verifiable rewards over a list of X-ray abnormalities. The model outputs reasoning that mirrors radiologists systematic thought process, uncertainty, and differential diagnosis. In out-of-distribution evaluation, the approach achieves competitive multi-label classification while improving interpretability. In a reader study with expert radiologists, full reasoning traces increased confidence, supported error auditing, and reduced time to finalize reports. We release code and the model NV-Reason-CXR-3B to support community progress toward trustworthy, explainable AI in chest radiography and other medical imaging tasks where reasoning quality is as critical as prediction quality.
>
---
#### [replaced 063] MaskCaptioner: Learning to Jointly Segment and Caption Object Trajectories in Videos
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2510.14904v2](http://arxiv.org/pdf/2510.14904v2)**

> **作者:** Gabriel Fiastre; Antoine Yang; Cordelia Schmid
>
> **备注:** 20 pages, 8 figures
>
> **摘要:** Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.
>
---
#### [replaced 064] Disentangled 4D Gaussian Splatting: Rendering High-Resolution Dynamic World at 343 FPS
- **分类: cs.GR; cs.CV**

- **链接: [http://arxiv.org/pdf/2503.22159v3](http://arxiv.org/pdf/2503.22159v3)**

> **作者:** Hao Feng; Hao Sun; Wei Xie; Zhi Zuo; Zhengzhe Liu
>
> **摘要:** While dynamic novel view synthesis from 2D videos has seen progress, achieving efficient reconstruction and rendering of dynamic scenes remains a challenging task. In this paper, we introduce Disentangled 4D Gaussian Splatting (Disentangled4DGS), a novel representation and rendering pipeline that achieves real-time performance without compromising visual fidelity. Disentangled4DGS decouples the temporal and spatial components of 4D Gaussians, avoiding the need for slicing first and four-dimensional matrix calculations in prior methods. By projecting temporal and spatial deformations into dynamic 2D Gaussians and deferring temporal processing, we minimize redundant computations of 4DGS. Our approach also features a gradient-guided flow loss and temporal splitting strategy to reduce artifacts. Experiments demonstrate a significant improvement in rendering speed and quality, achieving 343 FPS when render 1352*1014 resolution images on a single RTX3090 while reducing storage requirements by at least 4.5%. Our approach sets a new benchmark for dynamic novel view synthesis, outperforming existing methods on both multi-view and monocular dynamic scene datasets.
>
---
#### [replaced 065] SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2510.16396v3](http://arxiv.org/pdf/2510.16396v3)**

> **作者:** Yeh Keng Hao; Hsu Tzu Wei; Sun Min
>
> **备注:** Accepted to AICCC 2025
>
> **摘要:** With the increasing ubiquity of AR/VR devices, the deployment of deep learning models on edge devices has become a critical challenge. These devices require real-time inference, low power consumption, and minimal latency. Many framework designers face the conundrum of balancing efficiency and performance. We design a light framework that adopts an encoder-decoder architecture and introduces several key contributions aimed at improving both efficiency and accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency improvement. Moreover, we propose our SPLite decoder. This new architecture significantly boosts the decoding process's frame rate by 3.1x on the Raspberry Pi 5, while maintaining accuracy on par. To further optimize performance, we apply quantization-aware training, reducing memory usage while preserving accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5 CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on compound benchmark datasets, demonstrating comparable accuracy to state-of-the-art approaches while significantly enhancing computational efficiency.
>
---
#### [replaced 066] DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.16239v2](http://arxiv.org/pdf/2505.16239v2)**

> **作者:** Zheng Chen; Zichen Zou; Kewei Zhang; Xiongfei Su; Xin Yuan; Yong Guo; Yulun Zhang
>
> **备注:** Accepted to NeurIPS 2025. Code is available at: https://github.com/zhengchen1999/DOVE
>
> **摘要:** Diffusion models have demonstrated promising performance in real-world video super-resolution (VSR). However, the dozens of sampling steps they require, make inference extremely slow. Sampling acceleration techniques, particularly single-step, provide a potential solution. Nonetheless, achieving one step in VSR remains challenging, due to the high training overhead on video data and stringent fidelity demands. To tackle the above issues, we propose DOVE, an efficient one-step diffusion model for real-world VSR. DOVE is obtained by fine-tuning a pretrained video diffusion model (i.e., CogVideoX). To effectively train DOVE, we introduce the latent-pixel training strategy. The strategy employs a two-stage scheme to gradually adapt the model to the video super-resolution task. Meanwhile, we design a video processing pipeline to construct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning on this dataset further enhances the restoration capability of DOVE. Extensive experiments show that DOVE exhibits comparable or superior performance to multi-step diffusion-based VSR methods. It also offers outstanding inference efficiency, achieving up to a 28$\times$ speed-up over existing methods such as MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.
>
---
#### [replaced 067] FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes
- **分类: eess.IV; cs.AI; cs.CV; I.4.6; I.4.8; J.3**

- **链接: [http://arxiv.org/pdf/2509.06159v3](http://arxiv.org/pdf/2509.06159v3)**

> **作者:** Muraam Abdel-Ghani; Mahmoud Ali; Mohamed Ali; Fatmaelzahraa Ahmed; Muhammad Arsalan; Abdulaziz Al-Ali; Shidin Balakrishnan
>
> **备注:** 8 pages, 6 figures, In Proceedings of European Conference on Artificial Intelligence (ECAI) 2025 <https://doi.org/10.3233/FAIA250908>
>
> **摘要:** The growing popularity of robotic minimally invasive surgeries has made deep learning-based surgical training a key area of research. A thorough understanding of the surgical scene components is crucial, which semantic segmentation models can help achieve. However, most existing work focuses on surgical tools and overlooks anatomical objects. Additionally, current state-of-the-art (SOTA) models struggle to balance capturing high-level contextual features and low-level edge features. We propose a Feature-Adaptive Spatial Localization model (FASL-Seg), designed to capture features at multiple levels of detail through two distinct processing streams, namely a Low-Level Feature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream, for varying feature resolutions - enabling precise segmentation of anatomy and surgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark datasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model achieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy segmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU of 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation, respectively, outperforming SOTA overall performance, with comparable per-class SOTA results in both datasets and consistent performance in various classes for anatomy and instruments, demonstrating the effectiveness of distinct processing streams for varying feature resolutions.
>
---
#### [replaced 068] SD-ReID: View-aware Stable Diffusion for Aerial-Ground Person Re-Identification
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.09549v2](http://arxiv.org/pdf/2504.09549v2)**

> **作者:** Yuhao Wang; Xiang Hu; Lixin Wang; Pingping Zhang; Huchuan Lu
>
> **备注:** More modifications may performed
>
> **摘要:** Aerial-Ground Person Re-IDentification (AG-ReID) aims to retrieve specific persons across cameras with different viewpoints. Previous works focus on designing discriminative models to maintain the identity consistency despite drastic changes in camera viewpoints. The core idea behind these methods is quite natural, but designing a view-robust model is a very challenging task. Moreover, they overlook the contribution of view-specific features in enhancing the model's ability to represent persons. To address these issues, we propose a novel generative framework named SD-ReID for AG-ReID, which leverages generative models to mimic the feature distribution of different views while extracting robust identity representations. More specifically, we first train a ViT-based model to extract person representations along with controllable conditions, including identity and view conditions. We then fine-tune the Stable Diffusion (SD) model to enhance person representations guided by these controllable conditions. Furthermore, we introduce the View-Refined Decoder (VRD) to bridge the gap between instance-level and global-level features. Finally, both person representations and all-view features are employed to retrieve target persons. Extensive experiments on five AG-ReID benchmarks (i.e., CARGO, AG-ReIDv1, AG-ReIDv2, LAGPeR and G2APS-ReID) demonstrate the effectiveness of our proposed method. The source code will be available.
>
---
#### [replaced 069] Buffer layers for Test-Time Adaptation
- **分类: cs.LG; cs.CV**

- **链接: [http://arxiv.org/pdf/2510.21271v2](http://arxiv.org/pdf/2510.21271v2)**

> **作者:** Hyeongyu Kim; Geonhui Han; Dosik Hwang
>
> **备注:** Accepted at NeurIPS 2025
>
> **摘要:** In recent advancements in Test Time Adaptation (TTA), most existing methodologies focus on updating normalization layers to adapt to the test domain. However, the reliance on normalization-based adaptation presents key challenges. First, normalization layers such as Batch Normalization (BN) are highly sensitive to small batch sizes, leading to unstable and inaccurate statistics. Moreover, normalization-based adaptation is inherently constrained by the structure of the pre-trained model, as it relies on training-time statistics that may not generalize well to unseen domains. These issues limit the effectiveness of normalization-based TTA approaches, especially under significant domain shift. In this paper, we introduce a novel paradigm based on the concept of a Buffer layer, which addresses the fundamental limitations of normalization layer updates. Unlike existing methods that modify the core parameters of the model, our approach preserves the integrity of the pre-trained backbone, inherently mitigating the risk of catastrophic forgetting during online adaptation. Through comprehensive experimentation, we demonstrate that our approach not only outperforms traditional methods in mitigating domain shift and enhancing model robustness, but also exhibits strong resilience to forgetting. Furthermore, our Buffer layer is modular and can be seamlessly integrated into nearly all existing TTA frameworks, resulting in consistent performance improvements across various architectures. These findings validate the effectiveness and versatility of the proposed solution in real-world domain adaptation scenarios. The code is available at https://github.com/hyeongyu-kim/Buffer_TTA.
>
---
