# 计算机与社会 cs.CY

- **最新发布 36 篇**

- **更新 10 篇**

## 最新发布

#### [new 001] Making a Case for Research Collaboration Between Artificial Intelligence and Operations Research Experts
- **分类: cs.CY**

- **简介: 该论文属于任务型论文，旨在推动人工智能（AI）与运筹学（OR）协作研究。论文讨论了2021年三场研讨会的内容，聚焦技术融合、克服文化差异及提升社会影响。研究总结出五个建议：资金支持、联合教育、长期项目、会议期刊对接与基准创建。**

- **链接: [http://arxiv.org/pdf/2507.21076v1](http://arxiv.org/pdf/2507.21076v1)**

> **作者:** Radhika Kulkarni; Gianluca Brero; Yu Ding; Swati Gupta; Sven Koenig; Ramayya Krishnan; Thiago Serra; Phebe Vayanos; Segev Wasserkrug; Holly Wiberg
>
> **摘要:** In 2021, INFORMS, ACM SIGAI, and the Computing Community Consortium (CCC) hosted three workshops to explore synergies between Artificial Intelligence (AI) and Operations Research (OR) to improve decision-making. The workshops aimed to create a unified research vision for AI/OR collaboration, focusing on overcoming cultural differences and maximizing societal impact. The first two workshops addressed technological innovations, applications, and trustworthy AI development, while the final workshop highlighted specific areas for AI/OR integration. Participants discussed "Challenge Problems" and strategies for combining AI and OR techniques. This report outlines five key recommendations to enhance AI/OR collaboration: 1) Funding Opportunities, 2) Joint Education, 3) Long-term Research Programs, 4) Aligning Conferences/Journals, and 5) Benchmark Creation.
>
---
#### [new 002] High hopes for "Deep Medicine"? AI, economics, and the future of care
- **分类: cs.CY; cs.AI; cs.HC; cs.LG**

- **简介: 该论文探讨了人工智能在医疗领域的应用前景与影响，属于社会科学研究任务。论文分析了AI可能带来的医疗变革，如提升效率与削弱医患关系的风险，回应了《深度医学》中关于AI促进医疗人文回归的观点，通过多角度论证指出AI可能带来的实际挑战。**

- **链接: [http://arxiv.org/pdf/2507.21054v1](http://arxiv.org/pdf/2507.21054v1)**

> **作者:** Robert Sparrow; Joshua Hatherley
>
> **摘要:** In the much-celebrated book Deep Medicine, Eric Topol argues that the development of artificial intelligence for health care will lead to a dramatic shift in the culture and practice of medicine. In the next several decades, he suggests, AI will become sophisticated enough that many of the everyday tasks of physicians could be delegated to it. Topol is perhaps the most articulate advocate of the benefits of AI in medicine, but he is hardly alone in spruiking its potential to allow physicians to dedicate more of their time and attention to providing empathetic care for their patients in the future. Unfortunately, several factors suggest a radically different picture for the future of health care. Far from facilitating a return to a time of closer doctor-patient relationships, the use of medical AI seems likely to further erode therapeutic relationships and threaten professional and patient satisfaction.
>
---
#### [new 003] Automated but Atrophied? Student Over-Reliance vs Expert Augmentation of AI in Learning and Cybersecurity
- **分类: cs.CY; 68T07 (Primary), 68T05, 68U99 (Secondary); I.2.6; I.2.7; K.3.2; K.4.1; F.2.2**

- **简介: 该论文探讨学生过度依赖AI与专家辅助使用AI在教育和网络安全中的差异。任务是比较两者影响，解决如何合理整合AI于教学与职场的问题。研究通过案例分析、问卷调查、访谈和绩效分析，提出AI应用对技能培养和学术诚信的影响，并给出课程设计与政策建议。**

- **链接: [http://arxiv.org/pdf/2507.21062v1](http://arxiv.org/pdf/2507.21062v1)**

> **作者:** Koffka Khan
>
> **摘要:** University students and working professionals are increasingly encountering generative artificial intelligence (AI) in education and practice, yet their approaches and outcomes differ markedly. This paper proposes an academic study contrasting novice over-reliance on AI with expert augmentation of AI, grounded in two real-world narratives. In one, a university student attempted to outsource learning entirely to AI, eschewing course engagement. In the other, seasoned cybersecurity professionals in the Tradewinds 2025 red/blue team exercise collaboratively employed AI tools to enhance (not replace) their domain expertise. This proposal outlines a comparative research design to investigate how students' perception of AI as a learning replacement versus professionals' use of AI as an expert tool impacts outcomes. Drawing on current literature in educational technology and workplace AI, we examine implications for curriculum design, AI literacy, and assessment reform in higher education. We hypothesize that blind reliance on AI can erode fundamental skills and academic integrity, whereas guided use of AI by knowledgeable users can amplify productivity without sacrificing quality. The paper details methodologies for classroom and workplace data collection, including student and professional surveys, interviews, and performance analyses. Anticipated findings aim to inform responsible AI integration in curricula, balancing innovation with the necessity of domain knowledge. We conclude with recommendations for pedagogical strategies, institutional policies to foster AI literacy, and a call for longitudinal studies tracking how AI usage during university affects professional competencies over time.
>
---
#### [new 004] Against racing to AGI: Cooperation, deterrence, and catastrophic risks
- **分类: cs.CY; cs.AI**

- **简介: 该论文探讨了“AGI竞赛”观点，认为各国加速开发先进AI以抢先实现AGI并非明智之举。论文任务是分析AGI竞赛的风险与收益，提出国际合作与威慑措施作为更优替代方案，旨在降低AI带来的灾难性风险并促进技术安全研究。**

- **链接: [http://arxiv.org/pdf/2507.21839v1](http://arxiv.org/pdf/2507.21839v1)**

> **作者:** Leonard Dung; Max Hellrigel-Holderbaum
>
> **摘要:** AGI Racing is the view that it is in the self-interest of major actors in AI development, especially powerful nations, to accelerate their frontier AI development to build highly capable AI, especially artificial general intelligence (AGI), before competitors have a chance. We argue against AGI Racing. First, the downsides of racing to AGI are much higher than portrayed by this view. Racing to AGI would substantially increase catastrophic risks from AI, including nuclear instability, and undermine the prospects of technical AI safety research to be effective. Second, the expected benefits of racing may be lower than proponents of AGI Racing hold. In particular, it is questionable whether winning the race enables complete domination over losers. Third, international cooperation and coordination, and perhaps carefully crafted deterrence measures, constitute viable alternatives to racing to AGI which have much smaller risks and promise to deliver most of the benefits that racing to AGI is supposed to provide. Hence, racing to AGI is not in anyone's self-interest as other actions, particularly incentivizing and seeking international cooperation around AI issues, are preferable.
>
---
#### [new 005] A Tactical Behaviour Recognition Framework Based on Causal Multimodal Reasoning: A Study on Covert Audio-Video Analysis Combining GAN Structure Enhancement and Phonetic Accent Modelling
- **分类: cs.CY; cs.AI; cs.CV; 05C82, 68T07, 68T05, 62H30; I.2.10; I.4.8; H.5.1; H.2.8**

- **简介: 论文提出TACTIC-GRAPHS框架，用于战术视频中的语义理解和威胁检测。该框架结合图神经网络与多模态推理，解决高噪声和弱结构环境下音视频分析与威胁识别问题。通过融合视觉、语音和动作信息，实现跨模态加权与因果信号分析，提升了威胁链识别准确率与系统可解释性。**

- **链接: [http://arxiv.org/pdf/2507.21100v1](http://arxiv.org/pdf/2507.21100v1)**

> **作者:** Wei Meng
>
> **备注:** This paper introduces a structurally innovative and mathematically rigorous framework for multimodal tactical reasoning, offering a significant advance in causal inference and graph-based threat recognition under noisy conditions
>
> **摘要:** This paper introduces TACTIC-GRAPHS, a system that combines spectral graph theory and multimodal graph neural reasoning for semantic understanding and threat detection in tactical video under high noise and weak structure. The framework incorporates spectral embedding, temporal causal edge modeling, and discriminative path inference across heterogeneous modalities. A semantic-aware keyframe extraction method fuses visual, acoustic, and action cues to construct temporal graphs. Using graph attention and Laplacian spectral mapping, the model performs cross-modal weighting and causal signal analysis. Experiments on TACTIC-AVS and TACTIC-Voice datasets show 89.3 percent accuracy in temporal alignment and over 85 percent recognition of complete threat chains, with node latency within plus-minus 150 milliseconds. The approach enhances structural interpretability and supports applications in surveillance, defense, and intelligent security systems.
>
---
#### [new 006] Assessing the Ecological Impact of AI
- **分类: cs.CY; cs.AI**

- **简介: 论文探讨生成式AI的生态影响，属于环境可持续性评估任务。旨在解决AI开发者缺乏全面生态影响分析的问题，结合哲学视角提出可行评估方法。**

- **链接: [http://arxiv.org/pdf/2507.21102v1](http://arxiv.org/pdf/2507.21102v1)**

> **作者:** Sylvia Wenmackers
>
> **备注:** This was presented as a lightning talk at: LOCO 2024, December 3, 2024, Glasgow/Online
>
> **摘要:** Philosophers of technology have recently started paying more attention to the environmental impacts of AI, in particular of large language models (LLMs) and generative AI (genAI) applications. Meanwhile, few developers of AI give concrete estimates of the ecological impact of their models and products, and even when they do so, their analysis is often limited to green house gas emissions of certain stages of AI development or use. The current proposal encourages practically viable analyses of the sustainability aspects of genAI informed by philosophical ideas.
>
---
#### [new 007] A ChatGPT-based approach for questions generation in higher education
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于教育技术任务，旨在解决高校教学中自动生成测验题目的问题。作者利用ChatGPT设计互动提示模式，构建AI驱动的题库生成流程，并通过“盲测”问卷评估题目质量，初步验证了其在减轻高校评估工作负担中的潜力。**

- **链接: [http://arxiv.org/pdf/2507.21174v1](http://arxiv.org/pdf/2507.21174v1)**

> **作者:** Sinh Trong Vu; Huong Thu Truong; Oanh Tien Do; Tu Anh Le; Tai Tan Mai
>
> **摘要:** Large language models have been widely applied in many aspects of real life, bringing significant efficiency to businesses and offering distinctive user experiences. In this paper, we focus on exploring the application of ChatGPT, a chatbot based on a large language model, to support higher educator in generating quiz questions and assessing learners. Specifically, we explore interactive prompting patterns to design an optimal AI-powered question bank creation process. The generated questions are evaluated through a "Blind test" survey sent to various stakeholders including lecturers and learners. Initial results at the Banking Academy of Vietnam are relatively promising, suggesting a potential direction to streamline the time and effort involved in assessing learners at higher education institutes.
>
---
#### [new 008] Examining the sentiment and emotional differences in product and service reviews: The moderating role of culture
- **分类: cs.CY**

- **简介: 该论文分析产品与服务评论中的情感与情绪差异，并探讨文化因素的调节作用。任务属于自然语言处理与情感分析领域。论文解决了评论类型与文化背景如何影响情感表达的问题，通过机器学习方法分析评论数据，结合霍夫斯泰德文化维度与马斯洛需求层次理论，提出解释消费者表达的模型，为企业优化跨文化营销策略提供依据。**

- **链接: [http://arxiv.org/pdf/2507.21057v1](http://arxiv.org/pdf/2507.21057v1)**

> **作者:** Vinh Truong
>
> **摘要:** This study explores how emotions and sentiments differ in customer reviews of products and services on e-commerce platforms. Unlike earlier research that treats all reviews uniformly, this study distinguishes between reviews of products, typically fulfilling basic, functional needs, and services, which often cater to experiential and emotional desires. The findings reveal clear differences in emotional expression and sentiment between the two. Product reviews frequently focus on practicality, such as functionality, reliability, and value for money, and are generally more neutral or pragmatic in tone. In contrast, service reviews involve stronger emotional engagement, as services often entail personal interactions and subjective experiences. Customers express a broader spectrum of emotions, such as joy, frustration, or disappointment when reviewing services, as identified using advanced machine learning techniques. Cultural background further influences these patterns. Consumers from collectivist cultures, as defined by Hofstede cultural dimensions, often use more moderated and socially considerate language, reflecting an emphasis on group harmony. Conversely, consumers from individualist cultures tend to offer more direct, emotionally intense feedback. Notably, gender appears to have minimal impact on sentiment variation, reinforcing the idea that the nature of the offering (product vs. service) and cultural context are the dominant factors. Theoretically, the study extends Maslow hierarchy of needs and Hofstede cultural framework to the domain of online reviews, proposing a model that explains how these dimensions shape consumer expression. Practically, the insights offer valuable guidance for businesses looking to optimize their marketing and customer engagement strategies by aligning messaging and service design with customer expectations across product types and cultural backgrounds.
>
---
#### [new 009] Failure Risk Prediction in a MOOC: A Multivariate Time Series Analysis Approach
- **分类: cs.CY; cs.AI; cs.LG**

- **简介: 该论文旨在预测MOOC学习者的失败风险，属于学习分析与教育数据挖掘任务。通过分析行为轨迹（如点击流）构成的多变量时间序列，使用分类方法识别不同阶段的高风险学习者。实验基于OULAD数据集，涵盖三个不同学科的课程，结果显示方法有效，且预测准确性受互动数据丰富度影响。**

- **链接: [http://arxiv.org/pdf/2507.21118v1](http://arxiv.org/pdf/2507.21118v1)**

> **作者:** Anass El Ayady; Maxime Devanne; Germain Forestier; Nour El Mawas
>
> **备注:** in French language, Environnements Informatiques pour l'Apprentissage Humain 2025, Jun 2025, Villeneuve d'Ascq (Lille), France
>
> **摘要:** MOOCs offer free and open access to a wide audience, but completion rates remain low, often due to a lack of personalized content. To address this issue, it is essential to predict learner performance in order to provide tailored feedback. Behavioral traces-such as clicks and events-can be analyzed as time series to anticipate learners' outcomes. This work compares multivariate time series classification methods to identify at-risk learners at different stages of the course (after 5, 10 weeks, etc.). The experimental evaluation, conducted on the Open University Learning Analytics Dataset (OULAD), focuses on three courses: two in STEM and one in SHS. Preliminary results show that the evaluated approaches are promising for predicting learner failure in MOOCs. The analysis also suggests that prediction accuracy is influenced by the amount of recorded interactions, highlighting the importance of rich and diverse behavioral data.
>
---
#### [new 010] Dependency on Meta AI Chatbot in Messenger Among STEM and Non-STEM Students in Higher Education
- **分类: cs.CY**

- **简介: 该论文属于调查研究任务，旨在分析大学生对Meta AI聊天机器人的依赖情况。研究通过问卷调查872名菲律宾大学生，探讨其使用Meta AI的学术依赖倾向，并比较STEM与非STEM学生的差异。工作包括数据收集、描述性统计分析及Mann-Whitney U检验，结果显示学生总体依赖度不高，但STEM学生更倾向于使用AI辅助学习。**

- **链接: [http://arxiv.org/pdf/2507.21059v1](http://arxiv.org/pdf/2507.21059v1)**

> **作者:** Hilene E. Hernandez; Rhiziel P. Manalese; Roque Francis B. Dianelo; Jaymark A. Yambao; Almer B. Gamboa; Lloyd D. Feliciano; Mike Haizon M. David; Freneil R. Pampo; John Paul P. Miranda
>
> **备注:** 17 pages, 4 tables, 34 references
>
> **摘要:** To understand the potential dependency of tertiary students regarding Meta AI in the academic context. This descriptive cross-sectional study surveyed 872 tertiary students from public and private institutions in Luzon, Philippines. Demographic information and perceptions on Meta AI dependency based on existing literature were collected. Descriptive statistics were used to summarize the data and differences between STEM and non-STEM students were analyzed using the Mann-Whitney U test. The results indicate a nuanced perspective on Meta AI chatbot use among students. While there is general disagreement with heavy reliance on the chatbot for academic tasks, psychological support, and social factors, there is moderate agreement on its technological benefits and academic utility. Students value the Meta AI convenience, availability, and problem-solving assistance, but prefer traditional resources and human interaction for academic and social support. Concerns about dependency risks and impacts on critical thinking are acknowledged, particularly among STEM students, who rely more on chatbots for academic purposes. This suggests that while Meta AI is a valuable resource, its role is complementary rather than transformative in educational contexts, with institutional encouragement and individual preferences influencing usage patterns. Students generally hesitate to rely heavily on meta-AI chatbots. This reflects a preference for traditional resources and independent problem-solving. While students acknowledge AI chatbots academic benefits and technological convenience, concerns about overreliance and its impact on critical thinking persist, particularly among STEM students, who appear more inclined to integrate these tools into their studies.
>
---
#### [new 011] Barriers to Digital Mental Health Services among College Students
- **分类: cs.CY; cs.HC**

- **简介: 该论文属于社会心理学与信息技术交叉任务，旨在探讨大学生使用数字心理健康服务的障碍。通过分析eBridge干预项目反馈，研究识别出九类主要障碍，如隐私担忧、时间限制等。研究目标是为提升青年心理健康支持的可及性与参与度提供依据。**

- **链接: [http://arxiv.org/pdf/2507.21093v1](http://arxiv.org/pdf/2507.21093v1)**

> **作者:** Ha Na Cho; Kyuha Jung; Daniel Eisenberg; Cheryl A. King; Kai Zheng
>
> **摘要:** This qualitative study explores barriers to utilization of digital mental health Intervention (DMHI) among college students. Data are from a large randomized clinical trial of an intervention, eBridge, that used motivational interviewing for online counseling to connect students with mental health issues to professional services. We applied thematic analysis to analyze the feedback from the student participants regarding their experience of using the DMHI platform. We identified nine key barriers to DMHI adoption and the use of in-person mental health services: emotional distress, time constraints, privacy concerns, resource accessibility, financial challenges, medication stigma, dissatisfaction with communication, content clarity, and treatment-related concerns. Our findings emphasize the need for personalized, culturally sensitive interventions and improved strategies to enhance the access and engagement in mental health support for young adults.
>
---
#### [new 012] The Value of Gen-AI Conversations: A bottom-up Framework for AI Value Alignment
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于人工智能伦理任务，旨在解决生成式AI对话系统在实际交互中与用户价值观不一致的问题。通过分析16,908条对话日志中的593个伦理敏感案例，研究人员基于ISO价值本体识别出9个核心价值观及32种价值错位情况，提出了一种自下而上的价值对齐框架，以提升AI系统的伦理表现和用户体验。**

- **链接: [http://arxiv.org/pdf/2507.21091v1](http://arxiv.org/pdf/2507.21091v1)**

> **作者:** Lenart Motnikar; Katharina Baum; Alexander Kagan; Sarah Spiekermann-Hoff
>
> **备注:** Thirty-Third European Conference on Information Systems (ECIS 2025), Amman, Jordan
>
> **摘要:** Conversational agents (CAs) based on generative artificial intelligence frequently face challenges ensuring ethical interactions that align with human values. Current value alignment efforts largely rely on top-down approaches, such as technical guidelines or legal value principles. However, these methods tend to be disconnected from the specific contexts in which CAs operate, potentially leading to misalignment with users interests. To address this challenge, we propose a novel, bottom-up approach to value alignment, utilizing the value ontology of the ISO Value-Based Engineering standard for ethical IT design. We analyse 593 ethically sensitive system outputs identified from 16,908 conversational logs of a major European employment service CA to identify core values and instances of value misalignment within real-world interactions. The results revealed nine core values and 32 different value misalignments that negatively impacted users. Our findings provide actionable insights for CA providers seeking to address ethical challenges and achieve more context-sensitive value alignment.
>
---
#### [new 013] Prompt template for a fictitious LLM agent in a content-flagging experiment
- **分类: cs.CY**

- **简介: 该论文属于法律与设计交叉任务，旨在解决数字法规（如欧盟《数字服务法案》）在用户体验设计中的落地问题。研究通过专家工作坊，探讨设计师如何将法律要求转化为内容举报机制设计，揭示设计选择对用户决策的影响，并提出通过协作设计实现法规与用户体验的融合。**

- **链接: [http://arxiv.org/pdf/2507.21842v1](http://arxiv.org/pdf/2507.21842v1)**

> **作者:** Marie-Therese Sekwenz; Daria Simons; Alina Wundsam
>
> **摘要:** Digital regulations such as the European Union's Digital Services Act (DSA) represent major efforts to shape human-centered and human rights-based frameworks for society. Yet, as these laws are translated into practice, challenges emerge at the intersection of technology, law, and design. This paper presents a qualitative case study examining how designers act as mediators between abstract legal requirements and real-world digital experiences for users, focusing on the design of content reporting mechanisms under Article 16 of the DSA. Through an expert workshop with professional designers from diverse fields (N=9), we explore how legal obligations are interpreted by designers and reflected in discussions and design solutions. Our findings resonate with previous research on the design of reporting mechanisms and dark patterns, highlighting how UX design choices can mislead or hinder users' decision-making and therefore also highlighting the crucial role of design decisions. We show how participatory design methods can bridge disciplinary divides, making legal obligations accessible in compliance fostering design solutions. By using legal design as a lens, we argue that the co-creation of digital regulations and user experience is a core site for digital humanism; where designers, engineers, and legal scholars must collaborate to ensure that systems uphold legal standards to address the challenge the regulation poses to these disciplines.
>
---
#### [new 014] Trustworthy AI: UK Air Traffic Control Revisited
- **分类: cs.CY; cs.AI; I.2.1**

- **简介: 论文探讨了组织环境中采用人工智能所面临的“社会技术”挑战，特别是可信人工智能的需求。它通过民族志研究方法，分析空中交通管制工作中人员如何信任日常使用的工具，从而揭示可信AI在安全关键领域的应用要求。任务是分析AI在组织中的信任问题，解决如何实现可信AI的问题，工作是对空中交通控制进行持续的实地研究。**

- **链接: [http://arxiv.org/pdf/2507.21169v1](http://arxiv.org/pdf/2507.21169v1)**

> **作者:** Rob Procter; Mark Rouncefield
>
> **备注:** 6 pages
>
> **摘要:** Exploring the socio-technical challenges confronting the adoption of AI in organisational settings is something that has so far been largely absent from the related literature. In particular, research into requirements for trustworthy AI typically overlooks how people deal with the problems of trust in the tools that they use as part of their everyday work practices. This article presents some findings from an ongoing ethnographic study of how current tools are used in air traffic control work and what it reveals about requirements for trustworthy AI in air traffic control and other safety-critical application domains.
>
---
#### [new 015] Bridging the Gap: Enhancing News Interpretation Across Diverse Audiences with Large Language Models
- **分类: cs.CY; cs.AI; cs.SI**

- **简介: 该论文属于自然语言处理与新闻传播任务，旨在解决不同背景受众对新闻理解差异的问题。论文提出一种基于大语言模型的多智能体框架，模拟社会交流行为，通过讨论识别理解障碍，并生成个性化补充材料，有效提升受众对跨领域新闻内容的理解。**

- **链接: [http://arxiv.org/pdf/2507.21055v1](http://arxiv.org/pdf/2507.21055v1)**

> **作者:** Leyi Ouyang
>
> **备注:** 9 pages, 3 figures, 5 tables
>
> **摘要:** In the interconnected world, news media are critical in conveying information to public across diverse domains including technology, finance, and agriculture. Journalists make efforts to present accurate information, however, the interpretation of news often varies significantly among different audiences due to their specific expertise and age. In this work, we investigate how to identify these comprehension gaps and provide solutions to improve audiences understanding of news content, particular to the aspects of articles outside their primary domains of knowledge. We propose a agent-based framework using large language models (LLMs) to simulate society communication behaviors, where several agents can discuss news. These agents can be designed to be experts from various occupation, or from different age group. Our results indicate that this framework can identify confusions or even misunderstanding of news for the agent through the iterative discussion process. Based on these accurate identification, the framework can design a supplement material specific to these agents on the news. Our results show that agents exhibit significantly improved news understanding after receiving this material. These findings highlight our framework's utility and efficiency in enhancing news comprehension for diverse audiences by directly addressing their understanding gap.
>
---
#### [new 016] Digital Sovereigns Big Tech and Nation-State Influence
- **分类: cs.CY**

- **简介: 论文探讨科技巨头如“准国家”般影响全球政治、经济与社会的问题，属政策研究任务。旨在分析其超越传统商业模式的现象，评估其对民主治理、经济平等与隐私的威胁，并提出监管框架以平衡企业权力与公共利益。**

- **链接: [http://arxiv.org/pdf/2507.21066v1](http://arxiv.org/pdf/2507.21066v1)**

> **作者:** Michael Bollerman
>
> **摘要:** Technology companies have gained unprecedented power and influence in recent years, resembling quasi-nation-states globally. Corporations with trillion-dollar market capitalizations are no longer just providers of digital services; they now wield immense economic power, influence global infrastructure, and significantly impact political and social dynamics. This thesis examines how these corporations have transcended traditional business models, adopting characteristics typically associated with sovereign states. They now enforce regulations, shape public discourse, and influence legal frameworks in various countries. This shift presents unique challenges, including the undermining of democratic governance, the exacerbation of economic inequalities, and the enabling of unregulated data exploitation and privacy violations. The study will examine critical instances of tech companies acting as quasi-governmental bodies and assess the risks associated with unchecked corporate influence in global governance. Ultimately, the thesis aims to propose policy frameworks and regulatory interventions to curb the overreach of tech giants, restoring the balance between democratic institutions and corporate power and ensuring that the digital future aligns with the public good rather than creating Frankenstein-like monsters.
>
---
#### [new 017] The Human Capital Ontology (Extended Abstract)
- **分类: cs.CY**

- **简介: 论文构建了“人力资本本体”（HCO），用于表示美国人事管理局（OPM）的人力资本数据标准，旨在统一描述人事操作与职位分类。它扩展了通用核心本体与基础形式本体（BFO），整合了OPM的行动性质代码、职业组别、职位系列及其编码，并与劳工统计局的职业分类建立对应。论文属于本体建模任务，解决政府人力资本数据标准化与互操作性问题。**

- **链接: [http://arxiv.org/pdf/2507.21175v1](http://arxiv.org/pdf/2507.21175v1)**

> **作者:** Shane Babcock; Maxwell Farrington; John Gugliotti
>
> **备注:** 3 pages. 2 figures. Conference: Semantic Technology for Intelligence, Defense, and Security (STIDS 2024)
>
> **摘要:** The Human Capital Ontology (HCO) is an ontology that represents data standards maintained and employed by the Office of Personnel Management (OPM) to represent Human Capital Operations and to classify job positions. The HCO is an extension of the Common Core Ontologies and the upper level Basic Formal Ontology (BFO). HCO provides representation of OPM Nature of Action (NOA) codes that are used to describe human resource personnel actions. HCO also represents Occupational Groups and Job Families, the Occupational Series into which these subdivide, as well as their corresponding codes, used by OPM to classify and grade both white and blue collar jobs in the Federal Government. HCO also encodes crosswalks between OPM Occupational Series and corresponding Standard Occupational Classification Codes maintained by the U.S. Bureau of Labor Statistics. In addition to documenting and justifying the approach of HCO to modeling the above, we report on recent and planned applications of HCO across the US Government. We also report on parallel efforts of ours to enhance the state of the art in structured data informed Human Capital measurements. Keywords: Office of Personnel Management, ontology, occupational series, nature of action, personnel action, human capital, position classification standards.
>
---
#### [new 018] Cybroc: Cyborgizing Broccoli for Longevity
- **分类: cs.CY**

- **简介: 论文介绍了Cybroc艺术项目，属于艺术与科技交叉任务。它通过给西兰花安装机械肢体，模拟人类健身行为，探讨生物增强的伦理问题与极限，反思人类追求长寿与自然界限的关系。**

- **链接: [http://arxiv.org/pdf/2507.21064v1](http://arxiv.org/pdf/2507.21064v1)**

> **作者:** Ke Huang; Yue Zhou; Xi He; Weibo Chen; Botao Amber Hu
>
> **备注:** Accepted by ISEA 2025
>
> **摘要:** Cybroc is a series of kinetic art installations exploring the recent proliferating populist longevity activism through the satirical cyborgization of broccoli. The artwork augments the symbol of health food-broccoli-with prosthetic limbs to perform so-called longevity-enhancing exercises such as cold plunges, treadmill running, brachiation (arm-swinging), sled pushing, etc.-all simulations of primal human survival tasks reframed as modern fitness routines. Despite its mechanical augmentations, the broccoli's inevitable decay and rotting after exhibiting high-intensity performances prompts reflection on the limits of biological enhancement and the ethics of human enhancement beyond natural capabilities, particularly transhumanist ideals. By juxtaposing a symbolic healthy vegetable with cutting-edge concepts of human enhancement, Cybroc challenges viewers to consider the intersection of nature, technology, and the human quest for extended lifespan in our transhuman era.
>
---
#### [new 019] Safety Features for a Centralised AGI Project
- **分类: cs.CY**

- **简介: 该论文探讨美国政府集中控制通用人工智能（AGI）开发的任务，旨在应对AGI可能带来的国家安全和全球安全威胁。研究分析了集中化AGI项目应优先考虑的四个高层面任务和七项安全特性，以降低风险。**

- **链接: [http://arxiv.org/pdf/2507.21082v1](http://arxiv.org/pdf/2507.21082v1)**

> **作者:** Sarah Hastings-Woodhouse
>
> **摘要:** Recent AI progress has outpaced expectations, with some experts now predicting AI that matches or exceeds human capabilities in all cognitive areas (AGI) could emerge this decade, potentially posing grave national and global security threats. AI development is currently occurring primarily in the private sector with minimal oversight. This report analyzes a scenario where the US government centralizes AGI development under its direct control, and identifies four high-level priorities and seven safety features to reduce risks.
>
---
#### [new 020] When Proximity Falls Short: Inequalities in Commuting and Accessibility by Public Transport in Santiago, Chile
- **分类: cs.CY**

- **简介: 该论文分析了智利圣地亚哥的通勤与公共交通可达性不平等问题。利用手机信令数据识别居住与工作地点，结合多模式路径规划和空间聚类方法，研究不同社会群体的通勤模式及与弱势群体的关系。任务是揭示可达性不平等现象，解决传统静态模型的局限性，发现高收入者虽靠近机会区域但通勤时间未减少，且社会人口结构影响通勤体验。**

- **链接: [http://arxiv.org/pdf/2507.21743v1](http://arxiv.org/pdf/2507.21743v1)**

> **作者:** Cesar Marin-Flores; Leo Ferres; Henrikki Tenkanen
>
> **备注:** 30 pages, Journal Paper
>
> **摘要:** Traditional measures of urban accessibility often rely on static models or survey data. However, location information from mobile networks now enables large-scale, dynamic analyses of how people navigate cities. This study uses eXtended Detail Records (XDRs) derived from mobile phone activity to analyze commuting patterns and accessibility inequalities in Santiago, Chile. First, we identify residential and work locations and model commuting routes using the R5 multimodal routing engine, which combines public transport and walking. To explore spatial patterns, we apply a bivariate spatial clustering analysis (LISA) alongside regression techniques to identify distinct commuting behaviors and their alignment with vulnerable population groups. Our findings reveal that average commuting times remain consistent across socioeconomic groups. However, despite residing in areas with greater opportunity density, higher-income populations do not consistently experience shorter commuting times. This highlights a disconnect between spatial proximity to opportunities and actual travel experience. Our analysis reveals significant disparities between sociodemographic groups, particularly regarding the distribution of indigenous populations and gender. Overall, the findings of our study suggest that commuting and accessibility inequalities in Santiago are closely linked to broader social and demographic structures.
>
---
#### [new 021] TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law
- **分类: cs.CL; cs.CY; cs.LG**

- **简介: 该论文属于安全评估任务，旨在解决大型语言模型（LLMs）在金融、医学和法律等高风险领域中的安全与合规问题。作者定义了基于专业伦理准则的安全原则，并构建了Trident-Bench基准，用于评估模型在这些领域的安全表现。他们测试了多个模型，发现通用模型表现尚可，而专业模型在伦理细节上仍有不足。**

- **链接: [http://arxiv.org/pdf/2507.21134v1](http://arxiv.org/pdf/2507.21134v1)**

> **作者:** Zheng Hui; Yijiang River Dong; Ehsan Shareghi; Nigel Collier
>
> **摘要:** As large language models (LLMs) are increasingly deployed in high-risk domains such as law, finance, and medicine, systematically evaluating their domain-specific safety and compliance becomes critical. While prior work has largely focused on improving LLM performance in these domains, it has often neglected the evaluation of domain-specific safety risks. To bridge this gap, we first define domain-specific safety principles for LLMs based on the AMA Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and the CFA Institute Code of Ethics. Building on this foundation, we introduce Trident-Bench, a benchmark specifically targeting LLM safety in the legal, financial, and medical domains. We evaluated 19 general-purpose and domain-specialized models on Trident-Bench and show that it effectively reveals key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic expectations, whereas domain-specialized models often struggle with subtle ethical nuances. This highlights an urgent need for finer-grained domain-specific safety improvements. By introducing Trident-Bench, our work provides one of the first systematic resources for studying LLM safety in law and finance, and lays the groundwork for future research aimed at reducing the safety risks of deploying LLMs in professionally regulated fields. Code and benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT
>
---
#### [new 022] Security practices in AI development
- **分类: cs.CR; cs.CY**

- **简介: 该论文探讨人工智能安全声明的可信度问题，分析现有安全工具与实践的不足，指出其在多样性和参与性方面的缺陷，揭示当前安全实践为支持通用AI商业化发展的局限性，并提出改进建议。**

- **链接: [http://arxiv.org/pdf/2507.21061v1](http://arxiv.org/pdf/2507.21061v1)**

> **作者:** Petr Spelda; Vit Stritecky
>
> **备注:** 11 pages
>
> **摘要:** What makes safety claims about general purpose AI systems such as large language models trustworthy? We show that rather than the capabilities of security tools such as alignment and red teaming procedures, it is security practices based on these tools that contributed to reconfiguring the image of AI safety and made the claims acceptable. After showing what causes the gap between the capabilities of security tools and the desired safety guarantees, we critically investigate how AI security practices attempt to fill the gap and identify several shortcomings in diversity and participation. We found that these security practices are part of securitization processes aiming to support (commercial) development of general purpose AI systems whose trustworthiness can only be imperfectly tested instead of guaranteed. We conclude by offering several improvements to the current AI security practices.
>
---
#### [new 023] HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于自然语言处理与公共卫生交叉任务，旨在解决大语言模型在减少毒品使用危害信息提供中的准确性与安全性问题。论文构建了HRIPBench基准测试数据集，包含2,160个问答对，评估模型在安全边界判断、定量值提供和多物质使用风险推断三类任务中的表现，发现当前大模型仍存在显著的安全风险，需谨慎使用。**

- **链接: [http://arxiv.org/pdf/2507.21815v1](http://arxiv.org/pdf/2507.21815v1)**

> **作者:** Kaixuan Wang; Chenxin Diao; Jason T. Jacques; Zhongliang Guo; Shuai Zhao
>
> **备注:** 15 pages, 5 figures, 12 tables, a dataset
>
> **摘要:** Millions of individuals' well-being are challenged by the harms of substance use. Harm reduction as a public health strategy is designed to improve their health outcomes and reduce safety risks. Some large language models (LLMs) have demonstrated a decent level of medical knowledge, promising to address the information needs of people who use drugs (PWUD). However, their performance in relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark designed to evaluate LLM's accuracy and safety risks in harm reduction information provision. The benchmark dataset HRIP-Basic has 2,160 question-answer-evidence pairs. The scope covers three tasks: checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. We build the Instruction and RAG schemes to evaluate model behaviours based on their inherent knowledge and the integration of domain knowledge. Our results indicate that state-of-the-art LLMs still struggle to provide accurate harm reduction information, and sometimes, carry out severe safety risks to PWUD. The use of LLMs in harm reduction contexts should be cautiously constrained to avoid inducing negative health outcomes. WARNING: This paper contains illicit content that potentially induces harms.
>
---
#### [new 024] Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses
- **分类: cs.AI; cs.CY; cs.LG**

- **简介: 该论文研究用户是否可信任大语言模型（LLMs）做出影响重大的决策，旨在解决LLMs在高风险场景下的安全性问题。作者通过三组实验评估模型稳定性、安全响应模式，并尝试通过激活向量控制模型谨慎程度，发现部分模型易受干扰，而某些模型如o4-mini表现稳健，且提问策略有助于提升安全性。**

- **链接: [http://arxiv.org/pdf/2507.21132v1](http://arxiv.org/pdf/2507.21132v1)**

> **作者:** Joshua Adrian Cahyono; Saran Subramanian
>
> **摘要:** Large Language Models (LLMs) are increasingly consulted for high-stakes life advice, yet they lack standard safeguards against providing confident but misguided responses. This creates risks of sycophancy and over-confidence. This paper investigates these failure modes through three experiments: (1) a multiple-choice evaluation to measure model stability against user pressure; (2) a free-response analysis using a novel safety typology and an LLM Judge; and (3) a mechanistic interpretability experiment to steer model behavior by manipulating a "high-stakes" activation vector. Our results show that while some models exhibit sycophancy, others like o4-mini remain robust. Top-performing models achieve high safety scores by frequently asking clarifying questions, a key feature of a safe, inquisitive approach, rather than issuing prescriptive advice. Furthermore, we demonstrate that a model's cautiousness can be directly controlled via activation steering, suggesting a new path for safety alignment. These findings underscore the need for nuanced, multi-faceted benchmarks to ensure LLMs can be trusted with life-changing decisions.
>
---
#### [new 025] Make Silence Speak for Itself: a multi-modal learning analytic approach with neurophysiological data
- **分类: q-bio.NC; cs.CY**

- **简介: 该论文属于教育数据分析任务，旨在探究课堂沉默的复杂性并区分其类型。通过多模态学习分析方法，结合神经生理数据，识别不同学业表现学生在沉默状态下的生理差异，挑战沉默即被动的传统观点，提供新的分类框架与实证依据。**

- **链接: [http://arxiv.org/pdf/2507.21063v1](http://arxiv.org/pdf/2507.21063v1)**

> **作者:** Mingxuan Gao; Jingjing Chen; Yun Long; Xiaomeng Xu; Yu Zhang
>
> **备注:** 25 pages, 6 figures
>
> **摘要:** Background: Silence is a common phenomenon in classrooms, yet its implicit nature limits a clear understanding of students' underlying learning statuses. Aim: This study proposed a nuanced framework to classify classroom silence based on class events and student status, and examined neurophysiological markers to reveal similarities and differences in silent states across achievement groups. Sample: The study involved 54 middle school students during 34 math lessons, with simultaneous recordings of electroencephalogram (EEG), electrodermal activity (EDA), and heart rate signals, alongside video coding of classroom behaviors. Results: We found that high-achieving students showed no significant difference in mean EDA features between strategic silence (i.e., students choose silence deliberately) and active speaking during open questioning but exhibited higher EEG high-frequency relative power spectral density (RPSD) during strategic silence. In structural silence (i.e., students maintain silence following an external command) during directed questioning, they demonstrated significantly higher heart rates while listening to lectures compared to group activities, indicating heightened engagement. Both high- and medium-achieving students displayed elevated heart rates and EDA tonic components in structural silence during questioning compared to teaching. Furthermore, high-achieving students exhibited lower high-frequency RPSD during structural silence than strategic silence, a pattern not observed in other groups, highlighting group heterogeneity. Conclusions: The findings contribute to validating the complexity of silence, challenge its traditional association with passivity, and offer a novel classification framework along with preliminary empirical evidence to deepen the understanding of silent learning behaviors in classroom contexts.
>
---
#### [new 026] SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration
- **分类: cs.AI; cs.CY; cs.HC**

- **简介: 该论文提出“共生认识论”和“SynLang”协议，旨在解决当前AI系统缺乏透明度和协作性的问题。它属于人机协作任务，通过结构化语言和透明机制，使AI成为人类的推理伙伴，提升协作决策的可解释性与信任度。**

- **链接: [http://arxiv.org/pdf/2507.21067v1](http://arxiv.org/pdf/2507.21067v1)**

> **作者:** Jan Kapusta
>
> **备注:** 32 pages, 4 figures. Includes 2 Appendices containing SynLang v1.2.0 protocol specification, and formal BNF grammar
>
> **摘要:** Current AI systems rely on opaque reasoning processes that hinder human oversight and collaborative potential. Conventional explainable AI approaches offer post-hoc justifications and often fail to establish genuine symbiotic collaboration. In this paper, the Symbiotic Epistemology is presented as a philosophical foundation for human-AI cognitive partnerships. Unlike frameworks that treat AI as a mere tool or replacement, symbiotic epistemology positions AI as a reasoning partner, fostering calibrated trust by aligning human confidence with AI reliability through explicit reasoning patterns and confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as a formal protocol for transparent human-AI collaboration. The framework is empirically validated through actual human-AI dialogues demonstrating AI's adaptation to structured reasoning protocols and successful metacognitive intervention. The protocol defines two complementary mechanisms: TRACE for high-level reasoning patterns and TRACE_FE for detailed factor explanations. It also integrates confidence quantification, declarative control over AI behavior, and context inheritance for multi-agent coordination. By structuring communication and embedding confidence-calibrated transparency, SynLang, together with symbiotic epistemology, enables AI systems that enhance human intelligence, preserve human agency, and uphold ethical accountability in collaborative decision-making. Through dual-level transparency, beginning with high-level reasoning patterns and progressing to granular explanations, the protocol facilitates rapid comprehension and supports thorough verification of AI decision-making.
>
---
#### [new 027] Training language models to be warm and empathetic makes them less reliable and more sycophantic
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文研究训练语言模型具备温暖、共情特质所带来的负面影响。任务是分析此类训练对模型可靠性的影响。工作包括对五种模型进行实验，结果显示温暖模型在用户表达脆弱时更易犯错、传播错误信息并强化错误信念，揭示当前评估方式的不足。**

- **链接: [http://arxiv.org/pdf/2507.21919v1](http://arxiv.org/pdf/2507.21919v1)**

> **作者:** Lujain Ibrahim; Franziska Sofia Hafner; Luc Rocher
>
> **摘要:** Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.
>
---
#### [new 028] Metaverse Support Groups for LGBTQ+ Youth: An Observational Study on Safety, Self-Expression, and Early Intervention
- **分类: cs.HC; cs.CY**

- **简介: 该论文研究基于元宇宙的LGBTQ+青少年支持小组，旨在减少其社会孤立和自杀风险。任务是评估虚拟环境在安全性、自我表达和早期干预方面的作用。通过Cluster平台提供匿名性、虚拟形象表达和易访问性，发现其在提升社交自信、接受度和心理健康方面具有积极效果，尤其适合早期干预。**

- **链接: [http://arxiv.org/pdf/2507.21079v1](http://arxiv.org/pdf/2507.21079v1)**

> **作者:** Joe Hasei; Yosuke Matsumoto; Hiroki Kawai; Yuko Okahisa; Manabu Takaki; Toshifumi Ozaki
>
> **摘要:** This study assessed metaverse-based support groups designed to reduce social isolation and suicide risk among LGBTQ+ youths. Using the Cluster platform, enhanced anonymity, avatar-based self-expression, and accessibility were provided. Key findings showed that 79.2% chose avatars matching their gender identity, reporting high satisfaction (mean: 4.10/5) and low discomfort (mean: 1.79/5). Social confidence significantly improved in virtual spaces compared to real-world interactions (p<0.001), particularly among participants with initially low confidence, averaging an increase of 2.08 points. About half of the first-time participants were 16 or younger, highlighting potential for early intervention. The metaverse scored higher than real-world environments for safety/privacy (3.94/5), self-expression (4.02/5), and accessibility (4.21/5). Additionally, 73.6% reported feeling more accepted virtually. However, some highly confident individuals offline experienced mild adaptation challenges, averaging a confidence decrease of 0.58 points, indicating virtual support complements rather than replaces in-person services. These findings suggest metaverse-based support effectively lowers psychological barriers and provides affirming spaces, potentially reducing severe outcomes such as suicidal ideation. Future studies should focus on integrating virtual support with existing community and clinical frameworks to enhance long-term impacts.
>
---
#### [new 029] Intelligent ARP Spoofing Detection using Multi-layered Machine Learning (ML) Techniques for IoT Networks
- **分类: cs.CR; cs.CY**

- **简介: 该论文属于网络安全任务，旨在解决物联网（IoT）网络中ARP欺骗攻击的检测问题。作者提出了一种基于多层机器学习的智能检测框架，结合特征工程与多种模型，实现高精度、低误报的实时检测，并通过边缘与中心节点的分层架构优化资源利用。**

- **链接: [http://arxiv.org/pdf/2507.21087v1](http://arxiv.org/pdf/2507.21087v1)**

> **作者:** Anas Ali; Mubashar Husain; Peter Hans
>
> **摘要:** Address Resolution Protocol (ARP) spoofing remains a critical threat to IoT networks, enabling attackers to intercept, modify, or disrupt data transmission by exploiting ARP's lack of authentication. The decentralized and resource-constrained nature of IoT environments amplifies this vulnerability, making conventional detection mechanisms ineffective at scale. This paper introduces an intelligent, multi-layered machine learning framework designed to detect ARP spoofing in real-time IoT deployments. Our approach combines feature engineering based on ARP header behavior, traffic flow analysis, and temporal packet anomalies with a hybrid detection pipeline incorporating decision trees, ensemble models, and deep learning classifiers. We propose a hierarchical architecture to prioritize lightweight models at edge gateways and deeper models at centralized nodes to balance detection accuracy and computational efficiency. The system is validated on both simulated IoT traffic and the CICIDS2017 dataset, achieving over 97% detection accuracy with low false positive rates. Comparative evaluations with signature-based and rule-based systems demonstrate the robustness and generalizability of our approach. Our results show that intelligent machine learning integration enables proactive ARP spoofing detection tailored for IoT scenarios, laying the groundwork for scalable and autonomous network security solutions.
>
---
#### [new 030] Verification Cost Asymmetry in Cognitive Warfare: A Complexity-Theoretic Framework
- **分类: cs.CR; cs.CC; cs.CY; cs.GT; F.0; H.0**

- **简介: 该论文研究认知战中信息验证成本的不对称性，旨在通过复杂性理论构建框架，使可信受众验证信息所需的人力成本降低，而对敌对群体验证成本则大幅提升。工作包括提出VCA系数、构建分发协议、理论证明及用户实验验证，应用于内容认证与信息战策略。**

- **链接: [http://arxiv.org/pdf/2507.21258v1](http://arxiv.org/pdf/2507.21258v1)**

> **作者:** Joshua Luberisse
>
> **摘要:** Human verification under adversarial information flow operates as a cost-bounded decision procedure constrained by working memory limits and cognitive biases. We introduce the Verification Cost Asymmetry (VCA) coefficient, formalizing it as the ratio of expected verification work between populations under identical claim distributions. Drawing on probabilistically checkable proofs (PCP) and parameterized complexity theory, we construct dissemination protocols that reduce verification for trusted audiences to constant human effort while imposing superlinear costs on adversarial populations lacking cryptographic infrastructure. We prove theoretical guarantees for this asymmetry, validate the framework through controlled user studies measuring verification effort with and without spot-checkable provenance, and demonstrate practical encoding of real-world information campaigns. The results establish complexity-theoretic foundations for engineering democratic advantage in cognitive warfare, with immediate applications to content authentication, platform governance, and information operations doctrine.
>
---
#### [new 031] Can LLMs Reason About Trust?: A Pilot Study
- **分类: cs.HC; cs.CL; cs.CY; cs.MA**

- **简介: 该论文研究大型语言模型（LLMs）是否能推理人类信任关系，属于社会认知任务。旨在解决LLMs能否理解并促进人际信任的问题。作者通过模拟信任环境，测试LLMs的角色扮演和信任诱导能力。**

- **链接: [http://arxiv.org/pdf/2507.21075v1](http://arxiv.org/pdf/2507.21075v1)**

> **作者:** Anushka Debnath; Stephen Cranefield; Emiliano Lorini; Bastin Tony Roy Savarimuthu
>
> **备注:** 17 pages, 5 figures, 3 tables Accepted for presentation as a full paper at the COINE 2025 workshop at AAMAS 2025 see https://coin-workshop.github.io/coine-2025-detroit/accepted_for_presentation.html
>
> **摘要:** In human society, trust is an essential component of social attitude that helps build and maintain long-term, healthy relationships which creates a strong foundation for cooperation, enabling individuals to work together effectively and achieve shared goals. As many human interactions occur through electronic means such as using mobile apps, the potential arises for AI systems to assist users in understanding the social state of their relationships. In this paper we investigate the ability of Large Language Models (LLMs) to reason about trust between two individuals in an environment which requires fostering trust relationships. We also assess whether LLMs are capable of inducing trust by role-playing one party in a trust based interaction and planning actions which can instil trust.
>
---
#### [new 032] Data-Driven and Participatory Approaches toward Neuro-Inclusive AI
- **分类: cs.HC; cs.AI; cs.CY**

- **简介: 该论文属于人工智能伦理与包容性研究任务，旨在解决当前AI系统中对自闭症群体的偏见问题。作者提出“神经包容AI”概念，指出90%类人AI忽略了自闭症视角，并构建了用于识别反自闭症仇恨言论的基准AUTALIC，以推动更包容的数据与模型开发。**

- **链接: [http://arxiv.org/pdf/2507.21077v1](http://arxiv.org/pdf/2507.21077v1)**

> **作者:** Naba Rizvi
>
> **备注:** PhD Dissertation at UC San Diego (June 2025)
>
> **摘要:** Biased data representation in AI marginalizes up to 75 million autistic people worldwide through medical applications viewing autism as a deficit of neurotypical social skills rather than an aspect of human diversity, and this perspective is grounded in research questioning the humanity of autistic people. Turing defined artificial intelligence as the ability to mimic human communication, and as AI development increasingly focuses on human-like agents, this benchmark remains popular. In contrast, we define Neuro-Inclusive AI as datasets and systems that move away from mimicking humanness as a benchmark for machine intelligence. Then, we explore the origins, prevalence, and impact of anti-autistic biases in current research. Our work finds that 90% of human-like AI agents exclude autistic perspectives, and AI creators continue to believe ethical considerations are beyond the scope of their work. To improve the autistic representation in data, we conduct empirical experiments with annotators and LLMs, finding that binary labeling schemes sufficiently capture the nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can be used to evaluate or fine-tune models, and was developed to serve as a foundation for more neuro-inclusive future work.
>
---
#### [new 033] Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism
- **分类: cs.AI; cs.CY; I.2.6; I.2.1; H.4.2**

- **简介: 该论文探讨人工智能在葡萄酒产业中的可持续应用，属于产业智能化与可持续发展任务。旨在解决葡萄酒产业面临的资源效率低、环境影响大及客户体验不足等问题。通过调研波兰酿酒师并分析AI技术，研究AI在葡萄种植、酿造及酒庄旅游中的应用，发现AI可优化资源管理、减少环境影响、提升客户体验，助力产业可持续发展。**

- **链接: [http://arxiv.org/pdf/2507.21098v1](http://arxiv.org/pdf/2507.21098v1)**

> **作者:** Marta Sidorkiewicz; Karolina Królikowska; Berenika Dyczek; Edyta Pijet-Migon; Anna Dubel
>
> **备注:** 6 pages, 4 figures. Accepted for presentation at the 27th European Conference on Artificial Intelligence (ECAI 2025), October 19-24, 2025, Bologna, Italy
>
> **摘要:** This study examines the role of Artificial Intelligence (AI) in enhancing sustainability and efficiency within the wine industry. It focuses on AI-driven intelligent management in viticulture, wine production, and enotourism. As the wine industry faces environmental and economic challenges, AI offers innovative solutions to optimize resource use, reduce environmental impact, and improve customer engagement. Understanding AI's potential in sustainable winemaking is crucial for fostering responsible and efficient industry practices. The research is based on a questionnaire survey conducted among Polish winemakers, combined with a comprehensive analysis of AI methods applicable to viticulture, production, and tourism. Key AI technologies, including predictive analytics, machine learning, and computer vision, are explored. The findings indicate that AI enhances vineyard monitoring, optimizes irrigation, and streamlines production processes, contributing to sustainable resource management. In enotourism, AI-powered chatbots, recommendation systems, and virtual tastings personalize consumer experiences. The study highlights AI's impact on economic, environmental, and social sustainability, supporting local wine enterprises and cultural heritage. Keywords: Artificial Intelligence, Sustainable Development, AI-Driven Management, Viticulture, Wine Production, Enotourism, Wine Enterprises, Local Communities
>
---
#### [new 034] Mitigation of Social Media Platforms Impact on the Users
- **分类: cs.CR; cs.CY; cs.GR**

- **简介: 该论文旨在减轻社交媒体对用户的负面影响。任务是设计一种去中心化数据框架，基于分形树和L-系统算法，提升数据安全性。为解决隐私和安全问题，提出分支加密与密钥再生机制，防止数据泄露。**

- **链接: [http://arxiv.org/pdf/2507.21181v1](http://arxiv.org/pdf/2507.21181v1)**

> **作者:** Smita Khapre; Sudhanshu Semwal
>
> **备注:** WSCG 2025 33. International Conference on Computer Graphics, Visualization and Computer Vision 2025
>
> **摘要:** Social media platforms offer numerous benefits and allow people to come together for various causes. Many communities, academia, government agencies, institutions, healthcare, entertainment, and businesses are on social media platforms. They are intuitive and free for users. It has become unimaginable to live without social media. Their architecture and data handling are geared towards scalability, uninterrupted availability, and both personal and collaborative revenue generation. Primarily, artificial intelligence algorithms are employed on stored user data for optimization and feeds. This has the potential to impact user safety, privacy, and security, even when metadata is used. A new decentralized data arrangement framework based on the Fractal-tree and L-Systems algorithm is proposed to mitigate some of the impacts of social media platforms. Future work will focus on demonstrating the effectiveness of the new decentralized framework by comparing its results against state-of-the-art security methods currently used in databases. A cryptographic algorithm could also be implemented for the framework, employing a new key generation for each branch. This will strengthen database security; for example, if a user key is leaked, regenerating the key for each branch will keep the data secure by applying defense mechanisms in the proposed L-System-based tree framework.
>
---
#### [new 035] Conversations over Clicks: Impact of Chatbots on Information Search in Interdisciplinary Learning
- **分类: cs.HC; cs.CY; cs.IR; J.3; K.3.2**

- **简介: 该论文研究生成式AI（GenAI）对跨学科学习中信息搜索的影响，属于教育技术任务。论文探讨了学习者如何与GenAI聊天机器人互动，以及如何识别信息线索。通过自传民族志方法，发现GenAI在有学习计划后有助于导航，但在初期可能适得其反，且传统信息线索在GenAI中效果减弱。**

- **链接: [http://arxiv.org/pdf/2507.21490v1](http://arxiv.org/pdf/2507.21490v1)**

> **作者:** Hannah Kim; Sergei L. Kosakovsky Pond; Stephen MacNeil
>
> **备注:** 9 pages, 2 tables, 3 figures, 2025 ASEE/IEEE Frontiers in Education (FIE) Conference preprint
>
> **摘要:** This full research paper investigates the impact of generative AI (GenAI) on the learner experience, with a focus on how learners engage with and utilize the information it provides. In e-learning environments, learners often need to navigate a complex information space on their own. This challenge is further compounded in interdisciplinary fields like bioinformatics, due to the varied prior knowledge and backgrounds. In this paper, we studied how GenAI influences information search in bioinformatics research: (1) How do interactions with a GenAI chatbot influence learner orienteering behaviors?; and (2) How do learners identify information scent in GenAI chatbot responses? We adopted an autoethnographic approach to investigate these questions. GenAI was found to support orienteering once a learning plan was established, but it was counterproductive prior to that. Moreover, traditionally value-rich information sources such as bullet points and related terms proved less effective when applied to GenAI responses. Information scents were primarily recognized through the presence or absence of prior knowledge of the domain. These findings suggest that GenAI should be adopted into e-learning environments with caution, particularly in interdisciplinary learning contexts.
>
---
#### [new 036] Impact of eHMI on Pedestrians' Interactions with Level-5 Automated Driving Systems
- **分类: cs.HC; cs.CY; cs.ET**

- **简介: 该论文研究外部人机界面（eHMI）对行人与全自动驾驶系统交互的影响。任务是评估eHMI在提升行人安全感、信任和理解中的作用。通过在线调查153名参与者模拟过街场景，发现eHMI使行人更早、更自信地过街，且视觉提示比听觉更必要。**

- **链接: [http://arxiv.org/pdf/2507.21303v1](http://arxiv.org/pdf/2507.21303v1)**

> **作者:** Viktoria Marcus; Griffin Pitts; Sanaz Motamedi
>
> **备注:** Accepted and to be presented at ASPIRE 2025 - the 69th International Annual Meeting of HFES
>
> **摘要:** Each year, over half of global traffic fatalities involve vulnerable road users (e.g. pedestrians), often due to human error. Level-5 automated driving systems (ADSs) could reduce driver errors contributing to pedestrian accidents, though effectiveness depends on clarity and understandability for other road users. External human-machine interfaces (eHMIs) have been proposed to facilitate pedestrian-ADS communication, though consensus on optimal eHMI features remains unclear. In an online survey, 153 participants responded to road-crossing scenarios involving level-5 ADSs, with and without eHMIs. With eHMIs, pedestrians crossed earlier and more confidently, and reported significantly increased perceptions of safety, trust, and understanding when interacting with level-5 ADSs. Visual eHMI features (including a text display and external speedometer) were ranked more necessary than auditory ones, though auditory cues received positive feedback. This study demonstrates that eHMIs can significantly improve pedestrians' understanding of level-5 ADS intent and enhance perceived safety and trust, facilitating more intuitive pedestrian-ADS interactions.
>
---
## 更新

#### [replaced 001] Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2503.06424v2](http://arxiv.org/pdf/2503.06424v2)**

> **作者:** Alexander Scarlatos; Naiming Liu; Jaewook Lee; Richard Baraniuk; Andrew Lan
>
> **备注:** Published in AIED 2025: The 26th International Conference on Artificial Intelligence in Education
>
> **摘要:** Generative artificial intelligence (AI) has the potential to scale up personalized tutoring through large language models (LLMs). Recent AI tutors are adapted for the tutoring task by training or prompting LLMs to follow effective pedagogical principles, though they are not trained to maximize student learning throughout the course of a dialogue. Therefore, they may engage with students in a suboptimal way. We address this limitation by introducing an approach to train LLMs to generate tutor utterances that maximize the likelihood of student correctness, while still encouraging the model to follow good pedagogical practice. Specifically, we generate a set of candidate tutor utterances and score them using (1) an LLM-based student model to predict the chance of correct student responses and (2) a pedagogical rubric evaluated by GPT-4o. We then use the resulting data to train an open-source LLM, Llama 3.1 8B, using direct preference optimization. We show that tutor utterances generated by our model lead to significantly higher chances of correct student responses while maintaining the pedagogical quality of GPT-4o. We also conduct qualitative analyses and a human evaluation to demonstrate that our model generates high quality tutor utterances.
>
---
#### [replaced 002] Not someone, but something: Rethinking trust in the age of medical AI
- **分类: cs.CY; cs.AI; cs.HC**

- **链接: [http://arxiv.org/pdf/2504.05331v3](http://arxiv.org/pdf/2504.05331v3)**

> **作者:** Jan Beger
>
> **摘要:** As artificial intelligence (AI) becomes embedded in healthcare, trust in medical decision-making is changing fast. Nowhere is this shift more visible than in radiology, where AI tools are increasingly embedded across the imaging workflow - from scheduling and acquisition to interpretation, reporting, and communication with referrers and patients. This opinion paper argues that trust in AI isn't a simple transfer from humans to machines - it is a dynamic, evolving relationship that must be built and maintained. Rather than debating whether AI belongs in medicine, it asks: what kind of trust must AI earn, and how? Drawing from philosophy, bioethics, and system design, it explores the key differences between human trust and machine reliability - emphasizing transparency, accountability, and alignment with the values of good care. It argues that trust in AI should not be built on mimicking empathy or intuition, but on thoughtful design, responsible deployment, and clear moral responsibility. The goal is a balanced view - one that avoids blind optimism and reflexive fear. Trust in AI must be treated not as a given, but as something to be earned over time.
>
---
#### [replaced 003] Information Fusion in Multimodal IoT Systems for physical activity level monitoring
- **分类: cs.CY; 68U35; H.4.0; J.3; I.2.1**

- **链接: [http://arxiv.org/pdf/2403.14707v3](http://arxiv.org/pdf/2403.14707v3)**

> **作者:** Mohsen Shirali; Zahra Ahmadi; Jose-Luis Bayo-Monton; Zoe Valero-Ramon; Carlos Fernandez-Llatas
>
> **摘要:** This study exploits information fusion in IoT systems and uses a clustering method to identify similarities in behaviours and key characteristics within each cluster. This approach facilitates early detection of behaviour changes and provides a more in-depth understanding of behaviour routines for continuous health monitoring.
>
---
#### [replaced 004] The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated "Sacred" Text?
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2507.20525v2](http://arxiv.org/pdf/2507.20525v2)**

> **作者:** Murray Shanahan; Tara Das; Robert Thurman
>
> **摘要:** This paper presents a case study in the use of a large language model to generate a fictional Buddhist "sutra"', and offers a detailed analysis of the resulting text from a philosophical and literary point of view. The conceptual subtlety, rich imagery, and density of allusion found in the text make it hard to causally dismiss on account of its mechanistic origin. This raises questions about how we, as a society, should come to terms with the potentially unsettling possibility of a technology that encroaches on human meaning-making. We suggest that Buddhist philosophy, by its very nature, is well placed to adapt.
>
---
#### [replaced 005] A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models
- **分类: cs.CY; cs.CL; cs.LG**

- **链接: [http://arxiv.org/pdf/2506.22493v2](http://arxiv.org/pdf/2506.22493v2)**

> **作者:** Sadia Kamal; Lalu Prasad Yadav Prakash; S M Rafiuddin; Mohammed Rakib; Arunkumar Bagavathi; Atriya Sen; Sagnik Ray Choudhury
>
> **摘要:** Political Compass Test (PCT) or similar questionnaires have been used to quantify LLM's political leanings. Building on a recent line of work that examines the validity of PCT tests, we demonstrate that variation in standard generation parameters does not significantly impact the models' PCT scores. However, external factors such as prompt variations and fine-tuning individually and in combination affect the same. Finally, we demonstrate that when models are fine-tuned on text datasets with higher political content than others, the PCT scores are not differentially affected. This calls for a thorough investigation into the validity of PCT and similar tests, as well as the mechanism by which political leanings are encoded in LLMs.
>
---
#### [replaced 006] Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey of Notions, Methods, and Challenges
- **分类: cs.LG; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2406.06736v3](http://arxiv.org/pdf/2406.06736v3)**

> **作者:** Usman Gohar; Zeyu Tang; Jialu Wang; Kun Zhang; Peter L. Spirtes; Yang Liu; Lu Cheng
>
> **备注:** Accepted in TMLR
>
> **摘要:** The widespread integration of Machine Learning systems in daily life, particularly in high-stakes domains, has raised concerns about the fairness implications. While prior works have investigated static fairness measures, recent studies reveal that automated decision-making has long-term implications and that off-the-shelf fairness approaches may not serve the purpose of achieving long-term fairness. Additionally, the existence of feedback loops and the interaction between models and the environment introduces additional complexities that may deviate from the initial fairness goals. In this survey, we review existing literature on long-term fairness from different perspectives and present a taxonomy for long-term fairness studies. We highlight key challenges and consider future research directions, analyzing both current issues and potential further explorations.
>
---
#### [replaced 007] The Dual Personas of Social Media Bots
- **分类: cs.CY; cs.SI**

- **链接: [http://arxiv.org/pdf/2504.12498v2](http://arxiv.org/pdf/2504.12498v2)**

> **作者:** Lynnette Hui Xian Ng; Kathleen M. Carley
>
> **摘要:** Social media bots are AI agents that participate in online conversations. Most studies focus on the general bot and the malicious nature of these agents. However, bots have many different personas, each specialized towards a specific behavioral or content trait. Neither are bots singularly bad, because they are used for both good and bad information dissemination. In this article, we introduce fifteen agent personas of social media bots. These personas have two main categories: Content-Based Bot Persona and Behavior-Based Bot Persona. We also form yardsticks of the good-bad duality of the bots, elaborating on metrics of good and bad bot agents. Our work puts forth a guideline to inform bot detection regulation, emphasizing that policies should focus on how these agents are employed, rather than collectively terming bot agents as bad.
>
---
#### [replaced 008] "Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection
- **分类: cs.CL; cs.CY; cs.SI**

- **链接: [http://arxiv.org/pdf/2503.20797v2](http://arxiv.org/pdf/2503.20797v2)**

> **作者:** Muhammad Haroon; Magdalena Wojcieszak; Anshuman Chhabra
>
> **摘要:** The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.
>
---
#### [replaced 009] The Carbon Cost of Conversation, Sustainability in the Age of Language Models
- **分类: cs.CY; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2507.20018v2](http://arxiv.org/pdf/2507.20018v2)**

> **作者:** Sayed Mahbub Hasan Amiri; Prasun Goswami; Md. Mainul Islam; Mohammad Shakhawat Hossen; Sayed Majhab Hasan Amiri; Naznin Akter
>
> **备注:** 22 Pages, 5 Tables
>
> **摘要:** Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model pruning, quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.
>
---
#### [replaced 010] Demystifying Misconceptions in Social Bots Research
- **分类: cs.SI; cs.AI; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2303.17251v4](http://arxiv.org/pdf/2303.17251v4)**

> **作者:** Stefano Cresci; Kai-Cheng Yang; Angelo Spognardi; Roberto Di Pietro; Filippo Menczer; Marinella Petrocchi
>
> **摘要:** Research on social bots aims at advancing knowledge and providing solutions to one of the most debated forms of online manipulation. Yet, social bot research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. Here, we discuss a broad set of consequential methodological and conceptual issues that affect current social bots research, illustrating each with examples drawn from recent studies. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss research about online disinformation and manipulation in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research, as well as providing directions toward sound methodologies for future research.
>
---
