# 计算机与社会 cs.CY

- **最新发布 32 篇**

- **更新 8 篇**

## 最新发布

#### [new 001] VArsity: Can Large Language Models Keep Power Engineering Students in Phase?
- **分类: cs.CY; cs.SY; eess.SY**

- **简介: 该论文属于教育案例研究任务，旨在探讨大语言模型（LLM）在电力工程教学中的应用与挑战。研究通过让学生识别ChatGPT在电力因数校正问题中的错误，评估其在不同版本（GPT-4与ChatGPT o1）下的表现。结果显示，学生较易识别GPT-4的错误，而对ChatGPT o1的错误识别难度增加，表明LLM在教学评估中的作用需进一步研究。**

- **链接: [http://arxiv.org/pdf/2507.20995v1](http://arxiv.org/pdf/2507.20995v1)**

> **作者:** Samuel Talkington; Daniel K. Molzahn
>
> **备注:** 9 pages, 4 figures
>
> **摘要:** This paper provides an educational case study regarding our experience in deploying ChatGPT Large Language Models (LLMs) in the Spring 2025 and Fall 2023 offerings of ECE 4320: Power System Analysis and Control at Georgia Tech. As part of course assessments, students were tasked with identifying, explaining, and correcting errors in the ChatGPT outputs corresponding to power factor correction problems. While most students successfully identified the errors in the outputs from the GPT-4 version of ChatGPT used in Fall 2023, students found the errors from the ChatGPT o1 version much more difficult to identify in Spring 2025. As shown in this case study, the role of LLMs in pedagogy, assessment, and learning in power engineering classrooms is an important topic deserving further investigation.
>
---
#### [new 002] Justifications for Democratizing AI Alignment and Their Prospects
- **分类: cs.CY; cs.AI**

- **简介: 论文探讨人工智能对齐问题中的民主化路径，分析其相较于专家主导模式的合理性与挑战，结合工具性与非工具性理由，提出混合框架以平衡专家判断与公众参与，应对规范性与元规范性不确定性。**

- **链接: [http://arxiv.org/pdf/2507.19548v1](http://arxiv.org/pdf/2507.19548v1)**

> **作者:** André Steingrüber; Kevin Baum
>
> **备注:** accepted for the LNCS on-site proceedings of the AISoLA 2025 conference
>
> **摘要:** The AI alignment problem comprises both technical and normative dimensions. While technical solutions focus on implementing normative constraints in AI systems, the normative problem concerns determining what these constraints should be. This paper examines justifications for democratic approaches to the normative problem -- where affected stakeholders determine AI alignment -- as opposed to epistocratic approaches that defer to normative experts. We analyze both instrumental justifications (democratic approaches produce better outcomes) and non-instrumental justifications (democratic approaches prevent illegitimate authority or coercion). We argue that normative and metanormative uncertainty create a justificatory gap that democratic approaches aim to fill through political rather than theoretical justification. However, we identify significant challenges for democratic approaches, particularly regarding the prevention of illegitimate coercion through AI alignment. Our analysis suggests that neither purely epistocratic nor purely democratic approaches may be sufficient on their own, pointing toward hybrid frameworks that combine expert judgment with participatory input alongside institutional safeguards against AI monopolization.
>
---
#### [new 003] The Carbon Cost of Conversation, Sustainability in the Age of Language Models
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 这篇论文属于环境影响评估与可持续发展任务，旨在解决大型语言模型（LLMs）带来的碳排放、水资源消耗和电子废弃物问题。论文通过案例研究量化其环境成本，分析问题成因，并提出技术、政策和文化层面的可持续路径。**

- **链接: [http://arxiv.org/pdf/2507.20018v1](http://arxiv.org/pdf/2507.20018v1)**

> **作者:** Sayed Mahbub Hasan Amiri; Prasun Goswami; Md. Mainul Islam; Mohammad Shakhawat Hossen; Sayed Majhab Hasan Amiri; Naznin Akter
>
> **备注:** 22 Pages, 5 Tables
>
> **摘要:** Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model pruning, quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.
>
---
#### [new 004] The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated "Sacred" Text?
- **分类: cs.CY; cs.AI**

- **简介: 该论文探讨人工智能生成的佛教“经文”是否具有意义与价值，属于自然语言处理与哲学交叉任务。论文分析了AI生成文本的思想深度与文学性，反思技术对人类意义建构的冲击，并探讨佛教哲学对此的适应性。**

- **链接: [http://arxiv.org/pdf/2507.20525v1](http://arxiv.org/pdf/2507.20525v1)**

> **作者:** Murray Shanahan; Tara Das; Robert Thurman
>
> **摘要:** This paper presents a case study in the use of a large language model to generate a fictional Buddhist "sutr"', and offers a detailed analysis of the resulting text from a philosophical and literary point of view. The conceptual subtlety, rich imagery, and density of allusion found in the text make it hard to causally dismiss on account of its mechanistic origin. This raises questions about how we, as a society, should come to terms with the potentially unsettling possibility of a technology that encroaches on human meaning-making. We suggest that Buddhist philosophy, by its very nature, is well placed to adapt.
>
---
#### [new 005] A Survey of Virtual Reality in Japan
- **分类: cs.CY**

- **简介: 该论文属于调研任务，旨在介绍日本虚拟现实研究现状。作者通过参与NSF项目，访问日本多个实验室并参加相关会议，对当地VR研究进行调查，并简要比较了中国和韩国的VR与图形学研究情况。**

- **链接: [http://arxiv.org/pdf/2507.19499v1](http://arxiv.org/pdf/2507.19499v1)**

> **作者:** Benjamin Watson
>
> **摘要:** The NSF Summer Institute in Japan program sends about 60 graduate students of all disciplines to Japan each summer. For two months, students participate in research at host labs, visit conferences and other labs of interest, and receive Japanese language and cultural instruction. Full financial support is provided by the American and Japanese governments. During the summer of 1993, the author participated in this program and took the opportunity to visit the Japanese virtual reality research community. He attended two virtual reality conferences and toured more than a dozen labs. After the program, he made short visits to VR and graphics labs in PR China and South Korea. This paper gives a detailed account of these experiences.
>
---
#### [new 006] Can You Share Your Story? Modeling Clients' Metacognition and Openness for LLM Therapist Evaluation
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于自然语言处理与心理辅导交叉任务，旨在解决当前对大语言模型（LLM）心理咨询师评估不足的问题。现有方法依赖内部状态明确的客户模拟器，难以评估LLM挖掘未表达观点的能力。论文提出MindVoyager框架，构建动态适应的真实客户模拟器，并设计新评估指标，衡量LLM对客户信念和思维的理解与探索能力。**

- **链接: [http://arxiv.org/pdf/2507.19643v1](http://arxiv.org/pdf/2507.19643v1)**

> **作者:** Minju Kim; Dongje Yoo; Yeonjun Hwang; Minseok Kang; Namyoung Kim; Minju Gwak; Beong-woo Kwak; Hyungjoo Chae; Harim Kim; Yunjoong Lee; Min Hee Kim; Dayi Jung; Kyong-Mee Chung; Jinyoung Yeo
>
> **备注:** Published at ACL 2025 Findings
>
> **摘要:** Understanding clients' thoughts and beliefs is fundamental in counseling, yet current evaluations of LLM therapists often fail to assess this ability. Existing evaluation methods rely on client simulators that clearly disclose internal states to the therapist, making it difficult to determine whether an LLM therapist can uncover unexpressed perspectives. To address this limitation, we introduce MindVoyager, a novel evaluation framework featuring a controllable and realistic client simulator which dynamically adapts itself based on the ongoing counseling session, offering a more realistic and challenging evaluation environment. We further introduce evaluation metrics that assess the exploration ability of LLM therapists by measuring their thorough understanding of client's beliefs and thoughts.
>
---
#### [new 007] EyeAI: AI-Assisted Ocular Disease Detection for Equitable Healthcare Access
- **分类: cs.CY**

- **简介: 该论文属于医疗图像分类任务，旨在解决眼科疾病诊断资源不均问题。作者开发了EyeAI系统，基于卷积神经网络对视网膜图像进行二分类，判断是否患有45种眼疾之一，准确率80%，F1分数0.8876。该系统可通过网页应用实现低成本远程诊断，助力医疗公平。**

- **链接: [http://arxiv.org/pdf/2507.20346v1](http://arxiv.org/pdf/2507.20346v1)**

> **作者:** Shiv Garg; Ginny Berkemeier
>
> **摘要:** Ocular disease affects billions of individuals unevenly worldwide. It continues to increase in prevalence with trends of growing populations of diabetic people, increasing life expectancies, decreasing ophthalmologist availability, and rising costs of care. We present EyeAI, a system designed to provide artificial intelligence-assisted detection of ocular diseases, thereby enhancing global health. EyeAI utilizes a convolutional neural network model trained on 1,920 retinal fundus images to automatically diagnose the presence of ocular disease based on a retinal fundus image input through a publicly accessible web-based application. EyeAI performs a binary classification to determine the presence of any of 45 distinct ocular diseases, including diabetic retinopathy, media haze, and optic disc cupping, with an accuracy of 80%, an AUROC of 0.698, and an F1-score of 0.8876. EyeAI addresses barriers to traditional ophthalmologic care by facilitating low-cost, remote, and real-time diagnoses, particularly for equitable access to care in underserved areas and for supporting physicians through a secondary diagnostic opinion. Results demonstrate the potential of EyeAI as a scalable, efficient, and accessible diagnostic tool. Future work will focus on expanding the training dataset to enhance the accuracy of the model further and improve its diagnostic capabilities.
>
---
#### [new 008] Programmable Virtual Humans Toward Human Physiologically-Based Drug Discovery
- **分类: cs.CY; cs.AI; cs.CE; cs.LG**

- **简介: 该论文提出“可编程虚拟人类”概念，旨在通过多尺度建模与AI技术，模拟药物在人体内的作用，解决药物研发中早期发现与后期开发脱节、预测药物人体效应困难的问题，属于药物发现任务，工作聚焦于构建虚拟人体模型以实现体外药物虚拟实验。**

- **链接: [http://arxiv.org/pdf/2507.19568v1](http://arxiv.org/pdf/2507.19568v1)**

> **作者:** You Wu; Philip E. Bourne; Lei Xie
>
> **备注:** Under Review
>
> **摘要:** Artificial intelligence (AI) has sparked immense interest in drug discovery, but most current approaches only digitize existing high-throughput experiments. They remain constrained by conventional pipelines. As a result, they do not address the fundamental challenges of predicting drug effects in humans. Similarly, biomedical digital twins, largely grounded in real-world data and mechanistic models, are tailored for late-phase drug development and lack the resolution to model molecular interactions or their systemic consequences, limiting their impact in early-stage discovery. This disconnect between early discovery and late development is one of the main drivers of high failure rates in drug discovery. The true promise of AI lies not in augmenting current experiments but in enabling virtual experiments that are impossible in the real world: testing novel compounds directly in silico in the human body. Recent advances in AI, high-throughput perturbation assays, and single-cell and spatial omics across species now make it possible to construct programmable virtual humans: dynamic, multiscale models that simulate drug actions from molecular to phenotypic levels. By bridging the translational gap, programmable virtual humans offer a transformative path to optimize therapeutic efficacy and safety earlier than ever before. This perspective introduces the concept of programmable virtual humans, explores their roles in a new paradigm of drug discovery centered on human physiology, and outlines key opportunities, challenges, and roadmaps for their realization.
>
---
#### [new 009] Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content
- **分类: cs.CY; cs.AI; cs.CV**

- **简介: 该论文属于多模态内容安全任务，旨在解决针对LGBTQ群体的仇恨模因检测中的鲁棒性问题。作者构建了一个包含文本攻击与图像干扰的测试基准，在PrideMM数据集上评估现有模型，并提出轻量级文本去噪适配器（TDA）提升模型鲁棒性。实验表明，加入TDA的MemeBLIP2表现最优，揭示了文本依赖性及架构、预训练数据对鲁棒性的影响。**

- **链接: [http://arxiv.org/pdf/2507.19551v1](http://arxiv.org/pdf/2507.19551v1)**

> **作者:** Ran Tong; Songtao Wei; Jiaqi Liu; Lanruo Wang
>
> **备注:** 9 pages, 1 figure
>
> **摘要:** Hateful memes aimed at LGBTQ\,+ communities often evade detection by tweaking either the caption, the image, or both. We build the first robustness benchmark for this setting, pairing four realistic caption attacks with three canonical image corruptions and testing all combinations on the PrideMM dataset. Two state-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and we introduce a lightweight \textbf{Text Denoising Adapter (TDA)} to enhance the latter's resilience. Across the grid, MemeCLIP degrades more gently, while MemeBLIP2 is particularly sensitive to the caption edits that disrupt its language processing. However, the addition of the TDA not only remedies this weakness but makes MemeBLIP2 the most robust model overall. Ablations reveal that all systems lean heavily on text, but architectural choices and pre-training data significantly impact robustness. Our benchmark exposes where current multimodal safety models crack and demonstrates that targeted, lightweight modules like the TDA offer a powerful path towards stronger defences.
>
---
#### [new 010] FlashGuard: Novel Method in Evaluating Differential Characteristics of Visual Stimuli for Deterring Seizure Triggers in Photosensitive Epilepsy
- **分类: cs.CY; cs.HC; cs.SI**

- **简介: 该论文属于辅助技术任务，旨在解决光敏性癫痫患者因数字媒体视觉刺激引发癫痫的问题。提出了FlashGuard方法，通过实时分析CIELAB颜色空间中的颜色变化率，减少屏幕闪光风险。工作包括检测颜色差异和缓解刺激，为制定更全面的WCAG指南提供依据。**

- **链接: [http://arxiv.org/pdf/2507.19692v1](http://arxiv.org/pdf/2507.19692v1)**

> **作者:** Ishan Pendyala
>
> **摘要:** In the virtual realm, individuals with photosensitive epilepsy (PSE) encounter challenges when using devices, resulting in exposure to unpredictable seizure-causing visual stimuli. The current norm for preventing epileptic flashes in media is to detect asynchronously when a flash will occur in a video, then notifying the user. However, there is a lack of a real-time and computationally efficient solution for dealing with this issue. To address this issue and enhance accessibility for photosensitive viewers, FlashGuard, a novel approach, was devised to assess the rate of change of colors in frames across the user's screen and appropriately mitigate stimuli, based on perceptually aligned color space analysis in the CIELAB color space. The detection system is built on analyzing differences in color, and the mitigation system works by reducing luminance and smoothing color transitions. This study provides novel insight into how intrinsic color properties contribute to perceptual differences in flashing for PSE individuals, calling for the adoption of broadened WCAG guidelines to better account for risk. These insights and implementations pave the way for stronger protections for individuals with PSE from dangerous triggers in digital media, both in policy and in software.
>
---
#### [new 011] Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective
- **分类: cs.CY; cs.AI; cs.CL; cs.HC; econ.GN; q-fin.EC**

- **简介: 该论文研究AI与人类建议如何影响自私行为的惩罚。通过行为经济学与心理学方法，实验发现行为和建议内容影响惩罚程度，而建议来源（AI或人类）不影响。重点在于揭示AI伦理问题中的责任归因与惩罚机制。**

- **链接: [http://arxiv.org/pdf/2507.19487v1](http://arxiv.org/pdf/2507.19487v1)**

> **作者:** Margarita Leib; Nils Köbis; Ivan Soraperra
>
> **摘要:** People increasingly rely on AI-advice when making decisions. At times, such advice can promote selfish behavior. When individuals abide by selfishness-promoting AI advice, how are they perceived and punished? To study this question, we build on theories from social psychology and combine machine-behavior and behavioral economic approaches. In a pre-registered, financially-incentivized experiment, evaluators could punish real decision-makers who (i) received AI, human, or no advice. The advice (ii) encouraged selfish or prosocial behavior, and decision-makers (iii) behaved selfishly or, in a control condition, behaved prosocially. Evaluators further assigned responsibility to decision-makers and their advisors. Results revealed that (i) prosocial behavior was punished very little, whereas selfish behavior was punished much more. Focusing on selfish behavior, (ii) compared to receiving no advice, selfish behavior was penalized more harshly after prosocial advice and more leniently after selfish advice. Lastly, (iii) whereas selfish decision-makers were seen as more responsible when they followed AI compared to human advice, punishment between the two advice sources did not vary. Overall, behavior and advice content shape punishment, whereas the advice source does not.
>
---
#### [new 012] AI-Driven Media & Synthetic Knowledge: Rethinking Society in Generative Futures
- **分类: cs.CY; K4.2, K4.3, A0; K.4.2; K.4.3; A.0**

- **简介: 该论文探讨生成式AI对社会的影响，属于社会伦理与教育任务，旨在应对AI合成媒体带来的信任、身份与责任问题。研究通过两部分课程，梳理概念并提出应对策略，为教育与政策提供参考。**

- **链接: [http://arxiv.org/pdf/2507.19877v1](http://arxiv.org/pdf/2507.19877v1)**

> **作者:** Katalin Feher
>
> **备注:** 5 pages, summary of a granted experimental PhD seminar
>
> **摘要:** Generative AI is not just a technological leap -- it is a societal stress test, reshaping trust, identity, equity, and authorship. This exploratory PhD seminar examined emerging academic trends in AI-driven synthetic media and worlds, emphasizing ethical risks and societal implications. In Part One, students explored core concepts such as generative AI, fake media, and synthetic knowledge production. In Part Two, they critically engaged with these challenges, producing actionable insights. The two-part format enabled deep reflection on power, responsibility, and education in AI-augmented communication. Outcomes offer practical guidance for educators, researchers, and institutions committed to fostering more responsible, human-centered AI use in media and society.
>
---
#### [new 013] Differentiating hype from practical applications of large language models in medicine -- a primer for healthcare professionals
- **分类: cs.CY; cs.AI**

- **简介: 该论文旨在帮助医疗从业者区分大型语言模型（LLMs）在医学中的炒作与实际应用。任务是探讨LLMs在临床培训、实践与研究中的潜力与风险，解决如何在提升效率的同时避免信息泄露与错误应用的问题。工作包括分析LLMs在医疗领域的适用场景与局限性，强调需谨慎评估其使用方式。**

- **链接: [http://arxiv.org/pdf/2507.19567v1](http://arxiv.org/pdf/2507.19567v1)**

> **作者:** Elisha D. O. Roberson
>
> **备注:** 7 pages main document text, 2 figures. A basic primer on the potential and dangers of AI generally and LLMs specifically in the medical care system. Targeted to *non-expert* healthcare workers without experience in AI or LLMs
>
> **摘要:** The medical ecosystem consists of the training of new clinicians and researchers, the practice of clinical medicine, and areas of adjacent research. There are many aspects of these domains that could benefit from the application of task automation and programmatic assistance. Machine learning and artificial intelligence techniques, including large language models (LLMs), have been promised to deliver on healthcare innovation, improving care speed and accuracy, and reducing the burden on staff for manual interventions. However, LLMs have no understanding of objective truth that is based in reality. They also represent real risks to the disclosure of protected information when used by clinicians and researchers. The use of AI in medicine in general, and the deployment of LLMs in particular, therefore requires careful consideration and thoughtful application to reap the benefits of these technologies while avoiding the dangers in each context.
>
---
#### [new 014] Origin-Destination Extraction from Large-Scale Route Search Records for Tourism Trend Analysis
- **分类: cs.CY**

- **简介: 该论文属于数据分析与旅游趋势预测任务，旨在通过分析380亿条高速公路路线搜索记录，提取起点-终点（OD）信息，构建三维OD地图，揭示旅游热点与搜索趋势的关系，为交通管理和旅游规划提供支持。**

- **链接: [http://arxiv.org/pdf/2507.19544v1](http://arxiv.org/pdf/2507.19544v1)**

> **作者:** Hangli Ge; Dizhi Huang; Xiaojie Yang; Lifeng Lin; Kazuma Hatano; Takeshi Kawasaki; Noboru Koshizuka
>
> **摘要:** This paper presents a novel method for transforming large-scale historical expressway route search records into a three-dimensional (3D) Origin-Destination (OD) map, enabling data compression, efficient spatiotemporal sampling and statistical analysis. The study analyzed over 380 million expressway route search logs to investigate online search behavior related to tourist destinations. Several expressway interchanges (ICs) near popular attractions, such as those associated with spring flower viewing, autumn foliage and winter skiing, are examined and visualized. The results reveal strong correlations between search volume trends and the duration of peak tourism seasons. This approach leverages cyberspace behavioral data as a leading indicator of physical movement, providing a proactive tool for traffic management and tourism planning.
>
---
#### [new 015] PEMUTA: Pedagogically-Enriched Multi-Granular Undergraduate Thesis Assessment
- **分类: cs.CY; cs.AI**

- **简介: 该论文提出PEMUTA框架，用于多粒度本科论文评估。任务是解决现有大模型评估单一、忽略教学细节的问题。工作包括构建基于教育理论的六维评估体系（SLOWPR）和使用提示技术提升评估准确性。**

- **链接: [http://arxiv.org/pdf/2507.19556v1](http://arxiv.org/pdf/2507.19556v1)**

> **作者:** Jialu Zhang; Qingyang Sun; Qianyi Wang; Weiyi Zhang; Zunjie Xiao; Xiaoqing Zhang; Jianfeng Ren; Jiang Liu
>
> **摘要:** The undergraduate thesis (UGTE) plays an indispensable role in assessing a student's cumulative academic development throughout their college years. Although large language models (LLMs) have advanced education intelligence, they typically focus on holistic assessment with only one single evaluation score, but ignore the intricate nuances across multifaceted criteria, limiting their ability to reflect structural criteria, pedagogical objectives, and diverse academic competencies. Meanwhile, pedagogical theories have long informed manual UGTE evaluation through multi-dimensional assessment of cognitive development, disciplinary thinking, and academic performance, yet remain underutilized in automated settings. Motivated by the research gap, we pioneer PEMUTA, a pedagogically-enriched framework that effectively activates domain-specific knowledge from LLMs for multi-granular UGTE assessment. Guided by Vygotsky's theory and Bloom's Taxonomy, PEMUTA incorporates a hierarchical prompting scheme that evaluates UGTEs across six fine-grained dimensions: Structure, Logic, Originality, Writing, Proficiency, and Rigor (SLOWPR), followed by holistic synthesis. Two in-context learning techniques, \ie, few-shot prompting and role-play prompting, are also incorporated to further enhance alignment with expert judgments without fine-tuning. We curate a dataset of authentic UGTEs with expert-provided SLOWPR-aligned annotations to support multi-granular UGTE assessment. Extensive experiments demonstrate that PEMUTA achieves strong alignment with expert evaluations, and exhibits strong potential for fine-grained, pedagogically-informed UGTE evaluations.
>
---
#### [new 016] Towards Sustainability Model Cards
- **分类: cs.CY; cs.AI; cs.LG**

- **简介: 论文提出了一种面向可持续性的质量模型，旨在解决机器学习模型在训练和使用中能耗过高的问题。通过结合绿色AI理念与现有模型报告机制，作者设计了一种新的领域特定语言，用于精确描述模型的可持续性特征，支持自动化分析与模型比较。**

- **链接: [http://arxiv.org/pdf/2507.19559v1](http://arxiv.org/pdf/2507.19559v1)**

> **作者:** Gwendal Jouneaux; Jordi Cabot
>
> **摘要:** The growth of machine learning (ML) models and associated datasets triggers a consequent dramatic increase in energy costs for the use and training of these models. In the current context of environmental awareness and global sustainability concerns involving ICT, Green AI is becoming an important research topic. Initiatives like the AI Energy Score Ratings are a good example. Nevertheless, these benchmarking attempts are still to be integrated with existing work on Quality Models and Service-Level Agreements common in other, more mature, ICT subfields. This limits the (automatic) analysis of this model energy descriptions and their use in (semi)automatic model comparison, selection, and certification processes. We aim to leverage the concept of quality models and merge it with existing ML model reporting initiatives and Green/Frugal AI proposals to formalize a Sustainable Quality Model for AI/ML models. As a first step, we propose a new Domain-Specific Language to precisely define the sustainability aspects of an ML model (including the energy costs for its different tasks). This information can then be exported as an extended version of the well-known Model Cards initiative while, at the same time, being formal enough to be input of any other model description automatic process.
>
---
#### [new 017] RestoreAI -- Pattern-based Risk Estimation Of Remaining Explosives
- **分类: cs.LG; cs.CY; stat.AP; stat.ML**

- **简介: 该论文属于人工智能在地雷清除领域的应用任务，旨在解决如何通过地雷分布模式预测残留地雷风险以提高清除效率的问题。作者提出了RestoreAI系统，包含线性、曲线和贝叶斯三种模式分析方法，利用模式信息进行风险预测。实验表明，该系统显著提升了清除效率和速度。**

- **链接: [http://arxiv.org/pdf/2507.19873v1](http://arxiv.org/pdf/2507.19873v1)**

> **作者:** Björn Kischelewski; Benjamin Guedj; David Wahl
>
> **摘要:** Landmine removal is a slow, resource-intensive process affecting over 60 countries. While AI has been proposed to enhance explosive ordnance (EO) detection, existing methods primarily focus on object recognition, with limited attention to prediction of landmine risk based on spatial pattern information. This work aims to answer the following research question: How can AI be used to predict landmine risk from landmine patterns to improve clearance time efficiency? To that effect, we introduce RestoreAI, an AI system for pattern-based risk estimation of remaining explosives. RestoreAI is the first AI system that leverages landmine patterns for risk prediction, improving the accuracy of estimating the residual risk of missing EO prior to land release. We particularly focus on the implementation of three instances of RestoreAI, respectively, linear, curved and Bayesian pattern deminers. First, the linear pattern deminer uses linear landmine patterns from a principal component analysis (PCA) for the landmine risk prediction. Second, the curved pattern deminer uses curved landmine patterns from principal curves. Finally, the Bayesian pattern deminer incorporates prior expert knowledge by using a Bayesian pattern risk prediction. Evaluated on real-world landmine data, RestoreAI significantly boosts clearance efficiency. The top-performing pattern-based deminers achieved a 14.37 percentage point increase in the average share of cleared landmines per timestep and required 24.45% less time than the best baseline deminer to locate all landmines. Interestingly, linear and curved pattern deminers showed no significant performance difference, suggesting that more efficient linear patterns are a viable option for risk prediction.
>
---
#### [new 018] Latent Representations of Intracardiac Electrograms for Atrial Fibrillation Driver Detection
- **分类: cs.LG; cs.CY**

- **简介: 该论文属于医学信号处理与心律失常分析任务，旨在解决房颤驱动源检测难题。研究采用卷积自编码器对房颤期间记录的心内电图进行无监督特征提取，提取的潜在表示可用于自动化检测房颤驱动区域，实现较高准确率，并可集成于临床电生理标测系统，辅助消融手术定位。**

- **链接: [http://arxiv.org/pdf/2507.19547v1](http://arxiv.org/pdf/2507.19547v1)**

> **作者:** Pablo Peiro-Corbacho; Long Lin; Pablo Ávila; Alejandro Carta-Bergaz; Ángel Arenal; Carlos Sevilla-Salcedo; Gonzalo R. Ríos-Muñoz
>
> **摘要:** Atrial Fibrillation (AF) is the most prevalent sustained arrhythmia, yet current ablation therapies, including pulmonary vein isolation, are frequently ineffective in persistent AF due to the involvement of non-pulmonary vein drivers. This study proposes a deep learning framework using convolutional autoencoders for unsupervised feature extraction from unipolar and bipolar intracavitary electrograms (EGMs) recorded during AF in ablation studies. These latent representations of atrial electrical activity enable the characterization and automation of EGM analysis, facilitating the detection of AF drivers. The database consisted of 11,404 acquisitions recorded from 291 patients, containing 228,080 unipolar EGMs and 171,060 bipolar EGMs. The autoencoders successfully learned latent representations with low reconstruction loss, preserving the morphological features. The extracted embeddings allowed downstream classifiers to detect rotational and focal activity with moderate performance (AUC 0.73-0.76) and achieved high discriminative performance in identifying atrial EGM entanglement (AUC 0.93). The proposed method can operate in real-time and enables integration into clinical electroanatomical mapping systems to assist in identifying arrhythmogenic regions during ablation procedures. This work highlights the potential of unsupervised learning to uncover physiologically meaningful features from intracardiac signals.
>
---
#### [new 019] A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting
- **分类: cs.LG; cs.CY**

- **简介: 该论文属于时间序列预测任务，旨在解决流感A疫情预测问题。论文比较了传统模型（ARIMA、ETS）与六种深度学习模型（如LSTM、Transformer等）在流感预测中的性能，发现深度学习模型，尤其是Transformer表现更优，可提升传染病预测准确性。**

- **链接: [http://arxiv.org/pdf/2507.19515v1](http://arxiv.org/pdf/2507.19515v1)**

> **作者:** Edmund F. Agyemang; Hansapani Rodrigo; Vincent Agbenyeavu
>
> **摘要:** Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year, though this estimate is an improvement from years past due to improvements in sanitation, healthcare practices, and vaccination programs. In this study, we perform a comparative analysis of traditional and deep learning models to predict Influenza A outbreaks. Using historical data from January 2009 to December 2023, we compared the performance of traditional ARIMA and Exponential Smoothing(ETS) models with six distinct deep learning architectures: Simple RNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear superiority of all the deep learning models, especially the state-of-the-art Transformer with respective average testing MSE and MAE of 0.0433 \pm 0.0020 and 0.1126 \pm 0.0016 for capturing the temporal complexities associated with Influenza A data, outperforming well known traditional baseline ARIMA and ETS models. These findings of this study provide evidence that state-of-the-art deep learning architectures can enhance predictive modeling for infectious diseases and indicate a more general trend toward using deep learning methods to enhance public health forecasting and intervention planning strategies. Future work should focus on how these models can be incorporated into real-time forecasting and preparedness systems at an epidemic level, and integrated into existing surveillance systems.
>
---
#### [new 020] Exploring the Alignment of Perceived and Measured Sleep Quality with Working Memory using Consumer Wearables
- **分类: cs.HC; cs.CY**

- **简介: 该论文研究主观睡眠质量评估与可穿戴设备客观数据（如REM睡眠、心率）之间的关联，分析其对工作记忆的影响。任务是探索睡眠感知与记忆表现的关系，解决设备数据是否能提升对个体睡眠理解的问题。工作包括收集29人多周数据并建模分析预测因素。**

- **链接: [http://arxiv.org/pdf/2507.19491v1](http://arxiv.org/pdf/2507.19491v1)**

> **作者:** Peter Neigel; David Antony Selby; Shota Arai; Benjamin Tag; Niels van Berkel; Sebastian Vollmer; Andrew Vargo; Koichi Kise
>
> **备注:** 18 pages, 6 figures, 7 tables
>
> **摘要:** Wearable devices offer detailed sleep-tracking data. However, whether this information enhances our understanding of sleep or simply quantifies already-known patterns remains unclear. This work explores the relationship between subjective sleep self-assessments and sensor data from an Oura ring over 4--8 weeks in-the-wild. 29 participants rated their sleep quality daily compared to the previous night and completed a working memory task. Our findings reveal that differences in REM sleep, nocturnal heart rate, N-Back scores, and bedtimes highly predict sleep self-assessment in significance and effect size. For N-Back performance, REM sleep duration, prior night's REM sleep, and sleep self-assessment are the strongest predictors. We demonstrate that self-report sensitivity towards sleep markers differs among participants. We identify three groups, highlighting that sleep trackers provide more information gain for some users than others. Additionally, we make all experiment data publicly available.
>
---
#### [new 021] FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models
- **分类: cs.CL; cs.AI; cs.CY; cs.SI; I.2**

- **简介: 该论文参与EXIST 2025挑战赛，旨在识别和分类社交媒体中的性别歧视内容。论文提出三种模型：SCBM、SCBMT和XLM-RoBERTa，用于解决三个子任务：性别歧视识别、意图识别和分类。重点在于使用可解释的瓶颈概念（如形容词）结合大语言模型和Transformer，提升模型解释性与性能，并探索元数据的辅助作用。**

- **链接: [http://arxiv.org/pdf/2507.20924v1](http://arxiv.org/pdf/2507.20924v1)**

> **作者:** Roberto Labadie-Tamayo; Adrian Jaques Böck; Djordje Slijepčević; Xihui Chen; Andreas Babic; Matthias Zeppelzauer
>
> **备注:** 12 pages
>
> **摘要:** Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.
>
---
#### [new 022] Technological Requirements for Videoconferencing Judicial Hearings: Enhancing the Credibility and Reliability of Remote Testimonies
- **分类: cs.HC; cs.CY**

- **简介: 该论文属于司法技术任务，旨在解决远程视频听证可信度与可靠性不足的问题。通过法官实践经验，分析现有平台的缺陷，提出增加眼动追踪、环境验证、应用程序屏蔽等功能，提升远程作证的安全性与监控效果，以实现远程与现场听证的等效可信度。**

- **链接: [http://arxiv.org/pdf/2507.19496v1](http://arxiv.org/pdf/2507.19496v1)**

> **作者:** Jorge Alberto Araujo
>
> **摘要:** This paper analyzes the technological requirements necessary to enhance the credibility and reliability of judicial hearings conducted via videoconference, from the internal perspective of the judiciary. Drawing on the practical experience of a judge who conducts daily hearings, this study identifies limitations in current platforms for verifying the authenticity of testimonies and proposes tailored functionalities for the judicial context. Recognizing that remote hearings represent a convenience for the parties without replacing the option of in-person attendance, the article suggests implementing features such as eye tracking, environment verification, and blocking of parallel applications, in addition to improvements in transmission quality. The study concludes that developing specific modules for witnesses - focusing on security and monitoring - can significantly contribute to equalizing the credibility between remote and in-person hearings, thus expanding access to justice without compromising procedural reliability.
>
---
#### [new 023] Unlimited Editions: Documenting Human Style in AI Art Generation
- **分类: cs.HC; cs.AI; cs.CY; cs.IR**

- **简介: 该论文探讨AI艺术生成中人类风格的记录问题。任务是分析当前AI艺术生成忽视艺术风格背后创造性挣扎的问题。工作是通过历史案例指出艺术风格不仅是视觉形式，更是创作过程的体现，提出HCI应关注风格来源与演变的自动记录，而非仅追求视觉效果复制。**

- **链接: [http://arxiv.org/pdf/2507.19497v1](http://arxiv.org/pdf/2507.19497v1)**

> **作者:** Alex Leitch; Celia Chen
>
> **备注:** alt.CHI 2025
>
> **摘要:** As AI art generation becomes increasingly sophisticated, HCI research has focused primarily on questions of detection, authenticity, and automation. This paper argues that such approaches fundamentally misunderstand how artistic value emerges from the concerns that drive human image production. Through examination of historical precedents, we demonstrate that artistic style is not only visual appearance but the resolution of creative struggle, as artists wrestle with influence and technical constraints to develop unique ways of seeing. Current AI systems flatten these human choices into reproducible patterns without preserving their provenance. We propose that HCI's role lies not only in perfecting visual output, but in developing means to document the origins and evolution of artistic style as it appears within generated visual traces. This reframing suggests new technical directions for HCI research in generative AI, focused on automatic documentation of stylistic lineage and creative choice rather than simple reproduction of aesthetic effects.
>
---
#### [new 024] Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition
- **分类: cs.AI; cs.CL; cs.CY**

- **简介: 该论文属于安全评估任务，旨在解决AI代理在现实环境中部署时的安全漏洞问题。通过举办大规模红队竞赛，收集大量提示注入攻击，构建了ART基准测试，评估19个最先进模型的安全性，发现多数代理在少量查询内即出现策略违规，表明现有AI代理存在普遍且严重的安全漏洞。**

- **链接: [http://arxiv.org/pdf/2507.20526v1](http://arxiv.org/pdf/2507.20526v1)**

> **作者:** Andy Zou; Maxwell Lin; Eliot Jones; Micha Nowak; Mateusz Dziemian; Nick Winter; Alexander Grattan; Valent Nathanael; Ayla Croft; Xander Davies; Jai Patel; Robert Kirk; Nate Burnikell; Yarin Gal; Dan Hendrycks; J. Zico Kolter; Matt Fredrikson
>
> **摘要:** Recent advances have enabled LLM-powered AI agents to autonomously execute complex tasks by combining language model reasoning with tools, memory, and web access. But can these systems be trusted to follow deployment policies in realistic environments, especially under attack? To investigate, we ran the largest public red-teaming competition to date, targeting 22 frontier AI agents across 44 realistic deployment scenarios. Participants submitted 1.8 million prompt-injection attacks, with over 60,000 successfully eliciting policy violations such as unauthorized data access, illicit financial actions, and regulatory noncompliance. We use these results to build the Agent Red Teaming (ART) benchmark - a curated set of high-impact attacks - and evaluate it across 19 state-of-the-art models. Nearly all agents exhibit policy violations for most behaviors within 10-100 queries, with high attack transferability across models and tasks. Importantly, we find limited correlation between agent robustness and model size, capability, or inference-time compute, suggesting that additional defenses are needed against adversarial misuse. Our findings highlight critical and persistent vulnerabilities in today's AI agents. By releasing the ART benchmark and accompanying evaluation framework, we aim to support more rigorous security assessment and drive progress toward safer agent deployment.
>
---
#### [new 025] Studying Disinformation Narratives on Social Media with LLMs and Semantic Similarity
- **分类: cs.SI; cs.CY; cs.ET**

- **简介: 该论文属于信息检测任务，旨在解决社交媒体上虚假信息识别问题。论文开发了两个基于语义相似度的工具：一是追踪工具，用于衡量推文与目标叙述的相似性并绘制成时间线；二是合成工具，用于聚类高相似度推文并提取主导叙述。两者集成于推文叙述分析仪表板，并通过案例研究验证其有效性。**

- **链接: [http://arxiv.org/pdf/2507.20066v1](http://arxiv.org/pdf/2507.20066v1)**

> **作者:** Chaytan Inman
>
> **备注:** 45 pages, 8 figures, 13 tables
>
> **摘要:** This thesis develops a continuous scale measurement of similarity to disinformation narratives that can serve to detect disinformation and capture the nuanced, partial truths that are characteristic of it. To do so, two tools are developed and their methodologies are documented. The tracing tool takes tweets and a target narrative, rates the similarities of each to the target narrative, and graphs it as a timeline. The second narrative synthesis tool clusters tweets above a similarity threshold and generates the dominant narratives within each cluster. These tools are combined into a Tweet Narrative Analysis Dashboard. The tracing tool is validated on the GLUE STS-B benchmark, and then the two tools are used to analyze two case studies for further empirical validation. The first case study uses the target narrative "The 2020 election was stolen" and analyzes a dataset of Donald Trump's tweets during 2020. The second case study uses the target narrative, "Transgender people are harmful to society" and analyzes tens of thousands of tweets from the media outlets The New York Times, The Guardian, The Gateway Pundit, and Fox News. Together, the empirical findings from these case studies demonstrate semantic similarity for nuanced disinformation detection, tracing, and characterization. The tools developed in this thesis are hosted and can be accessed through the permission of the author. Please explain your use case in your request. The HTML friendly version of this paper is at https://chaytanc.github.io/projects/disinfo-research (Inman, 2025).
>
---
#### [new 026] Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文属于多模态推理任务，旨在解决视觉语言模型（VLM）在社会情境中推理能力不足的问题。作者提出了Cognitive Chain-of-Thought（CoCoT）提示策略，通过感知、情境和规范三个阶段提升模型的社会意识与可解释性，实验证明其在多个任务上优于传统方法。**

- **链接: [http://arxiv.org/pdf/2507.20409v1](http://arxiv.org/pdf/2507.20409v1)**

> **作者:** Eunkyu Park; Wesley Hanwen Deng; Gunhee Kim; Motahhare Eslami; Maarten Sap
>
> **备注:** Under review; 17 pages
>
> **摘要:** Chain-of-Thought (CoT) prompting helps models think step by step. But what happens when they must see, understand, and judge-all at once? In visual tasks grounded in social context, where bridging perception with norm-grounded judgments is essential, flat CoT often breaks down. We introduce Cognitive Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments show that, across multiple multimodal benchmarks (including intent disambiguation, commonsense reasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\% on average). Our findings demonstrate that cognitively grounded reasoning stages enhance interpretability and social awareness in VLMs, paving the way for safer and more reliable multimodal systems.
>
---
#### [new 027] Customize Multi-modal RAI Guardrails with Precedent-based predictions
- **分类: cs.LG; cs.CL; cs.CY**

- **简介: 该论文属于多模态内容审核任务，旨在根据用户自定义策略过滤有害图像内容。现有方法难以适应多样化的策略或需大量样本。论文提出基于先例（precedent）的预测方法，通过收集高质量先例并设计优化机制，提升模型灵活性与泛化能力，有效应对新策略和少量样本场景。**

- **链接: [http://arxiv.org/pdf/2507.20503v1](http://arxiv.org/pdf/2507.20503v1)**

> **作者:** Cheng-Fu Yang; Thanh Tran; Christos Christodoulopoulos; Weitong Ruan; Rahul Gupta; Kai-Wei Chang
>
> **备注:** Accepted to COLM 2025
>
> **摘要:** A multi-modal guardrail must effectively filter image content based on user-defined policies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit material, or spread misinformation. Deploying such guardrails in real-world applications, however, poses significant challenges. Users often require varied and highly customizable policies and typically cannot provide abundant examples for each custom policy. Consequently, an ideal guardrail should be scalable to the multiple policies and adaptable to evolving user standards with minimal retraining. Existing fine-tuning methods typically condition predictions on pre-defined policies, restricting their generalizability to new policies or necessitating extensive retraining to adapt. Conversely, training-free methods struggle with limited context lengths, making it difficult to incorporate all the policies comprehensively. To overcome these limitations, we propose to condition model's judgment on "precedents", which are the reasoning processes of prior data points similar to the given input. By leveraging precedents instead of fixed policies, our approach greatly enhances the flexibility and adaptability of the guardrail. In this paper, we introduce a critique-revise mechanism for collecting high-quality precedents and two strategies that utilize precedents for robust prediction. Experimental results demonstrate that our approach outperforms previous methods across both few-shot and full-dataset scenarios and exhibits superior generalization to novel policies.
>
---
#### [new 028] Street network sub-patterns and travel mode
- **分类: physics.soc-ph; cs.CY; cs.LG**

- **简介: 该论文属于城市形态与交通行为关联研究任务，旨在揭示不同城市形态对出行方式的影响。通过聚类分析九个美国都市区的建成环境特征，识别出不同城市形态类型，并利用统计方法分析其与出行模式的关系，发现网状结构促进公共交通使用，有机形态则增加小汽车依赖，为城市规划提供实证依据。**

- **链接: [http://arxiv.org/pdf/2507.19648v1](http://arxiv.org/pdf/2507.19648v1)**

> **作者:** Juan Fernando Riascos Goyes; Michael Lowry; Nicolás Guarín Zapata; Juan Pablo Ospina
>
> **摘要:** Urban morphology has long been recognized as a factor shaping human mobility, yet comparative and formal classifications of urban form across metropolitan areas remain limited. Building on theoretical principles of urban structure and advances in unsupervised learning, we systematically classified the built environment of nine U.S. metropolitan areas using structural indicators such as density, connectivity, and spatial configuration. The resulting morphological types were linked to mobility patterns through descriptive statistics, marginal effects estimation, and post hoc statistical testing. Here we show that distinct urban forms are systematically associated with different mobility behaviors, such as reticular morphologies being linked to significantly higher public transport use (marginal effect = 0.49) and reduced car dependence (-0.41), while organic forms are associated with increased car usage (0.44), and substantial declines in public transport (-0.47) and active mobility (-0.30). These effects are statistically robust (p < 1e-19), highlighting that the spatial configuration of urban areas plays a fundamental role in shaping transportation choices. Our findings extend previous work by offering a reproducible framework for classifying urban form and demonstrate the added value of morphological analysis in comparative urban research. These results suggest that urban form should be treated as a key variable in mobility planning and provide empirical support for incorporating spatial typologies into sustainable urban policy design.
>
---
#### [new 029] Learning the Value Systems of Societies from Preferences
- **分类: cs.AI; cs.CY; cs.LG**

- **简介: 论文研究如何从人类偏好中自动学习社会价值体系，属价值对齐任务。旨在解决AI系统难以手动构建个体与社会价值模型的问题。作者提出基于启发式深度聚类的方法，从群体行为中学习共享价值基础及多样化的价值体系，并通过旅行决策数据验证方法有效性。**

- **链接: [http://arxiv.org/pdf/2507.20728v1](http://arxiv.org/pdf/2507.20728v1)**

> **作者:** Andrés Holgado-Sánchez; Holger Billhardt; Sascha Ossowski; Sara Degli-Esposti
>
> **备注:** Full version of publication under the same accepted at ECAI 2025 conference (Submission 6755). 8 pages + 2 supplementary material
>
> **摘要:** Aligning AI systems with human values and the value-based preferences of various stakeholders (their value systems) is key in ethical AI. In value-aware AI systems, decision-making draws upon explicit computational representations of individual values (groundings) and their aggregation into value systems. As these are notoriously difficult to elicit and calibrate manually, value learning approaches aim to automatically derive computational models of an agent's values and value system from demonstrations of human behaviour. Nonetheless, social science and humanities literature suggest that it is more adequate to conceive the value system of a society as a set of value systems of different groups, rather than as the simple aggregation of individual value systems. Accordingly, here we formalize the problem of learning the value systems of societies and propose a method to address it based on heuristic deep clustering. The method learns socially shared value groundings and a set of diverse value systems representing a given society by observing qualitative value-based preferences from a sample of agents. We evaluate the proposal in a use case with real data about travelling decisions.
>
---
#### [new 030] Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning
- **分类: cs.LG; cs.AI; cs.CY**

- **简介: 论文属于人类移动预测任务，旨在解决灾害场景下移动模式变化导致现有模型失效的问题。工作提出DisasterMobLLM框架，结合大语言模型与跨城市知识迁移，提升灾害中人类移动预测准确性。**

- **链接: [http://arxiv.org/pdf/2507.19737v1](http://arxiv.org/pdf/2507.19737v1)**

> **作者:** Yinzhou Tang; Huandong Wang; Xiaochen Fan; Yong Li
>
> **摘要:** The vulnerability of cities to natural disasters has increased with urbanization and climate change, making it more important to predict human mobility in the disaster scenarios for downstream tasks including location-based early disaster warning and pre-allocating rescue resources, etc. However, existing human mobility prediction models are mainly designed for normal scenarios, and fail to adapt to disaster scenarios due to the shift of human mobility patterns under disaster. To address this issue, we introduce \textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios that can be integrated into existing deep mobility prediction methods by leveraging LLMs to model the mobility intention and transferring the common knowledge of how different disasters affect mobility intentions between cities. This framework utilizes a RAG-Enhanced Intention Predictor to forecast the next intention, refines it with an LLM-based Intention Refiner, and then maps the intention to an exact location using an Intention-Modulated Location Predictor. Extensive experiments illustrate that DisasterMobLLM can achieve a 32.8\% improvement in terms of Acc@1 and a 35.0\% improvement in terms of the F1-score of predicting immobility compared to the baselines. The code is available at https://github.com/tsinghua-fib-lab/DisasterMobLLM.
>
---
#### [new 031] Rural School Bus Routing and Scheduling
- **分类: math.OC; cs.CY**

- **简介: 该论文属于路径规划与调度任务，旨在解决农村校车行程过长导致的学生成绩下降、家长自驾增多及交通拥堵问题。作者提出一种结合路网特征的聚类-路径规划启发式方法，有效缩短乘车时间，提高校车利用率，缓解学校周边拥堵。**

- **链接: [http://arxiv.org/pdf/2507.19538v1](http://arxiv.org/pdf/2507.19538v1)**

> **作者:** Prabhat Hegde; Vikrant Vaze
>
> **摘要:** Long school bus rides adversely affect student performance and well-being. Rural school bus rides are particularly long, incentivizing parents to drive their children to school rather than to opt for the school bus. This in turn exacerbates the traffic congestion around schools, further compounding the problem of long bus rides, creating a vicious cycle. It also results in underutilized school buses and higher bus operating costs per rider. To address these challenges, this paper focuses on the design of rural school bus routes and schedules, a particularly challenging problem due to its unique operational complexities, including mixed loading and irregular road networks. We formalize a rural school bus routing and scheduling model that tackles these complexities while minimizing the total bus ride time of students. We develop an original road network-aware cluster-then-route heuristic that leverages our problem formulation to produce high-quality solutions. For real-world case studies, our approach outperforms status quo solutions by reducing the bus ride times of students by 37-39 %. Our solutions also make the school bus more attractive, helping address both the underutilization of school buses and the prevalence of private commutes. Our routing and scheduling approach can improve school bus use by 17-19 % and reduce car trips that induce congestion near schools by 12-17 %. Many rural school districts share the operational characteristics modeled in this study, including long bus rides, high operational expenditures, mixed loading, and a high proportion of car-based school commutes, suggesting the broad applicability of our approach. Ultimately, by reducing student travel times, increasing school bus utilization, and alleviating congestion near schools, our approach enables rural school district planners to address transportation-related barriers to student performance and well-being.
>
---
#### [new 032] E-polis: Gamifying Sociological Surveys through Serious Games -- A Data Analysis Approach Applied to Multiple-Choice Question Responses Datasets
- **分类: cs.HC; cs.CY; K.6.3; C.5.2; C.5.3; C.5.5; C.5.m; C.5.0**

- **简介: 论文提出了一款名为E-polis的严肃游戏，旨在通过游戏化方式收集和分析青少年政治观点数据。该研究属于社会学调查与游戏设计交叉任务，解决传统问卷形式枯燥、参与度低的问题。论文主要介绍了游戏的中间件架构及其数据层设计，并基于玩家选择进行数据分析，以探索游戏化在社会科学研究中的应用潜力。**

- **链接: [http://arxiv.org/pdf/2507.19488v1](http://arxiv.org/pdf/2507.19488v1)**

> **作者:** Alexandros Gazis; Eleftheria Katsiri
>
> **备注:** The article is under review by MDPI, Electronics journal. 36 pages, 20 figures, 67 references
>
> **摘要:** E-polis is a serious digital game designed to gamify sociological surveys studying young people's political opinions. In this platform game, players navigate a digital world, encountering quests posing sociological questions. Players' answers shape the city-game world, altering building structures based on their choices. E-polis is a serious game, not a government simulation, aiming to understand players' behaviors and opinions thus we do not train the players but rather understand them and help them visualize their choices in shaping a city's future. Also, it is noticed that no correct or incorrect answers apply. Moreover, our game utilizes a novel middleware architecture for development, diverging from typical asset prefab scene and script segregation. This article presents the data layer of our game's middleware, specifically focusing on data analysis based on respondents' gameplay answers. E-polis represents an innovative approach to gamifying sociological research, providing a unique platform for gathering and analyzing data on political opinions among youth and contributing to the broader field of serious games.
>
---
## 更新

#### [replaced 001] ShaRP: Explaining Rankings and Preferences with Shapley Values
- **分类: cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2401.16744v5](http://arxiv.org/pdf/2401.16744v5)**

> **作者:** Venetia Pliatsika; Joao Fonseca; Kateryna Akhynko; Ivan Shevchenko; Julia Stoyanovich
>
> **备注:** Accepted in VLDB
>
> **摘要:** Algorithmic decisions in critical domains such as hiring, college admissions, and lending are often based on rankings. Given the impact of these decisions on individuals, organizations, and population groups, it is essential to understand them - to help individuals improve their ranking position, design better ranking procedures, and ensure legal compliance. In this paper, we argue that explainability methods for classification and regression, such as SHAP, are insufficient for ranking tasks, and present ShaRP - Shapley Values for Rankings and Preferences - a framework that explains the contributions of features to various aspects of a ranked outcome. ShaRP computes feature contributions for various ranking-specific profit functions, such as rank and top-k, and also includes a novel Shapley value-based method for explaining pairwise preference outcomes. We provide a flexible implementation of ShaRP, capable of efficiently and comprehensively explaining ranked and pairwise outcomes over tabular data, in score-based ranking and learning-to-rank tasks. Finally, we develop a comprehensive evaluation methodology for ranking explainability methods, showing through qualitative, quantitative, and usability studies that our rank-aware QoIs offer complementary insights, scale effectively, and help users interpret ranked outcomes in practice.
>
---
#### [replaced 002] PRISM: A Personalized, Rapid, and Immersive Skill Mastery framework for personalizing experiential learning through Generative AI
- **分类: cs.CY; cs.AI; cs.CR; cs.HC**

- **链接: [http://arxiv.org/pdf/2411.14433v2](http://arxiv.org/pdf/2411.14433v2)**

> **作者:** Yu-Zheng Lin; Karan Patel; Ahmed Hussain J Alhamadah; Bono Po-Jen Shih; Matthew William Redondo; David Rafael Vidal Corona; Banafsheh Saber Latibari; Jesus Pacheco; Soheil Salehi; Pratik Satam
>
> **备注:** 24 pages, 7 figures
>
> **摘要:** The rise of generative AI (gen-AI) is transforming industries, particularly in education and workforce training. This chapter introduces PRISM (Personalized, Rapid, and Immersive Skill Mastery), a scalable framework leveraging gen-AI and Digital Twins (DTs) to deliver adaptive, experiential learning. PRISM integrates sentiment analysis and Retrieval-Augmented Generation (RAG) to monitor learner comprehension and dynamically adjust content to meet course objectives. We further present the Multi-Fidelity Digital Twin for Education (MFDT-E) framework, aligning DT fidelity levels with Bloom's Taxonomy and the Kirkpatrick evaluation model to support undergraduate, master's, and doctoral training. Experimental validation shows that GPT-4 achieves 91 percent F1 in zero-shot sentiment analysis of teacher-student dialogues, while GPT-3.5 performs robustly in informal language contexts. Additionally, the system's effectiveness and scalability for immersive Industry 4.0 training are demonstrated through four VR modules: Home Scene, Factory Floor Tour, Capping Station DT, and PPE Inspection Training. These results highlight the potential of integrating generative AI with digital twins to enable personalized, efficient, and scalable education.
>
---
#### [replaced 003] AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants
- **分类: cs.HC; cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2504.13887v2](http://arxiv.org/pdf/2504.13887v2)**

> **作者:** Isabel Villanueva; Tara Bobinac; Binwei Yao; Junjie Hu; Kaiping Chen
>
> **摘要:** Despite increasing AI chatbot deployment in public discourse, empirical evidence on their capacity to foster intercultural empathy remains limited. Through a randomized experiment, we assessed how different AI deliberation approaches--cross-cultural deliberation (presenting other-culture perspectives), own-culture deliberation (representing participants' own culture), and non-deliberative control--affect intercultural empathy across American and Latin American participants. Cross-cultural deliberation increased intercultural empathy among American participants through positive emotional engagement, but produced no such effects for Latin American participants, who perceived AI responses as culturally inauthentic despite explicit prompting to represent their cultural perspectives. Our analysis of participant-driven feedback, where users directly flagged and explained culturally inappropriate AI responses, revealed systematic gaps in AI's representation of Latin American contexts that persist despite sophisticated prompt engineering. These findings demonstrate that current approaches to AI cultural alignment--including linguistic adaptation and explicit cultural prompting--cannot fully address deeper representational asymmetries in AI systems. Our work advances both deliberation theory and AI alignment research by revealing how the same AI system can simultaneously promote intercultural understanding for one cultural group while failing for another, with critical implications for designing equitable AI systems for cross-cultural democratic discourse.
>
---
#### [replaced 004] Navigating the Risks of Using Large Language Models for Text Annotation in Social Science Research
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2503.22040v2](http://arxiv.org/pdf/2503.22040v2)**

> **作者:** Hao Lin; Yongjun Zhang
>
> **摘要:** Large language models (LLMs) have the potential to revolutionize computational social science, particularly in automated textual analysis. In this paper, we conduct a systematic evaluation of the promises and risks associated with using LLMs for text classification tasks, using social movement studies as an example. We propose a framework for social scientists to incorporate LLMs into text annotation, either as the primary coding decision-maker or as a coding assistant. This framework offers researchers tools to develop the potential best-performing prompt, and to systematically examine and report the validity and reliability of LLMs as a methodological tool. Additionally, we evaluate and discuss its epistemic risks associated with validity, reliability, replicability, and transparency. We conclude with several practical guidelines for using LLMs in text annotation tasks and offer recommendations for more effectively communicating epistemic risks in research.
>
---
#### [replaced 005] Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions
- **分类: cs.LG; cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.02087v2](http://arxiv.org/pdf/2507.02087v2)**

> **作者:** Eitan Anzenberg; Arunava Samajpati; Sivasankaran Chandrasekar; Varun Kacholia
>
> **备注:** 10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025
>
> **摘要:** The use of large language models (LLMs) in hiring promises to streamline candidate screening, but it also raises serious concerns regarding accuracy and algorithmic bias where sufficient safeguards are not in place. In this work, we benchmark several state-of-the-art foundational LLMs - including models from OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our proprietary domain-specific hiring model (Match Score) for job candidate matching. We evaluate each model's predictive accuracy (ROC AUC, Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis across declared gender, race, and intersectional subgroups). Our experiments on a dataset of roughly 10,000 real-world recent candidate-job pairs show that Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs 0.77) and achieves significantly more equitable outcomes across demographic groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957 (near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the intersectionals, respectively). We discuss why pretraining biases may cause LLMs with insufficient safeguards to propagate societal biases in hiring scenarios, whereas a bespoke supervised model can more effectively mitigate these biases. Our findings highlight the importance of domain-specific modeling and bias auditing when deploying AI in high-stakes domains such as hiring, and caution against relying on off-the-shelf LLMs for such tasks without extensive fairness safeguards. Furthermore, we show with empirical evidence that there shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a well-designed algorithm can achieve both accuracy in hiring and fairness in outcomes.
>
---
#### [replaced 006] Risks & Benefits of LLMs & GenAI for Platform Integrity, Healthcare Diagnostics, Financial Trust and Compliance, Cybersecurity, Privacy & AI Safety: A Comprehensive Survey, Roadmap & Implementation Blueprint
- **分类: cs.CR; cs.CY**

- **链接: [http://arxiv.org/pdf/2506.12088v2](http://arxiv.org/pdf/2506.12088v2)**

> **作者:** Kiarash Ahi
>
> **摘要:** Large Language Models (LLMs) and generative AI (GenAI) systems, such as ChatGPT, Claude, Gemini, LLaMA, and Copilot (by OpenAI, Anthropic, Google, Meta, and Microsoft, respectively), are reshaping digital platforms and app ecosystems while introducing critical challenges in cybersecurity, privacy, and platform integrity. Our analysis reveals alarming trends: LLM-assisted malware is projected to rise from 2% (2021) to 50% (2025); AI-generated Google reviews grew nearly tenfold (1.2% in 2021 to 12.21% in 2023, expected to reach 30% by 2025); AI scam reports surged 456%; misinformation sites increased over 1500%; and deepfake attacks are projected to rise over 900% in 2025. In finance, LLM-driven threats like synthetic identity fraud and AI-generated scams are accelerating. Platforms such as JPMorgan Chase, Stripe, and Plaid deploy LLMs for fraud detection, regulation parsing, and KYC/AML automation, reducing fraud loss by up to 21% and accelerating onboarding by 40-60%. LLM-facilitated code development has driven mobile app submissions from 1.8 million (2020) to 3.0 million (2024), projected to reach 3.6 million (2025). To address AI threats, platforms like Google Play, Apple App Store, GitHub Copilot, TikTok, Facebook, and Amazon deploy LLM-based defenses, highlighting their dual nature as both threat sources and mitigation tools. In clinical diagnostics, LLMs raise concerns about accuracy, bias, and safety, necessitating strong governance. Drawing on 445 references, this paper surveys LLM/GenAI and proposes a strategic roadmap and operational blueprint integrating policy auditing (such as CCPA and GDPR compliance), fraud detection, and demonstrates an advanced LLM-DA stack with modular components, multi-LLM routing, agentic memory, and governance layers. We provide actionable insights, best practices, and real-world case studies for scalable trust and responsible innovation.
>
---
#### [replaced 007] Epitome: Pioneering an Experimental Platform for AI-Social Science Integration
- **分类: cs.CY; cs.AI; cs.HC**

- **链接: [http://arxiv.org/pdf/2507.01061v2](http://arxiv.org/pdf/2507.01061v2)**

> **作者:** Jingjing Qu; Kejia Hu; Jun Zhu; Wenhao Li; Teng Wang; Zhiyun Chen; Yulei Ye; Chaochao Lu; Aimin Zhou; Xiangfeng Wang; James Evans
>
> **备注:** 18 pages, 5figures
>
> **摘要:** The integration of Large Language Models (LLMs) into social science experiments represents a transformative approach to understanding human-AI interactions and their societal impacts. We introduce Epitome, the world's first open experimental platform dedicated to the deep integration of artificial intelligence and social science. Rooted in theoretical foundations from management, communication studies, sociology, psychology, and ethics, Epitome focuses on the interactive impacts of AI on individuals, organizations, and society during its real-world deployment. It constructs a theoretical support system through cross-disciplinary experiments. The platform offers a one-stop comprehensive experimental solution spanning "foundation models-complex application development-user feedback" through seven core modules, while embedding the classical "control-comparison-comparative causal logic" of social science experiments into multilevel human-computer interaction environments, including dialogues, group chats, and multi-agent virtual scenarios. With its canvas-style, user-friendly interface, Epitome enables researchers to easily design and run complex experimental scenarios, facilitating systematic investigations into the social impacts of AI and exploration of integrated solutions.To demonstrate its capabilities, we replicated three seminal social science experiments involving LLMs, showcasing Epitome's potential to streamline complex experimental designs and produce robust results, suitable for publishing in the top selective journals. Our findings highlight the platform's utility in enhancing the efficiency and quality of human-AI interactions, providing valuable insights into the societal implications of AI technologies. Epitome thus offers a powerful tool for advancing interdisciplinary research at the intersection of AI and social science, with potential applications in policy-making, ...
>
---
#### [replaced 008] Verifying International Agreements on AI: Six Layers of Verification for Rules on Large-Scale AI Development and Deployment
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2507.15916v2](http://arxiv.org/pdf/2507.15916v2)**

> **作者:** Mauricio Baker; Gabriel Kulp; Oliver Marks; Miles Brundage; Lennart Heim
>
> **备注:** 80 pages, summary included
>
> **摘要:** The risks of frontier AI may require international cooperation, which in turn may require verification: checking that all parties follow agreed-on rules. For instance, states might need to verify that powerful AI models are widely deployed only after their risks to international security have been evaluated and deemed manageable. However, research on AI verification could benefit from greater clarity and detail. To address this, this report provides an in-depth overview of AI verification, intended for both policy professionals and technical researchers. We present novel conceptual frameworks, detailed implementation options, and key R&D challenges. These draw on existing literature, expert interviews, and original analysis, all within the scope of confidentially overseeing AI development and deployment that uses thousands of high-end AI chips. We find that states could eventually verify compliance by using six largely independent verification approaches with substantial redundancy: (1) built-in security features in AI chips; (2-3) separate monitoring devices attached to AI chips; and (4-6) personnel-based mechanisms, such as whistleblower programs. While promising, these approaches require guardrails to protect against abuse and power concentration, and many of these technologies have yet to be built or stress-tested. To enable states to confidently verify compliance with rules on large-scale AI development and deployment, the R&D challenges we list need significant progress.
>
---
