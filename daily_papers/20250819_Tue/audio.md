# 音频 cs.SD;  eess.SP

- **最新发布 19 篇**

- **更新 13 篇**

## 最新发布

#### [new 001] Data-driven RF Tomography via Cross-modal Sensing and Continual Learning
- **分类: eess.SP; cs.CV**

- **简介: 论文提出DRIFT框架，用于地下根茎类目标的RF断层成像。针对动态环境中RF信号变化导致成像不准的问题，结合跨模态感知与持续学习，提升模型鲁棒性与准确性。**

- **链接: [http://arxiv.org/pdf/2508.11654v1](http://arxiv.org/pdf/2508.11654v1)**

> **作者:** Yang Zhao; Tao Wang; Said Elhadi
>
> **备注:** 6 pages, 4 figures, to be published in IEEE AVSS Conference
>
> **摘要:** Data-driven radio frequency (RF) tomography has demonstrated significant potential for underground target detection, due to the penetrative nature of RF signals through soil. However, it is still challenging to achieve accurate and robust performance in dynamic environments. In this work, we propose a data-driven radio frequency tomography (DRIFT) framework with the following key components to reconstruct cross section images of underground root tubers, even with significant changes in RF signals. First, we design a cross-modal sensing system with RF and visual sensors, and propose to train an RF tomography deep neural network (DNN) model following the cross-modal learning approach. Then we propose to apply continual learning to automatically update the DNN model, once environment changes are detected in a dynamic environment. Experimental results show that our approach achieves an average equivalent diameter error of 2.29 cm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and dataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.
>
---
#### [new 002] FoleySpace: Vision-Aligned Binaural Spatial Audio Generation
- **分类: cs.SD**

- **简介: 论文提出FoleySpace框架，解决视频到单声道音频生成缺乏空间感的问题。通过视觉信息估计声源位置并生成3D轨迹，结合预训练V2A模型输出，用扩散模型生成空间一致的双耳音频，提升沉浸感。**

- **链接: [http://arxiv.org/pdf/2508.12918v1](http://arxiv.org/pdf/2508.12918v1)**

> **作者:** Lei Zhao; Rujin Chen; Chi Zhang; Xiao-Lei Zhang; Xuelong Li
>
> **摘要:** Recently, with the advancement of AIGC, deep learning-based video-to-audio (V2A) technology has garnered significant attention. However, existing research mostly focuses on mono audio generation that lacks spatial perception, while the exploration of binaural spatial audio generation technologies, which can provide a stronger sense of immersion, remains insufficient. To solve this problem, we propose FoleySpace, a framework for video-to-binaural audio generation that produces immersive and spatially consistent stereo sound guided by visual information. Specifically, we develop a sound source estimation method to determine the sound source 2D coordinates and depth in each video frame, and then employ a coordinate mapping mechanism to convert the 2D source positions into a 3D trajectory. This 3D trajectory, together with the monaural audio generated by a pre-trained V2A model, serves as a conditioning input for a diffusion model to generate spatially consistent binaural audio. To support the generation of dynamic sound fields, we constructed a training dataset based on recorded Head-Related Impulse Responses that includes various sound source movement scenarios. Experimental results demonstrate that the proposed method outperforms existing approaches in spatial perception consistency, effectively enhancing the immersive quality of the audio-visual experience.
>
---
#### [new 003] HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization
- **分类: cs.SD; cs.AI; eess.AS**

- **简介: 该论文针对语音基础模型在噪声环境下识别性能下降的问题，提出HuBERT-VIC模型，通过方差、不变性和协方差正则化调整噪声语音表征统计特性，提升模型鲁棒性与泛化能力。**

- **链接: [http://arxiv.org/pdf/2508.12292v1](http://arxiv.org/pdf/2508.12292v1)**

> **作者:** Hyebin Ahn; Kangwook Jang; Hoirin Kim
>
> **备注:** Accepted at Interspeech 2025
>
> **摘要:** Noise robustness in speech foundation models (SFMs) has been a critical challenge, as most models are primarily trained on clean data and experience performance degradation when the models are exposed to noisy speech. To address this issue, we propose HuBERT-VIC, a noise-robust SFM with variance, in-variance, and covariance regularization (VICReg) objectives. These objectives adjust the statistics of noisy speech representations, enabling the model to capture diverse acoustic characteristics and improving the generalization ability across different types of noise. When applied to HuBERT, our model shows relative performance improvements of 23.3% on LibriSpeech test-clean and 13.2% on test-other, compared to the baseline model pre-trained on noisy speech.
>
---
#### [new 004] Audio Flamingo Sound-CoT Technical Report: Improving Chain-of-Thought Reasoning in Sound Understanding
- **分类: cs.SD; cs.LG**

- **简介: 论文研究音频理解中的链式思维推理任务，旨在提升模型对声音的逻辑推理能力。提出AF-Reasoning-Eval基准和AF-CoT-Train训练数据集，通过微调Audio Flamingo模型显著改善了多个推理任务表现。**

- **链接: [http://arxiv.org/pdf/2508.11818v1](http://arxiv.org/pdf/2508.11818v1)**

> **作者:** Zhifeng Kong; Arushi Goel; Joao Felipe Santos; Sreyan Ghosh; Rafael Valle; Wei Ping; Bryan Catanzaro
>
> **摘要:** Chain-of-thought reasoning has demonstrated significant improvements in large language models and vision language models, yet its potential for audio language models remains largely unexplored. In this technical report, we take a preliminary step towards closing this gap. For better assessment of sound reasoning, we propose AF-Reasoning-Eval, a benchmark targeting common-sense reasoning and the ability to discriminate among closely related choices. To prepare training corpus for sound reasoning abilities, we propose automatic pipelines that transform existing audio question answering and classification data into explicit reasoning chains, yielding AF-CoT-Train with 1.24M samples. We study the effect of finetuning Audio Flamingo series on AF-CoT-Train and observe considerable improvements on several reasoning benchmarks, validating the effectiveness of chain-of-thought finetuning on advanced sound understanding.
>
---
#### [new 005] What Matters for Bioacoustic Encoding
- **分类: cs.SD; cs.AI; cs.IR; cs.LG**

- **简介: 该论文研究生物声学编码问题，旨在解决标注数据少导致的模型泛化差难题。通过大规模实验证明，自监督预训练结合混合生物声学与通用音频的监督微调能显著提升多种任务性能。**

- **链接: [http://arxiv.org/pdf/2508.11845v1](http://arxiv.org/pdf/2508.11845v1)**

> **作者:** Marius Miron; David Robinson; Milad Alizadeh; Ellen Gilsenan-McMahon; Gagan Narula; Olivier Pietquin; Matthieu Geist; Emmanuel Chemla; Maddie Cusimano; Felix Effenberger; Masato Hagiwara; Benjamin Hoffman; Sara Keen; Diane Kim; Jane Lawton; Jen-Yu Liu; Aza Raskin
>
> **摘要:** Bioacoustics, the study of sounds produced by living organisms, plays a vital role in conservation, biodiversity monitoring, and behavioral studies. Many tasks in this field, such as species, individual, and behavior classification and detection, are well-suited to machine learning. However, they often suffer from limited annotated data, highlighting the need for a general-purpose bioacoustic encoder capable of extracting useful representations for diverse downstream tasks. Such encoders have been proposed before, but are often limited in scope due to a focus on a narrow range of species (typically birds), and a reliance on a single model architecture or training paradigm. Moreover, they are usually evaluated on a small set of tasks and datasets. In this work, we present a large-scale empirical study that covers aspects of bioacoustics that are relevant to research but have previously been scarcely considered: training data diversity and scale, model architectures and training recipes, and the breadth of evaluation tasks and datasets. We obtain encoders that are state-of-the-art on the existing and proposed benchmarks. We also identify what matters for training these encoders, such that this work can be extended when more data are available or better architectures are proposed. Specifically, across 26 datasets with tasks including species classification, detection, individual ID, and vocal repertoire discovery, we find self-supervised pre-training followed by supervised post-training on a mixed bioacoustics + general-audio corpus yields the strongest in- and out-of-distribution performance. We show the importance of data diversity in both stages. To support ongoing research and application, we will release the model checkpoints.
>
---
#### [new 006] Optimizing Neural Architectures for Hindi Speech Separation and Enhancement in Noisy Environments
- **分类: cs.SD; cs.LG**

- **简介: 论文针对印度语境下的 Hindi 语音分离与增强任务，解决噪声环境下语音清晰度低的问题。通过改进 DEMUCS 模型并融合 U-Net 和 LSTM，利用大规模数据训练，并采用量化技术优化部署于边缘设备的性能。**

- **链接: [http://arxiv.org/pdf/2508.12009v1](http://arxiv.org/pdf/2508.12009v1)**

> **作者:** Arnav Ramamoorthy
>
> **备注:** ICAD 2025
>
> **摘要:** This paper addresses the challenges of Hindi speech separation and enhancement using advanced neural network architectures, with a focus on edge devices. We propose a refined approach leveraging the DEMUCS model to overcome limitations of traditional methods, achieving substantial improvements in speech clarity and intelligibility. The model is fine-tuned with U-Net and LSTM layers, trained on a dataset of 400,000 Hindi speech clips augmented with ESC-50 and MS-SNSD for diverse acoustic environments. Evaluation using PESQ and STOI metrics shows superior performance, particularly under extreme noise conditions. To ensure deployment on resource-constrained devices like TWS earbuds, we explore quantization techniques to reduce computational requirements. This research highlights the effectiveness of customized AI algorithms for speech processing in Indian contexts and suggests future directions for optimizing edge-based architectures.
>
---
#### [new 007] Towards Automatic Evaluation and High-Quality Pseudo-Parallel Dataset Construction for Audio Editing: A Human-in-the-Loop Method
- **分类: cs.SD**

- **简介: 论文聚焦音频编辑任务，解决缺乏高质量基准数据集和评估指标的问题。提出AuditScore主观评价数据集、AuditEval自动评分模型，并利用其构建高质量伪平行数据集，提升音频编辑质量评估与数据生成效果。**

- **链接: [http://arxiv.org/pdf/2508.11966v1](http://arxiv.org/pdf/2508.11966v1)**

> **作者:** Yuhang Jia; Hui Wang; Xin Nie; Yujie Guo; Lianru Gao; Yong Qin
>
> **摘要:** Audio editing aims to manipulate audio content based on textual descriptions, supporting tasks such as adding, removing, or replacing audio events. Despite recent progress, the lack of high-quality benchmark datasets and comprehensive evaluation metrics remains a major challenge for both assessing audio editing quality and improving the task itself. In this work, we propose a novel approach for audio editing task by incorporating expert knowledge into both the evaluation and dataset construction processes: 1) First, we establish AuditScore, the first comprehensive dataset for subjective evaluation of audio editing, consisting of over 6,300 edited samples generated from 7 representative audio editing frameworks and 23 system configurations. Each sample is annotated by professional raters on three key aspects of audio editing quality: overall Quality, Relevance to editing intent, and Faithfulness to original features. 2) Based on this dataset, we train AuditEval, the first model designed for automatic MOS-style scoring tailored to audio editing tasks. AuditEval addresses the critical lack of objective evaluation metrics and the prohibitive cost of subjective assessment in this field. 3) We further leverage AuditEval to evaluate and filter a large amount of synthetically mixed editing pairs, constructing a high-quality pseudo-parallel dataset by selecting the most plausible samples. Objective experiments validate the effectiveness of our expert-informed filtering strategy in yielding higher-quality data, while also revealing the limitations of relying solely on objective metrics. The dataset, codes and tools can be found at: https://github.com/NKU-HLT/AuditEval.
>
---
#### [new 008] MATPAC++: Enhanced Masked Latent Prediction for Self-Supervised Audio Representation Learning
- **分类: cs.SD; cs.AI**

- **简介: 该论文属于自监督音频表征学习任务，旨在解决音频中多声源导致的预测歧义问题。通过引入多选学习（MCL）增强掩码潜在预测机制，提出MATPAC++模型，在下游任务和AudioSet微调上均取得SOTA性能。**

- **链接: [http://arxiv.org/pdf/2508.12709v1](http://arxiv.org/pdf/2508.12709v1)**

> **作者:** Aurian Quelennec; Pierre Chouteau; Geoffroy Peeters; Slim Essid
>
> **备注:** Under review
>
> **摘要:** Masked latent prediction has emerged as a leading paradigm in self-supervised learning (SSL), especially for general audio and music representation learning. While recent methods have demonstrated strong performance, the role of the predictor module used at the output of such SSL systems remains mainly overlooked, despite being crucial for solving the pretext task at hand. In particular, this module should be able to deal with the ambiguity inherent in audio content, especially when it is composed of multiple sound sources. This work proposes a novel enhancement: integrating Multiple Choice Learning (MCL) to explicitly model prediction ambiguity and improve representation quality. We build on top of the recently proposed MATPAC system, improving its prediction and unsupervised classification pretext tasks with MCL. We extensively evaluate our method, MATPAC++, through both linear probing across multiple downstream tasks and fine-tuning on AudioSet, employing a unified protocol that enables rigorous and fair comparisons with state-of-the-art SSL approaches. Results show that our proposal achieves state-of-the-art when fine-tuned on AudioSet and overall state-of-the-art scores on downstream tasks. Additionally, we examine domain specialisation by training exclusively on music data, where our model achieves state-of-the-art performance with significantly improved efficiency.
>
---
#### [new 009] On the Extension of Differential Beamforming Theory to Arbitrary Planar Arrays of First-Order Elements
- **分类: eess.SP; eess.AS**

- **简介: 论文提出一种广义模匹配框架，用于实现任意平面一阶定向阵元的频不变差分波束成形。解决了传统方法忽略传感器方向性导致性能下降的问题，可合成任意阶数和指向的波束，适用于多种几何布局。**

- **链接: [http://arxiv.org/pdf/2508.12403v1](http://arxiv.org/pdf/2508.12403v1)**

> **作者:** Federico Miotello; Davide Albertini; Alberto Bernardini
>
> **摘要:** Small-size acoustic arrays exploit spatial diversity to achieve capabilities beyond those of single-element devices, with applications ranging from teleconferencing to immersive multimedia. A key requirement for broadband array processing is a frequency-invariant spatial response, which ensures consistent directivity across wide bandwidths and prevents spectral coloration. Differential beamforming offers an inherently frequency-invariant solution by leveraging pressure differences between closely spaced elements of small-size arrays. Traditional approaches, however, assume the array elements to be omnidirectional, whereas real transducers exhibit frequency-dependent directivity that can degrade performance if not properly modeled. To address this limitation, we propose a generalized modal matching framework for frequency-invariant differential beamforming, applicable to unconstrained planar arrays of first-order directional elements. By representing the desired beampattern as a truncated circular harmonic expansion and fitting it to the actual element responses, our method accommodates arbitrary planar geometries and element orientations. This approach enables the synthesis of beampatterns of any order and steering direction without imposing rigid layout requirements. Simulations confirm that accounting for sensor directivity at the design stage yields accurate and robust performance across varying frequencies, geometries, and noise conditions.
>
---
#### [new 010] Exploring Self-Supervised Audio Models for Generalized Anomalous Sound Detection
- **分类: cs.SD; eess.AS**

- **简介: 论文针对机器异常声音检测（ASD）任务，解决数据稀缺和跨设备泛化差的问题。提出基于自监督预训练模型、LoRA微调、机器感知分组适配器及动态聚类目标函数，显著提升检测性能。**

- **链接: [http://arxiv.org/pdf/2508.12230v1](http://arxiv.org/pdf/2508.12230v1)**

> **作者:** Bing Han; Anbai Jiang; Xinhu Zheng; Wei-Qiang Zhang; Jia Liu; Pingyi Fan; Yanmin Qian
>
> **备注:** Accepted by TASLP. 15 pages, 7 figures;
>
> **摘要:** Machine anomalous sound detection (ASD) is a valuable technique across various applications. However, its generalization performance is often limited due to challenges in data collection and the complexity of acoustic environments. Inspired by the success of large pre-trained models in numerous fields, this paper introduces a robust ASD model that leverages self-supervised pre-trained models trained on large-scale speech and audio datasets. Although there are inconsistencies between the pre-training datasets and the ASD task, our findings indicate that pre-training still provides substantial benefits for ASD. To mitigate overfitting and retain learned knowledge when fine-tuning with limited data, we explore Fully-Connected Low-Rank Adaptation (LoRA) as an alternative to full fine-tuning. Additionally, we propose a Machine-aware Group Adapter module, which enables the model to capture differences between various machines within a unified framework, thereby enhancing the generalization performance of ASD systems. To address the challenge of missing attribute labels, we design a novel objective function that dynamically clusters unattributed data using vector quantization and optimizes through a dual-level contrastive learning loss. The proposed methods are evaluated on all benchmark datasets, including the DCASE 2020-2024 five ASD challenges, and the experimental results show significant improvements of our new approach and demonstrate the effectiveness of our proposed strategies.
>
---
#### [new 011] Prediction of Spotify Chart Success Using Audio and Streaming Features
- **分类: cs.SD; eess.AS**

- **简介: 该论文属于分类任务，旨在预测歌曲在Spotify排行榜上的成功与否。作者基于音频特征和早期流媒体数据构建模型，通过对比多种算法发现树模型表现最优，即使仅用音频特征也能有效预测。**

- **链接: [http://arxiv.org/pdf/2508.11632v1](http://arxiv.org/pdf/2508.11632v1)**

> **作者:** Ian Jacob Cabansag; Paul Ntegeka
>
> **摘要:** Spotify's streaming charts offer a real-time lens into music popularity, driving discovery, playlists, and even revenue potential. Understanding what influences a song's rise in ranks on these charts-especially early on-can guide marketing efforts, investment decisions, and even artistic direction. In this project, we developed a classification pipeline to predict a song's chart success based on its musical characteristics and early engagement data. Using all 2024 U.S. Top 200 Spotify Daily Charts and the Spotify Web API, we built a dataset containing both metadata and audio features for 14,639 unique songs. The project was structured in two phases. First, we benchmarked four models: Logistic Regression, K Nearest Neighbors, Random Forest, and XGBoost-using a standard train-test split. In the second phase, we incorporated cross-validation, hyperparameter tuning, and detailed class-level evaluation to ensure robustness. Tree-based models consistently outperformed the rest, with Random Forest and XGBoost achieving macro F1-scores near 0.95 and accuracy around 97%. Even when stream count and rank history were excluded, models trained solely on audio attributes retained predictive power. These findings validate the potential of audio-based modeling in A&R scouting, playlist optimization, and hit forecasting-long before a track reaches critical mass.
>
---
#### [new 012] Cross-Modal Knowledge Distillation with Multi-Level Data Augmentation for Low-Resource Audio-Visual Sound Event Localization and Detection
- **分类: cs.SD; cs.MM**

- **简介: 论文提出跨模态知识蒸馏框架，结合多级数据增强，解决低资源音频-视觉声事件定位与检测问题，提升学生模型性能。**

- **链接: [http://arxiv.org/pdf/2508.12334v1](http://arxiv.org/pdf/2508.12334v1)**

> **作者:** Qing Wang; Ya Jiang; Hang Chen; Sabato Marco Siniscalchi; Jun Du; Jianqing Gao
>
> **备注:** 34 pages, 7 figures
>
> **摘要:** This work presents a cross-modal knowledge distillation (CMKD) framework combined with multi-level data augmentation for low-resource audio-visual (AV) sound event localization and detection (SELD). An audio-only SELD model acts as the teacher, transferring knowledge to an AV student model through both output responses and intermediate feature representations. To enhance learning, data augmentation is applied by mixing features randomly selected from multiple network layers and associated loss functions tailored to the SELD task. Extensive experiments on the DCASE 2023 and 2024 SELD datasets show that the proposed method significantly improves AV SELD performance, yielding relative gains of 22%~36% in the overall metric over the baseline. Notably, our approach achieves results comparable to or better than teacher models trained on much larger datasets, surpassing state-of-the-art methods on both DCASE 2023 and 2024 SELD tasks.
>
---
#### [new 013] Exploring the Feasibility of LLMs for Automated Music Emotion Annotation
- **分类: cs.SD**

- **简介: 该论文研究大语言模型（GPT-4o）在音乐情感标注中的可行性，旨在解决人工标注成本高、数据规模受限的问题。作者用GPT-4o对古典钢琴音乐数据集进行四象限情感标注，并与三位专家标注对比，评估其准确性、一致性及分布相似性，发现GPT虽略逊于人类但具可扩展潜力。**

- **链接: [http://arxiv.org/pdf/2508.12626v1](http://arxiv.org/pdf/2508.12626v1)**

> **作者:** Meng Yang; Jon McCormack; Maria Teresa Llano; Wanchao Su
>
> **备注:** Accepted to be published at ISMIR 2025
>
> **摘要:** Current approaches to music emotion annotation remain heavily reliant on manual labelling, a process that imposes significant resource and labour burdens, severely limiting the scale of available annotated data. This study examines the feasibility and reliability of employing a large language model (GPT-4o) for music emotion annotation. In this study, we annotated GiantMIDI-Piano, a classical MIDI piano music dataset, in a four-quadrant valence-arousal framework using GPT-4o, and compared against annotations provided by three human experts. We conducted extensive evaluations to assess the performance and reliability of GPT-generated music emotion annotations, including standard accuracy, weighted accuracy that accounts for inter-expert agreement, inter-annotator agreement metrics, and distributional similarity of the generated labels. While GPT's annotation performance fell short of human experts in overall accuracy and exhibited less nuance in categorizing specific emotional states, inter-rater reliability metrics indicate that GPT's variability remains within the range of natural disagreement among experts. These findings underscore both the limitations and potential of GPT-based annotation: despite its current shortcomings relative to human performance, its cost-effectiveness and efficiency render it a promising scalable alternative for music emotion annotation.
>
---
#### [new 014] CarelessWhisper: Turning Whisper into a Causal Streaming Model
- **分类: cs.CL; cs.LG; cs.SD; eess.AS**

- **简介: 论文将非因果的Whisper模型改造为低延迟流式ASR模型，解决其无法实时处理语音的问题。通过LoRA微调和弱对齐数据训练因果编码器，并设计优化推理机制，实现高效低延迟语音识别与词级时间戳提取。**

- **链接: [http://arxiv.org/pdf/2508.12301v1](http://arxiv.org/pdf/2508.12301v1)**

> **作者:** Tomer Krichli; Bhiksha Raj; Joseph Keshet
>
> **备注:** 17 pages, 7 Figures, This work has been submitted to the IEEE for possible publication
>
> **摘要:** Automatic Speech Recognition (ASR) has seen remarkable progress, with models like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA) performance in offline transcription. However, these models are not designed for streaming (online or real-time) transcription, due to limitations in their architecture and training methodology. We propose a method to turn the transformer encoder-decoder model into a low-latency streaming model that is careless about future context. We present an analysis explaining why it is not straightforward to convert an encoder-decoder transformer to a low-latency streaming model. Our proposed method modifies the existing (non-causal) encoder to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated inference mechanism that utilizes the fine-tune causal encoder and decoder to yield greedy and beam-search decoding, and is shown to be locally optimal. Experiments on low-latency chunk sizes (less than 300 msec) show that our fine-tuned model outperforms existing non-fine-tuned streaming approaches in most cases, while using a lower complexity. Additionally, we observe that our training process yields better alignment, enabling a simple method for extracting word-level timestamps. We release our training and inference code, along with the fine-tuned models, to support further research and development in streaming ASR.
>
---
#### [new 015] CEM-Net: Cross-Emotion Memory Network for Emotional Talking Face Generation
- **分类: cs.MM; cs.SD**

- **简介: 该论文属于情感驱动人脸动画生成任务，解决参考图像情绪与音频情绪冲突导致的表情不准确问题。提出CEM-Net，通过音频情绪增强模块和情绪桥接记忆模块，提升情感一致性与生成质量。**

- **链接: [http://arxiv.org/pdf/2508.12368v1](http://arxiv.org/pdf/2508.12368v1)**

> **作者:** Kangyi Wu; Pengna Li; Jingwen Fu; Yang Wu; Yuhan Liu; Sanping Zhou; Jinjun Wang
>
> **摘要:** Emotional talking face generation aims to animate a human face in given reference images and generate a talking video that matches the content and emotion of driving audio. However, existing methods neglect that reference images may have a strong emotion that conflicts with the audio emotion, leading to severe emotion inaccuracy and distorted generated results. To tackle the issue, we introduce a cross-emotion memory network(CEM-Net), designed to generate emotional talking faces aligned with the driving audio when reference images exhibit strong emotion. Specifically, an Audio Emotion Enhancement module(AEE) is first devised with the cross-reconstruction training strategy to enhance audio emotion, overcoming the disruption from reference image emotion. Secondly, since reference images cannot provide sufficient facial motion information of the speaker under audio emotion, an Emotion Bridging Memory module(EBM) is utilized to compensate for the lacked information. It brings in expression displacement from the reference image emotion to the audio emotion and stores it in the memory.Given a cross-emotion feature as a query, the matching displacement can be retrieved at inference time. Extensive experiments have demonstrated that our CEM-Net can synthesize expressive, natural and lip-synced talking face videos with better emotion accuracy.
>
---
#### [new 016] Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning
- **分类: cs.CL; cs.AI; cs.SD**

- **简介: 论文研究多模态大语言模型（MLLM）在自动口语评估中的应用，解决传统方法因模态限制导致的性能瓶颈。提出Speech-First Multimodal Training（SFMT）策略，通过课程学习提升语音特征建模，显著改善整体评估效果，尤其在发音等表达维度上取得突破。**

- **链接: [http://arxiv.org/pdf/2508.12591v1](http://arxiv.org/pdf/2508.12591v1)**

> **作者:** Yu-Hsuan Fang; Tien-Hong Lo; Yao-Ting Sung; Berlin Chen
>
> **备注:** Accepted at IEEE ASRU 2025
>
> **摘要:** Traditional Automated Speaking Assessment (ASA) systems exhibit inherent modality limitations: text-based approaches lack acoustic information while audio-based methods miss semantic context. Multimodal Large Language Models (MLLM) offer unprecedented opportunities for comprehensive ASA by simultaneously processing audio and text within unified frameworks. This paper presents a very first systematic study of MLLM for comprehensive ASA, demonstrating the superior performance of MLLM across the aspects of content and language use . However, assessment on the delivery aspect reveals unique challenges, which is deemed to require specialized training strategies. We thus propose Speech-First Multimodal Training (SFMT), leveraging a curriculum learning principle to establish more robust modeling foundations of speech before cross-modal synergetic fusion. A series of experiments on a benchmark dataset show MLLM-based systems can elevate the holistic assessment performance from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the evaluation of the delivery aspect, achieving an absolute accuracy improvement of 4% over conventional training approaches, which also paves a new avenue for ASA.
>
---
#### [new 017] On the Importance of Behavioral Nuances: Amplifying Non-Obvious Motor Noise Under True Empirical Considerations May Lead to Briefer Assays and Faster Classification Processes
- **分类: q-bio.QM; cs.CV; cs.LG; eess.SP; nlin.CD**

- **简介: 该论文属于情感计算任务，旨在通过分析面部微表情中的非明显运动噪声，提升短时数据（5秒）下的分类效率与个性化统计功效。工作包括开发新数据类型和几何动力学方法，捕捉面部微峰值差异，区分自闭症与神经典型个体的动态模式。**

- **链接: [http://arxiv.org/pdf/2508.12742v1](http://arxiv.org/pdf/2508.12742v1)**

> **作者:** Theodoros Bermperidis; Joe Vero; Elizabeth B Torres
>
> **备注:** This paper is under review in IEEE Transactions on Affective Computing
>
> **摘要:** There is a tradeoff between attaining statistical power with large, difficult to gather data sets, and producing highly scalable assays that register brief data samples. Often, as grand-averaging techniques a priori assume normally-distributed parameters and linear, stationary processes in biorhythmic, time series data, important information is lost, averaged out as gross data. We developed an affective computing platform that enables taking brief data samples while maintaining personalized statistical power. This is achieved by combining a new data type derived from the micropeaks present in time series data registered from brief (5-second-long) face videos with recent advances in AI-driven face-grid estimation methods. By adopting geometric and nonlinear dynamical systems approaches to analyze the kinematics, especially the speed data, the new methods capture all facial micropeaks. These include as well the nuances of different affective micro expressions. We offer new ways to differentiate dynamical and geometric patterns present in autistic individuals from those found more commonly in neurotypical development.
>
---
#### [new 018] Music and Artificial Intelligence: Artistic Trends
- **分类: cs.CY; cs.SD; eess.AS**

- **简介: 该论文属于艺术与AI交叉研究任务，旨在揭示AI在音乐创作中的应用趋势。通过分析337件音乐作品，分类整理AI用于作曲、歌词生成等场景，识别出其作为共创工具、艺术媒介及表演形式的新趋势与挑战。**

- **链接: [http://arxiv.org/pdf/2508.11694v1](http://arxiv.org/pdf/2508.11694v1)**

> **作者:** Jordi Pons; Zack Zukowski; Julian D. Parker; CJ Carr; Josiah Taylor; Zach Evans
>
> **摘要:** We study how musicians use artificial intelligence (AI) across formats like singles, albums, performances, installations, voices, ballets, operas, or soundtracks. We collect 337 music artworks and categorize them based on AI usage: AI composition, co-composition, sound design, lyrics generation, and translation. We find that AI is employed as a co-creative tool, as an artistic medium, and in live performances and installations. Innovative uses of AI include exploring uncanny aesthetics, multilingual and multigenre song releases, and new formats such as online installations. This research provides a comprehensive overview of current AI music practices, offering insights into emerging artistic trends and the challenges faced by AI musicians.
>
---
#### [new 019] MASSLOC: A Massive Sound Source Localization System based on Direction-of-Arrival Estimation
- **分类: eess.AS; eess.SP**

- **简介: 论文提出MASSLOC系统，用于多声源定位任务，解决传统RF方案硬件要求高、安装复杂问题。通过稀疏二维阵列和Zadoff-Chu序列实现高精度方向估计与多源识别，在混响环境中仍保持稳定性能。**

- **链接: [http://arxiv.org/pdf/2508.12024v1](http://arxiv.org/pdf/2508.12024v1)**

> **作者:** Georg K. J. Fischer; Thomas Schaechtle; Moritz Schabinger; Alexander Richter; Ivo Häring; Fabian Höflinger; Stefan J. Rupitsch
>
> **备注:** IEEE Transactions on Instrumentation and Measurement
>
> **摘要:** Acoustic indoor localization offers the potential for highly accurate position estimation while generally exhibiting low hardware requirements compared to Radio Frequency (RF)-based solutions. Furthermore, angular-based localization significantly reduces installation effort by minimizing the number of required fixed anchor nodes. In this contribution, we propose the so-called MASSLOC system, which leverages sparse two-dimensional array geometries to localize and identify a large number of concurrently active sources. Additionally, the use of complementary Zadoff-Chu sequences is introduced to enable efficient, beamforming-based source identification. These sequences provide a trade-off between favorable correlation properties and accurate, unsynchronized direction-of-arrival estimation by exhibiting a spectrally balanced waveform. The system is evaluated in both a controlled anechoic chamber and a highly reverberant lobby environment with a reverberation time of 1.6 s. In a laboratory setting, successful direction-of-arrival estimation and identification of up to 14 simultaneously emitting sources are demonstrated. Adopting a Perspective-n-Point (PnP) calibration approach, the system achieves a median three-dimensional localization error of 55.7 mm and a median angular error of 0.84 deg with dynamic source movement of up to 1.9 mps in the challenging reverberant environment. The multi-source capability is also demonstrated and evaluated in that environment with a total of three tags. These results indicate the scalability and robustness of the MASSLOC system, even under challenging acoustic conditions.
>
---
## 更新

#### [replaced 001] Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model
- **分类: eess.AS; cs.SD**

- **链接: [http://arxiv.org/pdf/2507.09372v3](http://arxiv.org/pdf/2507.09372v3)**

> **作者:** Philippe Gonzalez; Torsten Dau; Tobias May
>
> **备注:** Accepted to Clarity 2025 Workshop
>
> **摘要:** Deep learning-based hearing loss compensation (HLC) seeks to enhance speech intelligibility and quality for hearing impaired listeners using neural networks. One major challenge of HLC is the lack of a ground-truth target. Recent works have used neural networks to emulate non-differentiable auditory peripheral models in closed-loop frameworks, but this approach lacks flexibility. Alternatively, differentiable auditory models allow direct optimization, yet previous studies focused on individual listener profiles, or joint noise reduction (NR) and HLC without balancing each task. This work formulates NR and HLC as a multi-task learning problem, training a system to simultaneously predict denoised and compensated signals from noisy speech and audiograms using a differentiable auditory model. Results show the system achieves similar objective metric performance to systems trained for each task separately, while being able to adjust the balance between NR and HLC during inference.
>
---
#### [replaced 002] Comparative Evaluation of Acoustic Feature Extraction Tools for Clinical Speech Analysis
- **分类: cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2506.01129v2](http://arxiv.org/pdf/2506.01129v2)**

> **作者:** Anna Seo Gyeong Choi; Alexander Richardson; Ryan Partlan; Sunny Tang; Sunghye Cho
>
> **备注:** Accepted to Interspeech 2025
>
> **摘要:** This study compares three acoustic feature extraction toolkits (OpenSMILE, Praat, and Librosa) applied to clinical speech data from individuals with schizophrenia spectrum disorders (SSD) and healthy controls (HC). By standardizing extraction parameters across the toolkits, we analyzed speech samples from 77 SSD and 87 HC participants and found significant toolkit-dependent variations. While F0 percentiles showed high cross-toolkit correlation (r=0.962 to 0.999), measures like F0 standard deviation and formant values often had poor, even negative, agreement. Additionally, correlation patterns differed between SSD and HC groups. Classification analysis identified F0 mean, HNR, and MFCC1 (AUC greater than 0.70) as promising discriminators. These findings underscore reproducibility concerns and advocate for standardized protocols, multi-toolkit cross-validation, and transparent reporting.
>
---
#### [replaced 003] Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning
- **分类: cs.SD; cs.LG; eess.AS**

- **链接: [http://arxiv.org/pdf/2505.09304v2](http://arxiv.org/pdf/2505.09304v2)**

> **作者:** Luciano Sebastian Martinez-Rau; Quynh Nguyen Phuong Vu; Yuxuan Zhang; Bengt Oelmann; Sebastian Bader
>
> **备注:** Preprint submitted to the IEEE 11th World Forum on Internet of Things
>
> **摘要:** Keyword spotting (KWS) is a key component of smart devices, enabling efficient and intuitive audio interaction. However, standard KWS systems deployed on embedded devices often suffer performance degradation under real-world operating conditions. Resilient KWS systems address this issue by enabling dynamic adaptation, with applications such as adding or replacing keywords, adjusting to specific users, and improving noise robustness. However, deploying resilient, standalone KWS systems with low latency on resource-constrained devices remains challenging due to limited memory and computational resources. This study proposes a low computational approach for continuous noise adaptation of pretrained neural networks used for KWS classification, requiring only 1-shot learning and one epoch. The proposed method was assessed using two pretrained models and three real-world noise sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted models consistently outperformed the pretrained models across all scenarios, especially at SNR $\leq$ 18 dB, achieving accuracy improvements of 4.9% to 46.0%. These results highlight the efficacy of the proposed methodology while being lightweight enough for deployment on resource-constrained devices.
>
---
#### [replaced 004] S2Cap: A Benchmark and a Baseline for Singing Style Captioning
- **分类: cs.CL; cs.AI; cs.LG; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2409.09866v3](http://arxiv.org/pdf/2409.09866v3)**

> **作者:** Hyunjong Ok; Jaeho Lee
>
> **备注:** CIKM 2025 Resource Paper
>
> **摘要:** Singing voices contain much richer information than common voices, including varied vocal and acoustic properties. However, current open-source audio-text datasets for singing voices capture only a narrow range of attributes and lack acoustic features, leading to limited utility towards downstream tasks, such as style captioning. To fill this gap, we formally define the singing style captioning task and present S2Cap, a dataset of singing voices with detailed descriptions covering diverse vocal, acoustic, and demographic characteristics. Using this dataset, we develop an efficient and straightforward baseline algorithm for singing style captioning. The dataset is available at https://zenodo.org/records/15673764.
>
---
#### [replaced 005] Fast Algorithm for Moving Sound Source
- **分类: eess.AS; cs.SD**

- **链接: [http://arxiv.org/pdf/2508.03065v2](http://arxiv.org/pdf/2508.03065v2)**

> **作者:** Dong Yang
>
> **摘要:** Modern neural network-based speech processing systems usually need to have reverberation resistance, so the training of such systems requires a large amount of reverberation data. In the process of system training, it is now more inclined to use sampling static systems to simulate dynamic systems, or to supplement data through actually recorded data. However, this cannot fundamentally solve the problem of simulating motion data that conforms to physical laws. Aiming at the core issue of insufficient training data for speech enhancement models in moving scenarios, this paper proposes Yang's motion spatio-temporal sampling reconstruction theory to realize efficient simulation of motion continuous time-varying reverberation. This theory breaks through the limitations of the traditional static Image-Source Method (ISM) in time-varying systems. By decomposing the impulse response of the moving image source into two parts: linear time-invariant modulation and discrete time-varying fractional delay, a moving sound field model conforming to physical laws is established. Based on the band-limited characteristics of motion displacement, a hierarchical sampling strategy is proposed: high sampling rate is used for low-order images to retain details, and low sampling rate is used for high-order images to reduce computational complexity. A fast synthesis architecture is designed to realize real-time simulation. Experiments show that compared with the open-source models, the proposed theory can more accurately restore the amplitude and phase changes in moving scenarios, solving the industry problem of motion sound source data simulation, and providing high-quality dynamic training data for speech enhancement models.
>
---
#### [replaced 006] Is Smaller Always Faster? Tradeoffs in Compressing Self-Supervised Speech Transformers
- **分类: cs.CL; cs.LG; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2211.09949v4](http://arxiv.org/pdf/2211.09949v4)**

> **作者:** Tzu-Quan Lin; Tsung-Huan Yang; Chun-Yao Chang; Kuang-Ming Chen; Tzu-hsun Feng; Hung-yi Lee; Hao Tang
>
> **备注:** Accepted at ASRU 2025. Code is available at https://github.com/nervjack2/Speech-SSL-Compression
>
> **摘要:** Transformer-based self-supervised models have achieved remarkable success in speech processing, but their large size and high inference cost present significant challenges for real-world deployment. While numerous compression techniques have been proposed, inconsistent evaluation metrics make it difficult to compare their practical effectiveness. In this work, we conduct a comprehensive study of four common compression methods, including weight pruning, head pruning, low-rank approximation, and knowledge distillation on self-supervised speech Transformers. We evaluate each method under three key metrics: parameter count, multiply-accumulate operations, and real-time factor. Results show that each method offers distinct advantages. In addition, we contextualize recent compression techniques, comparing DistilHuBERT, FitHuBERT, LightHuBERT, ARMHuBERT, and STaRHuBERT under the same framework, offering practical guidance on compression for deployment.
>
---
#### [replaced 007] Towards Generalized Source Tracing for Codec-Based Deepfake Speech
- **分类: cs.SD; cs.CR; cs.LG; eess.AS**

- **链接: [http://arxiv.org/pdf/2506.07294v3](http://arxiv.org/pdf/2506.07294v3)**

> **作者:** Xuanjun Chen; I-Ming Lin; Lin Zhang; Haibin Wu; Hung-yi Lee; Jyh-Shing Roger Jang
>
> **备注:** IEEE ASRU 2025
>
> **摘要:** Recent attempts at source tracing for codec-based deepfake speech (CodecFake), generated by neural audio codec-based speech generation (CoSG) models, have exhibited suboptimal performance. However, how to train source tracing models using simulated CoSG data while maintaining strong performance on real CoSG-generated audio remains an open challenge. In this paper, we show that models trained solely on codec-resynthesized data tend to overfit to non-speech regions and struggle to generalize to unseen content. To mitigate these challenges, we introduce the Semantic-Acoustic Source Tracing Network (SASTNet), which jointly leverages Whisper for semantic feature encoding and Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet achieves state-of-the-art performance on the CoSG test set of the CodecFake+ dataset, demonstrating its effectiveness for reliable source tracing.
>
---
#### [replaced 008] DiffVox: A Differentiable Model for Capturing and Analysing Vocal Effects Distributions
- **分类: cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2504.14735v2](http://arxiv.org/pdf/2504.14735v2)**

> **作者:** Chin-Yun Yu; Marco A. Martínez-Ramírez; Junghyun Koo; Ben Hayes; Wei-Hsiang Liao; György Fazekas; Yuki Mitsufuji
>
> **备注:** Accepted at DAFx 2025
>
> **摘要:** This study introduces a novel and interpretable model, DiffVox, for matching vocal effects in music production. DiffVox, short for ``Differentiable Vocal Fx", integrates parametric equalisation, dynamic range control, delay, and reverb with efficient differentiable implementations to enable gradient-based optimisation for parameter estimation. Vocal presets are retrieved from two datasets, comprising 70 tracks from MedleyDB and 365 tracks from a private collection. Analysis of parameter correlations reveals strong relationships between effects and parameters, such as the high-pass and low-shelf filters often working together to shape the low end, and the delay time correlating with the intensity of the delayed signals. Principal component analysis reveals connections to McAdams' timbre dimensions, where the most crucial component modulates the perceived spaciousness while the secondary components influence spectral brightness. Statistical testing confirms the non-Gaussian nature of the parameter distribution, highlighting the complexity of the vocal effects space. These initial findings on the parameter distributions set the foundation for future research in vocal effects modelling and automatic mixing. Our source code and datasets are accessible at https://github.com/SonyResearch/diffvox.
>
---
#### [replaced 009] Radif Corpus: A Symbolic Dataset for Non-Metric Iranian Classical Music
- **分类: cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2507.10456v3](http://arxiv.org/pdf/2507.10456v3)**

> **作者:** Maziar Kanani; Sean O Leary; James McDermott
>
> **摘要:** Non-metric music forms the core of the repertoire in Iranian classical music. Dastgahi music serves as the underlying theoretical system for both Iranian art music and certain folk traditions. At the heart of Iranian classical music lies the radif, a foundational repertoire that organizes melodic material central to performance and pedagogy. In this study, we introduce a digital corpus representing the complete non-metrical radif repertoire, covering all 13 existing components of this repertoire. We provide MIDI files (about 281 minutes in total) and data spreadsheets describing notes, note durations, intervals, and hierarchical structures for 228 pieces of music. We faithfully represent the tonality including quarter-tones, and the non-metric aspect. Furthermore, we provide supporting basic statistics, and measures of complexity and similarity over the corpus. Our corpus provides a platform for computational studies of Iranian classical music. Researchers might employ it in studying melodic patterns, investigating improvisational styles, or for other tasks in music information retrieval, music theory, and computational (ethno)musicology.
>
---
#### [replaced 010] Differentiable Room Acoustic Rendering with Multi-View Vision Priors
- **分类: cs.CV; cs.SD**

- **链接: [http://arxiv.org/pdf/2504.21847v2](http://arxiv.org/pdf/2504.21847v2)**

> **作者:** Derong Jin; Ruohan Gao
>
> **备注:** ICCV 2025 (Oral); Project Page: https://humathe.github.io/avdar/
>
> **摘要:** An immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. However, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic Rendering (AV-DAR), a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. Experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from 16.6% to 50.9% when trained at the same scale.
>
---
#### [replaced 011] Communicate Less, Synthesize the Rest: Latency-aware Intent-based Generative Semantic Multicasting with Diffusion Models
- **分类: cs.IT; cs.CV; cs.MM; eess.SP; math.IT**

- **链接: [http://arxiv.org/pdf/2411.02334v2](http://arxiv.org/pdf/2411.02334v2)**

> **作者:** Xinkai Liu; Mahdi Boloursaz Mashhadi; Li Qiao; Yi Ma; Rahim Tafazolli; Mehdi Bennis
>
> **备注:** Submitted to IEEE Journals
>
> **摘要:** Generative diffusion models (GDMs) have recently shown great success in synthesizing multimedia signals with high perceptual quality, enabling highly efficient semantic communications in future wireless networks. In this paper, we develop an intent-aware generative semantic multicasting framework utilizing pre-trained diffusion models. In the proposed framework, the transmitter decomposes the source signal into multiple semantic classes based on the multi-user intent, i.e. each user is assumed to be interested in details of only a subset of the semantic classes. To better utilize the wireless resources, the transmitter sends to each user only its intended classes, and multicasts a highly compressed semantic map to all users over shared wireless resources that allows them to locally synthesize the other classes, namely non-intended classes, utilizing pre-trained diffusion models. The signal retrieved at each user is thereby partially reconstructed and partially synthesized utilizing the received semantic map. We design a communication/computation-aware scheme for per-class adaptation of the communication parameters, such as the transmission power and compression rate, to minimize the total latency of retrieving signals at multiple receivers, tailored to the prevailing channel conditions as well as the users' reconstruction/synthesis distortion/perception requirements. The simulation results demonstrate significantly reduced per-user latency compared with non-generative and intent-unaware multicasting benchmarks while maintaining high perceptual quality of the signals retrieved at the users.
>
---
#### [replaced 012] USAD: Universal Speech and Audio Representation via Distillation
- **分类: cs.SD; cs.CL; eess.AS**

- **链接: [http://arxiv.org/pdf/2506.18843v2](http://arxiv.org/pdf/2506.18843v2)**

> **作者:** Heng-Jui Chang; Saurabhchand Bhati; James Glass; Alexander H. Liu
>
> **备注:** Accepted to ASRU 2025
>
> **摘要:** Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.
>
---
#### [replaced 013] SEF-MK: Speaker-Embedding-Free Voice Anonymization through Multi-k-means Quantization
- **分类: cs.SD; cs.LG; eess.AS**

- **链接: [http://arxiv.org/pdf/2508.07086v2](http://arxiv.org/pdf/2508.07086v2)**

> **作者:** Beilong Tang; Xiaoxiao Miao; Xin Wang; Ming Li
>
> **备注:** 8 pages, 3 figures, accepted by 2025 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)
>
> **摘要:** Voice anonymization protects speaker privacy by concealing identity while preserving linguistic and paralinguistic content. Self-supervised learning (SSL) representations encode linguistic features but preserve speaker traits. We propose a novel speaker-embedding-free framework called SEF-MK. Instead of using a single k-means model trained on the entire dataset, SEF-MK anonymizes SSL representations for each utterance by randomly selecting one of multiple k-means models, each trained on a different subset of speakers. We explore this approach from both attacker and user perspectives. Extensive experiments show that, compared to a single k-means model, SEF-MK with multiple k-means models better preserves linguistic and emotional content from the user's viewpoint. However, from the attacker's perspective, utilizing multiple k-means models boosts the effectiveness of privacy attacks. These insights can aid users in designing voice anonymization systems to mitigate attacker threats.
>
---
