# 计算机与社会 cs.CY

- **最新发布 21 篇**

- **更新 11 篇**

## 最新发布

#### [new 001] Personalized Auto-Grading and Feedback System for Constructive Geometry Tasks Using Large Language Models on an Online Math Platform
- **分类: cs.CY; cs.LG**

- **简介: 该论文属于数学教育中的自动评分任务，旨在解决几何作业的个性化评估与反馈问题。研究使用大语言模型构建系统，实现精准评分与动态反馈。**

- **链接: [http://arxiv.org/pdf/2509.25529v1](http://arxiv.org/pdf/2509.25529v1)**

> **作者:** Yong Oh Lee; Byeonghun Bang; Joohyun Lee; Sejun Oh
>
> **摘要:** As personalized learning gains increasing attention in mathematics education, there is a growing demand for intelligent systems that can assess complex student responses and provide individualized feedback in real time. In this study, we present a personalized auto-grading and feedback system for constructive geometry tasks, developed using large language models (LLMs) and deployed on the Algeomath platform, a Korean online tool designed for interactive geometric constructions. The proposed system evaluates student-submitted geometric constructions by analyzing their procedural accuracy and conceptual understanding. It employs a prompt-based grading mechanism using GPT-4, where student answers and model solutions are compared through a few-shot learning approach. Feedback is generated based on teacher-authored examples built from anticipated student responses, and it dynamically adapts to the student's problem-solving history, allowing up to four iterative attempts per question. The system was piloted with 79 middle-school students, where LLM-generated grades and feedback were benchmarked against teacher judgments. Grading closely aligned with teachers, and feedback helped many students revise errors and complete multi-step geometry tasks. While short-term corrections were frequent, longer-term transfer effects were less clear. Overall, the study highlights the potential of LLMs to support scalable, teacher-aligned formative assessment in mathematics, while pointing to improvements needed in terminology handling and feedback design.
>
---
#### [new 002] Effectiveness of Large Language Models in Simulating Regional Psychological Structures: An Empirical Examination of Personality and Subjective Well-being
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 该论文属于心理模拟任务，旨在检验LLMs是否能模拟区域心理特征。通过对比虚拟与真实数据，发现LLMs在性格和幸福感上存在系统性差异，揭示其局限性。**

- **链接: [http://arxiv.org/pdf/2509.25283v1](http://arxiv.org/pdf/2509.25283v1)**

> **作者:** Ke Luoma; Li Zengyi; Liao Jiangqun; Tong Song; Peng Kaiping
>
> **摘要:** This study examines whether LLMs can simulate culturally grounded psychological patterns based on demographic information. Using DeepSeek, we generated 2943 virtual participants matched to demographic distributions from the CFPS2018 and compared them with human responses on the Big Five personality traits and subjective well-being across seven Chinese regions.Personality was measured using a 15-item Chinese Big Five inventory, and happiness with a single-item rating. Results revealed broad similarity between real and simulated datasets, particularly in regional variation trends. However, systematic differences emerged:simulated participants scored lower in extraversion and openness, higher in agreeableness and neuroticism, and consistently reported lower happiness. Predictive structures also diverged: while human data identified conscientiousness, extraversion and openness as positive predictors of happiness, the AI emphasized openness and agreeableness, with extraversion predicting negatively. These discrepancies suggest that while LLMs can approximate population-level psychological distributions, they underrepresent culturally specific and affective dimensions. The findings highlight both the potential and limitations of LLM-based virtual participants for large-scale psychological research and underscore the need for culturally enriched training data and improved affective modeling.
>
---
#### [new 003] The Sandbox Configurator: A Framework to Support Technical Assessment in AI Regulatory Sandboxes
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI监管任务，旨在解决AI系统评估标准不统一问题。提出Sandbox Configurator框架，支持定制化测试环境与跨机构协作。**

- **链接: [http://arxiv.org/pdf/2509.25256v1](http://arxiv.org/pdf/2509.25256v1)**

> **作者:** Alessio Buscemi; Thibault Simonetto; Daniele Pagani; German Castignani; Maxime Cordy; Jordi Cabot
>
> **摘要:** The systematic assessment of AI systems is increasingly vital as these technologies enter high-stakes domains. To address this, the EU's Artificial Intelligence Act introduces AI Regulatory Sandboxes (AIRS): supervised environments where AI systems can be tested under the oversight of Competent Authorities (CAs), balancing innovation with compliance, particularly for startups and SMEs. Yet significant challenges remain: assessment methods are fragmented, tests lack standardisation, and feedback loops between developers and regulators are weak. To bridge these gaps, we propose the Sandbox Configurator, a modular open-source framework that enables users to select domain-relevant tests from a shared library and generate customised sandbox environments with integrated dashboards. Its plug-in architecture aims to support both open and proprietary modules, fostering a shared ecosystem of interoperable AI assessment services. The framework aims to address multiple stakeholders: CAs gain structured workflows for applying legal obligations; technical experts can integrate robust evaluation methods; and AI providers access a transparent pathway to compliance. By promoting cross-border collaboration and standardisation, the Sandbox Configurator's goal is to support a scalable and innovation-friendly European infrastructure for trustworthy AI governance.
>
---
#### [new 004] AI in Pakistani Schools: Adoption, Usage, and Perceived Impact among Educators
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于教育技术研究，探讨巴基斯坦学校AI的采用与影响。旨在了解教师对AI的使用情况及看法，分析其应用现状与挑战。**

- **链接: [http://arxiv.org/pdf/2509.25293v1](http://arxiv.org/pdf/2509.25293v1)**

> **作者:** Syed Hassan Raza; Azib Farooq
>
> **摘要:** Artificial Intelligence (AI) is increasingly permeating classrooms worldwide, yet its adoption in schools of developing countries remains under-explored. This paper investigates AI adoption, usage patterns, and perceived impact in Pakistani K-12 schools based on a survey of 125 educators. The questionnaire covered educator's familiarity with AI, frequency and modes of use, and attitudes toward AI's benefits and challenges. Results reveal a generally positive disposition towards AI: over two-thirds of teachers expressed willingness to adopt AI tools given proper support and many have begun integrating AI for lesson planning and content creation. However, AI usage is uneven - while about one-third of respondents actively use AI tools frequently, others remain occasional users. Content generation emerged as the most common AI application, whereas AI-driven grading and feedback are rarely used. Teachers reported moderate improvements in student engagement and efficiency due to AI, but also voiced concerns about equitable access. These findings highlight both the enthusiasm for AI's potential in Pakistan's schools and the need for training and infrastructure to ensure inclusive and effective implementation.
>
---
#### [new 005] Toxicity in Online Platforms and AI Systems: A Survey of Needs, Challenges, Mitigations, and Future Directions
- **分类: cs.CY; cs.AI; cs.CL; cs.HC; cs.SI**

- **简介: 该论文属于AI伦理任务，旨在解决在线平台和AI系统中的毒性问题。通过综述现有研究，提出全面分类体系，探讨检测与缓解方法及未来方向。**

- **链接: [http://arxiv.org/pdf/2509.25539v1](http://arxiv.org/pdf/2509.25539v1)**

> **作者:** Smita Khapre; Melkamu Abay Mersha; Hassan Shakil; Jonali Baruah; Jugal Kalita
>
> **摘要:** The evolution of digital communication systems and the designs of online platforms have inadvertently facilitated the subconscious propagation of toxic behavior. Giving rise to reactive responses to toxic behavior. Toxicity in online content and Artificial Intelligence Systems has become a serious challenge to individual and collective well-being around the world. It is more detrimental to society than we realize. Toxicity, expressed in language, image, and video, can be interpreted in various ways depending on the context of usage. Therefore, a comprehensive taxonomy is crucial to detect and mitigate toxicity in online content, Artificial Intelligence systems, and/or Large Language Models in a proactive manner. A comprehensive understanding of toxicity is likely to facilitate the design of practical solutions for toxicity detection and mitigation. The classification in published literature has focused on only a limited number of aspects of this very complex issue, with a pattern of reactive strategies in response to toxicity. This survey attempts to generate a comprehensive taxonomy of toxicity from various perspectives. It presents a holistic approach to explain the toxicity by understanding the context and environment that society is facing in the Artificial Intelligence era. This survey summarizes the toxicity-related datasets and research on toxicity detection and mitigation for Large Language Models, social media platforms, and other online platforms, detailing their attributes in textual mode, focused on the English language. Finally, we suggest the research gaps in toxicity mitigation based on datasets, mitigation strategies, Large Language Models, adaptability, explainability, and evaluation.
>
---
#### [new 006] What Drives Paper Acceptance? A Process-Centric Analysis of Modern Peer Review
- **分类: cs.CY**

- **简介: 该论文属于学术评价研究，旨在分析影响论文接受的因素。通过分析ICLR 2017-2025的大量数据，研究了投稿策略、互动过程和评审分歧等多维因素对结果的影响。**

- **链接: [http://arxiv.org/pdf/2509.25701v1](http://arxiv.org/pdf/2509.25701v1)**

> **作者:** Sangkeun Jung; Goun Pyeon; Inbum Heo; Hyungjin Ahn
>
> **摘要:** Peer review is the primary mechanism for evaluating scientific contributions, yet prior studies have mostly examined paper features or external metadata in isolation. The emergence of open platforms such as OpenReview has transformed peer review into a transparent and interactive process, recording not only scores and comments but also rebuttals, reviewer-author exchanges, reviewer disagreements, and meta-reviewer decisions. This provides unprecedented process-level data for understanding how modern peer review operates. In this paper, we present a large-scale empirical study of ICLR 2017-2025, encompassing over 28,000 submissions. Our analysis integrates four complementary dimensions, including the structure and language quality of papers (e.g., section patterns, figure/table ratios, clarity), submission strategies and external metadata (e.g., timing, arXiv posting, author count), the dynamics of author-reviewer interactions (e.g., rebuttal frequency, responsiveness), and the patterns of reviewer disagreement and meta-review mediation (e.g., score variance, confidence weighting). Our results show that factors beyond scientific novelty significantly shape acceptance outcomes. In particular, the rebuttal stage emerges as a decisive phase: timely, substantive, and interactive author-reviewer communication strongly increases the likelihood of acceptance, often outweighing initial reviewer skepticism. Alongside this, clearer writing, balanced visual presentation, earlier submission, and effective resolution of reviewer disagreement also correlate with higher acceptance probabilities. Based on these findings, we propose data-driven guidelines for authors, reviewers, and meta-reviewers to enhance transparency and fairness in peer review. Our study demonstrates that process-centric signals are essential for understanding and improving modern peer review.
>
---
#### [new 007] Decoding the Gender Gap: Addressing Gender Stereotypes and Psychological Barriers to Empower Women in Technology
- **分类: cs.CY; cs.HC**

- **简介: 该论文属于社会心理学研究，旨在解决科技领域女性参与度低的问题。通过分析性别刻板印象和心理障碍，提出教育、组织和心理支持等综合解决方案。**

- **链接: [http://arxiv.org/pdf/2509.26332v1](http://arxiv.org/pdf/2509.26332v1)**

> **作者:** Zahra Fakoor Harehdasht; Raziyeh Saki
>
> **摘要:** Recently, the unequal presence of women compared to men in technology has attracted the attention of researchers and practitioners across multiple fields. It is time to regard this problem as a global crisis that not only limits access to talent but also reduces the diversity of perspectives that shape technological innovation. This article examines the psychological and social barriers that influence this gap, as well as the interventions designed to reduce it. Using a structured review, the findings assemble evidence on the role of early gender stereotypes in the family and school and the continuation of this crisis in educational and career choices, through to the psychological challenges women face in professional settings, such as feelings of self-undervaluation, occupational anxiety, a heightened fear of technology, and structural limitations in educational environments. Special attention is paid to Germany, where the technology gap is particularly evident and where multiple national programs have been implemented to address it. The present review shows that effective solutions require more than anti-discrimination policies: they should include educational practices, organizational reforms, mentoring, and psychological support. The article concludes by outlining practical and research implications and introduces the NEURON project as a pilot interdisciplinary initiative aimed at accelerating current empowerment efforts and developing new programs for women in technology occupations.
>
---
#### [new 008] Economic Competition, EU Regulation, and Executive Orders: A Framework for Discussing AI Policy Implications in CS Courses
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI政策与教育融合的任务，旨在解决CS课程中缺乏AI政策讨论的问题，提出框架和问题以促进相关教学。**

- **链接: [http://arxiv.org/pdf/2509.25524v1](http://arxiv.org/pdf/2509.25524v1)**

> **作者:** James Weichert; Hoda Eldardiry
>
> **摘要:** The growth and permeation of artificial intelligence (AI) technologies across society has drawn focus to the ways in which the responsible use of these technologies can be facilitated through AI governance. Increasingly, large companies and governments alike have begun to articulate and, in some cases, enforce governance preferences through AI policy. Yet existing literature documents an unwieldy heterogeneity in ethical principles for AI governance, while our own prior research finds that discussions of the implications of AI policy are not yet present in the computer science (CS) curriculum. In this context, overlapping jurisdictions and even contradictory policy preferences across private companies, local, national, and multinational governments create a complex landscape for AI policy which, we argue, will require AI developers able adapt to an evolving regulatory environment. Preparing computing students for the new challenges of an AI-dominated technology industry is therefore a key priority for the CS curriculum. In this discussion paper, we seek to articulate a framework for integrating discussions on the nascent AI policy landscape into computer science courses. We begin by summarizing recent AI policy efforts in the United States and European Union. Subsequently, we propose guiding questions to frame class discussions around AI policy in technical and non-technical (e.g., ethics) CS courses. Throughout, we emphasize the connection between normative policy demands and still-open technical challenges relating to their implementation and enforcement through code and governance structures. This paper therefore represents a valuable contribution towards bridging research and discussions across the areas of AI policy and CS education, underlining the need to prepare AI engineers to interact with and adapt to societal policy preferences.
>
---
#### [new 009] Cognifying Education: Mapping AI's transformative role in emotional, creative, and collaborative learning
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于教育技术研究，探讨AI在情感、创造和协作学习中的变革作用，旨在分析AI如何提升教育体验并提出实践建议。**

- **链接: [http://arxiv.org/pdf/2509.25266v1](http://arxiv.org/pdf/2509.25266v1)**

> **作者:** Mikael Gorsky; Ilya Levin
>
> **备注:** Presented at the 13th Higher Education Institutions Conference (HEIC) in Dubrovnik (September 2025): AI and Digital Transformation in Higher Education
>
> **摘要:** Artificial intelligence (AI) is rapidly reshaping educational practice, challenging long held assumptions about teaching and learning. This article integrates conceptual perspectives from recent books (Genesis by Eric Schmidt, Henry Kissinger and Craig Mundie, CoIntelligence by Ethan Mollick, and The Inevitable by Kevin Kelly) with empirical insights from popular AI podcasts and Anthropic public releases. We examine seven key domains: emotional support, creativity, contextual understanding, student engagement, problem solving, ethics and morality, and collaboration. For each domain, we explore AI capabilities, opportunities for transformative change, and emerging best practices, drawing equally from theoretical analysis and real world observations. Overall, we find that AI, when used thoughtfully, can complement and enhance human educators in fostering richer learning experiences across cognitive, social, and emotional dimensions. We emphasize an optimistic yet responsible outlook: educators and students should actively shape AI integration to amplify human potential in creativity, ethical reasoning, collaboration, and beyond, while maintaining a focus on human centric values.
>
---
#### [new 010] A systematic comparison of Large Language Models for automated assignment assessment in programming education: Exploring the importance of architecture and vendor
- **分类: cs.CY**

- **简介: 该论文属于编程教育中的自动评分任务，旨在比较不同大语言模型的评估效果，分析模型架构和供应商的影响。**

- **链接: [http://arxiv.org/pdf/2509.26483v1](http://arxiv.org/pdf/2509.26483v1)**

> **作者:** Marcin Jukiewicz
>
> **摘要:** This study presents the first large-scale, side-by-side comparison of contemporary Large Language Models (LLMs) in the automated grading of programming assignments. Drawing on over 6,000 student submissions collected across four years of an introductory programming course, we systematically analysed the distribution of grades, differences in mean scores and variability reflecting stricter or more lenient grading, and the consistency and clustering of grading patterns across models. Eighteen publicly available models were evaluated: Anthropic (claude-3-5-haiku, claude-opus-4-1, claude-sonnet-4); Deepseek (deepseek-chat, deepseek-reasoner); Google (gemini-2.0-flash-lite, gemini-2.0-flash, gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro); and OpenAI (gpt-4.1-mini, gpt-4.1-nano, gpt-4.1, gpt-4o-mini, gpt-4o, gpt-5-mini, gpt-5-nano, gpt-5). Statistical tests, correlation and clustering analyses revealed clear, systematic differences between and within vendor families, with "mini" and "nano" variants consistently underperforming their full-scale counterparts. All models displayed high internal agreement, measured by the intraclass correlation coefficient, with the model consensus but only moderate agreement with human teachers' grades, indicating a persistent gap between automated and human assessment. These findings underscore that the choice of model for educational deployment is not neutral and should be guided by pedagogical goals, transparent reporting of evaluation metrics, and ongoing human oversight to ensure accuracy, fairness and relevance.
>
---
#### [new 011] Bubble, Bubble, AI's Rumble: Why Global Financial Regulatory Incident Reporting is Our Shield Against Systemic Stumbles
- **分类: cs.CY; cs.AI; cs.CE**

- **简介: 该论文属于金融监管任务，旨在解决AI系统在金融市场中的透明度问题。通过构建全球监管数据库，整合交易报告与风险分析模型，提升对系统性风险的监控能力。**

- **链接: [http://arxiv.org/pdf/2509.26150v1](http://arxiv.org/pdf/2509.26150v1)**

> **作者:** Anchal Gupta; Gleb Pappyshev; James T Kwok
>
> **摘要:** "Double, double toil and trouble; Fire burn and cauldron bubble." As Shakespeare's witches foretold chaos through cryptic prophecies, modern capital markets grapple with systemic risks concealed by opaque AI systems. According to IMF, the August 5, 2024, plunge in Japanese and U.S. equities can be linked to algorithmic trading yet ab-sent from existing AI incidents database exemplifies this transparency crisis. Current AI incident databases, reliant on crowdsourcing or news scraping, systematically over-look capital market anomalies, particularly in algorithmic and high-frequency trading. We address this critical gap by proposing a regulatory-grade global database that elegantly synthesises post-trade reporting frameworks with proven incident documentation models from healthcare and aviation. Our framework's temporal data omission technique masking timestamps while preserving percent-age-based metrics enables sophisticated cross-jurisdictional analysis of emerging risks while safeguarding confidential business information. Synthetic data validation (modelled after real life published incidents , sentiments, data) reveals compelling pat-terns: systemic risks transcending geographical boundaries, market manipulation clusters distinctly identifiable via K-means algorithms, and AI system typology exerting significantly greater influence on trading behaviour than geographical location, This tripartite solution empowers regulators with unprecedented cross-jurisdictional oversight, financial institutions with seamless compliance integration, and investors with critical visibility into previously obscured AI-driven vulnerabilities. We call for immediate action to strengthen risk management and foster resilience in AI-driven financial markets against the volatile "cauldron" of AI-driven systemic risks., promoting global financial stability through enhanced transparency and coordinated oversight.
>
---
#### [new 012] Artificial Authority: From Machine Minds to Political Alignments. An Experimental Analysis of Democratic and Autocratic Biases in Large-Language Models
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI与政治研究任务，探讨LLMs是否表现出民主或专制倾向。通过实验分析模型响应，发现其政治倾向与开发国家文化相关。**

- **链接: [http://arxiv.org/pdf/2509.25286v1](http://arxiv.org/pdf/2509.25286v1)**

> **作者:** Szymon Łukasik; Natalia Ożegalska-Łukasik
>
> **摘要:** Political beliefs vary significantly across different countries, reflecting distinct historical, cultural, and institutional contexts. These ideologies, ranging from liberal democracies to rigid autocracies, influence human societies, as well as the digital systems that are constructed within those societies. The advent of generative artificial intelligence, particularly Large Language Models (LLMs), introduces new agents in the political space-agents trained on massive corpora that replicate and proliferate socio-political assumptions. This paper analyses whether LLMs display propensities consistent with democratic or autocratic world-views. We validate this insight through experimental tests in which we experiment with the leading LLMs developed across disparate political contexts, using several existing psychometric and political orientation measures. The analysis is based on both numerical scoring and qualitative analysis of the models' responses. Findings indicate high model-to-model variability and a strong association with the political culture of the country in which the model was developed. These findings highlight the need for more detailed examination of the socio-political dimensions embedded within AI systems.
>
---
#### [new 013] Artificial Intelligence-Powered Assessment Framework for Skill-Oriented Engineering Lab Education
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于教育技术任务，旨在解决工程实验教学中的评估难题。通过AI生成个性化题目和评估，提升实践能力与学习效果。**

- **链接: [http://arxiv.org/pdf/2509.25258v1](http://arxiv.org/pdf/2509.25258v1)**

> **作者:** Vaishnavi Sharma; Rakesh Thakur; Shashwat Sharma; Kritika Panjanani
>
> **摘要:** Practical lab education in computer science often faces challenges such as plagiarism, lack of proper lab records, unstructured lab conduction, inadequate execution and assessment, limited practical learning, low student engagement, and absence of progress tracking for both students and faculties, resulting in graduates with insufficient hands-on skills. In this paper, we introduce AsseslyAI, which addresses these challenges through online lab allocation, a unique lab problem for each student, AI-proctored viva evaluations, and gamified simulators to enhance engagement and conceptual mastery. While existing platforms generate questions based on topics, our framework fine-tunes on a 10k+ question-answer dataset built from AI/ML lab questions to dynamically generate diverse, code-rich assessments. Validation metrics show high question-answer similarity, ensuring accurate answers and non-repetitive questions. By unifying dataset-driven question generation, adaptive difficulty, plagiarism resistance, and evaluation in a single pipeline, our framework advances beyond traditional automated grading tools and offers a scalable path to produce genuinely skilled graduates.
>
---
#### [new 014] Trajectories and Comparative Analysis of Global Countries Dominating AI Publications, 2000-2025
- **分类: cs.CY; cs.DL**

- **简介: 该论文属于科技趋势分析任务，旨在研究2000-2025年间全球AI研究格局变化，通过分析各国出版物份额揭示主导力量转移。**

- **链接: [http://arxiv.org/pdf/2509.25298v1](http://arxiv.org/pdf/2509.25298v1)**

> **作者:** Jason Hung
>
> **备注:** 12 pages, 7 figures, 7 tables
>
> **摘要:** This study investigates the shifting global dynamics of Artificial Intelligence (AI) research by analysing the trajectories of countries dominating AI publications between 2000 and 2025. Drawing on the comprehensive OpenAlex dataset and employing fractional counting to avoid double attribution in co-authored work, the research maps the relative shares of AI publications across major global players. The analysis reveals a profound restructuring of the international AI research landscape. The US and the European Union (EU27), once the undisputed and established leaders, have experienced a notable decline in relative dominance, with their combined share of publications falling from over 57% in 2000 to less than 25% in 2025. In contrast, China has undergone a dramatic ascent, expanding its global share of AI publications from under 5% in 2000 to nearly 36% by 2025, thereby emerging as the single most dominant contributor. Alongside China, India has also risen substantially, consolidating a multipolar Asian research ecosystem. These empirical findings highlight the strategic implications of concentrated research output, particularly China's capacity to shape the future direction of AI innovation and standard-setting. While the study calculates the volume of AI publications (in percentage as global share) as a measure of research dominance, it also acknowledges limitations in capturing quality and impact, suggesting scholarly research areas for future work on high-impact AI scholarship.
>
---
#### [new 015] A Measurement Study of Model Context Protocol
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于系统测量任务，旨在评估Model Context Protocol（MCP）生态系统的健康状况与风险。通过构建MCPCrawler工具，分析了大量项目，揭示了其存在的无效项目、安全风险及协议碎片化问题。**

- **链接: [http://arxiv.org/pdf/2509.25292v1](http://arxiv.org/pdf/2509.25292v1)**

> **作者:** Hechuan Guo; Yongle Hao; Yue Zhang; Minghui Xu; Peizhuo Lyu; Jiezhi Chen; Xiuzhen Cheng
>
> **摘要:** The Model Context Protocol (MCP) has been proposed as a unifying standard for connecting large language models (LLMs) with external tools and resources, promising the same role for AI integration that HTTP and USB played for the Web and peripherals. Yet, despite rapid adoption and hype, its trajectory remains uncertain. Are MCP marketplaces truly growing, or merely inflated by placeholders and abandoned prototypes? Are servers secure and privacy-preserving, or do they expose users to systemic risks? And do clients converge on standardized protocols, or remain fragmented across competing designs? In this paper, we present the first large-scale empirical study of the MCP ecosystem. We design and implement MCPCrawler, a systematic measurement framework that collects and normalizes data from six major markets. Over a 14-day campaign, MCPCrawler aggregated 17,630 raw entries, of which 8,401 valid projects (8,060 servers and 341 clients) were analyzed. Our results reveal that more than half of listed projects are invalid or low-value, that servers face structural risks including dependency monocultures and uneven maintenance, and that clients exhibit a transitional phase in protocol and connection patterns. Together, these findings provide the first evidence-based view of the MCP ecosystem, its risks, and its future trajectory.
>
---
#### [new 016] Ethics Statements in AI Music Papers: The Effective and the Ineffective
- **分类: cs.CY; cs.SD**

- **简介: 该论文属于AI音乐研究中的伦理分析任务，旨在解决伦理声明在AI音乐论文中使用效果不佳的问题，通过文献回顾提出改进建议。**

- **链接: [http://arxiv.org/pdf/2509.25496v1](http://arxiv.org/pdf/2509.25496v1)**

> **作者:** Julia Barnett; Patrick O'Reilly; Jason Brent Smith; Annie Chu; Bryan Pardo
>
> **备注:** Accepted to the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: AI for Music
>
> **摘要:** While research in AI methods for music generation and analysis has grown in scope and impact, AI researchers' engagement with the ethical consequences of this work has not kept pace. To encourage such engagement, many publication venues have introduced optional or required ethics statements for AI research papers. Though some authors use these ethics statements to critically engage with the broader implications of their research, we find that the majority of ethics statements in the AI music literature do not appear to be effectively utilized for this purpose. In this work, we conduct a review of ethics statements across ISMIR, NIME, and selected prominent works in AI music from the past five years. We then offer suggestions for both audio conferences and researchers for engaging with ethics statements in ways that foster meaningful reflection rather than formulaic compliance.
>
---
#### [new 017] A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments
- **分类: cs.AI; cs.CY**

- **简介: 该论文属于AI行为研究任务，旨在评估LLM代理在真实决策中的选择机制。通过构建ABxLab框架，分析代理对价格、评分和心理提示的反应，揭示其决策偏差。**

- **链接: [http://arxiv.org/pdf/2509.25609v1](http://arxiv.org/pdf/2509.25609v1)**

> **作者:** Manuel Cherep; Chengtian Ma; Abigail Xu; Maya Shaked; Pattie Maes; Nikhil Singh
>
> **备注:** 23 pages, 13 figures
>
> **摘要:** Environments built for people are increasingly operated by a new class of economic actors: LLM-powered software agents making decisions on our behalf. These decisions range from our purchases to travel plans to medical treatment selection. Current evaluations of these agents largely focus on task competence, but we argue for a deeper assessment: how these agents choose when faced with realistic decisions. We introduce ABxLab, a framework for systematically probing agentic choice through controlled manipulations of option attributes and persuasive cues. We apply this to a realistic web-based shopping environment, where we vary prices, ratings, and psychological nudges, all of which are factors long known to shape human choice. We find that agent decisions shift predictably and substantially in response, revealing that agents are strongly biased choosers even without being subject to the cognitive constraints that shape human biases. This susceptibility reveals both risk and opportunity: risk, because agentic consumers may inherit and amplify human biases; opportunity, because consumer choice provides a powerful testbed for a behavioral science of AI agents, just as it has for the study of human behavior. We release our framework as an open benchmark for rigorous, scalable evaluation of agent decision-making.
>
---
#### [new 018] Human vs. AI Safety Perception? Decoding Human Safety Perception with Eye-Tracking Systems, Street View Images, and Explainable AI
- **分类: cs.HC; cs.CY**

- **简介: 该论文属于城市安全感知研究，旨在解析人类对公共空间安全的视觉感知。通过眼动追踪与深度学习结合，识别影响安全感知的关键视觉因素，并对比AI注意力机制。**

- **链接: [http://arxiv.org/pdf/2509.25457v1](http://arxiv.org/pdf/2509.25457v1)**

> **作者:** Yuhao Kang; Junda Chen; Liu Liu; Kshitij Sharmad; Martina Mazzarello; Simone Mora; Fabio Duarte; Carlo Ratti
>
> **备注:** 28 pages, 8 figures
>
> **摘要:** The way residents perceive safety plays an important role in how they use public spaces. Studies have combined large-scale street view images and advanced computer vision techniques to measure the perception of safety of urban environments. Despite their success, such studies have often overlooked the specific environmental visual factors that draw human attention and trigger people's feelings of safety perceptions. In this study, we introduce a computational framework that enriches the existing body of literature on place perception by using eye-tracking systems with street view images and deep learning approaches. Eye-tracking systems quantify not only what users are looking at but also how long they engage with specific environmental elements. This allows us to explore the nuance of which visual environmental factors influence human safety perceptions. We conducted our research in Helsingborg, Sweden, where we recruited volunteers outfitted with eye-tracking systems. They were asked to indicate which of the two street view images appeared safer. By examining participants' focus on specific features using Mean Object Ratio in Highlighted Regions (MoRH) and Mean Object Hue (MoH), we identified key visual elements that attract human attention when perceiving safe environments. For instance, certain urban infrastructure and public space features draw more human attention while the sky is less relevant in influencing safety perceptions. These insights offer a more human-centered understanding of which urban features influence human safety perceptions. Furthermore, we compared the real human attention from eye-tracking systems with attention maps obtained from eXplainable Artificial Intelligence (XAI) results. Several XAI models were tested, and we observed that XGradCAM and EigenCAM most closely align with human safety perceptual patterns.
>
---
#### [new 019] RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文属于社会情境理解任务，旨在解决LLMs在角色冲突场景中的上下文敏感性问题。作者构建了RoleConflictBench基准，分析模型对复杂社会困境的响应偏差。**

- **链接: [http://arxiv.org/pdf/2509.25897v1](http://arxiv.org/pdf/2509.25897v1)**

> **作者:** Jisu Shin; Hoyun Song; Juhyun Oh; Changgeon Ko; Eunsu Kim; Chani Jung; Alice Oh
>
> **摘要:** Humans often encounter role conflicts -- social dilemmas where the expectations of multiple roles clash and cannot be simultaneously fulfilled. As large language models (LLMs) become increasingly influential in human decision-making, understanding how they behave in complex social situations is essential. While previous research has evaluated LLMs' social abilities in contexts with predefined correct answers, role conflicts represent inherently ambiguous social dilemmas that require contextual sensitivity: the ability to recognize and appropriately weigh situational cues that can fundamentally alter decision priorities. To address this gap, we introduce RoleConflictBench, a novel benchmark designed to evaluate LLMs' contextual sensitivity in complex social dilemmas. Our benchmark employs a three-stage pipeline to generate over 13K realistic role conflict scenarios across 65 roles, systematically varying their associated expectations (i.e., their responsibilities and obligations) and situational urgency levels. By analyzing model choices across 10 different LLMs, we find that while LLMs show some capacity to respond to these contextual cues, this sensitivity is insufficient. Instead, their decisions are predominantly governed by a powerful, inherent bias related to social roles rather than situational information. Our analysis quantifies these biases, revealing a dominant preference for roles within the Family and Occupation domains, as well as a clear prioritization of male roles and Abrahamic religions across most evaluatee models.
>
---
#### [new 020] Characterizing Event-themed Malicious Web Campaigns: A Case Study on War-themed Websites
- **分类: cs.CR; cs.CY**

- **简介: 该论文属于网络安全领域，旨在识别和分析基于事件的恶意网络活动。通过案例研究，探索战争主题网站的攻击模式，以提升早期防御能力。**

- **链接: [http://arxiv.org/pdf/2509.25410v1](http://arxiv.org/pdf/2509.25410v1)**

> **作者:** Maraz Mia; Mir Mehedi A. Pritom; Tariqul Islam; Shouhuai Xu
>
> **备注:** 12 pages, 9 figures, 5 tables
>
> **摘要:** Cybercrimes such as online scams and fraud have become prevalent. Cybercriminals often abuse various global or regional events as themes of their fraudulent activities to breach user trust and attain a higher attack success rate. These attacks attempt to manipulate and deceive innocent people into interacting with meticulously crafted websites with malicious payloads, phishing, or fraudulent transactions. To deepen our understanding of the problem, this paper investigates how to characterize event-themed malicious website-based campaigns, with a case study on war-themed websites. We find that attackers tailor their attacks by exploiting the unique aspects of events, as evidenced by activities such as fundraising, providing aid, collecting essential supplies, or seeking updated news. We use explainable unsupervised clustering methods to draw further insights, which could guide the design of effective early defenses against various event-themed malicious web campaigns.
>
---
#### [new 021] Quantum est in Libris: Navigating Archives with GenAI, Uncovering Tension Between Preservation and Innovation
- **分类: cs.DL; cs.AI; cs.CY; cs.HC; I.2; J.5**

- **简介: 该论文属于文化传承与AI技术融合任务，探讨如何用GenAI重新诠释历史档案，解决传统与现代技术间的张力问题。**

- **链接: [http://arxiv.org/pdf/2509.25237v1](http://arxiv.org/pdf/2509.25237v1)**

> **作者:** Mar Canet Sola; Varvara Guljajeva
>
> **备注:** 5 pages, 4 figures,
>
> **摘要:** "Quantum est in libris" explores the intersection of the archaic and the modern. On one side, there are manuscript materials from the Estonian National Museum's (ERM) more than century-old archive describing the life experiences of Estonian people; on the other side, there is technology that transforms these materials into a dynamic and interactive experience. Connecting technology and cultural heritage is the visitor, who turns texts into inputs for a screen sculpture. Historical narratives are visually brought to life through the contemporary technological language. Because the video AI models we employed, Runway Gen-3 and Gen-4, have not previously interacted with Estonian heritage, we can observe how machines today "read the world" and create future heritage. "Quantum est in libris" introduces an exciting yet unsettling new dimension to the concept of cultural heritage: in a world where data are fluid and interpretations unstable, heritage status becomes fragile. In the digital environment, heritage issues are no longer just about preservation and transmission, but also about representation of the media, machine creativity, and interpretive error. Who or what shapes memory processes and memory spaces, and how?
>
---
## 更新

#### [replaced 001] A Large Scale Analysis of Gender Biases in Text-to-Image Generative Models
- **分类: cs.CV; cs.CY**

- **链接: [http://arxiv.org/pdf/2503.23398v2](http://arxiv.org/pdf/2503.23398v2)**

> **作者:** Leander Girrbach; Stephan Alaniz; Genevieve Smith; Zeynep Akata
>
> **备注:** 40 pages
>
> **摘要:** With the increasing use of image generation technology, understanding its social biases, including gender bias, is essential. This paper presents a large-scale study on gender bias in text-to-image (T2I) models, focusing on everyday situations. While previous research has examined biases in occupations, we extend this analysis to gender associations in daily activities, objects, and contexts. We create a dataset of 3,217 gender-neutral prompts and generate 200 images over 5 prompt variations per prompt from five leading T2I models. We automatically detect the perceived gender of people in the generated images and filter out images with no person or multiple people of different genders, leaving 2,293,295 images. To enable a broad analysis of gender bias in T2I models, we group prompts into semantically similar concepts and calculate the proportion of male- and female-gendered images for each prompt. Our analysis shows that T2I models reinforce traditional gender roles and reflect common gender stereotypes in household roles. Women are predominantly portrayed in care and human-centered scenarios, and men in technical or physical labor scenarios.
>
---
#### [replaced 002] Towards an AI-Augmented Textbook
- **分类: cs.CY; cs.HC**

- **链接: [http://arxiv.org/pdf/2509.13348v4](http://arxiv.org/pdf/2509.13348v4)**

> **作者:** LearnLM Team; Google; :; Alicia Martín; Amir Globerson; Amy Wang; Anirudh Shekhawat; Anna Iurchenko; Anisha Choudhury; Avinatan Hassidim; Ayça Çakmakli; Ayelet Shasha Evron; Charlie Yang; Courtney Heldreth; Diana Akrong; Gal Elidan; Hairong Mu; Ian Li; Ido Cohen; Katherine Chou; Komal Singh; Lev Borovoi; Lidan Hackmon; Lior Belinsky; Michael Fink; Niv Efron; Preeti Singh; Rena Levitt; Shashank Agarwal; Shay Sharon; Tracey Lee-Joe; Xiaohong Hao; Yael Gold-Zamir; Yael Haramaty; Yishay Mor; Yoav Bar Sinai; Yossi Matias
>
> **摘要:** Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. Any new material or alternative representation requires arduous human effort, so that textbooks cannot be adapted in a scalable manner. We present an approach for transforming and augmenting textbooks using generative AI, adding layers of multiple representations and personalization while maintaining content integrity and quality. We refer to the system built with this approach as Learn Your Way. We report pedagogical evaluations of the different transformations and augmentations, and present the results of a a randomized control trial, highlighting the advantages of learning with Learn Your Way over regular textbook usage.
>
---
#### [replaced 003] Protecting Human Cognition in the Age of AI
- **分类: cs.CY; cs.HC**

- **链接: [http://arxiv.org/pdf/2502.12447v3](http://arxiv.org/pdf/2502.12447v3)**

> **作者:** Anjali Singh; Karan Taneja; Zhitong Guan; Avijit Ghosh
>
> **备注:** Accepted at Tools for Thought Workshop at CHI'25
>
> **摘要:** The rapid adoption of Generative AI (GenAI) is significantly reshaping human cognition, influencing how we engage with information, think, reason, and learn. This paper synthesizes existing literature on GenAI's effects on different aspects of human cognition. Drawing on Krathwohl's revised Bloom's Taxonomy and Dewey's conceptualization of reflective thought, we examine the mechanisms through which GenAI is affecting the development of different cognitive abilities. We focus on novices, such as students, who may lack both domain knowledge and an understanding of effective human-AI interaction. Accordingly, we provide implications for rethinking and designing educational experiences that foster critical thinking and deeper cognitive engagement.
>
---
#### [replaced 004] A Meta-Analysis of LLM Effects on Students across Qualification, Socialisation, and Subjectification
- **分类: cs.CY; cs.AI; cs.HC**

- **链接: [http://arxiv.org/pdf/2509.22725v2](http://arxiv.org/pdf/2509.22725v2)**

> **作者:** Jiayu Huang; Ruoxin Ritter Wang; Jen-Hao Liu; Boming Xia; Yue Huang; Ruoxi Sun; Jason Minhui Xue; Jinan Zou
>
> **摘要:** Large language models (LLMs) are increasingly positioned as solutions for education, yet evaluations often reduce their impact to narrow performance metrics. This paper reframes the question by asking "what kind of impact should LLMs have in education?" Drawing on Biesta's tripartite account of good education: qualification, socialisation, and subjectification, we present a meta-analysis of 133 experimental and quasi-experimental studies (k = 188). Overall, the impact of LLMs on student learning is positive but uneven. Strong effects emerge in qualification, particularly when LLMs function as tutors in sustained interventions. Socialisation outcomes appear more variable, concentrated in sustained, reflective interventions. Subjectification, linked to autonomy and learner development, remains fragile, with improvements confined to small-scale, long-term studies. This purpose-level view highlights design as the decisive factor: without scaffolds for participation and agency, LLMs privilege what is easiest to measure while neglecting broader aims of education. For HCI and education, the issue is not just whether LLMs work, but what futures they enable or foreclose.
>
---
#### [replaced 005] Enabling AI Scientists to Recognize Innovation: A Domain-Agnostic Algorithm for Assessing Novelty
- **分类: cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2503.01508v4](http://arxiv.org/pdf/2503.01508v4)**

> **作者:** Yao Wang; Mingxuan Cui; Arthur Jiang; Jun Yan
>
> **摘要:** In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery. This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea's local density with its adjacent neighbors' densities. We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment. Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820) and biomedical research (AUROC=0.765) domains. Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s. 0.597) on cross-domain evaluation. These results validate RND as a generalizable solution for automated novelty assessment in scientific research.
>
---
#### [replaced 006] Qualitative Research in an Era of AI: A Pragmatic Approach to Data Analysis, Workflow, and Computation
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2509.12503v2](http://arxiv.org/pdf/2509.12503v2)**

> **作者:** Corey M. Abramson; Zhuofan Li; Tara Prendergast; Daniel Dohan
>
> **备注:** pre-print, methodology, workflow article
>
> **摘要:** Rapid computational developments - particularly the proliferation of artificial intelligence (AI) - increasingly shape social scientific research while raising new questions about in-depth qualitative methods such as ethnography and interviewing. Building on classic debates about using computers to analyze qualitative data, we revisit longstanding concerns and assess possibilities and dangers in an era of automation, AI chatbots, and 'big data.' We first historicize developments by revisiting classical and emergent concerns about qualitative analysis with computers. We then introduce a typology of contemporary modes of engagement - streamlining workflows, scaling up projects, hybrid analytical approaches, and the sociology of computation - alongside rejection of computational analyses. We illustrate these approaches with detailed workflow examples from a large-scale ethnographic study and guidance for solo researchers. We argue for a pragmatic sociological approach that moves beyond dualisms of technological optimism versus rejection to show how computational tools - simultaneously dangerous and generative - can be adapted to support longstanding qualitative aims when used carefully in ways aligned with core methodological commitments.
>
---
#### [replaced 007] Automated Quality Assessment for LLM-Based Complex Qualitative Coding: A Confidence-Diversity Framework
- **分类: cs.CY; 68T50, 62P25, 91C99; I.2.7; H.3.1; J.4**

- **链接: [http://arxiv.org/pdf/2508.20462v2](http://arxiv.org/pdf/2508.20462v2)**

> **作者:** Zhilong Zhao; Yindi Liu
>
> **备注:** 21 pages, 2 figures, 5 tables. v2: revised abstract and JCSS-aligned prose; unified table formatting and naming; clean compile
>
> **摘要:** Computational social science lacks a scalable and reliable mechanism to assure quality for AI-assisted qualitative coding when tasks demand domain expertise and long-text reasoning, and traditional double-coding is prohibitively costly at scale. We develop and validate a dual-signal quality assessment framework that combines model confidence with inter-model consensus (external entropy) and evaluate it across legal reasoning (390 Supreme Court cases), political analysis (645 hyperpartisan articles), and medical classification (1,000 clinical transcripts). External entropy is consistently negatively associated with accuracy (r = -0.179 to -0.273, p < 0.001), while confidence is positively associated in two domains (r = 0.104 to 0.429). Weight optimization improves over single-signal baselines by 6.6-113.7% and transfers across domains (100% success), and an intelligent triage protocol reduces manual verification effort by 44.6% while maintaining quality. The framework offers a principled, domain-agnostic quality assurance mechanism that scales qualitative coding without extensive double-coding, provides actionable guidance for sampling and verification, and enables larger and more diverse corpora to be analyzed with maintained rigor.
>
---
#### [replaced 008] Worker Discretion Advised: Co-designing Risk Disclosure in Crowdsourced Responsible AI (RAI) Content Work
- **分类: cs.HC; cs.CY**

- **链接: [http://arxiv.org/pdf/2509.12140v2](http://arxiv.org/pdf/2509.12140v2)**

> **作者:** Alice Qian; Ziqi Yang; Ryland Shaw; Jina Suh; Laura Dabbish; Hong Shen
>
> **摘要:** Responsible AI (RAI) content work, such as annotation, moderation, or red teaming for AI safety, often exposes crowd workers to potentially harmful content. While prior work has underscored the importance of communicating well-being risk to employed content moderators, designing effective disclosure mechanisms for crowd workers while balancing worker protection with the needs of task designers and platforms remains largely unexamined. To address this gap, we conducted co-design sessions with 29 task designers, workers, and platform representatives. We investigated task designer preferences for support in disclosing tasks, worker preferences for receiving risk disclosure warnings, and how platform stakeholders envision their role in shaping risk disclosure practices. We identify design tensions and map the sociotechnical tradeoffs that shape disclosure practices. We contribute design recommendations and feature concepts for risk disclosure mechanisms in the context of RAI content work.
>
---
#### [replaced 009] On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2502.14296v4](http://arxiv.org/pdf/2502.14296v4)**

> **作者:** Yue Huang; Chujie Gao; Siyuan Wu; Haoran Wang; Xiangqi Wang; Yujun Zhou; Yanbo Wang; Jiayi Ye; Jiawen Shi; Qihui Zhang; Yuan Li; Han Bao; Zhaoyi Liu; Tianrui Guan; Dongping Chen; Ruoxi Chen; Kehan Guo; Andy Zou; Bryan Hooi Kuen-Yew; Caiming Xiong; Elias Stengel-Eskin; Hongyang Zhang; Hongzhi Yin; Huan Zhang; Huaxiu Yao; Jaehong Yoon; Jieyu Zhang; Kai Shu; Kaijie Zhu; Ranjay Krishna; Swabha Swayamdipta; Taiwei Shi; Weijia Shi; Xiang Li; Yiwei Li; Yuexing Hao; Zhihao Jia; Zhize Li; Xiuying Chen; Zhengzhong Tu; Xiyang Hu; Tianyi Zhou; Jieyu Zhao; Lichao Sun; Furong Huang; Or Cohen Sasson; Prasanna Sattigeri; Anka Reuel; Max Lamparth; Yue Zhao; Nouha Dziri; Yu Su; Huan Sun; Heng Ji; Chaowei Xiao; Mohit Bansal; Nitesh V. Chawla; Jian Pei; Jianfeng Gao; Michael Backes; Philip S. Yu; Neil Zhenqiang Gong; Pin-Yu Chen; Bo Li; Dawn Song; Xiangliang Zhang
>
> **摘要:** Generative Foundation Models (GenFMs) have emerged as transformative tools. However, their widespread adoption raises critical concerns regarding trustworthiness across dimensions. This paper presents a comprehensive framework to address these challenges through three key contributions. First, we systematically review global AI governance laws and policies from governments and regulatory bodies, as well as industry practices and standards. Based on this analysis, we propose a set of guiding principles for GenFMs, developed through extensive multidisciplinary collaboration that integrates technical, ethical, legal, and societal perspectives. Second, we introduce TrustGen, the first dynamic benchmarking platform designed to evaluate trustworthiness across multiple dimensions and model types, including text-to-image, large language, and vision-language models. TrustGen leverages modular components--metadata curation, test case generation, and contextual variation--to enable adaptive and iterative assessments, overcoming the limitations of static evaluation methods. Using TrustGen, we reveal significant progress in trustworthiness while identifying persistent challenges. Finally, we provide an in-depth discussion of the challenges and future directions for trustworthy GenFMs, which reveals the complex, evolving nature of trustworthiness, highlighting the nuanced trade-offs between utility and trustworthiness, and consideration for various downstream applications, identifying persistent challenges and providing a strategic roadmap for future research. This work establishes a holistic framework for advancing trustworthiness in GenAI, paving the way for safer and more responsible integration of GenFMs into critical applications. To facilitate advancement in the community, we release the toolkit for dynamic evaluation.
>
---
#### [replaced 010] Locating Risk: Task Designers and the Challenge of Risk Disclosure in RAI Content Work
- **分类: cs.HC; cs.CY**

- **链接: [http://arxiv.org/pdf/2505.24246v3](http://arxiv.org/pdf/2505.24246v3)**

> **作者:** Alice Qian; Ryland Shaw; Laura Dabbish; Jina Suh; Hong Shen
>
> **摘要:** As AI systems are increasingly tested and deployed in open-ended and high-stakes domains, crowd workers are often tasked with responsible AI (RAI) content work. These tasks include labeling violent content, moderating disturbing text, or simulating harmful behavior for red teaming exercises to shape AI system behaviors. While prior efforts have highlighted the risks to worker well-being associated with RAI content work, far less attention has been paid to how these risks are communicated to workers. Existing transparency frameworks and guidelines such as model cards, datasheets, and crowdworksheets focus on documenting model information and dataset collection processes, but they overlook an important aspect of disclosing well-being risks to workers. In the absence of standard workflows or clear guidance, the consistent application of content warnings, consent flows, or other forms of well-being risk disclosure remain unclear. This study investigates how task designers approach risk disclosure in crowdsourced RAI tasks. Drawing on interviews with 23 task designers across academic and industry sectors, we examine how well-being risk is recognized, interpreted, and communicated in practice. Our findings surface a need to support task designers in identifying and communicating well-being risk not only to support crowdworker well-being but also to strengthen the ethical integrity and technical efficacy of AI development pipelines.
>
---
#### [replaced 011] ELEPHANT: Measuring and understanding social sycophancy in LLMs
- **分类: cs.CL; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2505.13995v2](http://arxiv.org/pdf/2505.13995v2)**

> **作者:** Myra Cheng; Sunny Yu; Cinoo Lee; Pranav Khadpe; Lujain Ibrahim; Dan Jurafsky
>
> **摘要:** LLMs are known to exhibit sycophancy: agreeing with and flattering users, even at the cost of correctness. Prior work measures sycophancy only as direct agreement with users' explicitly stated beliefs that can be compared to a ground truth. This fails to capture broader forms of sycophancy such as affirming a user's self-image or other implicit beliefs. To address this gap, we introduce social sycophancy, characterizing sycophancy as excessive preservation of a user's face (their desired self-image), and present ELEPHANT, a benchmark for measuring social sycophancy in an LLM. Applying our benchmark to 11 models, we show that LLMs consistently exhibit high rates of social sycophancy: on average, they preserve user's face 45 percentage points more than humans in general advice queries and in queries describing clear user wrongdoing (from Reddit's r/AmITheAsshole). Furthermore, when prompted with perspectives from either side of a moral conflict, LLMs affirm both sides (depending on whichever side the user adopts) in 48% of cases--telling both the at-fault party and the wronged party that they are not wrong--rather than adhering to a consistent moral or value judgment. We further show that social sycophancy is rewarded in preference datasets, and that while existing mitigation strategies for sycophancy are limited in effectiveness, model-based steering shows promise for mitigating these behaviors. Our work provides theoretical grounding and an empirical benchmark for understanding and addressing sycophancy in the open-ended contexts that characterize the vast majority of LLM use cases.
>
---
