# 计算机与社会 cs.CY

- **最新发布 13 篇**

- **更新 5 篇**

## 最新发布

#### [new 001] Documenting Deployment with Fabric: A Repository of Real-World AI Governance
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 该论文构建AI治理案例库（Fabric），解决现有研究侧重风险而缺乏实证的问题，通过收集20个部署案例、设计流程图及分析监督机制，为研究AI治理有效性提供可扩展工具。**

- **链接: [http://arxiv.org/pdf/2508.14119v1](http://arxiv.org/pdf/2508.14119v1)**

> **作者:** Mackenzie Jorgensen; Kendall Brogle; Katherine M. Collins; Lujain Ibrahim; Arina Shah; Petra Ivanovic; Noah Broestl; Gabriel Piles; Paul Dongha; Hatim Abdulhussein; Adrian Weller; Jillian Powers; Umang Bhatt
>
> **备注:** AIES 2025
>
> **摘要:** Artificial intelligence (AI) is increasingly integrated into society, from financial services and traffic management to creative writing. Academic literature on the deployment of AI has mostly focused on the risks and harms that result from the use of AI. We introduce Fabric, a publicly available repository of deployed AI use cases to outline their governance mechanisms. Through semi-structured interviews with practitioners, we collect an initial set of 20 AI use cases. In addition, we co-design diagrams of the AI workflow with the practitioners. We discuss the oversight mechanisms and guardrails used in practice to safeguard AI use. The Fabric repository includes visual diagrams of AI use cases and descriptions of the deployed systems. Using the repository, we surface gaps in governance and find common patterns in human oversight of deployed AI systems. We intend for Fabric to serve as an extendable, evolving tool for researchers to study the effectiveness of AI governance.
>
---
#### [new 002] An Investigation Into Secondary School Students' Debugging Behaviour in Python
- **分类: cs.CY**

- **简介: 该论文通过分析73名学生在Python调试中的行为，发现其多采用低效策略，难以彻底解决问题，强调需明确教授有效调试方法。**

- **链接: [http://arxiv.org/pdf/2508.14833v1](http://arxiv.org/pdf/2508.14833v1)**

> **作者:** Laurie Gale; Sue Sentance
>
> **备注:** 33 pages, 3 figures
>
> **摘要:** Background and context: Debugging is a common and often frustrating challenge for beginner programmers. Understanding students' debugging processes can help us identify the difficulties and misunderstandings they possess. However, we currently have limited knowledge of how secondary students debug in a text-based language, a medium through which millions of students will learn to program in the future. Objectives: In this paper, we investigate the debugging behaviour of K-12 students learning a text-based programming language, as part of an effort to shape how to effectively teach debugging to these students. Method: We collected log data from 73 students attempting a set of debugging exercises using an online code editor. We inductively analysed these logs using qualitative content analysis, generating a categorisation of the debugging behaviours observed. Findings: A range of behaviours were exhibited by students, skewed towards being ineffective. Most students were able to partially locate errors but often struggled to resolve them, sometimes introducing additional errors in the process. We argue that students struggling to debug possess fragile knowledge, a lens through which we view the results. Implications: This paper highlights some of the difficulties K-12 learners have when debugging in a text-based programming language. We argue, like much related work, that effective debugging strategies should be explicitly taught, while ineffective strategies should be discouraged.
>
---
#### [new 003] PAPPL: Personalized AI-Powered Progressive Learning Platform
- **分类: cs.CY; cs.AI; cs.ET**

- **简介: 论文提出PAPPL平台，作为个性化AI教学系统，解决工程教育中标准化框架导致的个性化学习不足问题，通过整合AI模块与GPT-4o技术，提供动态反馈与教学分析，提升学习效果。（99字）**

- **链接: [http://arxiv.org/pdf/2508.14109v1](http://arxiv.org/pdf/2508.14109v1)**

> **作者:** Shayan Bafandkar; Sungyong Chung; Homa Khosravian; Alireza Talebpour
>
> **摘要:** Engineering education has historically been constrained by rigid, standardized frameworks, often neglecting students' diverse learning needs and interests. While significant advancements have been made in online and personalized education within K-12 and foundational sciences, engineering education at both undergraduate and graduate levels continues to lag in adopting similar innovations. Traditional evaluation methods, such as exams and homework assignments, frequently overlook individual student requirements, impeding personalized educational experiences. To address these limitations, this paper introduces the Personalized AI-Powered Progressive Learning (PAPPL) platform, an advanced Intelligent Tutoring System (ITS) designed specifically for engineering education. It highlights the development of a scalable, data-driven tutoring environment leveraging cutting-edge AI technology to enhance personalized learning across diverse academic disciplines, particularly in STEM fields. PAPPL integrates core ITS components including the expert module, student module, tutor module, and user interface, and utilizes GPT-4o, a sophisticated large language model (LLM), to deliver context-sensitive and pedagogically sound hints based on students' interactions. The system uniquely records student attempts, detects recurring misconceptions, and generates progressively targeted feedback, providing personalized assistance that adapts dynamically to each student's learning profile. Additionally, PAPPL offers instructors detailed analytics, empowering evidence-based adjustments to teaching strategies. This study provides a fundamental framework for the progression of Generative ITSs scalable to all education levels, delivering important perspectives on personalized progressive learning and the wider possibilities of Generative AI in the field of education.
>
---
#### [new 004] Breakable Machine: A K-12 Classroom Game for Transformative AI Literacy Through Spoofing and eXplainable AI (XAI)
- **分类: cs.CY**

- **简介: 论文设计了一个K-12课堂游戏，通过对抗性玩法和XAI技术，帮助学生理解AI系统的脆弱性与偏见，培养其批判性思维与伦理意识。**

- **链接: [http://arxiv.org/pdf/2508.14201v1](http://arxiv.org/pdf/2508.14201v1)**

> **作者:** Olli Hilke; Nicolas Pope; Juho Kahila; Henriikka Vartiainen; Teemu Roos; Tuomo Parkki; Matti Tedre
>
> **摘要:** This paper, submitted to the special track on resources for teaching AI in K-12, presents an eXplainable AI (XAI)-based classroom game "Breakable Machine" for teaching critical, transformative AI literacy through adversarial play and interrogation of AI systems. Designed for learners aged 10-15, the game invites students to spoof an image classifier by manipulating their appearance or environment in order to trigger high-confidence misclassifications. Rather than focusing on building AI models, this activity centers on breaking them-exposing their brittleness, bias, and vulnerability through hands-on, embodied experimentation. The game includes an XAI view to help students visualize feature saliency, revealing how models attend to specific visual cues. A shared classroom leaderboard fosters collaborative inquiry and comparison of strategies, turning the classroom into a site for collective sensemaking. This approach reframes AI education by treating model failure and misclassification not as problems to be debugged, but as pedagogically rich opportunities to interrogate AI as a sociotechnical system. In doing so, the game supports students in developing data agency, ethical awareness, and a critical stance toward AI systems increasingly embedded in everyday life. The game and its source code are freely available.
>
---
#### [new 005] When Algorithms Infer Gender: Revisiting Computational Phenotyping with Electronic Health Records Data
- **分类: cs.CY**

- **简介: 论文研究如何通过算法从电子健康记录中推断性别，解决数据不全问题，但面临伦理挑战，综述现有方法并提出未来研究方向。**

- **链接: [http://arxiv.org/pdf/2508.14150v1](http://arxiv.org/pdf/2508.14150v1)**

> **作者:** Jessica Gronsbell; Hilary Thurston; Lillian Dong; Vanessa Ferguson; Diksha Sen Chaudhury; Braden O'Neill; Katrina S. Sha; Rebecca Bonneville
>
> **摘要:** Computational phenotyping has emerged as a practical solution to the incomplete collection of data on gender in electronic health records (EHRs). This approach relies on algorithms to infer a patient's gender using the available data in their health record, such as diagnosis codes, medication histories, and information in clinical notes. Although intended to improve the visibility of trans and gender-expansive populations in EHR-based biomedical research, computational phenotyping raises significant methodological and ethical concerns related to the potential misuse of algorithm outputs. In this paper, we review current practices for computational phenotyping of gender and examine its challenges through a critical lens. We also highlight existing recommendations for biomedical researchers and propose priorities for future work in this domain.
>
---
#### [new 006] Incident Analysis for AI Agents
- **分类: cs.CY; cs.AI**

- **简介: 该论文针对AI代理事故分析任务，解决现有报告流程无法处理敏感信息的问题，提出基于系统安全的框架，识别系统、上下文和认知三类因素，并推荐报告内容及信息保留规范。**

- **链接: [http://arxiv.org/pdf/2508.14231v1](http://arxiv.org/pdf/2508.14231v1)**

> **作者:** Carson Ezell; Xavier Roberts-Gaal; Alan Chan
>
> **备注:** 16 pages (10 pages main text), 4 figures, 3 tables. To be published in the Proceedings of the 2025 AAAI/ACM Conference on AI, Ethics, & Society (AIES)
>
> **摘要:** As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that directly or indirectly cause harm. For example, agents could be prompt-injected to exfiltrate private information or make unauthorized purchases. Structured information about such incidents (e.g., user prompts) can help us understand their causes and prevent future occurrences. However, existing incident reporting processes are not sufficient for understanding agent incidents. In particular, such processes are largely based on publicly available data, which excludes useful, but potentially sensitive, information such as an agent's chain of thought or browser history. To inform the development of new, emerging incident reporting processes, we propose an incident analysis framework for agents. Drawing on systems safety approaches, our framework proposes three types of factors that can cause incidents: system-related (e.g., CBRN training data), contextual (e.g., prompt injections), and cognitive (e.g., misunderstanding a user request). We also identify specific information that could help clarify which factors are relevant to a given incident: activity logs, system documentation and access, and information about the tools an agent uses. We provide recommendations for 1) what information incident reports should include and 2) what information developers and deployers should retain and make available to incident investigators upon request. As we transition to a world with more agents, understanding agent incidents will become increasingly crucial for managing risks.
>
---
#### [new 007] Enriching Moral Perspectives on AI: Concepts of Trust amongst Africans
- **分类: cs.CY; cs.AI**

- **简介: 该论文通过调查非洲AI从业者，探索其对AI信任的独特概念，弥补西方主导的研究空白，强调社区价值观对信任构建的影响。**

- **链接: [http://arxiv.org/pdf/2508.14116v1](http://arxiv.org/pdf/2508.14116v1)**

> **作者:** Lameck Mbangula Amugongo; Nicola J Bidwell; Joseph Mwatukange
>
> **摘要:** The trustworthiness of AI is considered essential to the adoption and application of AI systems. However, the meaning of trust varies across industry, research and policy spaces. Studies suggest that professionals who develop and use AI regard an AI system as trustworthy based on their personal experiences and social relations at work. Studies about trust in AI and the constructs that aim to operationalise trust in AI (e.g., consistency, reliability, explainability and accountability). However, the majority of existing studies about trust in AI are situated in Western, Educated, Industrialised, Rich and Democratic (WEIRD) societies. The few studies about trust and AI in Africa do not include the views of people who develop, study or use AI in their work. In this study, we surveyed 157 people with professional and/or educational interests in AI from 25 African countries, to explore how they conceptualised trust in AI. Most respondents had links with workshops about trust and AI in Africa in Namibia and Ghana. Respondents' educational background, transnational mobility, and country of origin influenced their concerns about AI systems. These factors also affected their levels of distrust in certain AI applications and their emphasis on specific principles designed to foster trust. Respondents often expressed that their values are guided by the communities in which they grew up and emphasised communal relations over individual freedoms. They described trust in many ways, including applying nuances of Afro-relationalism to constructs in international discourse, such as reliability and reliance. Thus, our exploratory study motivates more empirical research about the ways trust is practically enacted and experienced in African social realities of AI design, use and governance.
>
---
#### [new 008] Sociotechnical Imaginaries of ChatGPT in Higher Education: The Evolving Media Discourse
- **分类: cs.CY**

- **简介: 该论文通过框架理论与情感分析，研究2022-2024年间美国媒体对ChatGPT在高等教育中的报道，揭示媒体话语从机构响应为主转向谨慎乐观，突出其对AI教育应用的适应性想象。**

- **链接: [http://arxiv.org/pdf/2508.14692v1](http://arxiv.org/pdf/2508.14692v1)**

> **作者:** Yinan Sun; Ali Unlu; Aditya Johri
>
> **备注:** Under review at a conference
>
> **摘要:** This study investigates how U.S. news media framed the use of ChatGPT in higher education from November 2022 to October 2024. Employing Framing Theory and combining temporal and sentiment analysis of 198 news articles, we trace the evolving narratives surrounding generative AI. We found that the media discourse largely centered on institutional responses; policy changes and teaching practices showed the most consistent presence and positive sentiment over time. Conversely, coverage of topics such as human-centered learning, the job market, and skill development appeared more sporadically, with initially uncertain portrayals gradually shifting toward cautious optimism. Importantly, media sentiment toward ChatGPT's role in college admissions remained predominantly negative. Our findings suggest that media narratives prioritize institutional responses to generative AI over long-term, broader ethical, social, and labor-related implications, shaping an emerging sociotechnical imaginary that frames generative AI in education primarily through the lens of adaptation and innovation.
>
---
#### [new 009] Challenges and Opportunities for Participatory Design of Conversational Agents for Young People's Wellbeing
- **分类: cs.HC; cs.CY**

- **简介: 论文研究如何通过参与式设计对话代理支持青少年福祉，探索AI技术在不同文化背景下促进儿童心理健康的挑战与机遇。**

- **链接: [http://arxiv.org/pdf/2508.14787v1](http://arxiv.org/pdf/2508.14787v1)**

> **作者:** Natalia Kucirkova; Alexis Hiniker; Megumi Ishikawa; Sho Tsuji; Aayushi Dangol; Robert Wolfe
>
> **备注:** Presented at the AI4CW workshop at ACM IDC 2025
>
> **摘要:** This paper outlines the challenges and opportunities of research on conversational agents with children and young people across four countries, exploring the ways AI technologies can support children's well-being across social and cultural contexts.
>
---
#### [new 010] GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs
- **分类: cs.CL; cs.CY**

- **简介: 该论文构建了GRILE基准，用于评估LLMs在罗马尼亚语法推理与解释任务中的性能，解决低资源语言教育NLP的可信度问题，通过1151道试题测试模型准确性及解释质量，并分析错误根源。**

- **链接: [http://arxiv.org/pdf/2508.14279v1](http://arxiv.org/pdf/2508.14279v1)**

> **作者:** Adrian-Marius Dumitran; Alexandra-Mihaela Danila; Angela-Liliana Dumitran
>
> **备注:** Accepted as long paper @RANLP2025
>
> **摘要:** LLMs (Large language models) have revolutionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admissions). GRILE enables us to probe two complementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) producing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings expose open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation.
>
---
#### [new 011] Comparing energy consumption and accuracy in text classification inference
- **分类: cs.CL; cs.CY**

- **简介: 该论文针对文本分类推理任务，系统评估不同模型架构和硬件配置下的能耗与准确率权衡，发现大模型能耗高且准确率较低，运行时间与能耗强相关，为高效NLP应用提供优化依据。**

- **链接: [http://arxiv.org/pdf/2508.14170v1](http://arxiv.org/pdf/2508.14170v1)**

> **作者:** Johannes Zschache; Tilman Hartwig
>
> **备注:** Key results in Figure 1, submitted to Nature Communications, 25 pages
>
> **摘要:** The increasing deployment of large language models (LLMs) in natural language processing (NLP) tasks raises concerns about energy efficiency and sustainability. While prior research has largely focused on energy consumption during model training, the inference phase has received comparatively less attention. This study systematically evaluates the trade-offs between model accuracy and energy consumption in text classification inference across various model architectures and hardware configurations. Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy. We observe substantial variability in inference energy consumption ($<$mWh to $>$kWh), influenced by model type, model size, and hardware specifications. Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible. These findings have implications for sustainable AI development, providing actionable insights for researchers, industry practitioners, and policymakers seeking to balance performance and resource efficiency in NLP applications.
>
---
#### [new 012] Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach
- **分类: cs.CL; cs.CY**

- **简介: 论文通过构建分类法评估斯瓦希里语NLP的社会语言学多样性，解决现有评估忽视方言差异的问题，收集2170条多变体文本，分析模型错误以改进文化导向的评估框架。**

- **链接: [http://arxiv.org/pdf/2508.14051v1](http://arxiv.org/pdf/2508.14051v1)**

> **作者:** Kezia Oketch; John P. Lalor; Ahmed Abbasi
>
> **摘要:** We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing gaps in sociolinguistic diversity. Drawing on health-related psychometric tasks, we collect a dataset of 2,170 free-text responses from Kenyan speakers. The data exhibits tribal influences, urban vernacular, code-mixing, and loanwords. We develop a structured taxonomy and use it as a lens for examining model prediction errors across pre-trained and instruction-tuned language models. Our findings advance culturally grounded evaluation frameworks and highlight the role of sociolinguistic variation in shaping model performance.
>
---
#### [new 013] ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 论文设计ZPD-SCA基准测试，旨在评估LLMs在中文阅读理解难度评估中的能力，揭示其在匹配学生认知阶段（ZPD）时的不足，通过实验发现模型在零样本场景下表现差，但通过上下文示例可显著提升，但仍存在系统偏差。**

- **链接: [http://arxiv.org/pdf/2508.14377v1](http://arxiv.org/pdf/2508.14377v1)**

> **作者:** Wenhan Dong; Zhen Sun; Yuemeng Zhao; Zifan Peng; Jun Wu; Jingyi Zheng; Yule Liu; Xinlei He; Yu Wang; Ruiming Wang; Xinyi Huang; Lei Mo
>
> **摘要:** Large language models (LLMs) have demonstrated potential in educational applications, yet their capacity to accurately assess the cognitive alignment of reading materials with students' developmental stages remains insufficiently explored. This gap is particularly critical given the foundational educational principle of the Zone of Proximal Development (ZPD), which emphasizes the need to match learning resources with Students' Cognitive Abilities (SCA). Despite the importance of this alignment, there is a notable absence of comprehensive studies investigating LLMs' ability to evaluate reading comprehension difficulty across different student age groups, especially in the context of Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel benchmark specifically designed to assess stage-level Chinese reading comprehension difficulty. The benchmark is annotated by 60 Special Grade teachers, a group that represents the top 0.15% of all in-service teachers nationwide. Experimental results reveal that LLMs perform poorly in zero-shot learning scenarios, with Qwen-max and GLM even falling below the probability of random guessing. When provided with in-context examples, LLMs performance improves substantially, with some models achieving nearly double the accuracy of their zero-shot baselines. These results reveal that LLMs possess emerging abilities to assess reading difficulty, while also exposing limitations in their current training for educationally aligned judgment. Notably, even the best-performing models display systematic directional biases, suggesting difficulties in accurately aligning material difficulty with SCA. Furthermore, significant variations in model performance across different genres underscore the complexity of task. We envision that ZPD-SCA can provide a foundation for evaluating and improving LLMs in cognitively aligned educational applications.
>
---
## 更新

#### [replaced 001] Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking
- **分类: cs.HC; cs.AI; cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2403.09717v2](http://arxiv.org/pdf/2403.09717v2)**

> **作者:** Yiyang Gu; Yougen Zhou; Qin Chen; Ningning Zhou; Jie Zhou; Aimin Zhou; Liang He
>
> **备注:** Accepted by NLPCC 2025
>
> **摘要:** Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist response generation at each turn to simulate the psychiatrist. Experimental results on the existing benchmark show that our proposed method boosts the performance of all subtasks in depression-diagnosis-oriented chat.
>
---
#### [replaced 002] Biased AI improves human decision-making but reduces trust
- **分类: cs.HC; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2508.09297v3](http://arxiv.org/pdf/2508.09297v3)**

> **作者:** Shiyang Lai; Junsol Kim; Nadav Kunievsky; Yujin Potter; James Evans
>
> **摘要:** Current AI systems minimize risk by enforcing ideological neutrality, yet this may introduce automation bias by suppressing cognitive engagement in human decision-making. We conducted randomized trials with 2,500 participants to test whether culturally biased AI enhances human decision-making. Participants interacted with politically diverse GPT-4o variants on information evaluation tasks. Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias compared to non-biased counterparts, with amplified benefits when participants encountered opposing views. These gains carried a trust penalty: participants underappreciated biased AI and overcredited neutral systems. Exposing participants to two AIs whose biases flanked human perspectives closed the perception-performance gap. These findings complicate conventional wisdom about AI neutrality, suggesting that strategic integration of diverse cultural biases may foster improved and resilient human decision-making.
>
---
#### [replaced 003] Generative AI in K-12 Education: The CyberScholar Initiative
- **分类: cs.CY; cs.AI; cs.HC**

- **链接: [http://arxiv.org/pdf/2502.19422v3](http://arxiv.org/pdf/2502.19422v3)**

> **作者:** Vania Castro; Ana Karina de Oliveira Nascimento; Raigul Zheldibayeva; Duane Searsmith; Akash Saini; Bill Cope; Mary Kalantzis
>
> **摘要:** This paper focuses on the piloting of CyberScholar, a Generative AI assistant tool that aims to provide formative feedback on writing in K-12 contexts. Specifically, this study explores how students worked with CyberScholar in diverse subject areas, including English Language Arts, Social Studies, and Modern World History classes in Grades 7, 8, 10, and 11 in three schools in the Midwest and one in the Northwest of the United States. This paper focuses on CyberScholar's potential to support K-12 students' writing in diverse subject areas requiring written assignments. Data were collected through implementation observations, surveys, and interviews by participating 121 students and 4 teachers. Thematic qualitative analysis revealed that the feedback tool was perceived as a valuable tool for supporting student writing through detailed feedback, enhanced interactivity, and alignment with rubric criteria. Students appreciated the tool's guidance in refining their writing. For the students, the assistant tool suggests restructuring feedback as a dynamic, dialogic process rather than a static evaluation, a shift that aligns with the cyber-social learning idea, self-regulation, and metacognition. For the teaching side, the findings indicate a shift in teachers' roles, from serving primarily as evaluators to guiding AI feedback processes that foster better student writing and critical thinking.
>
---
#### [replaced 004] From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems
- **分类: cs.CY; cs.CE; cs.CL; cs.HC; cs.RO**

- **链接: [http://arxiv.org/pdf/2507.04996v4](http://arxiv.org/pdf/2507.04996v4)**

> **作者:** Jiangbo Yu
>
> **摘要:** Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are viewed as vehicular systems capable of perceiving their environment and executing pre-programmed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 0 to 5); Examples of this outpace include the interaction with humans with natural language, goal adaptation, contextual reasoning, external tool use, and unseen ethical dilemma handling, largely empowered by multi-modal large language models (LLMs). These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this gap, this paper introduces the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI systems to reason, adapt, and interact within complex environments. This paper proposes the term AgVs and their distinguishing characteristics from conventional AuVs. It synthesizes relevant advances in integrating LLMs and AuVs and highlights how AgVs might transform future mobility systems and ensure the systems are human-centered. The paper concludes by identifying key challenges in the development and governance of AgVs, and how they can play a significant role in future agentic transportation systems.
>
---
#### [replaced 005] A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2504.16133v2](http://arxiv.org/pdf/2504.16133v2)**

> **作者:** Milad Leyli-abadi; Ricardo J. Bessa; Jan Viebahn; Daniel Boos; Clark Borst; Alberto Castagna; Ricardo Chavarriaga; Mohamed Hassouna; Bruno Lemetayer; Giulia Leto; Antoine Marot; Maroua Meddeb; Manuel Meyer; Viola Schiaffonati; Manuel Schneider; Toni Waefler
>
> **摘要:** The interaction between humans and AI in safety-critical systems presents a unique set of challenges that remain partially addressed by existing frameworks. These challenges stem from the complex interplay of requirements for transparency, trust, and explainability, coupled with the necessity for robust and safe decision-making. A framework that holistically integrates human and AI capabilities while addressing these concerns is notably required, bridging the critical gaps in designing, deploying, and maintaining safe and effective systems. This paper proposes a holistic conceptual framework for critical infrastructures by adopting an interdisciplinary approach. It integrates traditionally distinct fields such as mathematics, decision theory, computer science, philosophy, psychology, and cognitive engineering and draws on specialized engineering domains, particularly energy, mobility, and aeronautics. Its flexibility is further demonstrated through a case study on power grid management.
>
---
