# 计算机与社会 cs.CY

- **最新发布 16 篇**

- **更新 9 篇**

## 最新发布

#### [new 001] Documenting Patterns of Exoticism of Marginalized Populations within Text-to-Image Generators
- **分类: cs.CY**

- **简介: 该论文旨在探讨文本到图像生成器中非西方群体的异域主义现象，解决AI公平性研究中对文化多样性忽视的问题，通过分析13国图像数据揭示衣着与文化特征的过度放大，验证了其对全球南北差异及弱势群体的双重异化影响，并提出设计改进方案。**

- **链接: [http://arxiv.org/pdf/2508.02937v1](http://arxiv.org/pdf/2508.02937v1)**

> **作者:** Sourojit Ghosh; Sanjana Gautam; Pranav Venkit; Avijit Ghosh
>
> **备注:** Upcoming Publication, AIES 2025
>
> **摘要:** A significant majority of AI fairness research studying the harmful outcomes of GAI tools have overlooked non-Western communities and contexts, necessitating a stronger coverage in this vein. We extend our previous work on exoticism (Ghosh et al., 2024) of 'Global South' countries from across the world, as depicted by GAI tools. We analyze generated images of individuals from 13 countries -- India, Bangladesh, Papua New Guinea, Egypt, Ethiopia, Tunisia, Sudan, Libya, Venezuela, Colombia, Indonesia, Honduras, and Mexico -- performing everyday activities (such as being at home, going to work, getting groceries, etc.), as opposed to images for the same activities being performed by persons from 3 'Global North' countries -- USA, UK, Australia. While outputs for 'Global North' demonstrate a difference across images and people clad in activity-appropriate attire, individuals from 'Global South' countries are depicted in similar attire irrespective of the performed activity, indicative of a pattern of exoticism where attire or other cultural features are overamplified at the cost of accuracy. We further show qualitatively-analyzed case studies that demonstrate how exoticism is not simply performed upon 'Global South' countries but also upon marginalized populations even in Western contexts, as we observe a similar exoticization of Indigenous populations in the 'Global North', and doubly upon marginalized populations within 'Global South' countries. We document implications for harm-aware usage patterns of such tools, and steps towards designing better GAI tools through community-centered endeavors.
>
---
#### [new 002] Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education
- **分类: cs.CY; cs.AI; cs.CL**

- **简介: 该论文提出了一种基于大语言模型的教学评估框架，旨在通过分层总结、匿名化和异常检测提取工程教育中的关键反馈，结合可视化分析提升教学效果，解决了传统人工评估效率低的问题，实现了跨学科、多维度的教学质量监测与持续改进。**

- **链接: [http://arxiv.org/pdf/2508.02731v1](http://arxiv.org/pdf/2508.02731v1)**

> **作者:** Jean-Francois Chamberland; Martin C. Carlisle; Arul Jayaraman; Krishna R. Narayanan; Sunay Palsole; Karan Watson
>
> **摘要:** Evaluating teaching effectiveness at scale remains a persistent challenge for large universities, particularly within engineering programs that enroll tens of thousands of students. Traditional methods, such as manual review of student evaluations, are often impractical, leading to overlooked insights and inconsistent data use. This article presents a scalable, AI-supported framework for synthesizing qualitative student feedback using large language models. The system employs hierarchical summarization, anonymization, and exception handling to extract actionable themes from open-ended comments while upholding ethical safeguards. Visual analytics contextualize numeric scores through percentile-based comparisons, historical trends, and instructional load. The approach supports meaningful evaluation and aligns with best practices in qualitative analysis and educational assessment, incorporating student, peer, and self-reflective inputs without automating personnel decisions. We report on its successful deployment across a large college of engineering. Preliminary validation through comparisons with human reviewers, faculty feedback, and longitudinal analysis suggests that LLM-generated summaries can reliably support formative evaluation and professional development. This work demonstrates how AI systems, when designed with transparency and shared governance, can promote teaching excellence and continuous improvement at scale within academic institutions.
>
---
#### [new 003] Web3 x AI Agents: Landscape, Integrations, and Foundational Challenges
- **分类: cs.CY; cs.AI; econ.GN; q-fin.EC**

- **简介: 该论文分析Web3与AI代理的融合，旨在探索其在去中心化生态中的整合模式及挑战，通过构建税收分类法、系统性地图和四个关键整合维度（金融、治理、安全、可靠性）的研究，揭示了跨领域技术融合的可行性路径与潜在问题。**

- **链接: [http://arxiv.org/pdf/2508.02773v1](http://arxiv.org/pdf/2508.02773v1)**

> **作者:** Yiming Shen; Jiashuo Zhang; Zhenzhe Shao; Wenxuan Luo; Yanlin Wang; Ting Chen; Zibin Zheng; Jiachi Chen
>
> **摘要:** The convergence of Web3 technologies and AI agents represents a rapidly evolving frontier poised to reshape decentralized ecosystems. This paper presents the first and most comprehensive analysis of the intersection between Web3 and AI agents, examining five critical dimensions: landscape, economics, governance, security, and trust mechanisms. Through an analysis of 133 existing projects, we first develop a taxonomy and systematically map the current market landscape (RQ1), identifying distinct patterns in project distribution and capitalization. Building upon these findings, we further investigate four key integrations: (1) the role of AI agents in participating in and optimizing decentralized finance (RQ2); (2) their contribution to enhancing Web3 governance mechanisms (RQ3); (3) their capacity to strengthen Web3 security via intelligent vulnerability detection and automated smart contract auditing (RQ4); and (4) the establishment of robust reliability frameworks for AI agent operations leveraging Web3's inherent trust infrastructure (RQ5). By synthesizing these dimensions, we identify key integration patterns, highlight foundational challenges related to scalability, security, and ethics, and outline critical considerations for future research toward building robust, intelligent, and trustworthy decentralized systems with effective AI agent interactions.
>
---
#### [new 004] Beyond risk: A proto-framework for assessing the societal impact of AI systems
- **分类: cs.CY; cs.AI; cs.ET**

- **简介: 该论文旨在构建一个基于自由概念的AI社会影响评估框架，解决当前风险导向方法不足的问题，通过Kantian哲学理论发展自由作为能力与机会的双重维度，结合可持续发展目标（SDGs）系统性分析AI对社会的影响。**

- **链接: [http://arxiv.org/pdf/2508.03666v1](http://arxiv.org/pdf/2508.03666v1)**

> **作者:** Willem Fourie
>
> **摘要:** In the discourse on AI regulation, 'responsible AI' is the dominant paradigm, with the focus on mitigating the risks related to AI systems. While this focus is important and necessary, it has limited use for a systematic consideration of AI's societal impact. This paper proposes a proto-framework for assessing the societal impact of AI systems by operationalising the concept of freedom. This proto-framework is intended as a step towards a fully operationalised framework to be used in policymaking contexts. By drawing on Kantian philosophy and related contemporary interpretations, freedom is developed as the counterpart to the concept of responsibility. Two dimensions of freedom are developed in further detail: freedom as capability and freedom as opportunity. These two dimensions of freedom are then applied in a proto-framework that systematically considers AI's impact on society using the Sustainable Development Goals. This proto-framework aims to complement current risk-based approaches and thereby offers a first step towards operationalising the concept of freedom in AI regulation.
>
---
#### [new 005] The Architecture of Trust: A Framework for AI-Augmented Real Estate Valuation in the Era of Structured Data
- **分类: cs.CY; cs.AI; cs.CV; I.2.1; H.4.2; K.5.2; I.2.10; I.4.8; K.4.1; J.1**

- **简介: 该论文旨在构建AI-增强房地产估值的框架，解决结构性数据转型与AI能力融合的技术挑战，通过三层次架构整合数据采集、语义理解与认知推理，开发信任机制并提出评价方法，推动系统效率与风险控制。**

- **链接: [http://arxiv.org/pdf/2508.02765v1](http://arxiv.org/pdf/2508.02765v1)**

> **作者:** Petteri Teikari; Mike Jarrell; Maryam Azh; Harri Pesola
>
> **备注:** 46 pages, 6 figures
>
> **摘要:** The Uniform Appraisal Dataset (UAD) 3.6's mandatory 2026 implementation transforms residential property valuation from narrative reporting to structured, machine-readable formats. This paper provides the first comprehensive analysis of this regulatory shift alongside concurrent AI advances in computer vision, natural language processing, and autonomous systems. We develop a three-layer framework for AI-augmented valuation addressing technical implementation and institutional trust requirements. Our analysis reveals how regulatory standardization converging with AI capabilities enables fundamental market restructuring with profound implications for professional practice, efficiency, and systemic risk. We make four key contributions: (1) documenting institutional failures including inter-appraiser variability and systematic biases undermining valuation reliability; (2) developing an architectural framework spanning physical data acquisition, semantic understanding, and cognitive reasoning that integrates emerging technologies while maintaining professional oversight; (3) addressing trust requirements for high-stakes financial applications including regulatory compliance, algorithmic fairness, and uncertainty quantification; (4) proposing evaluation methodologies beyond generic AI benchmarks toward domain-specific protocols. Our findings indicate successful transformation requires not merely technological sophistication but careful human-AI collaboration, creating systems that augment rather than replace professional expertise while addressing historical biases and information asymmetries in real estate markets.
>
---
#### [new 006] Towards a Manifesto for Cyber Humanities: Paradigms, Ethics, and Prospects
- **分类: cs.CY; cs.AI; cs.DL; I.2.0; K.3.m; K.4.0**

- **简介: 该论文旨在构建"Cyber Humanities"新范式，解决传统人文研究在数字时代面临的伦理与技术挑战，通过Decalogue框架整合伦理设计、可持续实践与人类中心主义知识体系，推动算法基础设施的反思与创新。**

- **链接: [http://arxiv.org/pdf/2508.02760v1](http://arxiv.org/pdf/2508.02760v1)**

> **作者:** Giovanni Adorni; Emanuele Bellini
>
> **备注:** 18 pages, 1 table, 48 references, to appear in: 1st. IEEE Int. Conf. on "Cyber Humanities"
>
> **摘要:** The accelerated evolution of digital infrastructures and algorithmic systems is reshaping how the humanities engage with knowledge and culture. Rooted in the traditions of Digital Humanities and Digital Humanism, the concept of "Cyber Humanities" proposes a critical reconfiguration of humanistic inquiry for the post-digital era. This Manifesto introduces a flexible framework that integrates ethical design, sustainable digital practices, and participatory knowledge systems grounded in human-centered approaches. By means of a Decalogue of foundational principles, the Manifesto invites the scientific community to critically examine and reimagine the algorithmic infrastructures that influence culture, creativity, and collective memory. Rather than being a simple extension of existing practices, "Cyber Humanities" should be understood as a foundational paradigm for humanistic inquiry in a computationally mediated world. Keywords: Cyber Humanities, Digital Humanities, Transdisciplinary Epistemology, Algorithmic Reflexivity, Human-centered AI, Ethics-by-Design, Knowledge Ecosystems, Digital Sovereignty, Cognitive Infrastructures
>
---
#### [new 007] The Silicon Reasonable Person: Can AI Predict How Ordinary People Judge Reasonableness?
- **分类: cs.CY; cs.AI**

- **简介: 该论文探讨了大语言模型是否能学习识别人类判断合理性中的决策架构，并通过实验验证其优先考虑社会信号的能力，以解决法律系统中法官直觉与社会认知差异的问题。**

- **链接: [http://arxiv.org/pdf/2508.02766v1](http://arxiv.org/pdf/2508.02766v1)**

> **作者:** Yonathan A. Arbel
>
> **备注:** 45 pages, 8 figures
>
> **摘要:** In everyday life, people make countless reasonableness judgments that determine appropriate behavior in various contexts. Predicting these judgments challenges the legal system, as judges' intuitions may not align with broader societal views. This Article investigates whether large language models (LLMs) can learn to identify patterns driving human reasonableness judgments. Using randomized controlled trials comparing humans and models across multiple legal contexts with over 10,000 simulated judgments, we demonstrate that certain models capture not just surface-level responses but potentially their underlying decisional architecture. Strikingly, these systems prioritize social cues over economic efficiency in negligence determinations, mirroring human behavior despite contradicting textbook treatments. These findings suggest practical applications: judges could calibrate intuitions against broader patterns, lawmakers could test policy interpretations, and resource-constrained litigants could preview argument reception. As AI agents increasingly make autonomous real-world decisions, understanding whether they've internalized recognizable ethical frameworks becomes essential for anticipating their behavior.
>
---
#### [new 008] Advancing Science- and Evidence-based AI Policy
- **分类: cs.CY**

- **简介: 该论文旨在推动科学与证据驱动的AI政策，解决如何优化证据与政策关系以应对AI机遇与挑战的任务，通过研究AI风险及倡导政策应对策略。**

- **链接: [http://arxiv.org/pdf/2508.02748v1](http://arxiv.org/pdf/2508.02748v1)**

> **作者:** Rishi Bommasani; Sanjeev Arora; Jennifer Chayes; Yejin Choi; Mariano-Florentino Cuéllar; Li Fei-Fei; Daniel E. Ho; Dan Jurafsky; Sanmi Koyejo; Hima Lakkaraju; Arvind Narayanan; Alondra Nelson; Emma Pierson; Joelle Pineau; Scott Singer; Gaël Varoquaux; Suresh Venkatasubramanian; Ion Stoica; Percy Liang; Dawn Song
>
> **备注:** This is the author's version of the work. It is posted here by permission of the AAAS for personal use, not for redistribution. The definitive version was published in Science on July 31, 2025
>
> **摘要:** AI policy should advance AI innovation by ensuring that its potential benefits are responsibly realized and widely shared. To achieve this, AI policymaking should place a premium on evidence: Scientific understanding and systematic analysis should inform policy, and policy should accelerate evidence generation. But policy outcomes reflect institutional constraints, political dynamics, electoral pressures, stakeholder interests, media environment, economic considerations, cultural contexts, and leadership perspectives. Adding to this complexity is the reality that the broad reach of AI may mean that evidence and policy are misaligned: Although some evidence and policy squarely address AI, much more partially intersects with AI. Well-designed policy should integrate evidence that reflects scientific understanding rather than hype. An increasing number of efforts address this problem by often either (i) contributing research into the risks of AI and their effective mitigation or (ii) advocating for policy to address these risks. This paper tackles the hard problem of how to optimize the relationship between evidence and policy to address the opportunities and challenges of increasingly powerful AI.
>
---
#### [new 009] Understanding Demand for Shared Autonomous Micro-Mobility
- **分类: cs.ET; cs.CY**

- **简介: 该论文旨在研究共享自动驾驶微交通系统的需求行为及环境影响，解决传统研究对替代模式、用户决策和环境效益的缺失，通过实证方法设计情境问卷并构建混合模型分析结果，揭示服务设计对采纳率与碳排放的影响机制。**

- **链接: [http://arxiv.org/pdf/2508.03521v1](http://arxiv.org/pdf/2508.03521v1)**

> **作者:** Naroa Coretti Sanchez; Kent Larson
>
> **摘要:** This study examines the behavioral and environmental implications of shared autonomous micro-mobility systems, focusing on autonomous bicycles and their integration with transit in the U.S. While prior research has addressed operational and lifecycle aspects, a critical gap remains in understanding which modes these services are likely to substitute, who is most inclined to adopt them, and how service attributes influence user decisions. We design a context-aware stated preference survey grounded in real-world trips and estimate discrete choice models, including a hybrid model incorporating latent attitudes. Findings indicate that adoption, mode shift, and environmental impacts are highly sensitive to service design. Scenarios with minimal wait and cost yield high adoption but increase emissions, while moderate waits are more likely to reduce impacts. Adoption likelihood varies with demographic characteristics, and outcomes depend on city type, context, and infrastructure assumptions. These insights can inform the development of more sustainable and equitable mobility systems.
>
---
#### [new 010] Real-World Receptivity to Adaptive Mental Health Interventions: Findings from an In-the-Wild Study
- **分类: cs.HC; cs.AI; cs.CY; eess.SP**

- **简介: 该论文旨在探讨智能设备在真实场景下的心理干预接收度，通过分析用户对即时反馈的接受与可行性的感知，构建基于强化学习的个性化干预系统。研究使用Android应用LogMe收集数据，验证其在动态情境中的有效性。**

- **链接: [http://arxiv.org/pdf/2508.02817v1](http://arxiv.org/pdf/2508.02817v1)**

> **作者:** Nilesh Kumar Sahu; Aditya Sneh; Snehil Gupta; Haroon R Lone
>
> **摘要:** The rise of mobile health (mHealth) technologies has enabled real-time monitoring and intervention for mental health conditions using passively sensed smartphone data. Building on these capabilities, Just-in-Time Adaptive Interventions (JITAIs) seek to deliver personalized support at opportune moments, adapting to users' evolving contexts and needs. Although prior research has examined how context affects user responses to generic notifications and general mHealth messages, relatively little work has explored its influence on engagement with actual mental health interventions. Furthermore, while much of the existing research has focused on detecting when users might benefit from an intervention, less attention has been paid to understanding receptivity, i.e., users' willingness and ability to engage with and act upon the intervention. In this study, we investigate user receptivity through two components: acceptance(acknowledging or engaging with a prompt) and feasibility (ability to act given situational constraints). We conducted a two-week in-the-wild study with 70 students using a custom Android app, LogMe, which collected passive sensor data and active context reports to prompt mental health interventions. The adaptive intervention module was built using Thompson Sampling, a reinforcement learning algorithm. We address four research questions relating smartphone features and self-reported contexts to acceptance and feasibility, and examine whether an adaptive reinforcement learning approach can optimize intervention delivery by maximizing a combined receptivity reward. Our results show that several types of passively sensed data significantly influenced user receptivity to interventions. Our findings contribute insights into the design of context-aware, adaptive interventions that are not only timely but also actionable in real-world settings.
>
---
#### [new 011] Classifying Epistemic Relationships in Human-AI Interaction: An Exploratory Approach
- **分类: cs.HC; cs.AI; cs.CY**

- **简介: 该论文旨在探索人类-AI互动中的语义关系，解决如何理解AI在知识生产中的角色问题，通过开发五种关系类型并分析动态性与情境依赖性，提出新的AI使用框架以丰富HCI的理解维度。**

- **链接: [http://arxiv.org/pdf/2508.03673v1](http://arxiv.org/pdf/2508.03673v1)**

> **作者:** Shengnan Yang; Rongqian Ma
>
> **摘要:** As AI systems become integral to knowledge-intensive work, questions arise not only about their functionality but also their epistemic roles in human-AI interaction. While HCI research has proposed various AI role typologies, it often overlooks how AI reshapes users' roles as knowledge contributors. This study examines how users form epistemic relationships with AI-how they assess, trust, and collaborate with it in research and teaching contexts. Based on 31 interviews with academics across disciplines, we developed a five-part codebook and identified five relationship types: Instrumental Reliance, Contingent Delegation, Co-agency Collaboration, Authority Displacement, and Epistemic Abstention. These reflect variations in trust, assessment modes, tasks, and human epistemic status. Our findings show that epistemic roles are dynamic and context-dependent. We argue for shifting beyond static metaphors of AI toward a more nuanced framework that captures how humans and AI co-construct knowledge, enriching HCI's understanding of the relational and normative dimensions of AI use.
>
---
#### [new 012] When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025
- **分类: cs.CL; cs.CY; cs.HC**

- **简介: 该论文分析了AI艺术领域中艺术家对创作权、伦理等问题的关注，通过数据挖掘揭示技术术语对公众话语的边缘化功能，并提出Bertopic方法和多模态基准以促进更透明的AI创作讨论。**

- **链接: [http://arxiv.org/pdf/2508.03037v1](http://arxiv.org/pdf/2508.03037v1)**

> **作者:** Ariya Mukherjee-Gandhi; Oliver Muellerklein
>
> **备注:** 18 pages, 5 figures, 5 tables
>
> **摘要:** As generative AI continues to reshape artistic production and alternate modes of human expression, artists whose livelihoods are most directly affected have raised urgent concerns about consent, transparency, and the future of creative labor. However, the voices of artists are often marginalized in dominant public and scholarly discourse. This study presents a twelve-year analysis, from 2013 to 2025, of English-language discourse surrounding AI-generated art. It draws from 439 curated 500-word excerpts sampled from opinion articles, news reports, blogs, legal filings, and spoken-word transcripts. Through a reproducible methodology, we identify five stable thematic clusters and uncover a misalignment between artists' perceptions and prevailing media narratives. Our findings highlight how the use of technical jargon can function as a subtle form of gatekeeping, often sidelining the very issues artists deem most urgent. Our work provides a BERTopic-based methodology and a multimodal baseline for future research, alongside a clear call for deeper, transparency-driven engagement with artist perspectives in the evolving AI-creative landscape.
>
---
#### [new 013] Who Gets Cited? Gender- and Majority-Bias in LLM-Driven Reference Selection
- **分类: cs.DL; cs.AI; cs.CY**

- **简介: 该论文旨在探讨LLM驱动的参考选择中性别与多数族裔偏见的存在及其影响，通过控制实验验证不同LLM对性别偏见的响应，发现两种形式的偏见（男性偏好和群体倾向），并分析其影响因素及缓解策略。**

- **链接: [http://arxiv.org/pdf/2508.02740v1](http://arxiv.org/pdf/2508.02740v1)**

> **作者:** Jiangen He
>
> **摘要:** Large language models (LLMs) are rapidly being adopted as research assistants, particularly for literature review and reference recommendation, yet little is known about whether they introduce demographic bias into citation workflows. This study systematically investigates gender bias in LLM-driven reference selection using controlled experiments with pseudonymous author names. We evaluate several LLMs (GPT-4o, GPT-4o-mini, Claude Sonnet, and Claude Haiku) by varying gender composition within candidate reference pools and analyzing selection patterns across fields. Our results reveal two forms of bias: a persistent preference for male-authored references and a majority-group bias that favors whichever gender is more prevalent in the candidate pool. These biases are amplified in larger candidate pools and only modestly attenuated by prompt-based mitigation strategies. Field-level analysis indicates that bias magnitude varies across scientific domains, with social sciences showing the least bias. Our findings indicate that LLMs can reinforce or exacerbate existing gender imbalances in scholarly recognition. Effective mitigation strategies are needed to avoid perpetuating existing gender disparities in scientific citation practices before integrating LLMs into high-stakes academic workflows.
>
---
#### [new 014] Can We Fix Social Media? Testing Prosocial Interventions using Generative Social Simulation
- **分类: cs.SI; cs.CY**

- **简介: 该论文旨在探讨社交媒体如何通过生成式社会模拟技术测试积极干预措施，以缓解偏见、促进对话和消除偏见。研究发现，现有干预效果有限，核心问题在于反馈机制的局限性，提出了重新架构平台架构的可能性。**

- **链接: [http://arxiv.org/pdf/2508.03385v1](http://arxiv.org/pdf/2508.03385v1)**

> **作者:** Maik Larooij; Petter Törnberg
>
> **摘要:** Social media platforms have been widely linked to societal harms, including rising polarization and the erosion of constructive debate. Can these problems be mitigated through prosocial interventions? We address this question using a novel method - generative social simulation - that embeds Large Language Models within Agent-Based Models to create socially rich synthetic platforms. We create a minimal platform where agents can post, repost, and follow others. We find that the resulting following-networks reproduce three well-documented dysfunctions: (1) partisan echo chambers; (2) concentrated influence among a small elite; and (3) the amplification of polarized voices - creating a 'social media prism' that distorts political discourse. We test six proposed interventions, from chronological feeds to bridging algorithms, finding only modest improvements - and in some cases, worsened outcomes. These results suggest that core dysfunctions may be rooted in the feedback between reactive engagement and network growth, raising the possibility that meaningful reform will require rethinking the foundational dynamics of platform architecture.
>
---
#### [new 015] Critical Challenges in Content Moderation for People Who Use Drugs (PWUD): Insights into Online Harm Reduction Practices from Moderators
- **分类: cs.HC; cs.CY**

- **简介: 该论文研究了在支持PWUD社区的内容审核中面临的三类挑战（专家风险评估、时间紧急响应、平台政策冲突），并通过访谈揭示其独特性，并提出自动化工具与高阶指令教学的两方面技术革新，旨在优化社会技术设计以提升危害减少实践效果。**

- **链接: [http://arxiv.org/pdf/2508.02868v1](http://arxiv.org/pdf/2508.02868v1)**

> **作者:** Kaixuan Wang; Loraine Clarke; Carl-Cyril J Dreue; Guancheng Zhou; Jason T. Jacques
>
> **备注:** 22 pages
>
> **摘要:** Online communities serve as essential support channels for People Who Use Drugs (PWUD), providing access to peer support and harm reduction information. The moderation of these communities involves consequential decisions affecting member safety, yet existing sociotechnical systems provide insufficient support for moderators. Through interviews with experienced moderators from PWUD forums on Reddit, we analyse the unique nature of this work. We argue that this work constitutes a distinct form of public health intervention characterised by three moderation challenges: the need for specialised, expert risk assessment; time-critical crisis response; and the navigation of a structural conflict between platform policies and community safety goals. We demonstrate how current moderation systems are insufficient in supporting PWUD communities. For example, policies minimising platforms' legal exposure to illicit activities can inadvertently push moderators to implement restrictive rules to protect community's existence, which can limit such a vulnerable group's ability to share potentially life-saving resources online. We conclude by identifying two necessary shifts in sociotechnical design to support moderators' work: first, moving to automated tools that support human sensemaking in contexts with competing interests; and second, shifting from systems that require moderators to perform low-level rule programming to those that enable high-level, example-based instruction. Further, we highlight how the design of sociotechnical systems in online spaces could impact harm reduction efforts aimed at improving health outcomes for PWUD communities.
>
---
#### [new 016] Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs
- **分类: cs.CL; cs.CY**

- **简介: 该论文探讨大语言模型（LLMs）在不同文化背景下对抑郁症症状表达的跨文化模式，旨在验证其能否模仿西方（心理症状）与东方（身体症状）的表达差异。研究通过实验分析发现，LLMs在英语提示下未能有效捕捉文化差异，但提示性语言（如中文、日语等）可改善效果，揭示了低敏感度及文化惯性机制。任务为评估LLMs的文化适应能力，解决如何提升其跨文化理解，工作包括多语言实验与文化因素分析。**

- **链接: [http://arxiv.org/pdf/2508.03247v1](http://arxiv.org/pdf/2508.03247v1)**

> **作者:** Shintaro Sakai; Jisun An; Migyeong Kang; Haewoon Kwak
>
> **摘要:** Prior clinical psychology research shows that Western individuals with depression tend to report psychological symptoms, while Eastern individuals report somatic ones. We test whether Large Language Models (LLMs), which are increasingly used in mental health, reproduce these cultural patterns by prompting them with Western or Eastern personas. Results show that LLMs largely fail to replicate the patterns when prompted in English, though prompting in major Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment in several configurations. Our analysis pinpoints two key reasons for this failure: the models' low sensitivity to cultural personas and a strong, culturally invariant symptom hierarchy that overrides cultural cues. These findings reveal that while prompt language is important, current general-purpose LLMs lack the robust, culture-aware capabilities essential for safe and effective mental health applications.
>
---
## 更新

#### [replaced 001] $\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection
- **分类: cs.SE; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2507.10583v2](http://arxiv.org/pdf/2507.10583v2)**

> **作者:** Daniil Orel; Indraneil Paul; Iryna Gurevych; Preslav Nakov
>
> **摘要:** In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most extensive open data suite for training and evaluating machine-generated code detectors, comprising over a million code samples, seven programming languages, outputs from 43 coding models, and over three real-world coding domains. Alongside fully AI-generated samples, our collection includes human-AI co-authored code, as well as adversarial samples explicitly crafted to evade detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite of encoder-only detectors trained using a multi-task objective over $\texttt{DroidCollection}$. Our experiments show that existing detectors' performance fails to generalise to diverse coding domains and programming languages outside of their narrow training data. Additionally, we demonstrate that while most detectors are easily compromised by humanising the output distributions using superficial prompting and alignment approaches, this problem can be easily amended by training on a small amount of adversarial data. Finally, we demonstrate the effectiveness of metric learning and uncertainty-based resampling as means to enhance detector training on possibly noisy distributions.
>
---
#### [replaced 002] Who Should Run Advanced AI Evaluations -- AISIs?
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2407.20847v3](http://arxiv.org/pdf/2407.20847v3)**

> **作者:** Merlin Stein; Milan Gandhi; Theresa Kriecherbauer; Amin Oueslati; Robert Trager
>
> **备注:** Accepted to AIES 2024 proceedings
>
> **摘要:** Artificial Intelligence (AI) Safety Institutes and governments worldwide are deciding whether they evaluate advanced AI themselves, support a private evaluation ecosystem or do both. Evaluation regimes have been established in a wide range of industry contexts to monitor and evaluate firms' compliance with regulation. Evaluation is a necessary governance tool to understand and manage the risks of a technology. This paper draws from nine such regimes to inform (i) who should evaluate which parts of advanced AI; and (ii) how much capacity public bodies may need to evaluate advanced AI effectively. First, the effective responsibility distribution between public and private evaluators depends heavily on specific industry and evaluation conditions. On the basis of advanced AI's risk profile, the sensitivity of information involved in the evaluation process, and the high costs of verifying safety and benefit claims of AI Labs, we recommend that public bodies become directly involved in safety critical, especially gray- and white-box, AI model evaluations. Governance and security audits, which are well-established in other industry contexts, as well as black-box model evaluations, may be more efficiently provided by a private market of evaluators and auditors under public oversight. Secondly, to effectively fulfil their role in advanced AI audits, public bodies need extensive access to models and facilities. AISI's capacity should scale with the industry's risk level, size and market concentration, potentially requiring 100s of employees for evaluations in large jurisdictions like the EU or US, like in nuclear safety and life sciences.
>
---
#### [replaced 003] A Foundational Schema.org Mapping for a Legal Knowledge Graph: Representing Brazilian Legal Norms as FRBR Works
- **分类: cs.DL; cs.AI; cs.CY; cs.IR**

- **链接: [http://arxiv.org/pdf/2508.00827v2](http://arxiv.org/pdf/2508.00827v2)**

> **作者:** Hudson de Martim
>
> **备注:** Substantial revision. Now grounded in the FRBR model, mapping the legal norm as an abstract Work. Scope narrowed to the Work -> sdo:Legislation mapping (LegislationObject section removed). Emphasizes creating a deterministic 'ground truth' for Legal AI and Graph RAG
>
> **摘要:** Structuring legal norms for machine readability is a critical prerequisite for building advanced AI and information retrieval systems, such as Legal Knowledge Graphs (LKGs). Grounded in the Functional Requirements for Bibliographic Records (FRBR) model, this paper proposes a foundational mapping for the abstract legal Work - which is materialized as the Norm node in our legal Graph RAG framework - to the interoperable schema.org/Legislation vocabulary. Using the Normas.leg.br portal as a practical case study, we demonstrate how to describe this Work entity via JSON-LD, considering stable URN identifiers, inter-norm relationships, and lifecycle properties. This structured, formal approach provides the essential first step toward creating a deterministic and verifiable knowledge graph, which can serve as a formalized "ground truth" for Legal AI applications, overcoming the limitations of purely probabilistic models.
>
---
#### [replaced 004] Beyond Platforms -- Growing Distributed Transaction Networks for Digital Commerce
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2504.18602v2](http://arxiv.org/pdf/2504.18602v2)**

> **作者:** Yvonne Dittrich; Kim Peiter Jørgensen; Ravi Prakash; Willard Rafnsson; Jonas Kastberg Hinrichsen
>
> **备注:** 55 pages, 1 figure, 3 tables. Submitted to Information and Software Technology
>
> **摘要:** We talk of the internet as digital infrastructure; but we leave the building of rails and roads to the quasi-monopolistic platform providers. Decentralised architectures provide a number of advantages: They are potentially more inclusive for small players; more resilient against adversarial events; and seem to generate more innovation. However, it is not well understood how to evolve, adapt and govern decentralised infrastructures. This article reports empirical research on the development and governance of the Beckn Protocol, an open source protocol for decentralised transactions, the successful development of domain-specific adaptations, and implementation and scaling of commercial infrastructures based on it. It explores how the architecture and governance support local innovation for specific business domains, and how the domain-specific innovations feed back into the development of the core concept The research applied a case study approach, combining interviews with core members of the Beckn community; triangulated by interviews with community leaders of domain specific adaptations and by analysis of online documents and the protocol itself. The article shows the possibility of such a decentralised approach to IT Infrastructures. It analyses the Beckn Protocol, domain specific adaptations, and networks built as a software ecosystem. Based on this analysis, a number of generative mechanisms, socio-technical arrangements that support adoption, innovation, and scaling of infrastructures are highlighted.
>
---
#### [replaced 005] Beyond Images: Adaptive Fusion of Visual and Textual Data for Food Classification
- **分类: cs.CV; cs.AI; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2308.02562v4](http://arxiv.org/pdf/2308.02562v4)**

> **作者:** Prateek Mittal; Puneet Goyal; Joohi Chauhan
>
> **摘要:** This study introduces a novel multimodal food recognition framework that effectively combines visual and textual modalities to enhance classification accuracy and robustness. The proposed approach employs a dynamic multimodal fusion strategy that adaptively integrates features from unimodal visual inputs and complementary textual metadata. This fusion mechanism is designed to maximize the use of informative content, while mitigating the adverse impact of missing or inconsistent modality data. The framework was rigorously evaluated on the UPMC Food-101 dataset and achieved unimodal classification accuracies of 73.60% for images and 88.84% for text. When both modalities were fused, the model achieved an accuracy of 97.84%, outperforming several state-of-the-art methods. Extensive experimental analysis demonstrated the robustness, adaptability, and computational efficiency of the proposed settings, highlighting its practical applicability to real-world multimodal food-recognition scenarios.
>
---
#### [replaced 006] The Xeno Sutra: Can Meaning and Value be Ascribed to an AI-Generated "Sacred" Text?
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2507.20525v4](http://arxiv.org/pdf/2507.20525v4)**

> **作者:** Murray Shanahan; Tara Das; Robert Thurman
>
> **摘要:** This paper presents a case study in the use of a large language model to generate a fictional Buddhist "sutra"', and offers a detailed analysis of the resulting text from a philosophical and literary point of view. The conceptual subtlety, rich imagery, and density of allusion found in the text make it hard to causally dismiss on account of its mechanistic origin. This raises questions about how we, as a society, should come to terms with the potentially unsettling possibility of a technology that encroaches on human meaning-making. We suggest that Buddhist philosophy, by its very nature, is well placed to adapt.
>
---
#### [replaced 007] Two Means to an End Goal: Connecting Explainability and Contestability in the Regulation of Public Sector AI
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2504.18236v2](http://arxiv.org/pdf/2504.18236v2)**

> **作者:** Timothée Schmude; Mireia Yurrita; Kars Alfrink; Thomas Le Goff; Sebastian Tschiatschek; Tiphaine Viard
>
> **备注:** 14 pages main text, 4 figures. Supplementary material is provided
>
> **摘要:** Explainability and its emerging counterpart contestability have become important normative and design principles for trustworthy AI as they enable users and subjects to understand and challenge AI decisions. However, realizing these principles is difficult, as they assume different meanings in technical, legal, and organizational dimensions of AI regulation. To resolve this conceptual polysemy, in this paper, we present the findings of an interview study with 14 experts to examine the intersection and implementation of explainability and contestability, and their understanding in different research communities. We outline differentiations between descriptive and normative explainability, judicial and non-judicial channels of contestation, and individual and collective contestation action. We further describe the main points of friction in the realization of both principles, including the alignment between top-down and bottom-up regulation, the assignment of responsibility, and the need for interdisciplinary collaboration. Lastly, we formulate three recommendations for AI policy to implement both principles through a Regulation by Design perspective. We believe our contributions can inform policy-making and regulation of these core principles and enable more effective and equitable design, development, and deployment of trustworthy public AI systems.
>
---
#### [replaced 008] Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power
- **分类: cs.AI; cs.CY; cs.LG; econ.TH; math.OC; 68Txx; I.2**

- **链接: [http://arxiv.org/pdf/2508.00159v2](http://arxiv.org/pdf/2508.00159v2)**

> **作者:** Jobst Heitzig; Ram Potham
>
> **摘要:** Power is a key concept in AI safety: power-seeking as an instrumental goal, sudden or gradual disempowerment of humans, power balance in human-AI interaction and international AI governance. At the same time, power as the ability to pursue diverse goals is essential for wellbeing. This paper explores the idea of promoting both safety and wellbeing by forcing AI agents explicitly to empower humans and to manage the power balance between humans and AI agents in a desirable way. Using a principled, partially axiomatic approach, we design a parametrizable and decomposable objective function that represents an inequality- and risk-averse long-term aggregate of human power. It takes into account humans' bounded rationality and social norms, and, crucially, considers a wide variety of possible human goals. We derive algorithms for computing that metric by backward induction or approximating it via a form of multi-agent reinforcement learning from a given world model. We exemplify the consequences of (softly) maximizing this metric in a variety of paradigmatic situations and describe what instrumental sub-goals it will likely imply. Our cautious assessment is that softly maximizing suitable aggregate metrics of human power might constitute a beneficial objective for agentic AI systems that is safer than direct utility-based objectives.
>
---
#### [replaced 009] Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content
- **分类: cs.CY; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2507.19551v2](http://arxiv.org/pdf/2507.19551v2)**

> **作者:** Ran Tong; Songtao Wei; Jiaqi Liu; Lanruo Wang
>
> **备注:** 9 pages, 1 figure
>
> **摘要:** Hateful memes aimed at LGBTQ\,+ communities often evade detection by tweaking either the caption, the image, or both. We build the first robustness benchmark for this setting, pairing four realistic caption attacks with three canonical image corruptions and testing all combinations on the PrideMM dataset. Two state-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and we introduce a lightweight \textbf{Text Denoising Adapter (TDA)} to enhance the latter's resilience. Across the grid, MemeCLIP degrades more gently, while MemeBLIP2 is particularly sensitive to the caption edits that disrupt its language processing. However, the addition of the TDA not only remedies this weakness but makes MemeBLIP2 the most robust model overall. Ablations reveal that all systems lean heavily on text, but architectural choices and pre-training data significantly impact robustness. Our benchmark exposes where current multimodal safety models crack and demonstrates that targeted, lightweight modules like the TDA offer a powerful path towards stronger defences.
>
---
