# 计算机与社会 cs.CY

- **最新发布 31 篇**

- **更新 6 篇**

## 最新发布

#### [new 001] The Economics of Information Pollution in the Age of AI: A General Equilibrium Approach to Welfare, Measurement, and Policy
- **分类: cs.CY; cs.GT**

- **简介: 该论文研究AI时代信息污染的经济影响，构建一般均衡模型分析其对福利的影响，提出信息污染指数及适应性治理框架，旨在解决信息市场失灵问题。**

- **链接: [http://arxiv.org/pdf/2509.13729v1](http://arxiv.org/pdf/2509.13729v1)**

> **作者:** Yukun Zhang; Tianyang Zhang
>
> **摘要:** The advent of Large Language Models (LLMs) represents a fundamental shock to the economics of information production. By asymmetrically collapsing the marginal cost of generating low-quality, synthetic content while leaving high-quality production costly, AI systematically incentivizes information pollution. This paper develops a general equilibrium framework to analyze this challenge. We model the strategic interactions among a monopolistic platform, profit-maximizing producers, and utility-maximizing consumers in a three-stage game. The core of our model is a production technology with differential elasticities of substitution ($\sigma_L > 1 > \sigma_H$), which formalizes the insight that AI is a substitute for labor in low-quality production but a complement in high-quality creation. We prove the existence of a unique "Polluted Information Equilibrium" and demonstrate its inefficiency, which is driven by a threefold market failure: a production externality, a platform governance failure, and an information commons externality. Methodologically, we derive a theoretically-grounded Information Pollution Index (IPI) with endogenous welfare weights to measure ecosystem health. From a policy perspective, we show that a first-best outcome requires a portfolio of instruments targeting each failure. Finally, considering the challenges of deep uncertainty, we advocate for an adaptive governance framework where policy instruments are dynamically adjusted based on real-time IPI readings, offering a robust blueprint for regulating information markets in the age of AI.
>
---
#### [new 002] The Intercepted Self: How Generative AI Challenges the Dynamics of the Relational Self
- **分类: cs.CY; cs.AI**

- **简介: 论文探讨生成式AI如何影响“关系自我”的三个维度，分析其在任务执行与行为预测中的作用。属于人机交互与自我认知研究，旨在揭示AI对人类自我关系的潜在影响。**

- **链接: [http://arxiv.org/pdf/2509.13391v1](http://arxiv.org/pdf/2509.13391v1)**

> **作者:** Sandrine R. Schiller; Camilo Miguel Signorelli; Filippos Stamatiou
>
> **备注:** 8 pages, accepted at the 8th AAAI/ACM Conference on AI, Ethics, and Society
>
> **摘要:** Generative AI is changing our way of interacting with technology, others, and ourselves. Systems such as Microsoft copilot, Gemini and the expected Apple intelligence still awaits our prompt for action. Yet, it is likely that AI assistant systems will only become better at predicting our behaviour and acting on our behalf. Imagine new generations of generative and predictive AI deciding what you might like best at a new restaurant, picking an outfit that increases your chances on your date with a partner also chosen by the same or a similar system. Far from a science fiction scenario, the goal of several research programs is to build systems capable of assisting us in exactly this manner. The prospect urges us to rethink human-technology relations, but it also invites us to question how such systems might change the way we relate to ourselves. Building on our conception of the relational self, we question the possible effects of generative AI with respect to what we call the sphere of externalised output, the contextual sphere and the sphere of self-relating. In this paper, we attempt to deepen the existential considerations accompanying the AI revolution by outlining how generative AI enables the fulfilment of tasks and also increasingly anticipates, i.e. intercepts, our initiatives in these different spheres.
>
---
#### [new 003] Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews
- **分类: cs.CY; cs.AI**

- **简介: 该论文研究LLM生成的同行评审中的偏见问题，属于公平性分析任务。通过控制实验，发现机构排名和性别偏见，揭示隐性偏见在评分中的影响，旨在提升评审公正性。**

- **链接: [http://arxiv.org/pdf/2509.13400v1](http://arxiv.org/pdf/2509.13400v1)**

> **作者:** Sai Suresh Marchala Vasu; Ivaxi Sheth; Hui-Po Wang; Ruta Binkyte; Mario Fritz
>
> **摘要:** The adoption of large language models (LLMs) is transforming the peer review process, from assisting reviewers in writing more detailed evaluations to generating entire reviews automatically. While these capabilities offer exciting opportunities, they also raise critical concerns about fairness and reliability. In this paper, we investigate bias in LLM-generated peer reviews by conducting controlled experiments on sensitive metadata, including author affiliation and gender. Our analysis consistently shows affiliation bias favoring institutions highly ranked on common academic rankings. Additionally, we find some gender preferences, which, even though subtle in magnitude, have the potential to compound over time. Notably, we uncover implicit biases that become more evident with token-based soft ratings.
>
---
#### [new 004] The threat of analytic flexibility in using large language models to simulate human data: A call to attention
- **分类: cs.CY; cs.AI**

- **简介: 论文探讨使用大语言模型生成合成数据（硅样本）时分析灵活性带来的威胁。研究指出不同配置对模拟数据质量影响显著，无统一最佳方案，呼吁重视分析选择对结果的影响。属于社会科学方法论研究，旨在提升合成数据可靠性。**

- **链接: [http://arxiv.org/pdf/2509.13397v1](http://arxiv.org/pdf/2509.13397v1)**

> **作者:** Jamie Cummins
>
> **备注:** 11 pages, 3 figures
>
> **摘要:** Social scientists are now using large language models to create "silicon samples" - synthetic datasets intended to stand in for human respondents, aimed at revolutionising human subjects research. However, there are many analytic choices which must be made to produce these samples. Though many of these choices are defensible, their impact on sample quality is poorly understood. I map out these analytic choices and demonstrate how a very small number of decisions can dramatically change the correspondence between silicon samples and human data. Configurations (N = 252) varied substantially in their capacity to estimate (i) rank ordering of participants, (ii) response distributions, and (iii) between-scale correlations. Most critically, configurations were not consistent in quality: those that performed well on one dimension often performed poorly on another, implying that there is no "one-size-fits-all" configuration that optimises the accuracy of these samples. I call for greater attention to the threat of analytic flexibility in using silicon samples.
>
---
#### [new 005] Interleaving Natural Language Prompting with Code Editing for Solving Programming Tasks with Generative AI Models
- **分类: cs.CY**

- **简介: 该论文研究学生在编程任务中如何结合自然语言提示与代码编辑。通过分析13,305次互动，发现学生常用提示生成初始代码，再通过简短编辑调试。任务复杂度越高，手动编辑越多，高水平学生更依赖提示，低水平学生更多编辑。研究揭示了两种方式在AI辅助编程中的互补作用。**

- **链接: [http://arxiv.org/pdf/2509.14088v1](http://arxiv.org/pdf/2509.14088v1)**

> **作者:** Victor-Alexandru Pădurean; Paul Denny; Andrew Luxton-Reilly; Alkis Gotovos; Adish Singla
>
> **摘要:** Nowadays, computing students often rely on both natural-language prompting and manual code editing to solve programming tasks. Yet we still lack a clear understanding of how these two modes are combined in practice, and how their usage varies with task complexity and student ability. In this paper, we investigate this through a large-scale study in an introductory programming course, collecting 13,305 interactions from 355 students during a three-day laboratory activity. Our analysis shows that students primarily use prompting to generate initial solutions, and then often enter short edit-run loops to refine their code following a failed execution. We find that manual editing becomes more frequent as task complexity increases, but most edits remain concise, with many affecting a single line of code. Higher-performing students tend to succeed using prompting alone, while lower-performing students rely more on edits. Student reflections confirm that prompting is helpful for structuring solutions, editing is effective for making targeted corrections, while both are useful for learning. These findings highlight the role of manual editing as a deliberate last-mile repair strategy, complementing prompting in AI-assisted programming workflows.
>
---
#### [new 006] Towards an AI-Augmented Textbook
- **分类: cs.CY; cs.HC**

- **简介: 论文提出利用生成式AI构建“Learn Your Way”系统，以增强教科书的多样性和个性化。该任务旨在解决传统教科书无法灵活适应学习需求的问题，通过AI实现内容扩展与个性化，提升教学效果。**

- **链接: [http://arxiv.org/pdf/2509.13348v1](http://arxiv.org/pdf/2509.13348v1)**

> **作者:** LearnLM Team; Google; :; Amy Wang; Anna Iurchenko; Anisha Choudhury; Alicia Martín; Amir Globerson; Avinatan Hassidim; Ayça Çakmakli; Ayelet Shasha Evron; Charlie Yang; Courtney Heldreth; Diana Akrong; Gal Elidan; Hairong Mu; Ian Li; Ido Cohen; Katherine Chou; Komal Singh; Lev Borovoi; Lidan Hackmon; Lior Belinsky; Michael Fink; Niv Efron; Preeti Singh; Rena Levitt; Shashank Agarwal; Shay Sharon; Tracey Lee-Joe; Xiaohong Hao; Yael Gold-Zamir; Yael Haramaty; Yishay Mor; Yoav Bar Sinai; Yossi Matias
>
> **摘要:** Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. Any new material or alternative representation requires arduous human effort, so that textbooks cannot be adapted in a scalable manner. We present an approach for transforming and augmenting textbooks using generative AI, adding layers of multiple representations and personalization while maintaining content integrity and quality. We refer to the system built with this approach as Learn Your Way. We report pedagogical evaluations of the different transformations and augmentations, and present the results of a a randomized control trial, highlighting the advantages of learning with Learn Your Way over regular textbook usage.
>
---
#### [new 007] Evaluating undergraduate mathematics examinations in the era of generative AI: a curriculum-level case study
- **分类: cs.CY; cs.AI**

- **简介: 该论文评估生成式AI在无监督环境下完成大学数学考试的表现，探讨传统闭卷考试的教育价值。通过模拟开放环境下的AI答题，分析其成绩与一致性，指出需改革当前评估方式以适应AI时代。**

- **链接: [http://arxiv.org/pdf/2509.13359v1](http://arxiv.org/pdf/2509.13359v1)**

> **作者:** Benjamin J. Walker; Beatriz Navarro Lameda; Ruth A. Reynolds
>
> **摘要:** Generative artificial intelligence (GenAI) tools such as OpenAI's ChatGPT are transforming the educational landscape, prompting reconsideration of traditional assessment practices. In parallel, universities are exploring alternatives to in-person, closed-book examinations, raising concerns about academic integrity and pedagogical alignment in uninvigilated settings. This study investigates whether traditional closed-book mathematics examinations retain their pedagogical relevance when hypothetically administered in uninvigilated, open-book settings with GenAI access. Adopting an empirical approach, we generate, transcribe, and blind-mark GenAI submissions to eight undergraduate mathematics examinations at a Russel Group university, spanning the entirety of the first-year curriculum. By combining independent GenAI responses to individual questions, we enable a meaningful evaluation of GenAI performance, both at the level of modules and across the first-year curriculum. We find that GenAI attainment is at the level of a first-class degree, though current performance can vary between modules. Further, we find that GenAI performance is remarkably consistent when viewed across the entire curriculum, significantly more so than that of students in invigilated examinations. Our findings evidence the need for redesigning assessments in mathematics for unsupervised settings, and highlight the potential reduction in pedagogical value of current standards in the era of generative artificial intelligence.
>
---
#### [new 008] CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI
- **分类: cs.CY; cs.CL**

- **简介: 该论文提出CogniAlign框架，通过多学科代理协同推理解决AI道德对齐问题。基于自然主义道德实在论，以生存性为根基，提升AI道德判断的透明度与准确性，优于GPT-4o。**

- **链接: [http://arxiv.org/pdf/2509.13356v1](http://arxiv.org/pdf/2509.13356v1)**

> **作者:** Hasin Jawad Ali; Ilhamul Azam; Ajwad Abrar; Md. Kamrul Hasan; Hasan Mahmud
>
> **摘要:** The challenge of aligning artificial intelligence (AI) with human values persists due to the abstract and often conflicting nature of moral principles and the opacity of existing approaches. This paper introduces CogniAlign, a multi-agent deliberation framework based on naturalistic moral realism, that grounds moral reasoning in survivability, defined across individual and collective dimensions, and operationalizes it through structured deliberations among discipline-specific scientist agents. Each agent, representing neuroscience, psychology, sociology, and evolutionary biology, provides arguments and rebuttals that are synthesized by an arbiter into transparent and empirically anchored judgments. We evaluate CogniAlign on classic and novel moral questions and compare its outputs against GPT-4o using a five-part ethical audit framework. Results show that CogniAlign consistently outperforms the baseline across more than sixty moral questions, with average performance gains of 16.2 points in analytic quality, 14.3 points in breadth, and 28.4 points in depth of explanation. In the Heinz dilemma, for example, CogniAlign achieved an overall score of 89.2 compared to GPT-4o's 69.2, demonstrating a decisive advantage in handling moral reasoning. By reducing black-box reasoning and avoiding deceptive alignment, CogniAlign highlights the potential of interdisciplinary deliberation as a scalable pathway for safe and transparent AI alignment.
>
---
#### [new 009] To whom did my vote go?
- **分类: cs.CY; cs.GT**

- **简介: 论文设计了一个演示系统，用于展示单转让投票（STV）中选民投票如何在多轮计票中转移和影响候选人得票。该系统帮助用户理解其投票在选举中的具体贡献，属于选举算法可视化任务，解决STV计票过程不透明的问题。**

- **链接: [http://arxiv.org/pdf/2509.13370v1](http://arxiv.org/pdf/2509.13370v1)**

> **作者:** Andrew Conway; Michelle Blom; Alexander Ek; Peter Stuckey; Vanessa Teague; Damjan Vukcevic
>
> **摘要:** Single Transferable Vote (STV) counting, used in several jurisdictions in Australia, is a system for choosing multiple election winners given voters' preferences among candidates. The system is complex and it is not always obvious how an individual's vote contributes to candidates' tallies across rounds of tabulation. This short paper presents a demonstration system that allows voters to enter an example vote in a past Australian STV election, and see: (i)~how that vote would have been transferred between candidates; and (ii)~how much that vote would have contributed to the tallies of relevant candidates, across rounds of tabulation.
>
---
#### [new 010] Behind India's ChatGPT Conversations: A Retrospective Analysis of 238 Unedited User Prompts
- **分类: cs.CY; H.5.2; K.4.3; J.4**

- **简介: 该论文分析印度城市专业人士使用ChatGPT的真实行为，通过238条未编辑用户提示，揭示其日常高频使用、跨领域整合及文化适应策略，提出真实AI使用模式的新研究方法。**

- **链接: [http://arxiv.org/pdf/2509.13337v1](http://arxiv.org/pdf/2509.13337v1)**

> **作者:** Kalyani Khona
>
> **备注:** 11 pages, 1 table. Behavioral analysis of authentic ChatGPT usage patterns among English-speaking urban professionals in India using retrospective prompt collection methodology
>
> **摘要:** Understanding how users authentically interact with Large Language Models (LLMs) remains a significant challenge in human-computer interaction research. Most existing studies rely on self-reported usage patterns or controlled experimental conditions, potentially missing genuine behavioral adaptations. This study presents a behavioral analysis of the use of English-speaking urban professional ChatGPT in India based on 238 authentic, unedited user prompts from 40 participants in 15+ Indian cities, collected using retrospective survey methodology in August 2025. Using authentic retrospective prompt collection via anonymous social media survey to minimize real-time observer effects, we analyzed genuine usage patterns. Key findings include: (1) 85\% daily usage rate (34/40 users) indicating mature adoption beyond experimental use, (2) evidence of cross-domain integration spanning professional, personal, health and creative contexts among the majority of users, (3) 42.5\% (17/40) primarily use ChatGPT for professional workflows with evidence of real-time problem solving integration, and (4) cultural context navigation strategies with users incorporating Indian cultural specifications in their prompts. Users develop sophisticated adaptation techniques and the formation of advisory relationships for personal guidance. The study reveals the progression from experimental to essential workflow dependency, with users treating ChatGPT as an integrated life assistant rather than a specialized tool. However, the findings are limited to urban professionals in English recruited through social media networks and require a larger demographic validation. This work contributes a novel methodology to capture authentic AI usage patterns and provides evidence-based insights into cultural adaptation strategies among this specific demographic of users.
>
---
#### [new 011] Synthetic Data and the Shifting Ground of Truth
- **分类: cs.CY; cs.AI; cs.LG**

- **简介: 论文探讨合成数据如何改变“真实”概念，分析其在机器学习中作为训练数据和真实基准的矛盾性，并研究研究人员如何在缺乏现实参照的情况下构建“地面真相”。属于人工智能与数据伦理领域，解决数据真实性与模型性能之间的矛盾问题。**

- **链接: [http://arxiv.org/pdf/2509.13355v1](http://arxiv.org/pdf/2509.13355v1)**

> **作者:** Dietmar Offenhuber
>
> **备注:** Talk presented at the Society for the Social Studies of Science (4S) 2025 meeting in Seattle, Sept. 3, 2025
>
> **摘要:** The emergence of synthetic data for privacy protection, training data generation, or simply convenient access to quasi-realistic data in any shape or volume complicates the concept of ground truth. Synthetic data mimic real-world observations, but do not refer to external features. This lack of a representational relationship, however, not prevent researchers from using synthetic data as training data for AI models and ground truth repositories. It is claimed that the lack of data realism is not merely an acceptable tradeoff, but often leads to better model performance than realistic data: compensate for known biases, prevent overfitting and support generalization, and make the models more robust in dealing with unexpected outliers. Indeed, injecting noisy and outright implausible data into training sets can be beneficial for the model. This greatly complicates usual assumptions based on which representational accuracy determines data fidelity (garbage in - garbage out). Furthermore, ground truth becomes a self-referential affair, in which the labels used as a ground truth repository are themselves synthetic products of a generative model and as such not connected to real-world observations. My paper examines how ML researchers and practitioners bootstrap ground truth under such paradoxical circumstances without relying on the stable ground of representation and real-world reference. It will also reflect on the broader implications of a shift from a representational to what could be described as a mimetic or iconic concept of data.
>
---
#### [new 012] The Provenance Problem: LLMs and the Breakdown of Citation Norms
- **分类: cs.CY; cs.AI**

- **简介: 论文探讨生成式AI在学术写作中引发的“来源问题”，即AI可能无引用地复现他人思想，导致学术归属混乱。论文分析该现象对学术诚信的影响，并提出应对策略，属于学术伦理与AI技术交叉领域的研究任务。**

- **链接: [http://arxiv.org/pdf/2509.13365v1](http://arxiv.org/pdf/2509.13365v1)**

> **作者:** Brian D. Earp; Haotian Yuan; Julian Koplin; Sebastian Porsdam Mann
>
> **备注:** 9 pages
>
> **摘要:** The increasing use of generative AI in scientific writing raises urgent questions about attribution and intellectual credit. When a researcher employs ChatGPT to draft a manuscript, the resulting text may echo ideas from sources the author has never encountered. If an AI system reproduces insights from, for example, an obscure 1975 paper without citation, does this constitute plagiarism? We argue that such cases exemplify the 'provenance problem': a systematic breakdown in the chain of scholarly credit. Unlike conventional plagiarism, this phenomenon does not involve intent to deceive (researchers may disclose AI use and act in good faith) yet still benefit from the uncredited intellectual contributions of others. This dynamic creates a novel category of attributional harm that current ethical and professional frameworks fail to address. As generative AI becomes embedded across disciplines, the risk that significant ideas will circulate without recognition threatens both the reputational economy of science and the demands of epistemic justice. This Perspective analyzes how AI challenges established norms of authorship, introduces conceptual tools for understanding the provenance problem, and proposes strategies to preserve integrity and fairness in scholarly communication.
>
---
#### [new 013] An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies
- **分类: cs.CY; cs.CL**

- **简介: 论文提出基于LLM的框架，分析协商会议中集体想法的演变过程，解决如何追踪理念演化及影响投票动态的问题，通过分析会议记录揭示高分辨率的协商动态。**

- **链接: [http://arxiv.org/pdf/2509.12577v1](http://arxiv.org/pdf/2509.12577v1)**

> **作者:** Elinor Poole-Dayan; Deb Roy; Jad Kabbara
>
> **摘要:** In an era of increasing societal fragmentation, political polarization, and erosion of public trust in institutions, representative deliberative assemblies are emerging as a promising democratic forum for developing effective policy outcomes on complex global issues. Despite theoretical attention, there remains limited empirical work that systematically traces how specific ideas evolve, are prioritized, or are discarded during deliberation to form policy recommendations. Addressing these gaps, this work poses two central questions: (1) How might we trace the evolution and distillation of ideas into concrete recommendations within deliberative assemblies? (2) How does the deliberative process shape delegate perspectives and influence voting dynamics over the course of the assembly? To address these questions, we develop LLM-based methodologies for empirically analyzing transcripts from a tech-enhanced in-person deliberative assembly. The framework identifies and visualizes the space of expressed suggestions. We also empirically reconstruct each delegate's evolving perspective throughout the assembly. Our methods contribute novel empirical insights into deliberative processes and demonstrate how LLMs can surface high-resolution dynamics otherwise invisible in traditional assembly outputs.
>
---
#### [new 014] Defining a classification system for augmentation technology in socio-technical terms
- **分类: cs.CY**

- **简介: 该论文提出一种从社会技术角度分类增强技术的方法，强调其作为社会、话语和修辞现象的特性。旨在提升数字与AI素养，识别技术设计中的潜在问题。**

- **链接: [http://arxiv.org/pdf/2509.13340v1](http://arxiv.org/pdf/2509.13340v1)**

> **作者:** Isabel Pedersen; Ann Hill Duin
>
> **备注:** 4 pages, accepted version for 2021 IEEE International Symposium on Technology and Society (ISTAS)
>
> **摘要:** This short paper provides a means to classify augmentation technologies to reconceptualize them as sociotechnical, discursive and rhetorical phenomena, rather than only through technological classifications. It identifies a set of value systems that constitute augmentation technologies within discourses, namely, the intent to enhance, automate, and build efficiency. This short paper makes a contribution to digital literacy surrounding augmentation technology emergence, as well as the more specific area of AI literacy, which can help identify unintended consequences implied at the design stages of these technologies.
>
---
#### [new 015] Uncovering AI Governance Themes in EU Policies using BERTopic and Thematic Analysis
- **分类: cs.CY; cs.AI**

- **简介: 该论文通过BERTopic和主题分析，研究欧盟AI治理政策中的主题演变。任务是梳理EU AI政策的核心议题，解决政策内容碎片化问题，工作包括文本分析与量化建模，以揭示治理主题的动态发展。**

- **链接: [http://arxiv.org/pdf/2509.13387v1](http://arxiv.org/pdf/2509.13387v1)**

> **作者:** Delaram Golpayegani; Marta Lasek-Markey; Arjumand Younus; Aphra Kerr; Dave Lewis
>
> **摘要:** The upsurge of policies and guidelines that aim to ensure Artificial Intelligence (AI) systems are safe and trustworthy has led to a fragmented landscape of AI governance. The European Union (EU) is a key actor in the development of such policies and guidelines. Its High-Level Expert Group (HLEG) issued an influential set of guidelines for trustworthy AI, followed in 2024 by the adoption of the EU AI Act. While the EU policies and guidelines are expected to be aligned, they may differ in their scope, areas of emphasis, degrees of normativity, and priorities in relation to AI. To gain a broad understanding of AI governance from the EU perspective, we leverage qualitative thematic analysis approaches to uncover prevalent themes in key EU documents, including the AI Act and the HLEG Ethics Guidelines. We further employ quantitative topic modelling approaches, specifically through the use of the BERTopic model, to enhance the results and increase the document sample to include EU AI policy documents published post-2018. We present a novel perspective on EU policies, tracking the evolution of its approach to addressing AI governance.
>
---
#### [new 016] Perspectives and potential issues in using artificial intelligence for computer science education
- **分类: cs.CY; K.3**

- **简介: 该论文探讨AI在计算机科学教育中的应用与挑战，分析其潜力与问题，提出教育机构应主动适应AI技术，优化教学方法，确保教育公平与质量。属于教育技术研究任务。**

- **链接: [http://arxiv.org/pdf/2509.13730v1](http://arxiv.org/pdf/2509.13730v1)**

> **作者:** Juho Vepsäläinen; Petri Juntunen
>
> **备注:** 12 pages, 1 figure, 2 tables, preprint (not approved for publication yet)
>
> **摘要:** Since its launch in late 2022, ChatGPT has ignited widespread interest in Large Language Models (LLMs) and broader Artificial Intelligence (AI) solutions. As this new wave of AI permeates various sectors of society, we are continually uncovering both the potential and the limitations of existing AI tools. The need for adjustment is particularly significant in Computer Science Education (CSEd), as LLMs have evolved into core coding tools themselves, blurring the line between programming aids and intelligent systems, and reinforcing CSEd's role as a nexus of technology and pedagogy. The findings of our survey indicate that while AI technologies hold potential for enhancing learning experiences, such as through personalized learning paths, intelligent tutoring systems, and automated assessments, there are also emerging concerns. These include the risk of over-reliance on technology, the potential erosion of fundamental cognitive skills, and the challenge of maintaining equitable access to such innovations. Recent advancements represent a paradigm shift, transforming not only the content we teach but also the methods by which teaching and learning take place. Rather than placing the burden of adapting to AI technologies on students, educational institutions must take a proactive role in verifying, integrating, and applying new pedagogical approaches. Such efforts can help ensure that both educators and learners are equipped with the skills needed to navigate the evolving educational landscape shaped by these technological innovations.
>
---
#### [new 017] Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI
- **分类: cs.CY; cs.AI; cs.CL; cs.HC; cs.LG**

- **简介: 论文探讨大语言模型中“准确性悖论”，指出过度依赖准确性会加剧幻觉风险。论文提出幻觉分类，分析其在输出、个体与社会层面的影响，并主张采用多元、情境化治理方法以提升AI可信度。**

- **链接: [http://arxiv.org/pdf/2509.13345v1](http://arxiv.org/pdf/2509.13345v1)**

> **作者:** Zihao Li; Weiwei Yi; Jiahong Chen
>
> **摘要:** As Large Language Models (LLMs) permeate everyday decision-making, their epistemic and societal risks demand urgent scrutiny. Hallucinations, the generation of fabricated, misleading, oversimplified or untrustworthy outputs, has emerged as imperative challenges. While regulatory, academic, and technical discourse position accuracy as the principal benchmark for mitigating such harms, this article contends that overreliance on accuracy misdiagnoses the problem and has counterproductive effect: the accuracy paradox. Drawing on interdisciplinary literatures, this article develops a taxonomy of hallucination types and shows the paradox along three intertwining dimensions: outputs, individuals and society. First, accuracy functions as a superficial proxy for reliability, incentivising the optimisation of rhetorical fluency and surface-level correctness over epistemic trustworthiness. This encourages passive user trust in outputs that appear accurate but epistemically untenable. Second, accuracy as a singular metric fails to detect harms that are not factually false but are nonetheless misleading, value-laden, or socially distorting, including consensus illusions, sycophantic alignment, and subtle manipulation. Third, regulatory overemphasis on accuracy obscures the wider societal consequences of hallucination, including social sorting, privacy violations, equity harms, epistemic convergence that marginalises dissent, reduces pluralism, and causes social deskilling. By examining the EU AI Act, GDPR, and DSA, the article argues that current regulations are not yet structurally equipped to address these epistemic, relational, and systemic harms and exacerbated by the overreliance on accuracy. By exposing such conceptual and practical challenges, this article calls for a fundamental shift towards pluralistic, context-aware, and manipulation-resilient approaches to AI trustworthy governance.
>
---
#### [new 018] AI and the Future of Academic Peer Review
- **分类: cs.CY**

- **简介: 论文探讨AI（特别是大语言模型）在学术同行评审中的应用。旨在解决传统评审效率低、偏见多等问题，分析AI辅助评审的潜力与挑战，并提出治理框架以确保其可靠性与公正性。**

- **链接: [http://arxiv.org/pdf/2509.14189v1](http://arxiv.org/pdf/2509.14189v1)**

> **作者:** Sebastian Porsdam Mann; Mateo Aboy; Joel Jiehao Seah; Zhicheng Lin; Xufei Luo; Dan Rodger; Hazem Zohny; Timo Minssen; Julian Savulescu; Brian D. Earp
>
> **备注:** 34 pages
>
> **摘要:** Peer review remains the central quality-control mechanism of science, yet its ability to fulfill this role is increasingly strained. Empirical studies document serious shortcomings: long publication delays, escalating reviewer burden concentrated on a small minority of scholars, inconsistent quality and low inter-reviewer agreement, and systematic biases by gender, language, and institutional prestige. Decades of human-centered reforms have yielded only marginal improvements. Meanwhile, artificial intelligence, especially large language models (LLMs), is being piloted across the peer-review pipeline by journals, funders, and individual reviewers. Early studies suggest that AI assistance can produce reviews comparable in quality to humans, accelerate reviewer selection and feedback, and reduce certain biases, but also raise distinctive concerns about hallucination, confidentiality, gaming, novelty recognition, and loss of trust. In this paper, we map the aims and persistent failure modes of peer review to specific LLM applications and systematically analyze the objections they raise alongside safeguards that could make their use acceptable. Drawing on emerging evidence, we show that targeted, supervised LLM assistance can plausibly improve error detection, timeliness, and reviewer workload without displacing human judgment. We highlight advanced architectures, including fine-tuned, retrieval-augmented, and multi-agent systems, that may enable more reliable, auditable, and interdisciplinary review. We argue that ethical and practical considerations are not peripheral but constitutive: the legitimacy of AI-assisted peer review depends on governance choices as much as technical capacity. The path forward is neither uncritical adoption nor reflexive rejection, but carefully scoped pilots with explicit evaluation metrics, transparency, and accountability.
>
---
#### [new 019] Reproducible workflow for online AI in digital health
- **分类: cs.CY; cs.AI**

- **简介: 论文提出一种可复现的科学工作流程，用于开发和部署在线AI算法于数字健康干预中。旨在解决在线AI适应性与可复现性之间的平衡问题，确保数据存储、算法行为审计及结果可比性，以支持科学研究与可信优化。**

- **链接: [http://arxiv.org/pdf/2509.13499v1](http://arxiv.org/pdf/2509.13499v1)**

> **作者:** Susobhan Ghosh; Bhanu T. Gulapalli; Daiqi Gao; Asim Gazi; Anna Trella; Ziping Xu; Kelly Zhang; Susan A. Murphy
>
> **摘要:** Online artificial intelligence (AI) algorithms are an important component of digital health interventions. These online algorithms are designed to continually learn and improve their performance as streaming data is collected on individuals. Deploying online AI presents a key challenge: balancing adaptability of online AI with reproducibility. Online AI in digital interventions is a rapidly evolving area, driven by advances in algorithms, sensors, software, and devices. Digital health intervention development and deployment is a continuous process, where implementation - including the AI decision-making algorithm - is interspersed with cycles of re-development and optimization. Each deployment informs the next, making iterative deployment a defining characteristic of this field. This iterative nature underscores the importance of reproducibility: data collected across deployments must be accurately stored to have scientific utility, algorithm behavior must be auditable, and results must be comparable over time to facilitate scientific discovery and trustworthy refinement. This paper proposes a reproducible scientific workflow for developing, deploying, and analyzing online AI decision-making algorithms in digital health interventions. Grounded in practical experience from multiple real-world deployments, this workflow addresses key challenges to reproducibility across all phases of the online AI algorithm development life-cycle.
>
---
#### [new 020] Understanding the Process of Human-AI Value Alignment
- **分类: cs.CY; cs.AI**

- **简介: 该论文通过系统文献综述，分析172篇价值对齐研究文章，提出更精准的定义，并归纳六大主题，旨在解决价值对齐过程中的挑战与认知限制问题。**

- **链接: [http://arxiv.org/pdf/2509.13854v1](http://arxiv.org/pdf/2509.13854v1)**

> **作者:** Jack McKinlay; Marina De Vos; Janina A. Hoffmann; Andreas Theodorou
>
> **备注:** 39 pages, 7 figures
>
> **摘要:** Background: Value alignment in computer science research is often used to refer to the process of aligning artificial intelligence with humans, but the way the phrase is used often lacks precision. Objectives: In this paper, we conduct a systematic literature review to advance the understanding of value alignment in artificial intelligence by characterising the topic in the context of its research literature. We use this to suggest a more precise definition of the term. Methods: We analyse 172 value alignment research articles that have been published in recent years and synthesise their content using thematic analyses. Results: Our analysis leads to six themes: value alignment drivers & approaches; challenges in value alignment; values in value alignment; cognitive processes in humans and AI; human-agent teaming; and designing and developing value-aligned systems. Conclusions: By analysing these themes in the context of the literature we define value alignment as an ongoing process between humans and autonomous agents that aims to express and implement abstract values in diverse contexts, while managing the cognitive limits of both humans and AI agents and also balancing the conflicting ethical and political demands generated by the values in different groups. Our analysis gives rise to a set of research challenges and opportunities in the field of value alignment for future work.
>
---
#### [new 021] All Models Are Wrong, But Can They Be Useful? Lessons from COVID-19 Agent-Based Models: A Systematic Review
- **分类: cs.MA; cs.CY**

- **简介: 该论文系统回顾了536篇新冠ABM研究，评估其对健康政策的实用性。发现多数模型缺乏透明度、代码共享和验证框架，需提升标准以增强未来公共卫生决策支持能力。**

- **链接: [http://arxiv.org/pdf/2509.13346v1](http://arxiv.org/pdf/2509.13346v1)**

> **作者:** Emma Von Hoene; Sara Von Hoene; Szandra Peter; Ethan Hopson; Emily Csizmadia; Faith Fenyk; Kai Barner; Timothy Leslie; Hamdi Kavak; Andreas Zufle; Amira Roess; Taylor Anderson
>
> **备注:** 20 pages, 5 figures, 3 tables, 8 supplemental files
>
> **摘要:** The COVID-19 pandemic prompted a surge in computational models to simulate disease dynamics and guide interventions. Agent-based models (ABMs) are well-suited to capture population and environmental heterogeneity, but their rapid deployment raised questions about utility for health policy. We systematically reviewed 536 COVID-19 ABM studies published from January 2020 to December 2023, retrieved from Web of Science, PubMed, and Wiley on January 30, 2024. Studies were included if they used ABMs to simulate COVID-19 transmission, where reviews were excluded. Studies were assessed against nine criteria of model usefulness, including transparency and re-use, interdisciplinary collaboration and stakeholder engagement, and evaluation practices. Publications peaked in late 2021 and were concentrated in a few countries. Most models explored behavioral or policy interventions (n = 294, 54.85%) rather than real-time forecasting (n = 9, 1.68%). While most described model assumptions (n = 491, 91.60%), fewer disclosed limitations (n = 349, 65.11%), shared code (n = 219, 40.86%), or built on existing models (n = 195, 36.38%). Standardized reporting protocols (n = 36, 6.72%) and stakeholder engagement were rare (13.62%, n = 73). Only 2.24% (n = 12) described a comprehensive validation framework, though uncertainty was often quantified (n = 407, 75.93%). Limitations of this review include underrepresentation of non-English studies, subjective data extraction, variability in study quality, and limited generalizability. Overall, COVID-19 ABMs advanced quickly, but lacked transparency, accessibility, and participatory engagement. Stronger standards are needed for ABMs to serve as reliable decision-support tools in future public health crises.
>
---
#### [new 022] AI For Privacy in Smart Homes: Exploring How Leveraging AI-Powered Smart Devices Enhances Privacy Protection
- **分类: cs.HC; cs.CY**

- **简介: 该论文研究AI如何增强智能家居隐私保护。通过23次访谈和扎根理论分析，提出用户对AI隐私工具的期望及伦理、安全挑战，为设计更隐私友好的智能设备提供指导。**

- **链接: [http://arxiv.org/pdf/2509.14050v1](http://arxiv.org/pdf/2509.14050v1)**

> **作者:** Wael Albayaydh; Ivan Flechais; Rui Zhao; Jood Albayaydh
>
> **摘要:** Privacy concerns and fears of unauthorized access in smart home devices often stem from misunderstandings about how data is collected, used, and protected. This study explores how AI-powered tools can offer innovative privacy protections through clear, personalized, and contextual support to users. Through 23 in-depth interviews with users, AI developers, designers, and regulators, and using Grounded Theory analysis, we identified two key themes: Aspirations for AI-Enhanced Privacy - how users perceive AI's potential to empower them, address power imbalances, and improve ease of use- and AI Ethical, Security, and Regulatory Considerations-challenges in strengthening data security, ensuring regulatory compliance, and promoting ethical AI practices. Our findings contribute to the field by uncovering user aspirations for AI-driven privacy solutions, identifying key security and ethical challenges, and providing actionable recommendations for all stakeholders, particularly targeting smart device designers and AI developers, to guide the co-design of AI tools that enhance privacy protection in smart home devices. By bridging the gap between user expectations, AI capabilities, and regulatory frameworks, this work offers practical insights for shaping the future of privacy-conscious AI integration in smart homes.
>
---
#### [new 023] Framing Migration: A Computational Analysis of UK Parliamentary Discourse
- **分类: cs.CL; cs.CY**

- **简介: 该论文通过计算方法分析英国议会75年移民相关辩论话语，比较美国国会讨论，利用大语言模型标注立场与叙事框架，揭示英国内政党态度趋同及叙事从整合转向安全化趋势，解决政治话语分析的可扩展性问题。**

- **链接: [http://arxiv.org/pdf/2509.14197v1](http://arxiv.org/pdf/2509.14197v1)**

> **作者:** Vahid Ghafouri; Robert McNeil; Teodor Yankov; Madeleine Sumption; Luc Rocher; Scott A. Hale; Adam Mahdi
>
> **摘要:** We present a large-scale computational analysis of migration-related discourse in UK parliamentary debates spanning over 75 years and compare it with US congressional discourse. Using open-weight LLMs, we annotate each statement with high-level stances toward migrants and track the net tone toward migrants across time and political parties. For the UK, we extend this with a semi-automated framework for extracting fine-grained narrative frames to capture nuances of migration discourse. Our findings show that, while US discourse has grown increasingly polarised, UK parliamentary attitudes remain relatively aligned across parties, with a persistent ideological gap between Labour and the Conservatives, reaching its most negative level in 2025. The analysis of narrative frames in the UK parliamentary statements reveals a shift toward securitised narratives such as border control and illegal immigration, while longer-term integration-oriented frames such as social integration have declined. Moreover, discussions of national law about immigration have been replaced over time by international law and human rights, revealing nuances in discourse trends. Taken together broadly, our findings demonstrate how LLMs can support scalable, fine-grained discourse analysis in political and historical contexts.
>
---
#### [new 024] Higher-order Network phenomena of cascading failures in resilient cities
- **分类: cs.SI; cs.CY**

- **简介: 该论文研究城市多模式交通网络中级联失效的高阶网络现象，旨在解决静态网络分析低估系统性风险的问题。通过实证分析，揭示静态结构与动态功能失效的脱节，提出需转向动态模型以提升城市韧性。**

- **链接: [http://arxiv.org/pdf/2509.13808v1](http://arxiv.org/pdf/2509.13808v1)**

> **作者:** Jinghua Song; Yuan Wang; Zimo Yan
>
> **摘要:** Modern urban resilience is threatened by cascading failures in multimodal transport networks, where localized shocks trigger widespread paralysis. Existing models, limited by their focus on pairwise interactions, often underestimate this systemic risk. To address this, we introduce a framework that confronts higher-order network theory with empirical evidence from a large-scale, real-world multimodal transport network. Our findings confirm a fundamental duality: network integration enhances static robustness metrics but simultaneously creates the structural pathways for catastrophic cascades. Crucially, we uncover the source of this paradox: a profound disconnect between static network structure and dynamic functional failure. We provide strong evidence that metrics derived from the network's static blueprint-encompassing both conventional low-order centrality and novel higher-order structural analyses-are fundamentally disconnected from and thus poor predictors of a system's dynamic functional resilience. This result highlights the inherent limitations of static analysis and underscores the need for a paradigm shift towards dynamic models to design and manage truly resilient urban systems.
>
---
#### [new 025] I, Robot? Socio-Technical Implications of Ultra-Personalized AI-Powered AAC; an Autoethnographic Account
- **分类: cs.HC; cs.CY**

- **简介: 该论文探讨超个性化AI辅助沟通系统对隐私与自我表达的影响。通过自传式研究，分析个性化AI在AAC设备中的应用，解决用户编辑负担问题，探索隐私、作者权与控制权的挑战。**

- **链接: [http://arxiv.org/pdf/2509.13671v1](http://arxiv.org/pdf/2509.13671v1)**

> **作者:** Tobias Weinberg; Ricardo E. Gonzalez Penuela; Stephanie Valencia; Thijs Roumen
>
> **备注:** 16 pages, 9 figures
>
> **摘要:** Generic AI auto-complete for message composition often fails to capture the nuance of personal identity, requiring significant editing. While harmless in low-stakes settings, for users of Augmentative and Alternative Communication (AAC) devices, who rely on such systems for everyday communication, this editing burden is particularly acute. Intuitively, the need for edits would be lower if language models were personalized to the communication of the specific user. While technically feasible, such personalization raises socio-technical questions: what are the implications of logging one's own conversations, and how does personalization affect privacy, authorship, and control? We explore these questions through an autoethnographic study in three phases: (1) seven months of collecting all the lead author's AAC communication data, (2) fine-tuning a model on this dataset, and (3) three months of daily use of personalized AI suggestions. We reflect on these phases through continuous diary entries and interaction logs. Our findings highlight the value of personalization as well as implications on privacy, authorship, and blurring the boundaries of self-expression.
>
---
#### [new 026] MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes
- **分类: cs.CV; cs.CY**

- **简介: 论文提出MINGLE模型，用于检测城市场景中语义复杂的社交群体区域。任务是识别图像中基于人际互动的群体区域，解决传统目标检测无法处理的社交关系问题。工作包括设计三阶段流程、构建新数据集，促进相关研究。**

- **链接: [http://arxiv.org/pdf/2509.13484v1](http://arxiv.org/pdf/2509.13484v1)**

> **作者:** Liu Liu; Alexandra Kudaeva; Marco Cipriano; Fatimeh Al Ghannam; Freya Tan; Gerard de Melo; Andres Sevtsuk
>
> **备注:** 13 pages, 4 figures, under review at AAAI 2026
>
> **摘要:** Understanding group-level social interactions in public spaces is crucial for urban planning, informing the design of socially vibrant and inclusive environments. Detecting such interactions from images involves interpreting subtle visual cues such as relations, proximity, and co-movement - semantically complex signals that go beyond traditional object detection. To address this challenge, we introduce a social group region detection task, which requires inferring and spatially grounding visual regions defined by abstract interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf human detection and depth estimation, (2) VLM-based reasoning to classify pairwise social affiliation, and (3) a lightweight spatial aggregation algorithm to localize socially connected groups. To support this task and encourage future research, we present a new dataset of 100K urban street-view images annotated with bounding boxes and labels for both individuals and socially interacting groups. The annotations combine human-created labels and outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage of real-world scenarios.
>
---
#### [new 027] WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data
- **分类: cs.LG; cs.CY**

- **简介: 该论文提出WatchAnxiety模型，利用智能手表数据预测社交焦虑者的即时焦虑状态。属于情绪识别任务，解决实时个性化干预需求。通过迁移学习结合心率数据和特质评估，实现较高准确率的焦虑检测。**

- **链接: [http://arxiv.org/pdf/2509.13725v1](http://arxiv.org/pdf/2509.13725v1)**

> **作者:** Md Sabbir Ahmed; Noah French; Mark Rucker; Zhiyuan Wang; Taylor Myers-Brower; Kaitlyn Petz; Mehdi Boukhechba; Bethany A. Teachman; Laura E. Barnes
>
> **摘要:** Social anxiety is a common mental health condition linked to significant challenges in academic, social, and occupational functioning. A core feature is elevated momentary (state) anxiety in social situations, yet little prior work has measured or predicted fluctuations in this anxiety throughout the day. Capturing these intra-day dynamics is critical for designing real-time, personalized interventions such as Just-In-Time Adaptive Interventions (JITAIs). To address this gap, we conducted a study with socially anxious college students (N=91; 72 after exclusions) using our custom smartwatch-based system over an average of 9.03 days (SD = 2.95). Participants received seven ecological momentary assessments (EMAs) per day to report state anxiety. We developed a base model on over 10,000 days of external heart rate data, transferred its representations to our dataset, and fine-tuned it to generate probabilistic predictions. These were combined with trait-level measures in a meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety detection in our dataset. To evaluate generalizability, we applied the training approach to a separate hold-out set from the TILES-18 dataset-the same dataset used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1% balanced accuracy, outperforming prior work by at least 7%.
>
---
#### [new 028] Practitioners' Perspectives on a Differential Privacy Deployment Registry
- **分类: cs.CR; cs.CY; cs.HC**

- **简介: 该论文旨在构建一个差分隐私部署注册表，以促进实践者共享和学习DP实施经验。研究设计了描述DP部署的架构与交互界面，并通过用户研究分析其潜在价值与挑战。**

- **链接: [http://arxiv.org/pdf/2509.13509v1](http://arxiv.org/pdf/2509.13509v1)**

> **作者:** Priyanka Nanayakkara; Elena Ghazi; Salil Vadhan
>
> **摘要:** Differential privacy (DP) -- a principled approach to producing statistical data products with strong, mathematically provable privacy guarantees for the individuals in the underlying dataset -- has seen substantial adoption in practice over the past decade. Applying DP requires making several implementation decisions, each with significant impacts on data privacy and/or utility. Hence, to promote shared learning and accountability around DP deployments, Dwork, Kohli, and Mulligan (2019) proposed a public-facing repository ("registry") of DP deployments. The DP community has recently started to work toward realizing this vision. We contribute to this effort by (1) developing a holistic, hierarchical schema to describe any given DP deployment and (2) designing and implementing an interactive interface to act as a registry where practitioners can access information about past DP deployments. We (3) populate our interface with 21 real-world DP deployments and (4) conduct an exploratory user study with DP practitioners ($n=16$) to understand how they would use the registry, as well as what challenges and opportunities they foresee around its adoption. We find that participants were enthusiastic about the registry as a valuable resource for evaluating prior deployments and making future deployments. They also identified several opportunities for the registry, including that it can become a "hub" for the community and support broader communication around DP (e.g., to legal teams). At the same time, they identified challenges around the registry gaining adoption, including the effort and risk involved with making implementation choices public and moderating the quality of entries. Based on our findings, we offer recommendations for encouraging adoption and increasing the registry's value not only to DP practitioners, but also to policymakers, data users, and data subjects.
>
---
#### [new 029] Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy
- **分类: cs.LG; cs.CY**

- **简介: 该论文通过机器学习预测再入狱风险，开展针对性心理健康干预，旨在降低再犯率，打破监禁循环。研究分析数据、构建模型，并评估干预效果，尤其对高风险群体成效显著。属于公共政策与机器学习结合的应用任务。**

- **链接: [http://arxiv.org/pdf/2509.14129v1](http://arxiv.org/pdf/2509.14129v1)**

> **作者:** Kit T. Rodolfa; Erika Salomon; Jin Yao; Steve Yoder; Robert Sullivan; Kevin McGuire; Allie Dickinson; Rob MacDougall; Brian Seidler; Christina Sung; Claire Herdeman; Rayid Ghani
>
> **摘要:** Many incarcerated individuals face significant and complex challenges, including mental illness, substance dependence, and homelessness, yet jails and prisons are often poorly equipped to address these needs. With little support from the existing criminal justice system, these needs can remain untreated and worsen, often leading to further offenses and a cycle of incarceration with adverse outcomes both for the individual and for public safety, with particularly large impacts on communities of color that continue to widen the already extensive racial disparities in criminal justice outcomes. Responding to these failures, a growing number of criminal justice stakeholders are seeking to break this cycle through innovative approaches such as community-driven and alternative approaches to policing, mentoring, community building, restorative justice, pretrial diversion, holistic defense, and social service connections. Here we report on a collaboration between Johnson County, Kansas, and Carnegie Mellon University to perform targeted, proactive mental health outreach in an effort to reduce reincarceration rates. This paper describes the data used, our predictive modeling approach and results, as well as the design and analysis of a field trial conducted to confirm our model's predictive power, evaluate the impact of this targeted outreach, and understand at what level of reincarceration risk outreach might be most effective. Through this trial, we find that our model is highly predictive of new jail bookings, with more than half of individuals in the trial's highest-risk group returning to jail in the following year. Outreach was most effective among these highest-risk individuals, with impacts on mental health utilization, EMS dispatches, and criminal justice involvement.
>
---
#### [new 030] Programmable Cognitive Bias in Social Agents
- **分类: cs.AI; cs.CE; cs.CY**

- **简介: 论文提出CoBRA工具包，用于编程社会智能体的认知偏差。任务是解决传统方法无法一致生成符合描述的行为问题。工作包括设计认知偏差指数和行为调节引擎，并通过评估验证其有效性。**

- **链接: [http://arxiv.org/pdf/2509.13588v1](http://arxiv.org/pdf/2509.13588v1)**

> **作者:** Xuan Liu; Haoyang Shang; Haojian Jin
>
> **摘要:** This paper introduces CoBRA, a novel toolkit for systematically specifying agent behavior in LLM-based social simulation. We found that conventional approaches that specify agent behaviors through implicit natural language descriptions cannot yield consistent behaviors across models, and the produced agent behaviors do not capture the nuances of the descriptions. In contrast, CoBRA presents a new approach to program agents' cognitive biases explicitly, by grounding agents' expected behaviors using classic social science experiments. CoBRA has two components: (1) Cognitive Bias Index that measures the cognitive bias of a social agent, by quantifying the agent's reactions in a set of validated classical social science experiments; (2) Behavioral Regulation Engine that aligns the agent's behavior to demonstrate controlled cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and technical benchmarks. Our results suggest that CoBRA can precisely program the cognitive bias demonstrated in a social agent in a model-agnostic manner.
>
---
#### [new 031] Right-to-Override for Critical Urban Control Systems: A Deliberative Audit Method for Buildings, Power, and Transport
- **分类: eess.SY; cs.CY; cs.HC; cs.SY**

- **简介: 论文提出“Right-to-Override”（R2O）机制与“Deliberative Audit Method”（DAM），解决城市自动化系统（如建筑、电力、交通）缺乏居民干预权的问题。通过模拟验证，R2O能减少系统性损害，提升公平性与安全性。**

- **链接: [http://arxiv.org/pdf/2509.13369v1](http://arxiv.org/pdf/2509.13369v1)**

> **作者:** Rashid Mushkani
>
> **摘要:** Automation now steers building HVAC, distribution grids, and traffic signals, yet residents rarely have authority to pause or redirect these systems when they harm inclusivity, safety, or accessibility. We formalize a Right-to-Override (R2O) - defining override authorities, evidentiary thresholds, and domain-validated safe fallback states - and introduce a Deliberative Audit Method (DAM) with playbooks for pre-deployment walkthroughs, shadow-mode trials, and post-incident review. We instantiate R2O/DAM in simulations of smart-grid load shedding, building HVAC under occupancy uncertainty, and multi-agent traffic signals. R2O reduces distributional harm with limited efficiency loss: load-shedding disparity in unserved energy drops from 5.61x to 0.69x with constant curtailment; an override eliminates two discomfort-hours for seniors at an energy cost of 77 kWh; and median pedestrian wait falls from 90.4 s to 55.9 s with a 6.0 s increase in mean vehicle delay. We also contribute a policy standard, audit worksheets, and a ModelOps integration pattern to make urban automation contestable and reviewable.
>
---
## 更新

#### [replaced 001] A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare
- **分类: cs.CY; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2502.15871v2](http://arxiv.org/pdf/2502.15871v2)**

> **作者:** Manar Aljohani; Jun Hou; Sindhura Kommu; Xuan Wang
>
> **摘要:** The application of large language models (LLMs) in healthcare holds significant promise for enhancing clinical decision-making, medical research, and patient care. However, their integration into real-world clinical settings raises critical concerns around trustworthiness, particularly around dimensions of truthfulness, privacy, safety, robustness, fairness, and explainability. These dimensions are essential for ensuring that LLMs generate reliable, unbiased, and ethically sound outputs. While researchers have recently begun developing benchmarks and evaluation frameworks to assess LLM trustworthiness, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights. This survey addresses that gap by providing a comprehensive review of current methodologies and solutions aimed at mitigating risks across key trust dimensions. We analyze how each dimension affects the reliability and ethical deployment of healthcare LLMs, synthesize ongoing research efforts, and identify critical gaps in existing approaches. We also identify emerging challenges posed by evolving paradigms, such as multi-agent collaboration, multi-modal reasoning, and the development of small open-source medical models. Our goal is to guide future research toward more trustworthy, transparent, and clinically viable LLMs.
>
---
#### [replaced 002] An Attention-Based Denoising Framework for Personality Detection in Social Media Texts
- **分类: cs.CY; cs.CL**

- **链接: [http://arxiv.org/pdf/2311.09945v2](http://arxiv.org/pdf/2311.09945v2)**

> **作者:** Lei Lin; Jizhao Zhu; Qirui Tang; Yihua Du
>
> **摘要:** In social media networks, users produce a large amount of text content anytime, providing researchers with an invaluable approach to digging for personality-related information. Personality detection based on user-generated text is a method with broad application prospects, such as for constructing user portraits. The presence of significant noise in social media texts hinders personality detection. However, previous studies have not delved deeper into addressing this challenge. Inspired by the scanning reading technique, we propose an attention-based information extraction mechanism (AIEM) for long texts, which is applied to quickly locate valuable pieces of text, and fully integrate beneficial semantic information. Then, we provide a novel attention-based denoising framework (ADF) for personality detection tasks and achieve state-of-the-art performance on two commonly used datasets. Notably, we obtain an average accuracy improvement of 10.2% on the gold standard Twitter-Myers-Briggs Type Indicator (Twitter-MBTI) dataset. We made our code publicly available on GitHub\footnote{https://github.com/Once2gain/PersonalityDetection}. We shed light on how AIEM works to magnify personality-related signals through a case study.
>
---
#### [replaced 003] MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform
- **分类: cs.CY; cs.AI; cs.CL; cs.HC**

- **链接: [http://arxiv.org/pdf/2506.00308v2](http://arxiv.org/pdf/2506.00308v2)**

> **作者:** Hayoung Jung; Shravika Mittal; Ananya Aatreya; Navreet Kaur; Munmun De Choudhury; Tanushree Mitra
>
> **备注:** To appear at EMNLP 2025. Please cite EMNLP version when proceedings are available
>
> **摘要:** Understanding the prevalence of misinformation in health topics online can inform public health policies and interventions. However, measuring such misinformation at scale remains a challenge, particularly for high-stakes but understudied topics like opioid-use disorder (OUD)--a leading cause of death in the U.S. We present the first large-scale study of OUD-related myths on YouTube, a widely-used platform for health information. With clinical experts, we validate 8 pervasive myths and release an expert-labeled video dataset. To scale labeling, we introduce MythTriage, an efficient triage pipeline that uses a lightweight model for routine cases and defers harder ones to a high-performing, but costlier, large language model (LLM). MythTriage achieves up to 0.86 macro F1-score while estimated to reduce annotation time and financial cost by over 76% compared to experts and full LLM labeling. We analyze 2.9K search results and 343K recommendations, uncovering how myths persist on YouTube and offering actionable insights for public health and platform moderation.
>
---
#### [replaced 004] Legal Knowledge Graph Foundations, Part I: URI-Addressable Abstract Works (LRMoo F1 to schema.org)
- **分类: cs.DL; cs.AI; cs.CY; cs.IR**

- **链接: [http://arxiv.org/pdf/2508.00827v3](http://arxiv.org/pdf/2508.00827v3)**

> **作者:** Hudson de Martim
>
> **备注:** Major revision. The paper is now Part I of a series, mapping a formal LRMoo-based legal ontology to the web. This part details the mapping of the abstract F1 Work to schema.org, clarifying its foundational contribution by removing application-specific dependencies
>
> **摘要:** Building upon a formal, event-centric model for the diachronic evolution of legal norms grounded in the IFLA Library Reference Model (LRMoo), this paper addresses the essential first step of publishing this model's foundational entity-the abstract legal Work (F1)-on the Semantic Web. We propose a detailed, property-by-property mapping of the LRMoo F1 Work to the widely adopted schema.org/Legislation vocabulary. Using Brazilian federal legislation from the Normas.leg.br portal as a practical case study, we demonstrate how to create interoperable, machine-readable descriptions via JSON-LD, focusing on stable URN identifiers, core metadata, and norm relationships. This structured mapping establishes a stable, URI-addressable anchor for each legal norm, creating a verifiable "ground truth". It provides the essential, interoperable foundation upon which subsequent layers of the model, such as temporal versions (Expressions) and internal components, can be built. By bridging formal ontology with web-native standards, this work paves the way for building deterministic and reliable Legal Knowledge Graphs (LKGs), overcoming the limitations of purely probabilistic models.
>
---
#### [replaced 005] Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem
- **分类: cs.MA; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2509.04537v3](http://arxiv.org/pdf/2509.04537v3)**

> **作者:** Ryosuke Takata; Atsushi Masumori; Takashi Ikegami
>
> **摘要:** We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.
>
---
#### [replaced 006] Designing AI-Agents with Personalities: A Psychometric Approach
- **分类: cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2410.19238v2](http://arxiv.org/pdf/2410.19238v2)**

> **作者:** Muhua Huang; Xijuan Zhang; Christopher Soto; James Evans
>
> **摘要:** We introduce a methodology for assigning quantifiable and psychometrically validated personalities to AI-Agents using the Big Five framework. Across three studies, we evaluate its feasibility and limits. In Study 1, we show that large language models (LLMs) capture semantic similarities among Big Five measures, providing a basis for personality assignment. In Study 2, we create AI-Agents using prompts designed based on the Big Five Inventory (BFI-2) in the Likert or Expanded format, and find that, when paired with newer LLMs (e.g., GPT-4, GPT-4o, Llama, DeepSeek), these AI-Agents align more closely with human responses on the Mini-Markers test than those generated with binary adjective prompts or older models, although the finer pattern of results (e.g., factor loading patterns) were not consistent between AI-Agents and human participants. In Study 3, we validate our AI-Agents with risk-taking and moral dilemma vignettes. We find that while fine-tuning shifts responses toward more moral judgment, AI-Agent correlations between the input Big Five traits and the output moral judgments mirror those from human participants. Overall, our results show that AI-Agents align with humans in correlations between input Big Five traits and output responses and may serve as useful tools for preliminary research. Nevertheless, discrepancies in finer response patterns indicate that AI-Agents cannot (yet) fully substitute for human participants in precision or high-stakes projects.
>
---
