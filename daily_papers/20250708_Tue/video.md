# 计算机视觉 cs.CV

- **最新发布 328 篇**

- **更新 190 篇**

## 最新发布

#### [new 001] Helping CLIP See Both the Forest and the Trees: A Decomposition and Description Approach
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视觉-语言模型任务，解决CLIP对局部细节感知不足的问题。通过多裁剪增强方法，提升其局部特征分析能力。**

- **链接: [http://arxiv.org/pdf/2507.03458v1](http://arxiv.org/pdf/2507.03458v1)**

> **作者:** Leyan Xue; Zongbo Han; Guangyu Wang; Qinghua Hu; Mingyue Cheng; Changqing Zhang
>
> **摘要:** Vision-Language Models (VLMs) like CLIP achieve cross-modal semantic alignment through contrastive learning, exhibiting robust zero-shot generalization. Traditional prompt engineering, however, predominantly relies on coarse-grained category labels, neglecting fine-grained local semantics. Existing approaches assume that VLMs inherently recognize localized visual details and attempt to enhance classification by augmenting text prompts with attribute descriptors generated by large language models. However, our systematic experiments reveal critical limitations: CLIP's strong bias toward global image patterns hinders its ability to process localized visual descriptors. To address this fundamental constraint, we propose a simple, effective, and plug-and-play solution that enables CLIP to ``See Both the Forest and the Trees." Specifically, we employ stochastic multi-crop augmentation to activate CLIP's latent capacity for localized feature analysis. By cropping only partial regions, the approach effectively constrains the model's receptive field and recalibrates its attention mechanism, thereby mitigating its inherent bias. We evaluate the proposed method under zero-shot, few-shot, and test-time adaptation settings, and extensive experiments demonstrate that D&D achieves promising performance.
>
---
#### [new 002] Zero-shot Inexact CAD Model Alignment from a Single Image
- **分类: cs.CV**

- **简介: 该论文属于3D模型对齐任务，解决单图像下无监督的3D模型匹配问题。通过自监督学习和纹理不变姿态优化，实现跨类别的准确对齐。**

- **链接: [http://arxiv.org/pdf/2507.03292v1](http://arxiv.org/pdf/2507.03292v1)**

> **作者:** Pattaramanee Arsomngern; Sasikarn Khwanmuang; Matthias Nießner; Supasorn Suwajanakorn
>
> **备注:** ICCV 2025. Project page: https://zerocad9d.github.io/
>
> **摘要:** One practical approach to infer 3D scene structure from a single image is to retrieve a closely matching 3D model from a database and align it with the object in the image. Existing methods rely on supervised training with images and pose annotations, which limits them to a narrow set of object categories. To address this, we propose a weakly supervised 9-DoF alignment method for inexact 3D models that requires no pose annotations and generalizes to unseen categories. Our approach derives a novel feature space based on foundation features that ensure multi-view consistency and overcome symmetry ambiguities inherent in foundation features using a self-supervised triplet loss. Additionally, we introduce a texture-invariant pose refinement technique that performs dense alignment in normalized object coordinates, estimated through the enhanced feature space. We conduct extensive evaluations on the real-world ScanNet25k dataset, where our method outperforms SOTA weakly supervised baselines by +4.3% mean alignment accuracy and is the only weakly supervised approach to surpass the supervised ROCA by +2.7%. To assess generalization, we introduce SUN2CAD, a real-world test set with 20 novel object categories, where our method achieves SOTA results without prior training on them.
>
---
#### [new 003] Predicting Asphalt Pavement Friction Using Texture-Based Image Indicator
- **分类: cs.CV; eess.IV**

- **简介: 该论文属于道路安全领域，旨在预测沥青路面摩擦力。通过图像纹理指标建立统计模型，准确反映摩擦变化，提升路面设计效率。**

- **链接: [http://arxiv.org/pdf/2507.03559v1](http://arxiv.org/pdf/2507.03559v1)**

> **作者:** Bingjie Lu; Zhengyang Lu; Yijiashun Qi; Hanzhe Guo; Tianyao Sun; Zunduo Zhao
>
> **摘要:** Pavement skid resistance is of vital importance for road safety. The objective of this study is to propose and validate a texture-based image indicator to predict pavement friction. This index enables pavement friction to be measured easily and inexpensively using digital images. Three different types of asphalt surfaces (dense-graded asphalt mix, open-grade friction course, and chip seal) were evaluated subject to various tire polishing cycles. Images were taken with corresponding friction measured using Dynamic Friction Tester (DFT) in the laboratory. The aggregate protrusion area is proposed as the indicator. Statistical models are established for each asphalt surface type to correlate the proposed indicator with friction coefficients. The results show that the adjusted R-square values of all relationships are above 0.90. Compared to other image-based indicators in the literature, the proposed image indicator more accurately reflects the changes in pavement friction with the number of polishing cycles, proving its cost-effective use for considering pavement friction in mix design stage.
>
---
#### [new 004] Parameterized Diffusion Optimization enabled Autoregressive Ordinal Regression for Diabetic Retinopathy Grading
- **分类: cs.CV**

- **简介: 该论文属于糖尿病视网膜病变分级任务，解决类别分布不均和边界模糊问题。提出AOR-DR方法，结合扩散优化与自回归机制提升分类性能。**

- **链接: [http://arxiv.org/pdf/2507.04978v1](http://arxiv.org/pdf/2507.04978v1)**

> **作者:** Qinkai Yu; Wei Zhou; Hantao Liu; Yanyu Xu; Meng Wang; Yitian Zhao; Huazhu Fu; Xujiong Ye; Yalin Zheng; Yanda Meng
>
> **备注:** MICCAI 2025
>
> **摘要:** As a long-term complication of diabetes, diabetic retinopathy (DR) progresses slowly, potentially taking years to threaten vision. An accurate and robust evaluation of its severity is vital to ensure prompt management and care. Ordinal regression leverages the underlying inherent order between categories to achieve superior performance beyond traditional classification. However, there exist challenges leading to lower DR classification performance: 1) The uneven distribution of DR severity levels, characterized by a long-tailed pattern, adds complexity to the grading process. 2)The ambiguity in defining category boundaries introduces additional challenges, making the classification process more complex and prone to inconsistencies. This work proposes a novel autoregressive ordinal regression method called AOR-DR to address the above challenges by leveraging the clinical knowledge of inherent ordinal information in DR grading dataset settings. Specifically, we decompose the DR grading task into a series of ordered steps by fusing the prediction of the previous steps with extracted image features as conditions for the current prediction step. Additionally, we exploit the diffusion process to facilitate conditional probability modeling, enabling the direct use of continuous global image features for autoregression without relearning contextual information from patch-level features. This ensures the effectiveness of the autoregressive process and leverages the capabilities of pre-trained large-scale foundation models. Extensive experiments were conducted on four large-scale publicly available color fundus datasets, demonstrating our model's effectiveness and superior performance over six recent state-of-the-art ordinal regression methods. The implementation code is available at https://github.com/Qinkaiyu/AOR-DR.
>
---
#### [new 005] TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation
- **分类: cs.CV**

- **简介: 该论文属于视频帧插值任务，旨在解决现有方法在效率和性能上的不足。提出TLB-VFI模型，提升时间信息提取效率，减少参数与计算量。**

- **链接: [http://arxiv.org/pdf/2507.04984v1](http://arxiv.org/pdf/2507.04984v1)**

> **作者:** Zonglin Lyu; Chen Chen
>
> **摘要:** Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$ (we use n to denote time in videos to avoid notation overload with the timestep $t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and $I_1$. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: https://zonglinl.github.io/tlbvfi_page.
>
---
#### [new 006] A Visual Leap in CLIP Compositionality Reasoning through Generation of Counterfactual Sets
- **分类: cs.CV**

- **简介: 该论文属于视觉语言模型任务，解决 compositional reasoning 问题。通过生成反事实数据集提升模型推理能力。**

- **链接: [http://arxiv.org/pdf/2507.04699v1](http://arxiv.org/pdf/2507.04699v1)**

> **作者:** Zexi Jia; Chuanwei Huang; Hongyan Fei; Yeshuang Zhu; Zhiqiang Yuan; Ying Deng; Jiapei Zhang; Jinchao Zhang; Jie Zhou
>
> **摘要:** Vision-language models (VLMs) often struggle with compositional reasoning due to insufficient high-quality image-text data. To tackle this challenge, we propose a novel block-based diffusion approach that automatically generates counterfactual datasets without manual annotation. Our method utilizes large language models to identify entities and their spatial relationships. It then independently generates image blocks as "puzzle pieces" coherently arranged according to specified compositional rules. This process creates diverse, high-fidelity counterfactual image-text pairs with precisely controlled variations. In addition, we introduce a specialized loss function that differentiates inter-set from intra-set samples, enhancing training efficiency and reducing the need for negative samples. Experiments demonstrate that fine-tuning VLMs with our counterfactual datasets significantly improves visual reasoning performance. Our approach achieves state-of-the-art results across multiple benchmarks while using substantially less training data than existing methods.
>
---
#### [new 007] ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls for Remote Sensing
- **分类: cs.CV**

- **简介: 该论文属于遥感图像生成任务，旨在通过多模态控制生成未来场景。提出ChangeBridge模型，解决基于历史图像生成未来图像的问题。**

- **链接: [http://arxiv.org/pdf/2507.04678v1](http://arxiv.org/pdf/2507.04678v1)**

> **作者:** Zhenghui Zhao; Chen Wu; Di Wang; Hongruixuan Chen; Zhuo Zheng
>
> **摘要:** Recent advancements in generative methods, especially diffusion models, have made great progress in remote sensing image synthesis. Despite these advancements, existing methods have not explored the simulation of future scenarios based on given scenario images. This simulation capability has wide applications for urban planning, land managementChangeBridge: Spatiotemporal Image Generation with Multimodal Controls, and beyond. In this work, we propose ChangeBridge, a conditional spatiotemporal diffusion model. Given pre-event images and conditioned on multimodal spatial controls (e.g., text prompts, instance layouts, and semantic maps), ChangeBridge can synthesize post-event images. The core idea behind ChangeBridge is to modeling the noise-to-image diffusion model, as a pre-to-post diffusion bridge. Conditioned on multimodal controls, ChangeBridge leverages a stochastic Brownian-bridge diffusion, directly modeling the spatiotemporal evolution between pre-event and post-event states. To the best of our knowledge, ChangeBridge is the first spatiotemporal generative model with multimodal controls for remote sensing. Experimental results demonstrate that ChangeBridge can simulate high-fidelity future scenarios aligned with given conditions, including event and event-driven background variations. Code will be available.
>
---
#### [new 008] Concept-based Adversarial Attack: a Probabilistic Perspective
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于对抗攻击任务，旨在生成多样化且保持概念一致的对抗样本。通过概率模型生成对抗例子，提升攻击效率与多样性。**

- **链接: [http://arxiv.org/pdf/2507.02965v1](http://arxiv.org/pdf/2507.02965v1)**

> **作者:** Andi Zhang; Xuan Ding; Steven McDonagh; Samuel Kaski
>
> **摘要:** We propose a concept-based adversarial attack framework that extends beyond single-image perturbations by adopting a probabilistic perspective. Rather than modifying a single image, our method operates on an entire concept -- represented by a probabilistic generative model or a set of images -- to generate diverse adversarial examples. Preserving the concept is essential, as it ensures that the resulting adversarial images remain identifiable as instances of the original underlying category or identity. By sampling from this concept-based adversarial distribution, we generate images that maintain the original concept but vary in pose, viewpoint, or background, thereby misleading the classifier. Mathematically, this framework remains consistent with traditional adversarial attacks in a principled manner. Our theoretical and empirical results demonstrate that concept-based adversarial attacks yield more diverse adversarial examples and effectively preserve the underlying concept, while achieving higher attack efficiency.
>
---
#### [new 009] ChestGPT: Integrating Large Language Models and Vision Transformers for Disease Detection and Localization in Chest X-Rays
- **分类: cs.CV**

- **简介: 该论文属于医学影像分析任务，旨在解决放射科医生短缺问题。通过整合视觉Transformer和语言模型，实现胸部X光片的疾病分类与病灶定位。**

- **链接: [http://arxiv.org/pdf/2507.03739v1](http://arxiv.org/pdf/2507.03739v1)**

> **作者:** Shehroz S. Khan; Petar Przulj; Ahmed Ashraf; Ali Abedi
>
> **备注:** 8 pages, 5 figures, 4 tables
>
> **摘要:** The global demand for radiologists is increasing rapidly due to a growing reliance on medical imaging services, while the supply of radiologists is not keeping pace. Advances in computer vision and image processing technologies present significant potential to address this gap by enhancing radiologists' capabilities and improving diagnostic accuracy. Large language models (LLMs), particularly generative pre-trained transformers (GPTs), have become the primary approach for understanding and generating textual data. In parallel, vision transformers (ViTs) have proven effective at converting visual data into a format that LLMs can process efficiently. In this paper, we present ChestGPT, a deep-learning framework that integrates the EVA ViT with the Llama 2 LLM to classify diseases and localize regions of interest in chest X-ray images. The ViT converts X-ray images into tokens, which are then fed, together with engineered prompts, into the LLM, enabling joint classification and localization of diseases. This approach incorporates transfer learning techniques to enhance both explainability and performance. The proposed method achieved strong global disease classification performance on the VinDr-CXR dataset, with an F1 score of 0.76, and successfully localized pathologies by generating bounding boxes around the regions of interest. We also outline several task-specific prompts, in addition to general-purpose prompts, for scenarios radiologists might encounter. Overall, this framework offers an assistive tool that can lighten radiologists' workload by providing preliminary findings and regions of interest to facilitate their diagnostic process.
>
---
#### [new 010] MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding
- **分类: cs.CV**

- **简介: 该论文属于多模态学习任务，针对多模态注意力不一致和衰减问题，提出MODA机制提升感知、认知与情感理解能力。**

- **链接: [http://arxiv.org/pdf/2507.04635v1](http://arxiv.org/pdf/2507.04635v1)**

> **作者:** Zhicheng Zhang; Wuyou Xia; Chenxi Zhao; Zhou Yan; Xiaoqiang Liu; Yongjie Zhu; Wenyu Qin; Pengfei Wan; Di Zhang; Jufeng Yang
>
> **备注:** ICML 2025 (Spotlight, Top 2.6%)
>
> **摘要:** Multimodal large language models (MLLMs) recently showed strong capacity in integrating data among multiple modalities, empowered by a generalizable attention architecture. Advanced methods predominantly focus on language-centric tuning while less exploring multimodal tokens mixed through attention, posing challenges in high-level tasks that require fine-grained cognition and emotion understanding. In this work, we identify the attention deficit disorder problem in multimodal learning, caused by inconsistent cross-modal attention and layer-by-layer decayed attention activation. To address this, we propose a novel attention mechanism, termed MOdular Duplex Attention (MODA), simultaneously conducting the inner-modal refinement and inter-modal interaction. MODA employs a correct-after-align strategy to effectively decouple modality alignment from cross-layer token mixing. In the alignment phase, tokens are mapped to duplex modality spaces based on the basis vectors, enabling the interaction between visual and language modality. Further, the correctness of attention scores is ensured through adaptive masked attention, which enhances the model's flexibility by allowing customizable masking patterns for different modalities. Extensive experiments on 21 benchmark datasets verify the effectiveness of MODA in perception, cognition, and emotion tasks. Source code and demo are available in https://zzcheng.top/MODA.
>
---
#### [new 011] Development of an Improved Capsule-Yolo Network for Automatic Tomato Plant Disease Early Detection and Diagnosis
- **分类: cs.CV**

- **简介: 该论文属于植物病害检测任务，旨在解决番茄早期病害识别问题。通过改进的Capsule-YOLO网络实现精准分割与诊断，提升检测性能并提供防治建议。**

- **链接: [http://arxiv.org/pdf/2507.03219v1](http://arxiv.org/pdf/2507.03219v1)**

> **作者:** Idris Ochijenu; Monday Abutu Idakwo; Sani Felix
>
> **摘要:** Like many countries, Nigeria is naturally endowed with fertile agricultural soil that supports large-scale tomato production. However, the prevalence of disease causing pathogens poses a significant threat to tomato health, often leading to reduced yields and, in severe cases, the extinction of certain species. These diseases jeopardise both the quality and quantity of tomato harvests, contributing to food insecurity. Fortunately, tomato diseases can often be visually identified through distinct forms, appearances, or textures, typically first visible on leaves and fruits. This study presents an enhanced Capsule-YOLO network architecture designed to automatically segment overlapping and occluded tomato leaf images from complex backgrounds using the YOLO framework. It identifies disease symptoms with impressive performance metrics: 99.31% accuracy, 98.78% recall, and 99.09% precision, and a 98.93% F1-score representing improvements of 2.91%, 1.84%, 5.64%, and 4.12% over existing state-of-the-art methods. Additionally, a user-friendly interface was developed to allow farmers and users to upload images of affected tomato plants and detect early disease symptoms. The system also provides recommendations for appropriate diagnosis and treatment. The effectiveness of this approach promises significant benefits for the agricultural sector by enhancing crop yields and strengthening food security.
>
---
#### [new 012] MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry
- **分类: cs.CV; cs.AI; 68T45, 65D18**

- **简介: 该论文属于粒子图像测速（PIV）任务，解决深度学习在PIV中评估不足的问题。构建了大规模合成PIV数据集，并提出MCFormer网络，显著提升PIV性能。**

- **链接: [http://arxiv.org/pdf/2507.04750v1](http://arxiv.org/pdf/2507.04750v1)**

> **作者:** Zicheng Lin; Xiaoqiang Li; Yichao Wang; Chuan Zhu
>
> **备注:** 20 pages, 13 figures, 5 tables. Comprehensive benchmark evaluation of optical flow models for PIV. Introduces MCFormer architecture with multi-frame temporal processing and multiple cost volumes. Includes large-scale synthetic PIV dataset based on JHTDB and Blasius CFD simulations. Code and dataset will be made publicly available
>
> **摘要:** Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep learning applications face significant hurdles. A critical gap exists: the lack of comprehensive evaluation of how diverse optical flow models perform specifically on PIV data, largely due to limitations in available datasets and the absence of a standardized benchmark. This prevents fair comparison and hinders progress. To address this, our primary contribution is a novel, large-scale synthetic PIV benchmark dataset generated from diverse CFD simulations (JHTDB and Blasius). It features unprecedented variety in particle densities, flow velocities, and continuous motion, enabling, for the first time, a standardized and rigorous evaluation of various optical flow and PIV algorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a new deep network architecture leveraging multi-frame temporal information and multiple cost volumes, specifically designed for PIV's sparse nature. Our comprehensive benchmark evaluation, the first of its kind, reveals significant performance variations among adapted optical flow models and demonstrates that MCFormer significantly outperforms existing methods, achieving the lowest overall normalized endpoint error (NEPE). This work provides both a foundational benchmark resource essential for future PIV research and a state-of-the-art method tailored for PIV challenges. We make our benchmark dataset and code publicly available to foster future research in this area.
>
---
#### [new 013] AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics
- **分类: cs.CV; cs.CL; cs.LG; I.2.10; I.4.9; J.3**

- **简介: 该论文属于医学图像合成任务，旨在解决数据不平衡和隐私问题。通过生成高质量合成图像，提升分类器性能，增强模型泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.05063v1](http://arxiv.org/pdf/2507.05063v1)**

> **作者:** Jan Carreras Boada; Rao Muhammad Umer; Carsten Marr
>
> **备注:** 8 pages, 6 figures, 2 tables. Final Degree Project (TFG) submitted at ESCI-UPF and conducted at Helmholtz Munich
>
> **摘要:** Biomedical datasets often contain a large sample imbalance and are subject to strict privacy constraints, which together hinder the development of accurate machine learning models. One potential solution is to generate synthetic images, as this can improve data availability while preserving patient privacy. However, it remains difficult to generate synthetic images of sufficient quality for training robust classifiers. In this work, we focus on the classification of single white blood cells, a key component in the diagnosis of hematological diseases such as acute myeloid leukemia (AML), a severe blood cancer. We demonstrate how synthetic images generated with a fine-tuned stable diffusion model using LoRA weights when guided by real few-shot samples of the target white blood cell classes, can enhance classifier performance for limited data. When training a ResNet classifier, accuracy increased from 27.3\% to 78.4\% (+51.1\%) by adding 5000 synthetic images per class to a small and highly imbalanced real dataset. For a CLIP-based classifier, the accuracy improved from 61.8\% to 76.8\% (+15.0\%). The synthetic images are highly similar to real images, and they can help overcome dataset limitations, enhancing model generalization. Our results establish synthetic images as a tool in biomedical research, improving machine learning models, and facilitating medical diagnosis and research.
>
---
#### [new 014] DESign: Dynamic Context-Aware Convolution and Efficient Subnet Regularization for Continuous Sign Language Recognition
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于连续手语识别任务，解决多样样本处理与时间依赖建模问题。提出DESign框架，结合DCAC和SR-CTC提升识别准确率。**

- **链接: [http://arxiv.org/pdf/2507.03339v1](http://arxiv.org/pdf/2507.03339v1)**

> **作者:** Sheng Liu; Yiheng Yu; Yuan Feng; Min Xu; Zhelun Jin; Yining Jiang; Tiantian Yuan
>
> **摘要:** Current continuous sign language recognition (CSLR) methods struggle with handling diverse samples. Although dynamic convolutions are ideal for this task, they mainly focus on spatial modeling and fail to capture the temporal dynamics and contextual dependencies. To address this, we propose DESign, a novel framework that incorporates Dynamic Context-Aware Convolution (DCAC) and Subnet Regularization Connectionist Temporal Classification (SR-CTC). DCAC dynamically captures the inter-frame motion cues that constitute signs and uniquely adapts convolutional weights in a fine-grained manner based on contextual information, enabling the model to better generalize across diverse signing behaviors and boost recognition accuracy. Furthermore, we observe that existing methods still rely on only a limited number of frames for parameter updates during training, indicating that CTC learning overfits to a dominant path. To address this, SR-CTC regularizes training by applying supervision to subnetworks, encouraging the model to explore diverse CTC alignment paths and effectively preventing overfitting. A classifier-sharing strategy in SR-CTC further strengthens multi-scale consistency. Notably, SR-CTC introduces no inference overhead and can be seamlessly integrated into existing CSLR models to boost performance. Extensive ablations and visualizations further validate the effectiveness of the proposed methods. Results on mainstream CSLR datasets (i.e., PHOENIX14, PHOENIX14-T, CSL-Daily) demonstrate that DESign achieves state-of-the-art performance.
>
---
#### [new 015] Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文属于医学图像分割任务，旨在解决模型在未见领域泛化能力差的问题。通过引入因果推理机制，提升分割模型的鲁棒性与可交互性。**

- **链接: [http://arxiv.org/pdf/2507.03585v1](http://arxiv.org/pdf/2507.03585v1)**

> **作者:** Tao Tang; Shijie Xu; Yiting Wu; Zhixiang Lu
>
> **摘要:** The clinical utility of deep learning models for medical image segmentation is severely constrained by their inability to generalize to unseen domains. This failure is often rooted in the models learning spurious correlations between anatomical content and domain-specific imaging styles. To overcome this fundamental challenge, we introduce Causal-SAM-LLM, a novel framework that elevates Large Language Models (LLMs) to the role of causal reasoners. Our framework, built upon a frozen Segment Anything Model (SAM) encoder, incorporates two synergistic innovations. First, Linguistic Adversarial Disentanglement (LAD) employs a Vision-Language Model to generate rich, textual descriptions of confounding image styles. By training the segmentation model's features to be contrastively dissimilar to these style descriptions, it learns a representation robustly purged of non-causal information. Second, Test-Time Causal Intervention (TCI) provides an interactive mechanism where an LLM interprets a clinician's natural language command to modulate the segmentation decoder's features in real-time, enabling targeted error correction. We conduct an extensive empirical evaluation on a composite benchmark from four public datasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under cross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM establishes a new state of the art in out-of-distribution (OOD) robustness, improving the average Dice score by up to 6.2 points and reducing the Hausdorff Distance by 15.8 mm over the strongest baseline, all while using less than 9% of the full model's trainable parameters. Our work charts a new course for building robust, efficient, and interactively controllable medical AI systems.
>
---
#### [new 016] FreqCross: A Multi-Modal Frequency-Spatial Fusion Network for Robust Detection of Stable Diffusion 3.5 Generated Images
- **分类: cs.CV; cs.CR**

- **简介: 该论文属于图像检测任务，旨在解决AI生成图像的检测问题。提出FreqCross网络，融合空间、频率和径向能量特征，提升检测准确性。**

- **链接: [http://arxiv.org/pdf/2507.02995v1](http://arxiv.org/pdf/2507.02995v1)**

> **作者:** Guang Yang
>
> **摘要:** The rapid advancement of diffusion models, particularly Stable Diffusion 3.5, has enabled the generation of highly photorealistic synthetic images that pose significant challenges to existing detection methods. This paper presents FreqCross, a novel multi-modal fusion network that combines spatial RGB features, frequency domain artifacts, and radial energy distribution patterns to achieve robust detection of AI-generated images. Our approach leverages a three-branch architecture: (1) a ResNet-18 backbone for spatial feature extraction, (2) a lightweight CNN for processing 2D FFT magnitude spectra, and (3) a multi-layer perceptron for analyzing radial energy profiles. We introduce a novel radial energy distribution analysis that captures characteristic frequency artifacts inherent in diffusion-generated images, and fuse it with spatial and spectral cues via simple feature concatenation followed by a compact classification head. Extensive experiments on a dataset of 10,000 paired real (MS-COCO) and synthetic (Stable Diffusion 3.5) images demonstrate that FreqCross achieves 97.8\% accuracy, outperforming state-of-the-art baselines by 5.2\%. The frequency analysis further reveals that synthetic images exhibit distinct spectral signatures in the 0.1--0.4 normalised frequency range, providing theoretical foundation for our approach. Code and pre-trained models are publicly available to facilitate reproducible research.
>
---
#### [new 017] Multimodal image registration for effective thermographic fever screening
- **分类: cs.CV**

- **简介: 该论文属于图像配准任务，旨在解决红外热成像中准确定位眼内眦区域的问题，通过多模态图像配准提高发热筛查精度。**

- **链接: [http://arxiv.org/pdf/2507.02955v1](http://arxiv.org/pdf/2507.02955v1)**

> **作者:** C. Y. N. Dwith; Pejhman Ghassemi; Joshua Pfefer; Jon Casamento; Quanzeng Wang
>
> **摘要:** Fever screening based on infrared thermographs (IRTs) is a viable mass screening approach during infectious disease pandemics, such as Ebola and SARS, for temperature monitoring in public places like hospitals and airports. IRTs have found to be powerful, quick and non-invasive methods to detect elevated temperatures. Moreover, regions medially adjacent to the inner canthi (called the canthi regions in this paper) are preferred sites for fever screening. Accurate localization of the canthi regions can be achieved through multi-modal registration of infrared (IR) and white-light images. We proposed a registration method through a coarse-fine registration strategy using different registration models based on landmarks and edge detection on eye contours. We evaluated the registration accuracy to be within 2.7 mm, which enables accurate localization of the canthi regions.
>
---
#### [new 018] INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视觉语言模型任务，旨在解决模型生成与图像内容不符的幻觉问题。通过交互引导采样方法，提升模型生成准确性。**

- **链接: [http://arxiv.org/pdf/2507.05056v1](http://arxiv.org/pdf/2507.05056v1)**

> **作者:** Xin Dong; Shichao Dong; Jin Wang; Jing Huang; Li Zhou; Zenghui Sun; Lihua Jing; Jingsong Lan; Xiaoyong Zhu; Bo Zheng
>
> **摘要:** Hallucinations in large vision-language models (LVLMs) pose significant challenges for real-world applications, as LVLMs may generate responses that appear plausible yet remain inconsistent with the associated visual content. This issue rarely occurs in human cognition. We argue that this discrepancy arises from humans' ability to effectively leverage multimodal interaction information in data samples. Specifically, humans typically first gather multimodal information, analyze the interactions across modalities for understanding, and then express their understanding through language. Motivated by this observation, we conduct extensive experiments on popular LVLMs and obtained insights that surprisingly reveal human-like, though less pronounced, cognitive behavior of LVLMs on multimodal samples. Building on these findings, we further propose \textbf{INTER}: \textbf{Inter}action Guidance Sampling, a novel training-free algorithm that mitigate hallucinations without requiring additional data. Specifically, INTER explicitly guides LVLMs to effectively reapply their understanding of multimodal interaction information when generating responses, thereby reducing potential hallucinations. On six benchmarks including VQA and image captioning tasks, INTER achieves an average improvement of up to 3.4\% on five LVLMs compared to the state-of-the-art decoding strategy. The code will be released when the paper is accepted.
>
---
#### [new 019] ICAS: Detecting Training Data from Autoregressive Image Generative Models
- **分类: cs.CV; cs.AI; cs.CR**

- **简介: 该论文属于数据检测任务，旨在识别生成模型训练数据。通过会员推理方法，提出隐式分类与自适应评分策略，有效检测训练样本。**

- **链接: [http://arxiv.org/pdf/2507.05068v1](http://arxiv.org/pdf/2507.05068v1)**

> **作者:** Hongyao Yu; Yixiang Qiu; Yiheng Yang; Hao Fang; Tianqu Zhuang; Jiaxin Hong; Bin Chen; Hao Wu; Shu-Tao Xia
>
> **备注:** ACM MM 2025
>
> **摘要:** Autoregressive image generation has witnessed rapid advancements, with prominent models such as scale-wise visual auto-regression pushing the boundaries of visual synthesis. However, these developments also raise significant concerns regarding data privacy and copyright. In response, training data detection has emerged as a critical task for identifying unauthorized data usage in model training. To better understand the vulnerability of autoregressive image generative models to such detection, we conduct the first study applying membership inference to this domain. Our approach comprises two key components: implicit classification and an adaptive score aggregation strategy. First, we compute the implicit token-wise classification score within the query image. Then we propose an adaptive score aggregation strategy to acquire a final score, which places greater emphasis on the tokens with lower scores. A higher final score indicates that the sample is more likely to be involved in the training set. To validate the effectiveness of our method, we adapt existing detection algorithms originally designed for LLMs to visual autoregressive models. Extensive experiments demonstrate the superiority of our method in both class-conditional and text-to-image scenarios. Moreover, our approach exhibits strong robustness and generalization under various data transformations. Furthermore, sufficient experiments suggest two novel key findings: (1) A linear scaling law on membership inference, exposing the vulnerability of large foundation models. (2) Training data from scale-wise visual autoregressive models is easier to detect than other autoregressive paradigms.Our code is available at https://github.com/Chrisqcwx/ImageAR-MIA.
>
---
#### [new 020] Comprehensive Information Bottleneck for Unveiling Universal Attribution to Interpret Vision Transformers
- **分类: cs.CV**

- **简介: 该论文属于视觉Transformer解释任务，解决多层特征 attribution 信息丢失问题。提出CoIBA方法，在多层共享参数压缩信息，提升解释的准确性与可信度。**

- **链接: [http://arxiv.org/pdf/2507.04388v1](http://arxiv.org/pdf/2507.04388v1)**

> **作者:** Jung-Ho Hong; Ho-Joong Kim; Kyu-Sung Jeon; Seong-Whan Lee
>
> **备注:** CVPR 2025 (highlight)
>
> **摘要:** The feature attribution method reveals the contribution of input variables to the decision-making process to provide an attribution map for explanation. Existing methods grounded on the information bottleneck principle compute information in a specific layer to obtain attributions, compressing the features by injecting noise via a parametric damping ratio. However, the attribution obtained in a specific layer neglects evidence of the decision-making process distributed across layers. In this paper, we introduce a comprehensive information bottleneck (CoIBA), which discovers the relevant information in each targeted layer to explain the decision-making process. Our core idea is applying information bottleneck in multiple targeted layers to estimate the comprehensive information by sharing a parametric damping ratio across the layers. Leveraging this shared ratio complements the over-compressed information to discover the omitted clues of the decision by sharing the relevant information across the targeted layers. We suggest the variational approach to fairly reflect the relevant information of each layer by upper bounding layer-wise information. Therefore, CoIBA guarantees that the discarded activation is unnecessary in every targeted layer to make a decision. The extensive experimental results demonstrate the enhancement in faithfulness of the feature attributions provided by CoIBA.
>
---
#### [new 021] VICI: VLM-Instructed Cross-view Image-localisation
- **分类: cs.CV**

- **简介: 该论文属于跨视图图像定位任务，解决有限视野街景图像与卫星图像匹配问题。通过两阶段检索与重排序方法提升定位精度。**

- **链接: [http://arxiv.org/pdf/2507.04107v1](http://arxiv.org/pdf/2507.04107v1)**

> **作者:** Xiaohan Zhang; Tavis Shore; Chen Chen; Oscar Mendez; Simon Hadfield; Safwan Wshah
>
> **摘要:** In this paper, we present a high-performing solution to the UAVM 2025 Challenge, which focuses on matching narrow FOV street-level images to corresponding satellite imagery using the University-1652 dataset. As panoramic Cross-View Geo-Localisation nears peak performance, it becomes increasingly important to explore more practical problem formulations. Real-world scenarios rarely offer panoramic street-level queries; instead, queries typically consist of limited-FOV images captured with unknown camera parameters. Our work prioritises discovering the highest achievable performance under these constraints, pushing the limits of existing architectures. Our method begins by retrieving candidate satellite image embeddings for a given query, followed by a re-ranking stage that selectively enhances retrieval accuracy within the top candidates. This two-stage approach enables more precise matching, even under the significant viewpoint and scale variations inherent in the task. Through experimentation, we demonstrate that our approach achieves competitive results -specifically attaining R@1 and R@10 retrieval rates of \topone\% and \topten\% respectively. This underscores the potential of optimised retrieval and re-ranking strategies in advancing practical geo-localisation performance. Code is available at https://github.com/tavisshore/VICI.
>
---
#### [new 022] Leveraging Self-Supervised Features for Efficient Flooded Region Identification in UAV Aerial Images
- **分类: cs.CV**

- **简介: 该论文属于图像分割任务，旨在解决无人机航拍图像中洪水区域识别问题。通过引入自监督学习特征，减少对人工标注数据的依赖，提升分割效率与准确性。**

- **链接: [http://arxiv.org/pdf/2507.04915v1](http://arxiv.org/pdf/2507.04915v1)**

> **作者:** Dibyabha Deb; Ujjwal Verma
>
> **备注:** 13 Pages, 4 Figures
>
> **摘要:** Identifying regions affected by disasters is a vital step in effectively managing and planning relief and rescue efforts. Unlike the traditional approaches of manually assessing post-disaster damage, analyzing images of Unmanned Aerial Vehicles (UAVs) offers an objective and reliable way to assess the damage. In the past, segmentation techniques have been adopted to identify post-flood damage in UAV aerial images. However, most of these supervised learning approaches rely on manually annotated datasets. Indeed, annotating images is a time-consuming and error-prone task that requires domain expertise. This work focuses on leveraging self-supervised features to accurately identify flooded regions in UAV aerial images. This work proposes two encoder-decoder-based segmentation approaches, which integrate the visual features learned from DINOv2 with the traditional encoder backbone. This study investigates the generalization of self-supervised features for UAV aerial images. Specifically, we evaluate the effectiveness of features from the DINOv2 model, trained on non-aerial images, for segmenting aerial images, noting the distinct perspectives between the two image types. Our results demonstrate that DINOv2's self-supervised pretraining on natural images generates transferable, general-purpose visual features that streamline the development of aerial segmentation workflows. By leveraging these features as a foundation, we significantly reduce reliance on labor-intensive manual annotation processes, enabling high-accuracy segmentation with limited labeled aerial data.
>
---
#### [new 023] Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions
- **分类: cs.CV; cs.AI; cs.GR; cs.HC; cs.MM**

- **简介: 该论文属于Talking Head Generation任务，旨在生成与输入同步的逼真人脸视频。工作包括方法分类、数据集评估、指标分析及未来方向探索。**

- **链接: [http://arxiv.org/pdf/2507.02900v1](http://arxiv.org/pdf/2507.02900v1)**

> **作者:** Vineet Kumar Rakesh; Soumya Mazumdar; Research Pratim Maity; Sarbajit Pal; Amitabha Das; Tapas Samanta
>
> **摘要:** Talking Head Generation (THG) has emerged as a transformative technology in computer vision, enabling the synthesis of realistic human faces synchronized with image, audio, text, or video inputs. This paper provides a comprehensive review of methodologies and frameworks for talking head generation, categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields (NeRF)--based, diffusion--based, parameter-driven techniques and many other techniques. It evaluates algorithms, datasets, and evaluation metrics while highlighting advancements in perceptual realism and technical efficiency critical for applications such as digital avatars, video dubbing, ultra-low bitrate video conferencing, and online education. The study identifies challenges such as reliance on pre--trained models, extreme pose handling, multilingual synthesis, and temporal consistency. Future directions include modular architectures, multilingual datasets, hybrid models blending pre--trained and task-specific layers, and innovative loss functions. By synthesizing existing research and exploring emerging trends, this paper aims to provide actionable insights for researchers and practitioners in the field of talking head generation. For the complete survey, code, and curated resource list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.
>
---
#### [new 024] MVL-Loc: Leveraging Vision-Language Model for Generalizable Multi-Scene Camera Relocalization
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于相机重定位任务，解决多场景下定位精度与泛化能力问题。通过引入视觉语言模型和多模态数据，提升定位的准确性和适应性。**

- **链接: [http://arxiv.org/pdf/2507.04509v1](http://arxiv.org/pdf/2507.04509v1)**

> **作者:** Zhendong Xiao; Wu Wei; Shujie Ji; Shan Yang; Changhao Chen
>
> **备注:** PRCV
>
> **摘要:** Camera relocalization, a cornerstone capability of modern computer vision, accurately determines a camera's position and orientation (6-DoF) from images and is essential for applications in augmented reality (AR), mixed reality (MR), autonomous driving, delivery drones, and robotic navigation. Unlike traditional deep learning-based methods that regress camera pose from images in a single scene, which often lack generalization and robustness in diverse environments, we propose MVL-Loc, a novel end-to-end multi-scene 6-DoF camera relocalization framework. MVL-Loc leverages pretrained world knowledge from vision-language models (VLMs) and incorporates multimodal data to generalize across both indoor and outdoor settings. Furthermore, natural language is employed as a directive tool to guide the multi-scene learning process, facilitating semantic understanding of complex scenes and capturing spatial relationships among objects. Extensive experiments on the 7Scenes and Cambridge Landmarks datasets demonstrate MVL-Loc's robustness and state-of-the-art performance in real-world multi-scene camera relocalization, with improved accuracy in both positional and orientational estimates.
>
---
#### [new 025] PointGAC: Geometric-Aware Codebook for Masked Point Cloud Modeling
- **分类: cs.CV**

- **简介: 该论文属于点云重建任务，解决传统方法过度约束模型的问题。提出PointGAC，通过聚类引导师生框架，提升特征泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.04801v1](http://arxiv.org/pdf/2507.04801v1)**

> **作者:** Abiao Li; Chenlei Lv; Yuming Fang; Yifan Zuo; Jian Zhang; Guofeng Mei
>
> **备注:** ICCV 2025
>
> **摘要:** Most masked point cloud modeling (MPM) methods follow a regression paradigm to reconstruct the coordinate or feature of masked regions. However, they tend to over-constrain the model to learn the details of the masked region, resulting in failure to capture generalized features. To address this limitation, we propose \textbf{\textit{PointGAC}}, a novel clustering-based MPM method that aims to align the feature distribution of masked regions. Specially, it features an online codebook-guided teacher-student framework. Firstly, it presents a geometry-aware partitioning strategy to extract initial patches. Then, the teacher model updates a codebook via online k-means based on features extracted from the complete patches. This procedure facilitates codebook vectors to become cluster centers. Afterward, we assigns the unmasked features to their corresponding cluster centers, and the student model aligns the assignment for the reconstructed masked features. This strategy focuses on identifying the cluster centers to which the masked features belong, enabling the model to learn more generalized feature representations. Benefiting from a proposed codebook maintenance mechanism, codebook vectors are actively updated, which further increases the efficiency of semantic feature learning. Experiments validate the effectiveness of the proposed method on various downstream tasks. Code is available at https://github.com/LAB123-tech/PointGAC
>
---
#### [new 026] Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于视频语言模型任务，解决模型无法拒绝无关问题的问题。提出对齐可回答性框架，使模型能评估问题相关性并拒绝超出视频内容的问题。**

- **链接: [http://arxiv.org/pdf/2507.04976v1](http://arxiv.org/pdf/2507.04976v1)**

> **作者:** Eunseop Yoon; Hee Suk Yoon; Mark A. Hasegawa-Johnson; Chang D. Yoo
>
> **备注:** ICLR 2025
>
> **摘要:** In the broader context of deep learning, Multimodal Large Language Models have achieved significant breakthroughs by leveraging powerful Large Language Models as a backbone to align different modalities into the language space. A prime exemplification is the development of Video Large Language Models (Video-LLMs). While numerous advancements have been proposed to enhance the video understanding capabilities of these models, they are predominantly trained on questions generated directly from video content. However, in real-world scenarios, users often pose questions that extend beyond the informational scope of the video, highlighting the need for Video-LLMs to assess the relevance of the question. We demonstrate that even the best-performing Video-LLMs fail to reject unfit questions-not necessarily due to a lack of video understanding, but because they have not been trained to identify and refuse such questions. To address this limitation, we propose alignment for answerability, a framework that equips Video-LLMs with the ability to evaluate the relevance of a question based on the input video and appropriately decline to answer when the question exceeds the scope of the video, as well as an evaluation framework with a comprehensive set of metrics designed to measure model behavior before and after alignment. Furthermore, we present a pipeline for creating a dataset specifically tailored for alignment for answerability, leveraging existing video-description paired datasets.
>
---
#### [new 027] HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context Attention Network for Colorectal Polyp Detection
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于结直肠息肉检测任务，旨在解决小病灶检测、边界定位和决策可解释性问题。提出HGNet模型，融合高阶空间超图和多尺度上下文注意力机制，提升检测性能与可解释性。**

- **链接: [http://arxiv.org/pdf/2507.04880v1](http://arxiv.org/pdf/2507.04880v1)**

> **作者:** Xiaofang Liu; Lingling Sun; Xuqing Zhang; Yuannong Ye; Bin zhao
>
> **摘要:** Colorectal cancer (CRC) is closely linked to the malignant transformation of colorectal polyps, making early detection essential. However, current models struggle with detecting small lesions, accurately localizing boundaries, and providing interpretable decisions. To address these issues, we propose HGNet, which integrates High-Order Spatial Awareness Hypergraph and Multi-Scale Context Attention. Key innovations include: (1) an Efficient Multi-Scale Context Attention (EMCA) module to enhance lesion feature representation and boundary modeling; (2) the deployment of a spatial hypergraph convolution module before the detection head to capture higher-order spatial relationships between nodes; (3) the application of transfer learning to address the scarcity of medical image data; and (4) Eigen Class Activation Map (Eigen-CAM) for decision visualization. Experimental results show that HGNet achieves 94% accuracy, 90.6% recall, and 90% mAP@0.5, significantly improving small lesion differentiation and clinical interpretability. The source code will be made publicly available upon publication of this paper.
>
---
#### [new 028] RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction
- **分类: cs.CV**

- **简介: 该论文属于关键点提取任务，解决弱监督下鲁棒关键点检测与描述问题。提出RIPE框架，仅需图像对是否同场景的二值标签，利用强化学习和多尺度特征融合实现高性能关键点提取。**

- **链接: [http://arxiv.org/pdf/2507.04839v1](http://arxiv.org/pdf/2507.04839v1)**

> **作者:** Johannes Künzel; Anna Hilsmann; Peter Eisert
>
> **备注:** ICCV 2025
>
> **摘要:** We introduce RIPE, an innovative reinforcement learning-based framework for weakly-supervised training of a keypoint extractor that excels in both detection and description tasks. In contrast to conventional training regimes that depend heavily on artificial transformations, pre-generated models, or 3D data, RIPE requires only a binary label indicating whether paired images represent the same scene. This minimal supervision significantly expands the pool of training data, enabling the creation of a highly generalized and robust keypoint extractor. RIPE utilizes the encoder's intermediate layers for the description of the keypoints with a hyper-column approach to integrate information from different scales. Additionally, we propose an auxiliary loss to enhance the discriminative capability of the learned descriptors. Comprehensive evaluations on standard benchmarks demonstrate that RIPE simplifies data preparation while achieving competitive performance compared to state-of-the-art techniques, marking a significant advancement in robust keypoint extraction and description. To support further research, we have made our code publicly available at https://github.com/fraunhoferhhi/RIPE.
>
---
#### [new 029] Interpretable Diffusion Models with B-cos Networks
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于文本到图像生成任务，旨在解决扩散模型难以准确反映提示语语义的问题。通过引入B-cos模块，实现模型的可解释性，揭示每个提示词对图像的影响区域。**

- **链接: [http://arxiv.org/pdf/2507.03846v1](http://arxiv.org/pdf/2507.03846v1)**

> **作者:** Nicola Bernold; Moritz Vandenhirtz; Alice Bizeul; Julia E. Vogt
>
> **摘要:** Text-to-image diffusion models generate images by iteratively denoising random noise, conditioned on a prompt. While these models have enabled impressive progress in image generation, they often fail to accurately reflect all semantic information described in the prompt -- failures that are difficult to detect automatically. In this work, we introduce a diffusion model architecture built with B-cos modules that offers inherent interpretability. Our approach provides insight into how individual prompt tokens affect the generated image by producing explanations that highlight the pixel regions influenced by each token. We demonstrate that B-cos diffusion models can produce high-quality images while providing meaningful insights into prompt-image alignment.
>
---
#### [new 030] Learning to Generate Vectorized Maps at Intersections with Multiple Roadside Cameras
- **分类: cs.CV**

- **简介: 该论文属于自动驾驶中的地图生成任务，旨在解决复杂交叉口地图构建问题。通过多摄像头输入，提出MRC-VMap模型，实现高效、高精度的矢量地图生成。**

- **链接: [http://arxiv.org/pdf/2507.02899v1](http://arxiv.org/pdf/2507.02899v1)**

> **作者:** Miao Fan; Quanxin Zheng; Shengtong Xu; Linghe Kong; Haoyi Xiong
>
> **摘要:** Vectorized maps are indispensable for precise navigation and the safe operation of autonomous vehicles. Traditional methods for constructing these maps fall into two categories: offline techniques, which rely on expensive, labor-intensive LiDAR data collection and manual annotation, and online approaches that use onboard cameras to reduce costs but suffer from limited performance, especially at complex intersections. To bridge this gap, we introduce MRC-VMap, a cost-effective, vision-centric, end-to-end neural network designed to generate high-definition vectorized maps directly at intersections. Leveraging existing roadside surveillance cameras, MRC-VMap directly converts time-aligned, multi-directional images into vectorized map representations. This integrated solution lowers the need for additional intermediate modules--such as separate feature extraction and Bird's-Eye View (BEV) conversion steps--thus reducing both computational overhead and error propagation. Moreover, the use of multiple camera views enhances mapping completeness, mitigates occlusions, and provides robust performance under practical deployment constraints. Extensive experiments conducted on 4,000 intersections across 4 major metropolitan areas in China demonstrate that MRC-VMap not only outperforms state-of-the-art online methods but also achieves accuracy comparable to high-cost LiDAR-based approaches, thereby offering a scalable and efficient solution for modern autonomous navigation systems.
>
---
#### [new 031] MGSfM: Multi-Camera Geometry Driven Global Structure-from-Motion
- **分类: cs.CV**

- **简介: 该论文属于多相机SfM任务，解决传统方法在鲁棒性和效率上的不足。提出双模块框架，提升多相机系统的结构恢复精度与效率。**

- **链接: [http://arxiv.org/pdf/2507.03306v1](http://arxiv.org/pdf/2507.03306v1)**

> **作者:** Peilin Tao; Hainan Cui; Diantao Tu; Shuhan Shen
>
> **备注:** Accepted at ICCV 2025, The code is available at https://github.com/3dv-casia/MGSfM/
>
> **摘要:** Multi-camera systems are increasingly vital in the environmental perception of autonomous vehicles and robotics. Their physical configuration offers inherent fixed relative pose constraints that benefit Structure-from-Motion (SfM). However, traditional global SfM systems struggle with robustness due to their optimization framework. We propose a novel global motion averaging framework for multi-camera systems, featuring two core components: a decoupled rotation averaging module and a hybrid translation averaging module. Our rotation averaging employs a hierarchical strategy by first estimating relative rotations within rigid camera units and then computing global rigid unit rotations. To enhance the robustness of translation averaging, we incorporate both camera-to-camera and camera-to-point constraints to initialize camera positions and 3D points with a convex distance-based objective function and refine them with an unbiased non-bilinear angle-based objective function. Experiments on large-scale datasets show that our system matches or exceeds incremental SfM accuracy while significantly improving efficiency. Our framework outperforms existing global SfM methods, establishing itself as a robust solution for real-world multi-camera SfM applications. The code is available at https://github.com/3dv-casia/MGSfM/.
>
---
#### [new 032] EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal and Multi-Task Human Animation
- **分类: cs.CV**

- **简介: 该论文属于人像动画任务，旨在解决多模态、多任务生成效率低的问题。通过统一模型架构和创新训练方法，实现高效高质量的动画生成。**

- **链接: [http://arxiv.org/pdf/2507.03905v1](http://arxiv.org/pdf/2507.03905v1)**

> **作者:** Rang Meng; Yan Wang; Weipeng Wu; Ruobing Zheng; Yuming Li; Chenguang Ma
>
> **摘要:** Human animation recently has advanced rapidly, achieving increasingly realistic and vivid results, especially with the integration of large-scale video generation models. However, the slow inference speed and high computational cost of these large models bring significant challenges for practical applications. Additionally, various tasks in human animation, such as lip-syncing, audio-driven full-body animation, and video generation from start and end frames, often require different specialized models. The introduction of large video models has not alleviated this dilemma. This raises an important question: Can we make human animation Faster, Higher in quality, Stronger in generalization, and make various tasks Together in one model? To address this, we dive into video generation models and discover that the devil lies in the details: Inspired by MAE, we propose a novel unified Multi-Task paradigm for human animation, treating diverse generation tasks as spatial-temporal local reconstructions, requiring modifications only on the input side; Given the interplay and division among multi-modal conditions including text, image, and audio, we introduce a multi-modal decoupled cross-attention module to fuse multi-modals in a divide-and-conquer manner; We propose a new SFT+Reward alternating training paradigm, enabling the minimal model with 1.3B parameters to achieve generation quality comparable to models with 10 times the parameters count. Through these innovations, our work paves the way for efficient, high-quality, and versatile digital human generation, addressing both performance and practicality challenges in the field. Extensive experiments demonstrate that EchoMimicV3 outperforms existing models in both facial and semi-body video generation, providing precise text-based control for creating videos in a wide range of scenarios.
>
---
#### [new 033] MolVision: Molecular Property Prediction with Vision Language Models
- **分类: cs.CV**

- **简介: 该论文属于分子属性预测任务，旨在解决传统文本表示不足的问题。通过引入视觉语言模型，结合分子结构图像与文本描述，提升预测性能。**

- **链接: [http://arxiv.org/pdf/2507.03283v1](http://arxiv.org/pdf/2507.03283v1)**

> **作者:** Deepan Adak; Yogesh Singh Rawat; Shruti Vyas
>
> **摘要:** Molecular property prediction is a fundamental task in computational chemistry with critical applications in drug discovery and materials science. While recent works have explored Large Language Models (LLMs) for this task, they primarily rely on textual molecular representations such as SMILES/SELFIES, which can be ambiguous and structurally less informative. In this work, we introduce MolVision, a novel approach that leverages Vision-Language Models (VLMs) by integrating both molecular structure as images and textual descriptions to enhance property prediction. We construct a benchmark spanning ten diverse datasets, covering classification, regression and description tasks. Evaluating nine different VLMs in zero-shot, few-shot, and fine-tuned settings, we find that visual information improves prediction performance, particularly when combined with efficient fine-tuning strategies such as LoRA. Our results reveal that while visual information alone is insufficient, multimodal fusion significantly enhances generalization across molecular properties. Adaptation of vision encoder for molecular images in conjunction with LoRA further improves the performance. The code and data is available at : $\href{https://molvision.github.io/MolVision/}{https://molvision.github.io/MolVision/}$.
>
---
#### [new 034] M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop Reasoning in Medical Instructional Video Understanding
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于医疗视频理解任务，旨在解决多语言、多模态和多跳推理问题。提出M3-Med基准，包含专家标注的医学问题与视频片段，评估模型深度跨模态理解能力。**

- **链接: [http://arxiv.org/pdf/2507.04289v1](http://arxiv.org/pdf/2507.04289v1)**

> **作者:** Shenxi Liu; Kan Li; Mingyang Zhao; Yuhang Tian; Bin Li; Shoujun Zhou; Hongliang Li; Fuxia Yang
>
> **备注:** 19 pages, 8 figures, 7 tables
>
> **摘要:** With the rapid progress of artificial intelligence (AI) in multi-modal understanding, there is increasing potential for video comprehension technologies to support professional domains such as medical education. However, existing benchmarks suffer from two primary limitations: (1) Linguistic Singularity: they are largely confined to English, neglecting the need for multilingual resources; and (2) Shallow Reasoning: their questions are often designed for surface-level information retrieval, failing to properly assess deep multi-modal integration. To address these limitations, we present M3-Med, the first benchmark for Multi-lingual, Multi-modal, and Multi-hop reasoning in Medical instructional video understanding. M3-Med consists of medical questions paired with corresponding video segments, annotated by a team of medical experts. A key innovation of M3-Med is its multi-hop reasoning task, which requires a model to first locate a key entity in the text, then find corresponding visual evidence in the video, and finally synthesize information across both modalities to derive the answer. This design moves beyond simple text matching and poses a substantial challenge to a model's deep cross-modal understanding capabilities. We define two tasks: Temporal Answer Grounding in Single Video (TAGSV) and Temporal Answer Grounding in Video Corpus (TAGVC). We evaluated several state-of-the-art models and Large Language Models (LLMs) on M3-Med. The results reveal a significant performance gap between all models and human experts, especially on the complex multi-hop questions where model performance drops sharply. M3-Med effectively highlights the current limitations of AI models in deep cross-modal reasoning within specialized domains and provides a new direction for future research.
>
---
#### [new 035] RainShift: A Benchmark for Precipitation Downscaling Across Geographies
- **分类: cs.CV**

- **简介: 该论文属于气候数据降尺度任务，旨在解决模型在不同地理区域泛化能力不足的问题。通过构建RainShift数据集，评估并改进模型的跨区域适应性。**

- **链接: [http://arxiv.org/pdf/2507.04930v1](http://arxiv.org/pdf/2507.04930v1)**

> **作者:** Paula Harder; Luca Schmidt; Francis Pelletier; Nicole Ludwig; Matthew Chantry; Christian Lessig; Alex Hernandez-Garcia; David Rolnick
>
> **摘要:** Earth System Models (ESM) are our main tool for projecting the impacts of climate change. However, running these models at sufficient resolution for local-scale risk-assessments is not computationally feasible. Deep learning-based super-resolution models offer a promising solution to downscale ESM outputs to higher resolutions by learning from data. Yet, due to regional variations in climatic processes, these models typically require retraining for each geographical area-demanding high-resolution observational data, which is unevenly available across the globe. This highlights the need to assess how well these models generalize across geographic regions. To address this, we introduce RainShift, a dataset and benchmark for evaluating downscaling under geographic distribution shifts. We evaluate state-of-the-art downscaling approaches including GANs and diffusion models in generalizing across data gaps between the Global North and Global South. Our findings reveal substantial performance drops in out-of-distribution regions, depending on model and geographic area. While expanding the training domain generally improves generalization, it is insufficient to overcome shifts between geographically distinct regions. We show that addressing these shifts through, for example, data alignment can improve spatial generalization. Our work advances the global applicability of downscaling methods and represents a step toward reducing inequities in access to high-resolution climate information.
>
---
#### [new 036] DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction
- **分类: cs.CV; cs.AI; cs.RO; I.4.8; I.2.7; I.2.10**

- **简介: 该论文属于自动驾驶中的运动风险预测任务，旨在提升VLM在长尾场景下的安全预测能力。通过合成高风险运动数据并设计通用框架，显著提高了模型的准确性和泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.02948v1](http://arxiv.org/pdf/2507.02948v1)**

> **作者:** Zhiyi Hou; Enhui Ma; Fang Li; Zhiyi Lai; Kalok Ho; Zhanqian Wu; Lijun Zhou; Long Chen; Chitian Sun; Haiyang Sun; Bing Wang; Guang Chen; Hangjun Ye; Kaicheng Yu
>
> **备注:** 12 pages, 4 figures. Code available at https://github.com/hzy138/DriveMRP
>
> **摘要:** Autonomous driving has seen significant progress, driven by extensive real-world data. However, in long-tail scenarios, accurately predicting the safety of the ego vehicle's future motion remains a major challenge due to uncertainties in dynamic environments and limitations in data coverage. In this work, we aim to explore whether it is possible to enhance the motion risk prediction capabilities of Vision-Language Models (VLM) by synthesizing high-risk motion data. Specifically, we introduce a Bird's-Eye View (BEV) based motion simulation method to model risks from three aspects: the ego-vehicle, other vehicles, and the environment. This allows us to synthesize plug-and-play, high-risk motion data suitable for VLM training, which we call DriveMRP-10K. Furthermore, we design a VLM-agnostic motion risk estimation framework, named DriveMRP-Agent. This framework incorporates a novel information injection strategy for global context, ego-vehicle perspective, and trajectory projection, enabling VLMs to effectively reason about the spatial relationships between motion waypoints and the environment. Extensive experiments demonstrate that by fine-tuning with DriveMRP-10K, our DriveMRP-Agent framework can significantly improve the motion risk prediction performance of multiple VLM baselines, with the accident recognition accuracy soaring from 27.13% to 88.03%. Moreover, when tested via zero-shot evaluation on an in-house real-world high-risk motion dataset, DriveMRP-Agent achieves a significant performance leap, boosting the accuracy from base_model's 29.42% to 68.50%, which showcases the strong generalization capabilities of our method in real-world scenarios.
>
---
#### [new 037] Detection of Rail Line Track and Human Beings Near the Track to Avoid Accidents
- **分类: cs.CV; cs.LG; 68T10; I.2.10; I.4.8**

- **简介: 该论文属于目标检测任务，旨在通过YOLOv5模型检测铁路轨道及附近行人，以预防事故。工作包括实时视频分析与精准识别，提升铁路安全。**

- **链接: [http://arxiv.org/pdf/2507.03040v1](http://arxiv.org/pdf/2507.03040v1)**

> **作者:** Mehrab Hosain; Rajiv Kapoor
>
> **备注:** Accepted at COMITCON 2023; Published in Lecture Notes in Electrical Engineering, Vol. 1191, Springer
>
> **摘要:** This paper presents an approach for rail line detection and the identification of human beings in proximity to the track, utilizing the YOLOv5 deep learning model to mitigate potential accidents. The technique incorporates real-time video data to identify railway tracks with impressive accuracy and recognizes nearby moving objects within a one-meter range, specifically targeting the identification of humans. This system aims to enhance safety measures in railway environments by providing real-time alerts for any detected human presence close to the track. The integration of a functionality to identify objects at a longer distance further fortifies the preventative capabilities of the system. With a precise focus on real-time object detection, this method is poised to deliver significant contributions to the existing technologies in railway safety. The effectiveness of the proposed method is demonstrated through a comprehensive evaluation, yielding a remarkable improvement in accuracy over existing methods. These results underscore the potential of this approach to revolutionize safety measures in railway environments, providing a substantial contribution to accident prevention strategies.
>
---
#### [new 038] RegistrationMamba: A Mamba-based Registration Framework Integrating Multi-Expert Feature Learning for Cross-Modal Remote Sensing Images
- **分类: cs.CV**

- **简介: 该论文属于跨模态遥感图像配准任务，解决因辐射差异大和纹理少导致的特征提取困难问题。提出RegistrationMamba框架，结合多专家特征学习和全局特征捕捉，提升配准精度与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.04397v1](http://arxiv.org/pdf/2507.04397v1)**

> **作者:** Wei Wang; Dou Quan; Chonghua Lv; Shuang Wang; Ning Huyan; Yunan Li; Licheng Jiao
>
> **摘要:** Cross-modal remote sensing image (CRSI) registration is critical for multi-modal image applications. However, CRSI mainly faces two challenges: significant nonlinear radiometric variations between cross-modal images and limited textures hindering the discriminative information extraction. Existing methods mainly adopt convolutional neural networks (CNNs) or Transformer architectures to extract discriminative features for registration. However, CNNs with the local receptive field fail to capture global contextual features, and Transformers have high computational complexity and restrict their application to high-resolution CRSI. To solve these issues, this paper proposes RegistrationMamba, a novel Mamba architecture based on state space models (SSMs) integrating multi-expert feature learning for improving the accuracy of CRSI registration. Specifically, RegistrationMamba employs a multi-directional cross-scanning strategy to capture global contextual relationships with linear complexity. To enhance the performance of RegistrationMamba under texture-limited scenarios, we propose a multi-expert feature learning (MEFL) strategy to capture features from various augmented image variants through multiple feature experts. MEFL leverages a learnable soft router to dynamically fuse the features from multiple experts, thereby enriching feature representations and improving registration performance. Notably, MEFL can be seamlessly integrated into various frameworks, substantially boosting registration performance. Additionally, RegistrationMamba integrates a multi-level feature aggregation (MFA) module to extract fine-grained local information and enable effective interaction between global and local features. Extensive experiments on CRSI with varying image resolutions have demonstrated that RegistrationMamba has superior performance and robustness compared to state-of-the-art methods.
>
---
#### [new 039] Sign Spotting Disambiguation using Large Language Models
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于手语识别任务，解决连续手语视频中符号识别的歧义问题。通过整合大语言模型和字典匹配，提升识别准确性和流畅性。**

- **链接: [http://arxiv.org/pdf/2507.03703v1](http://arxiv.org/pdf/2507.03703v1)**

> **作者:** JianHe Low; Ozge Mercanoglu Sincan; Richard Bowden
>
> **摘要:** Sign spotting, the task of identifying and localizing individual signs within continuous sign language video, plays a pivotal role in scaling dataset annotations and addressing the severe data scarcity issue in sign language translation. While automatic sign spotting holds great promise for enabling frame-level supervision at scale, it grapples with challenges such as vocabulary inflexibility and ambiguity inherent in continuous sign streams. Hence, we introduce a novel, training-free framework that integrates Large Language Models (LLMs) to significantly enhance sign spotting quality. Our approach extracts global spatio-temporal and hand shape features, which are then matched against a large-scale sign dictionary using dynamic time warping and cosine similarity. This dictionary-based matching inherently offers superior vocabulary flexibility without requiring model retraining. To mitigate noise and ambiguity from the matching process, an LLM performs context-aware gloss disambiguation via beam search, notably without fine-tuning. Extensive experiments on both synthetic and real-world sign language datasets demonstrate our method's superior accuracy and sentence fluency compared to traditional approaches, highlighting the potential of LLMs in advancing sign spotting.
>
---
#### [new 040] Transparent Machine Learning: Training and Refining an Explainable Boosting Machine to Identify Overshooting Tops in Satellite Imagery
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于气象图像识别任务，旨在解决overshooting tops检测问题。通过结合特征工程与EBM算法，构建可解释的机器学习模型。**

- **链接: [http://arxiv.org/pdf/2507.03183v1](http://arxiv.org/pdf/2507.03183v1)**

> **作者:** Nathan Mitchell; Lander Ver Hoef; Imme Ebert-Uphoff; Kristina Moen; Kyle Hilburn; Yoonjin Lee; Emily J. King
>
> **备注:** 38 pages, 19 figures
>
> **摘要:** An Explainable Boosting Machine (EBM) is an interpretable machine learning (ML) algorithm that has benefits in high risk applications but has not yet found much use in atmospheric science. The overall goal of this work is twofold: (1) explore the use of EBMs, in combination with feature engineering, to obtain interpretable, physics-based machine learning algorithms for meteorological applications; (2) illustrate these methods for the detection of overshooting top (OTs) in satellite imagery. Specifically, we seek to simplify the process of OT detection by first using mathematical methods to extract key features, such as cloud texture using Gray-Level Co-occurrence Matrices, followed by applying an EBM. Our EBM focuses on the classification task of predicting OT regions, utilizing Channel 2 (visible imagery) and Channel 13 (infrared imagery) of the Advanced Baseline Imager sensor of the Geostationary Operational Environmental Satellite 16. Multi-Radar/Multi-Sensor system convection flags are used as labels to train the EBM model. Note, however, that detecting convection, while related, is different from detecting OTs. Once trained, the EBM was examined and minimally altered to more closely match strategies used by domain scientists to identify OTs. The result of our efforts is a fully interpretable ML algorithm that was developed in a human-machine collaboration. While the final model does not reach the accuracy of more complex approaches, it performs well and represents a significant step toward building fully interpretable ML algorithms for this and other meteorological applications.
>
---
#### [new 041] Voyaging into Unbounded Dynamic Scenes from a Single View
- **分类: cs.CV**

- **简介: 该论文属于动态场景生成任务，旨在从单视角生成连贯的无限动态场景。通过引入射线上下文和点云优化，实现3D运动一致性，支持自由视角浏览。**

- **链接: [http://arxiv.org/pdf/2507.04183v1](http://arxiv.org/pdf/2507.04183v1)**

> **作者:** Fengrui Tian; Tianjiao Ding; Jinqi Luo; Hancheng Min; René Vidal
>
> **备注:** Accepted by International Conference on Computer Vision (ICCV) 2025. Project Page: https://tianfr.github.io/DynamicVoyager
>
> **摘要:** This paper studies the problem of generating an unbounded dynamic scene from a single view, which has wide applications in augmented/virtual reality and robotics. Since the scene is changing over time, different generated views need to be consistent with the underlying 3D motions. While previous works learn such consistency by training from multiple views, the generated scene regions are bounded to be close to the training views with limited camera movements. To address this issue, we propose DynamicVoyager that reformulates the dynamic scene generation as a scene outpainting process for new dynamic content. As 2D outpainting models can hardly generate 3D consistent motions from only 2D pixels at a single view, we consider pixels as rays to enrich the pixel input with the ray context, so that the 3D motion consistency can be learned from the ray information. More specifically, we first map the single-view video input to a dynamic point cloud with the estimated video depths. Then we render the partial video at a novel view and outpaint the video with ray contexts from the point cloud to generate 3D consistent motions. We employ the outpainted video to update the point cloud, which is used for scene outpainting from future novel views. Experiments show that our model is able to generate unbounded scenes with consistent motions along fly-through cameras, and the generated contents can be controlled with scene prompts.
>
---
#### [new 042] What's Making That Sound Right Now? Video-centric Audio-Visual Localization
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于音频-视觉定位任务，解决现有方法忽视时间动态和复杂场景的问题。提出AVATAR基准和TAVLO模型，增强视频中的时序建模与定位精度。**

- **链接: [http://arxiv.org/pdf/2507.04667v1](http://arxiv.org/pdf/2507.04667v1)**

> **作者:** Hahyeon Choi; Junhoo Lee; Nojun Kwak
>
> **备注:** Published at ICCV 2025. Project page: https://hahyeon610.github.io/Video-centric_Audio_Visual_Localization/
>
> **摘要:** Audio-Visual Localization (AVL) aims to identify sound-emitting sources within a visual scene. However, existing studies focus on image-level audio-visual associations, failing to capture temporal dynamics. Moreover, they assume simplified scenarios where sound sources are always visible and involve only a single object. To address these limitations, we propose AVATAR, a video-centric AVL benchmark that incorporates high-resolution temporal information. AVATAR introduces four distinct scenarios -- Single-sound, Mixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive evaluation of AVL models. Additionally, we present TAVLO, a novel video-centric AVL model that explicitly integrates temporal information. Experimental results show that conventional methods struggle to track temporal variations due to their reliance on global audio features and frame-level mappings. In contrast, TAVLO achieves robust and precise audio-visual alignment by leveraging high-resolution temporal modeling. Our work empirically demonstrates the importance of temporal dynamics in AVL and establishes a new standard for video-centric audio-visual localization.
>
---
#### [new 043] Learning Normals of Noisy Points by Local Gradient-Aware Surface Filtering
- **分类: cs.CV**

- **简介: 该论文属于3D几何处理任务，旨在解决噪声点云法向量估计问题。通过局部梯度感知的表面滤波方法，提升法向量估计精度。**

- **链接: [http://arxiv.org/pdf/2507.03394v1](http://arxiv.org/pdf/2507.03394v1)**

> **作者:** Qing Li; Huifang Feng; Xun Gong; Yu-Shen Liu
>
> **备注:** Accepted by ICCV 2025. Code: https://github.com/LeoQLi/LGSF
>
> **摘要:** Estimating normals for noisy point clouds is a persistent challenge in 3D geometry processing, particularly for end-to-end oriented normal estimation. Existing methods generally address relatively clean data and rely on supervised priors to fit local surfaces within specific neighborhoods. In this paper, we propose a novel approach for learning normals from noisy point clouds through local gradient-aware surface filtering. Our method projects noisy points onto the underlying surface by utilizing normals and distances derived from an implicit function constrained by local gradients. We start by introducing a distance measurement operator for global surface fitting on noisy data, which integrates projected distances along normals. Following this, we develop an implicit field-based filtering approach for surface point construction, adding projection constraints on these points during filtering. To address issues of over-smoothing and gradient degradation, we further incorporate local gradient consistency constraints, as well as local gradient orientation and aggregation. Comprehensive experiments on normal estimation, surface reconstruction, and point cloud denoising demonstrate the state-of-the-art performance of our method. The source code and trained models are available at https://github.com/LeoQLi/LGSF.
>
---
#### [new 044] Masked Temporal Interpolation Diffusion for Procedure Planning in Instructional Videos
- **分类: cs.CV**

- **简介: 该论文属于视频动作规划任务，解决从起始和结束视觉状态生成连贯动作序列的问题。提出MTID模型，结合时间插值和掩码机制提升动作预测准确性。**

- **链接: [http://arxiv.org/pdf/2507.03393v1](http://arxiv.org/pdf/2507.03393v1)**

> **作者:** Yufan Zhou; Zhaobo Qi; Lingshuai Lin; Junqi Jing; Tingting Chai; Beichen Zhang; Shuhui Wang; Weigang Zhang
>
> **摘要:** In this paper, we address the challenge of procedure planning in instructional videos, aiming to generate coherent and task-aligned action sequences from start and end visual observations. Previous work has mainly relied on text-level supervision to bridge the gap between observed states and unobserved actions, but it struggles with capturing intricate temporal relationships among actions. Building on these efforts, we propose the Masked Temporal Interpolation Diffusion (MTID) model that introduces a latent space temporal interpolation module within the diffusion model. This module leverages a learnable interpolation matrix to generate intermediate latent features, thereby augmenting visual supervision with richer mid-state details. By integrating this enriched supervision into the model, we enable end-to-end training tailored to task-specific requirements, significantly enhancing the model's capacity to predict temporally coherent action sequences. Additionally, we introduce an action-aware mask projection mechanism to restrict the action generation space, combined with a task-adaptive masked proximity loss to prioritize more accurate reasoning results close to the given start and end states over those in intermediate steps. Simultaneously, it filters out task-irrelevant action predictions, leading to contextually aware action sequences. Experimental results across three widely used benchmark datasets demonstrate that our MTID achieves promising action planning performance on most metrics. The code is available at https://github.com/WiserZhou/MTID.
>
---
#### [new 045] 2.5D Object Detection for Intelligent Roadside Infrastructure
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于目标检测任务，解决自动驾驶中传感器受限问题，提出2.5D检测框架，通过图像平面检测车辆轮廓，提升跨视角和恶劣环境下的检测性能。**

- **链接: [http://arxiv.org/pdf/2507.03564v1](http://arxiv.org/pdf/2507.03564v1)**

> **作者:** Nikolai Polley; Yacin Boualili; Ferdinand Mütsch; Maximilian Zipfl; Tobias Fleck; J. Marius Zöllner
>
> **备注:** Accepted at 2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC)
>
> **摘要:** On-board sensors of autonomous vehicles can be obstructed, occluded, or limited by restricted fields of view, complicating downstream driving decisions. Intelligent roadside infrastructure perception systems, installed at elevated vantage points, can provide wide, unobstructed intersection coverage, supplying a complementary information stream to autonomous vehicles via vehicle-to-everything (V2X) communication. However, conventional 3D object-detection algorithms struggle to generalize under the domain shift introduced by top-down perspectives and steep camera angles. We introduce a 2.5D object detection framework, tailored specifically for infrastructure roadside-mounted cameras. Unlike conventional 2D or 3D object detection, we employ a prediction approach to detect ground planes of vehicles as parallelograms in the image frame. The parallelogram preserves the planar position, size, and orientation of objects while omitting their height, which is unnecessary for most downstream applications. For training, a mix of real-world and synthetically generated scenes is leveraged. We evaluate generalizability on a held-out camera viewpoint and in adverse-weather scenarios absent from the training set. Our results show high detection accuracy, strong cross-viewpoint generalization, and robustness to diverse lighting and weather conditions. Model weights and inference code are provided at: https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection
>
---
#### [new 046] Markerless Stride Length estimation in Athletic using Pose Estimation with monocular vision
- **分类: cs.CV**

- **简介: 该论文属于计算机视觉任务，旨在无需标记测量运动员步长。通过姿态估计和单目视觉技术，准确估算步长和速度，辅助训练与监测。**

- **链接: [http://arxiv.org/pdf/2507.03016v1](http://arxiv.org/pdf/2507.03016v1)**

> **作者:** Patryk Skorupski; Cosimo Distante; Pier Luigi Mazzeo
>
> **摘要:** Performance measures such as stride length in athletics and the pace of runners can be estimated using different tricks such as measuring the number of steps divided by the running length or helping with markers printed on the track. Monitoring individual performance is essential for supporting staff coaches in establishing a proper training schedule for each athlete. The aim of this paper is to investigate a computer vision-based approach for estimating stride length and speed transition from video sequences and assessing video analysis processing among athletes. Using some well-known image processing methodologies such as probabilistic hough transform combined with a human pose detection algorithm, we estimate the leg joint position of runners. In this way, applying a homography transformation, we can estimate the runner stride length. Experiments on various race videos with three different runners demonstrated that the proposed system represents a useful tool for coaching and training. This suggests its potential value in measuring and monitoring the gait parameters of athletes.
>
---
#### [new 047] Semantically Consistent Discrete Diffusion for 3D Biological Graph Modeling
- **分类: cs.CV**

- **简介: 该论文属于3D生物图生成任务，解决生成过程中保持解剖有效性的问题。提出一种新方法，通过投影操作和边删除噪声提升生成质量与下游任务性能。**

- **链接: [http://arxiv.org/pdf/2507.04856v1](http://arxiv.org/pdf/2507.04856v1)**

> **作者:** Chinmay Prabhakar; Suprosanna Shit; Tamaz Amiranashvili; Hongwei Bran Li; Bjoern Menze
>
> **备注:** Accepted to MICCAI 2025
>
> **摘要:** 3D spatial graphs play a crucial role in biological and clinical research by modeling anatomical networks such as blood vessels,neurons, and airways. However, generating 3D biological graphs while maintaining anatomical validity remains challenging, a key limitation of existing diffusion-based methods. In this work, we propose a novel 3D biological graph generation method that adheres to structural and semantic plausibility conditions. We achieve this by using a novel projection operator during sampling that stochastically fixes inconsistencies. Further, we adopt a superior edge-deletion-based noising procedure suitable for sparse biological graphs. Our method demonstrates superior performance on two real-world datasets, human circle of Willis and lung airways, compared to previous approaches. Importantly, we demonstrate that the generated samples significantly enhance downstream graph labeling performance. Furthermore, we show that our generative model is a reasonable out-of-the-box link predictior.
>
---
#### [new 048] CMET: Clustering guided METric for quantifying embedding quality
- **分类: cs.CV**

- **简介: 该论文属于嵌入质量评估任务，旨在解决现有度量方法计算成本高、效率低的问题，提出CMET指标，有效评估嵌入数据的局部和全局结构保留能力。**

- **链接: [http://arxiv.org/pdf/2507.04840v1](http://arxiv.org/pdf/2507.04840v1)**

> **作者:** Sourav Ghosh; Chayan Maitra; Rajat K. De
>
> **备注:** 22 pages, 19 figures
>
> **摘要:** Due to rapid advancements in technology, datasets are available from various domains. In order to carry out more relevant and appropriate analysis, it is often necessary to project the dataset into a higher or lower dimensional space based on requirement. Projecting the data in a higher-dimensional space helps in unfolding intricate patterns, enhancing the performance of the underlying models. On the other hand, dimensionality reduction is helpful in denoising data while capturing maximal information, as well as reducing execution time and memory.In this context, it is not always statistically evident whether the transformed embedding retains the local and global structure of the original data. Most of the existing metrics that are used for comparing the local and global shape of the embedding against the original one are highly expensive in terms of time and space complexity. In order to address this issue, the objective of this study is to formulate a novel metric, called Clustering guided METric (CMET), for quantifying embedding quality. It is effective to serve the purpose of quantitative comparison between an embedding and the original data. CMET consists of two scores, viz., CMET_L and CMET_G, that measure the degree of local and global shape preservation capability, respectively. The efficacy of CMET has been demonstrated on a wide variety of datasets, including four synthetic, two biological, and two image datasets. Results reflect the favorable performance of CMET against the state-of-the-art methods. Capability to handle both small and large data, low algorithmic complexity, better and stable performance across all kinds of data, and different choices of hyper-parameters feature CMET as a reliable metric.
>
---
#### [new 049] Learning Disentangled Stain and Structural Representations for Semi-Supervised Histopathology Segmentation
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于医学图像分割任务，旨在解决H&E染色差异和标注数据不足导致的腺体分割难题。通过引入CSDS框架，学习分离的染色和结构表示，提升半监督分割效果。**

- **链接: [http://arxiv.org/pdf/2507.03923v1](http://arxiv.org/pdf/2507.03923v1)**

> **作者:** Ha-Hieu Pham; Nguyen Lan Vi Vu; Thanh-Huy Nguyen; Ulas Bagci; Min Xu; Trung-Nghia Le; Huy-Hieu Pham
>
> **摘要:** Accurate gland segmentation in histopathology images is essential for cancer diagnosis and prognosis. However, significant variability in Hematoxylin and Eosin (H&E) staining and tissue morphology, combined with limited annotated data, poses major challenges for automated segmentation. To address this, we propose Color-Structure Dual-Student (CSDS), a novel semi-supervised segmentation framework designed to learn disentangled representations of stain appearance and tissue structure. CSDS comprises two specialized student networks: one trained on stain-augmented inputs to model chromatic variation, and the other on structure-augmented inputs to capture morphological cues. A shared teacher network, updated via Exponential Moving Average (EMA), supervises both students through pseudo-labels. To further improve label reliability, we introduce stain-aware and structure-aware uncertainty estimation modules that adaptively modulate the contribution of each student during training. Experiments on the GlaS and CRAG datasets show that CSDS achieves state-of-the-art performance in low-label settings, with Dice score improvements of up to 1.2% on GlaS and 0.7% on CRAG at 5% labeled data, and 0.7% and 1.4% at 10%. Our code and pre-trained models are available at https://github.com/hieuphamha19/CSDS.
>
---
#### [new 050] LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains
- **分类: cs.CV; cs.AI; cs.CR**

- **简介: 该论文属于AI生成图像检测任务，旨在解决现有方法计算成本高的问题。通过评估轻量级模型，在保证精度的同时降低资源消耗。**

- **链接: [http://arxiv.org/pdf/2507.05162v1](http://arxiv.org/pdf/2507.05162v1)**

> **作者:** Nicholas Chivaran; Jianbing Ni
>
> **备注:** To appear in the proceedings of PST2025
>
> **摘要:** The recent proliferation of photorealistic AI-generated images (AIGI) has raised urgent concerns about their potential misuse, particularly on social media platforms. Current state-of-the-art AIGI detection methods typically rely on large, deep neural architectures, creating significant computational barriers to real-time, large-scale deployment on platforms like social media. To challenge this reliance on computationally intensive models, we introduce LAID, the first framework -- to our knowledge -- that benchmarks and evaluates the detection performance and efficiency of off-the-shelf lightweight neural networks. In this framework, we comprehensively train and evaluate selected models on a representative subset of the GenImage dataset across spatial, spectral, and fusion image domains. Our results demonstrate that lightweight models can achieve competitive accuracy, even under adversarial conditions, while incurring substantially lower memory and computation costs compared to current state-of-the-art methods. This study offers valuable insight into the trade-off between efficiency and performance in AIGI detection and lays a foundation for the development of practical, scalable, and trustworthy detection systems. The source code of LAID can be found at: https://github.com/nchivar/LAID.
>
---
#### [new 051] FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于视觉预训练任务，旨在提升DINOv2的训练效率与鲁棒性。通过频率课程学习和高斯噪声增强，减少计算量并保持性能。**

- **链接: [http://arxiv.org/pdf/2507.03779v1](http://arxiv.org/pdf/2507.03779v1)**

> **作者:** Jiaqi Zhang; Juntuo Wang; Zhixin Sun; John Zou; Randall Balestriero
>
> **摘要:** Large-scale vision foundation models such as DINOv2 boast impressive performances by leveraging massive architectures and training datasets. But numerous scenarios require practitioners to reproduce those pre-training solutions, such as on private data, new modalities, or simply for scientific questioning--which is currently extremely demanding computation-wise. We thus propose a novel pre-training strategy for DINOv2 that simultaneously accelerates convergence--and strengthens robustness to common corruptions as a by-product. Our approach involves a frequency filtering curriculum--low-frequency being seen first--and the Gaussian noise patching augmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while pre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still achieves matching robustness in corruption benchmarks (ImageNet-C) and maintains competitive linear probing performance compared with baseline. This dual benefit of efficiency and robustness makes large-scale self-supervised foundation modeling more attainable, while opening the door to novel exploration around data curriculum and augmentation as means to improve self-supervised learning models robustness. The code is available at https://github.com/KevinZ0217/fast_dinov2
>
---
#### [new 052] Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于多模态视觉推理任务，旨在将语言模型的推理能力迁移至视觉领域。通过两阶段训练方法，提升模型在视觉推理任务上的表现。**

- **链接: [http://arxiv.org/pdf/2507.05255v1](http://arxiv.org/pdf/2507.05255v1)**

> **作者:** Yana Wei; Liang Zhao; Jianjian Sun; Kangheng Lin; Jisheng Yin; Jingcheng Hu; Yinmin Zhang; En Yu; Haoran Lv; Zejia Weng; Jia Wang; Chunrui Han; Yuang Peng; Qi Han; Zheng Ge; Xiangyu Zhang; Daxin Jiang; Vishal M. Patel
>
> **摘要:** The remarkable reasoning capability of large language models (LLMs) stems from cognitive behaviors that emerge through reinforcement with verifiable rewards. This work investigates how to transfer this principle to Multimodal LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning, followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps, surpassing all previous open-source efforts in scale. This pioneering work reveals three fundamental insights: 1) Behavior transfer emerges surprisingly early in cold start due to linguistic mental imagery. 2) Cold start broadly memorizes visual behaviors, while RL critically discerns and scales up effective patterns. 3) Transfer strategically favors high-utility behaviors such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR), achieves state-of-the-art performance on a suite of reasoning benchmarks, including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We release our model, data, and training dynamics to catalyze the development of more capable, behavior-aligned multimodal reasoners.
>
---
#### [new 053] Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing
- **分类: cs.CV**

- **简介: 该论文属于SAR船舶检测任务，旨在解决卫星上部署高效低功耗模型的问题。通过优化YOLOv8架构，在FPGA上实现高性能检测。**

- **链接: [http://arxiv.org/pdf/2507.04842v1](http://arxiv.org/pdf/2507.04842v1)**

> **作者:** Colin Laganier; Liam Fletcher; Elim Kwan; Richard Walters; Victoria Nockles
>
> **备注:** 14 pages, 5 figures, 3 table
>
> **摘要:** Rapid analysis of satellite data is vital for many remote sensing applications, from disaster response to environmental monitoring, but is becoming harder to achieve with the increasing volumes of data generated by modern satellites. On-satellite machine learning (ML) offers a potential solution, by reducing latency associated with transmission of these large data volumes to ground stations, but state-of-the-art models are often too large or power-hungry for satellite deployment. Vessel detection using Synthetic Aperture Radar (SAR) is a critical time-sensitive task for maritime security that exemplifies this challenge. SAR vessel detection has previously been demonstrated only by ML models that either are too large for satellite deployment, have not been developed for sufficiently low-power hardware, or have only been developed and tested on small SAR datasets that do not sufficiently represent the real-world task. Here we address this issue by developing and deploying a new efficient and highly performant SAR vessel detection model, using a customised YOLOv8 architecture specifically optimized for FPGA-based processing within common satellite power constraints (<10W). We train and evaluate our model on the largest and most diverse open SAR vessel dataset, xView3-SAR, and deploy it on a Kria KV260 MPSoC. We show that our FPGA-based model has detection and classification performance only ~2% and 3% lower than values from state-of-the-art GPU-based models, despite being two to three orders of magnitude smaller in size. This work demonstrates small yet highly performant ML models for time-critical SAR analysis, paving the way for more autonomous, responsive, and scalable Earth observation systems.
>
---
#### [new 054] 4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture
- **分类: cs.CV**

- **简介: 该论文属于4D重建任务，解决高帧率场景下低帧率相机的重建问题。通过异步采集和生成模型提升重建质量与帧率。**

- **链接: [http://arxiv.org/pdf/2507.05163v1](http://arxiv.org/pdf/2507.05163v1)**

> **作者:** Yutian Chen; Shi Guo; Tianshuo Yang; Lihe Ding; Xiuyuan Yu; Jinwei Gu; Tianfan Xue
>
> **备注:** Webpage: https://openimaginglab.github.io/4DSloMo/
>
> **摘要:** Reconstructing fast-dynamic scenes from multi-view videos is crucial for high-speed motion analysis and realistic 4D reconstruction. However, the majority of 4D capture systems are limited to frame rates below 30 FPS (frames per second), and a direct 4D reconstruction of high-speed motion from low FPS input may lead to undesirable results. In this work, we propose a high-speed 4D capturing system only using low FPS cameras, through novel capturing and processing modules. On the capturing side, we propose an asynchronous capture scheme that increases the effective frame rate by staggering the start times of cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our method achieves an equivalent frame rate of 100-200 FPS without requiring specialized high-speed cameras. On processing side, we also propose a novel generative model to fix artifacts caused by 4D sparse-view reconstruction, as asynchrony reduces the number of viewpoints at each timestamp. Specifically, we propose to train a video-diffusion-based artifact-fix model for sparse 4D reconstruction, which refines missing details, maintains temporal consistency, and improves overall reconstruction quality. Experimental results demonstrate that our method significantly enhances high-speed 4D reconstruction compared to synchronous capture.
>
---
#### [new 055] MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images
- **分类: cs.CV; 68U05; I.3.7; I.3.3; I.4.1**

- **简介: 该论文属于3D重建与材质分解任务，解决从多视角图像中恢复高保真几何和物理材质的问题。通过联合优化神经SDF、材质场和光照模型，实现精准的逆渲染与可编辑资产生成。**

- **链接: [http://arxiv.org/pdf/2507.04749v1](http://arxiv.org/pdf/2507.04749v1)**

> **作者:** Chengyu Wang; Isabella Bennett; Henry Scott; Liang Zhang; Mei Chen; Hao Li; Rui Zhao
>
> **备注:** 12 pages, 4 figures
>
> **摘要:** We present MatDecompSDF, a novel framework for recovering high-fidelity 3D shapes and decomposing their physically-based material properties from multi-view images. The core challenge of inverse rendering lies in the ill-posed disentanglement of geometry, materials, and illumination from 2D observations. Our method addresses this by jointly optimizing three neural components: a neural Signed Distance Function (SDF) to represent complex geometry, a spatially-varying neural field for predicting PBR material parameters (albedo, roughness, metallic), and an MLP-based model for capturing unknown environmental lighting. The key to our approach is a physically-based differentiable rendering layer that connects these 3D properties to the input images, allowing for end-to-end optimization. We introduce a set of carefully designed physical priors and geometric regularizations, including a material smoothness loss and an Eikonal loss, to effectively constrain the problem and achieve robust decomposition. Extensive experiments on both synthetic and real-world datasets (e.g., DTU) demonstrate that MatDecompSDF surpasses state-of-the-art methods in geometric accuracy, material fidelity, and novel view synthesis. Crucially, our method produces editable and relightable assets that can be seamlessly integrated into standard graphics pipelines, validating its practical utility for digital content creation.
>
---
#### [new 056] Structure-Guided Diffusion Models for High-Fidelity Portrait Shadow Removal
- **分类: cs.CV**

- **简介: 该论文属于图像修复任务，旨在解决人像阴影去除问题。通过结构引导的扩散模型，有效去除阴影并保留细节，避免常见问题。**

- **链接: [http://arxiv.org/pdf/2507.04692v1](http://arxiv.org/pdf/2507.04692v1)**

> **作者:** Wanchang Yu; Qing Zhang; Rongjia Zheng; Wei-Shi Zheng
>
> **摘要:** We present a diffusion-based portrait shadow removal approach that can robustly produce high-fidelity results. Unlike previous methods, we cast shadow removal as diffusion-based inpainting. To this end, we first train a shadow-independent structure extraction network on a real-world portrait dataset with various synthetic lighting conditions, which allows to generate a shadow-independent structure map including facial details while excluding the unwanted shadow boundaries. The structure map is then used as condition to train a structure-guided inpainting diffusion model for removing shadows in a generative manner. Finally, to restore the fine-scale details (e.g., eyelashes, moles and spots) that may not be captured by the structure map, we take the gradients inside the shadow regions as guidance and train a detail restoration diffusion model to refine the shadow removal result. Extensive experiments on the benchmark datasets show that our method clearly outperforms existing methods, and is effective to avoid previously common issues such as facial identity tampering, shadow residual, color distortion, structure blurring, and loss of details. Our code is available at https://github.com/wanchang-yu/Structure-Guided-Diffusion-for-Portrait-Shadow-Removal.
>
---
#### [new 057] $\varphi$-Adapt: A Physics-Informed Adaptation Learning Approach to 2D Quantum Material Discovery
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于2D量子材料识别任务，解决数据稀缺和模型泛化问题。提出$\varphi$-Adapt方法，结合物理信息与域适应，提升合成数据在真实场景中的应用效果。**

- **链接: [http://arxiv.org/pdf/2507.05184v1](http://arxiv.org/pdf/2507.05184v1)**

> **作者:** Hoang-Quan Nguyen; Xuan Bac Nguyen; Sankalp Pandey; Tim Faltermeier; Nicholas Borys; Hugh Churchill; Khoa Luu
>
> **摘要:** Characterizing quantum flakes is a critical step in quantum hardware engineering because the quality of these flakes directly influences qubit performance. Although computer vision methods for identifying two-dimensional quantum flakes have emerged, they still face significant challenges in estimating flake thickness. These challenges include limited data, poor generalization, sensitivity to domain shifts, and a lack of physical interpretability. In this paper, we introduce one of the first Physics-informed Adaptation Learning approaches to overcome these obstacles. We focus on two main issues, i.e., data scarcity and generalization. First, we propose a new synthetic data generation framework that produces diverse quantum flake samples across various materials and configurations, reducing the need for time-consuming manual collection. Second, we present $\varphi$-Adapt, a physics-informed adaptation method that bridges the performance gap between models trained on synthetic data and those deployed in real-world settings. Experimental results show that our approach achieves state-of-the-art performance on multiple benchmarks, outperforming existing methods. Our proposed approach advances the integration of physics-based modeling and domain adaptation. It also addresses a critical gap in leveraging synthesized data for real-world 2D material analysis, offering impactful tools for deep learning and materials science communities.
>
---
#### [new 058] MambaFusion: Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection
- **分类: cs.CV**

- **简介: 该论文属于多模态3D目标检测任务，解决融合效率与精度问题，提出MambaFusion方法提升相机-激光雷达对齐与特征融合。**

- **链接: [http://arxiv.org/pdf/2507.04369v1](http://arxiv.org/pdf/2507.04369v1)**

> **作者:** Hanshi Wang; Jin Gao; Weiming Hu; Zhipeng Zhang
>
> **备注:** 10 pages
>
> **摘要:** We present the first work demonstrating that a pure Mamba block can achieve efficient Dense Global Fusion, meanwhile guaranteeing top performance for camera-LiDAR multi-modal 3D object detection. Our motivation stems from the observation that existing fusion strategies are constrained by their inability to simultaneously achieve efficiency, long-range modeling, and retaining complete scene information. Inspired by recent advances in state-space models (SSMs) and linear attention, we leverage their linear complexity and long-range modeling capabilities to address these challenges. However, this is non-trivial since our experiments reveal that simply adopting efficient linear-complexity methods does not necessarily yield improvements and may even degrade performance. We attribute this degradation to the loss of height information during multi-modal alignment, leading to deviations in sequence order. To resolve this, we propose height-fidelity LiDAR encoding that preserves precise height information through voxel compression in continuous space, thereby enhancing camera-LiDAR alignment. Subsequently, we introduce the Hybrid Mamba Block, which leverages the enriched height-informed features to conduct local and global contextual learning. By integrating these components, our method achieves state-of-the-art performance with the top-tire NDS score of 75.0 on the nuScenes validation benchmark, even surpassing methods that utilize high-resolution inputs. Meanwhile, our method maintains efficiency, achieving faster inference speed than most recent state-of-the-art methods.
>
---
#### [new 059] DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于机器人操作任务，旨在解决现有方法在图像预测中信息冗余和缺乏全面世界知识的问题。提出DreamVLA框架，整合动态、空间和语义信息，提升动作规划与执行效果。**

- **链接: [http://arxiv.org/pdf/2507.04447v1](http://arxiv.org/pdf/2507.04447v1)**

> **作者:** Wenyao Zhang; Hongsi Liu; Zekun Qi; Yunnan Wang; XinQiang Yu; Jiazhao Zhang; Runpei Dong; Jiawei He; He Wang; Zhizheng Zhang; Li Yi; Wenjun Zeng; Xin Jin
>
> **摘要:** Recent advances in vision-language-action (VLA) models have shown promise in integrating image generation with action prediction to improve generalization and reasoning in robot manipulation. However, existing methods are limited to challenging image-based forecasting, which suffers from redundant information and lacks comprehensive and critical world knowledge, including dynamic, spatial and semantic information. To address these limitations, we propose DreamVLA, a novel VLA framework that integrates comprehensive world knowledge forecasting to enable inverse dynamics modeling, thereby establishing a perception-prediction-action loop for manipulation tasks. Specifically, DreamVLA introduces a dynamic-region-guided world knowledge prediction, integrated with the spatial and semantic cues, which provide compact yet comprehensive representations for action planning. This design aligns with how humans interact with the world by first forming abstract multimodal reasoning chains before acting. To mitigate interference among the dynamic, spatial and semantic information during training, we adopt a block-wise structured attention mechanism that masks their mutual attention, preventing information leakage and keeping each representation clean and disentangled. Moreover, to model the conditional distribution over future actions, we employ a diffusion-based transformer that disentangles action representations from shared latent features. Extensive experiments on both real-world and simulation environments demonstrate that DreamVLA achieves 76.7% success rate on real robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.
>
---
#### [new 060] From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving
- **分类: cs.CV; cs.AI; cs.LG; cs.MA; cs.RO**

- **简介: 该论文属于自动驾驶中的轨迹预测任务，旨在解决多智能体交互下的运动预测问题。通过对比分析不同联合预测方法，提升预测的准确性与一致性。**

- **链接: [http://arxiv.org/pdf/2507.05254v1](http://arxiv.org/pdf/2507.05254v1)**

> **作者:** Fabian Konstantinidis; Ariel Dallari Guerreiro; Raphael Trumpp; Moritz Sackmann; Ulrich Hofmann; Marco Caccamo; Christoph Stiller
>
> **备注:** Accepted at International Conference on Intelligent Transportation Systems 2025 (ITSC 2025)
>
> **摘要:** Accurate motion prediction of surrounding traffic participants is crucial for the safe and efficient operation of automated vehicles in dynamic environments. Marginal prediction models commonly forecast each agent's future trajectories independently, often leading to sub-optimal planning decisions for an automated vehicle. In contrast, joint prediction models explicitly account for the interactions between agents, yielding socially and physically consistent predictions on a scene level. However, existing approaches differ not only in their problem formulation but also in the model architectures and implementation details used, making it difficult to compare them. In this work, we systematically investigate different approaches to joint motion prediction, including post-processing of the marginal predictions, explicitly training the model for joint predictions, and framing the problem as a generative task. We evaluate each approach in terms of prediction accuracy, multi-modality, and inference efficiency, offering a comprehensive analysis of the strengths and limitations of each approach. Several prediction examples are available at https://frommarginaltojointpred.github.io/.
>
---
#### [new 061] AI for the Routine, Humans for the Complex: Accuracy-Driven Data Labelling with Mixed Integer Linear Programming
- **分类: cs.CV; cs.SE**

- **简介: 该论文属于数据标注任务，解决准确标注数据稀缺问题。通过混合整数线性规划方法OPAL，在保证高精度的同时减少人工标注工作量。**

- **链接: [http://arxiv.org/pdf/2507.04990v1](http://arxiv.org/pdf/2507.04990v1)**

> **作者:** Mohammad Hossein Amini; Mehrdad Sabetzadeh; Shiva Nejati
>
> **摘要:** The scarcity of accurately labelled data remains a major challenge in deep learning (DL). Many DL approaches rely on semi-supervised methods, which focus on constructing large datasets that require only a minimal amount of human-labelled data. Since DL training algorithms can tolerate moderate label noise, it has generally been acceptable for the accuracy of labels in large training datasets to fall well short of a perfect 100%. However, when it comes to testing DL models, achieving high label accuracy-as close to 100% as possible-is paramount for reliable verification. In this article, we introduce OPAL, a human-assisted labelling method that can be configured to target a desired accuracy level while minimizing the manual effort required for labelling. The main contribution of OPAL is a mixed-integer linear programming (MILP) formulation that minimizes labelling effort subject to a specified accuracy target. We evaluate OPAL for two tasks in the context of testing vision systems: automatic labelling of test data and automated validation of test data. Our evaluation, based on more than 2500 experiments performed on seven datasets, comparing OPAL with eight baseline methods, shows that OPAL, relying on its MILP formulation, achieves an average accuracy of 98.8%, just 1.2% below perfect accuracy, while cutting manual labelling by more than half. Further, OPAL significantly outperforms automated labelling baselines in labelling accuracy across all seven datasets, with large effect sizes, when all methods are provided with the same manual-labelling budget. For automated test-input validation, on average, OPAL reduces manual effort by 28.8% while achieving 4.5% higher accuracy than the SOTA validation baselines. Finally, we show that augmenting OPAL with an active learning loop leads to an additional 4.5% reduction in required manual labelling, without compromising accuracy.
>
---
#### [new 062] Flow-Anchored Consistency Models
- **分类: cs.CV**

- **简介: 该论文属于生成模型任务，解决连续时间一致性模型训练不稳定问题。通过引入流锚定策略，提升生成效率与质量。**

- **链接: [http://arxiv.org/pdf/2507.03738v1](http://arxiv.org/pdf/2507.03738v1)**

> **作者:** Yansong Peng; Kai Zhu; Yu Liu; Pingyu Wu; Hebei Li; Xiaoyan Sun; Feng Wu
>
> **摘要:** Continuous-time Consistency Models (CMs) promise efficient few-step generation but face significant challenges with training instability. We argue this instability stems from a fundamental conflict: by training a network to learn only a shortcut across a probability flow, the model loses its grasp on the instantaneous velocity field that defines the flow. Our solution is to explicitly anchor the model in the underlying flow during training. We introduce the Flow-Anchored Consistency Model (FACM), a simple but effective training strategy that uses a Flow Matching (FM) task as an anchor for the primary CM shortcut objective. This Flow-Anchoring approach requires no architectural modifications and is broadly compatible with standard model architectures. By distilling a pre-trained LightningDiT model, our method achieves a state-of-the-art FID of 1.32 with two steps (NFE=2) and 1.76 with just one step (NFE=1) on ImageNet 256x256, significantly outperforming previous methods. This provides a general and effective recipe for building high-performance, few-step generative models. Our code and pretrained models: https://github.com/ali-vilab/FACM.
>
---
#### [new 063] Subject Invariant Contrastive Learning for Human Activity Recognition
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于人体活动识别任务，解决因个体差异导致的模型泛化问题。提出SICL损失函数，通过重加权负样本提升活动特征学习。**

- **链接: [http://arxiv.org/pdf/2507.03250v1](http://arxiv.org/pdf/2507.03250v1)**

> **作者:** Yavuz Yarici; Kiran Kokilepersaud; Mohit Prabhushankar; Ghassan AlRegib
>
> **摘要:** The high cost of annotating data makes self-supervised approaches, such as contrastive learning methods, appealing for Human Activity Recognition (HAR). Effective contrastive learning relies on selecting informative positive and negative samples. However, HAR sensor signals are subject to significant domain shifts caused by subject variability. These domain shifts hinder model generalization to unseen subjects by embedding subject-specific variations rather than activity-specific features. As a result, human activity recognition models trained with contrastive learning often struggle to generalize to new subjects. We introduce Subject-Invariant Contrastive Learning (SICL), a simple yet effective loss function to improve generalization in human activity recognition. SICL re-weights negative pairs drawn from the same subject to suppress subject-specific cues and emphasize activity-specific information. We evaluate our loss function on three public benchmarks: UTD-MHAD, MMAct, and DARai. We show that SICL improves performance by up to 11% over traditional contrastive learning methods. Additionally, we demonstrate the adaptability of our loss function across various settings, including multiple self-supervised methods, multimodal scenarios, and supervised learning frameworks.
>
---
#### [new 064] Exploring Kolmogorov-Arnold Network Expansions in Vision Transformers for Mitigating Catastrophic Forgetting in Continual Learning
- **分类: cs.CV**

- **简介: 该论文属于持续学习任务，旨在解决视觉变压器中的灾难性遗忘问题。通过用Kolmogorov-Arnold网络替代MLP，提升模型在新任务中保持旧知识的能力。**

- **链接: [http://arxiv.org/pdf/2507.04020v1](http://arxiv.org/pdf/2507.04020v1)**

> **作者:** Zahid Ullah; Jihie Kim
>
> **摘要:** Continual learning (CL), the ability of a model to learn new tasks without forgetting previously acquired knowledge, remains a critical challenge in artificial intelligence, particularly for vision transformers (ViTs) utilizing Multilayer Perceptrons (MLPs) for global representation learning. Catastrophic forgetting, where new information overwrites prior knowledge, is especially problematic in these models. This research proposes replacing MLPs in ViTs with Kolmogorov-Arnold Network (KANs) to address this issue. KANs leverage local plasticity through spline-based activations, ensuring that only a subset of parameters is updated per sample, thereby preserving previously learned knowledge. The study investigates the efficacy of KAN-based ViTs in CL scenarios across benchmark datasets (MNIST, CIFAR100), focusing on their ability to retain accuracy on earlier tasks while adapting to new ones. Experimental results demonstrate that KAN-based ViTs significantly mitigate catastrophic forgetting, outperforming traditional MLP-based ViTs in knowledge retention and task adaptation. This novel integration of KANs into ViTs represents a promising step toward more robust and adaptable models for dynamic environments.
>
---
#### [new 065] LACONIC: A 3D Layout Adapter for Controllable Image Creation
- **分类: cs.CV**

- **简介: 该论文属于图像生成任务，解决多物体场景中保持三维结构一致性的难题。通过引入3D感知适配器，增强文本到图像模型的3D理解能力，支持更精确的场景控制与编辑。**

- **链接: [http://arxiv.org/pdf/2507.03257v1](http://arxiv.org/pdf/2507.03257v1)**

> **作者:** Léopold Maillard; Tom Durand; Adrien Ramanana Rahary; Maks Ovsjanikov
>
> **备注:** Accepted to ICCV 2025. Preprint version
>
> **摘要:** Existing generative approaches for guided image synthesis of multi-object scenes typically rely on 2D controls in the image or text space. As a result, these methods struggle to maintain and respect consistent three-dimensional geometric structure, underlying the scene. In this paper, we propose a novel conditioning approach, training method and adapter network that can be plugged into pretrained text-to-image diffusion models. Our approach provides a way to endow such models with 3D-awareness, while leveraging their rich prior knowledge. Our method supports camera control, conditioning on explicit 3D geometries and, for the first time, accounts for the entire context of a scene, i.e., both on and off-screen items, to synthesize plausible and semantically rich images. Despite its multi-modal nature, our model is lightweight, requires a reasonable number of data for supervised learning and shows remarkable generalization power. We also introduce methods for intuitive and consistent image editing and restyling, e.g., by positioning, rotating or resizing individual objects in a scene. Our method integrates well within various image creation workflows and enables a richer set of applications compared to previous approaches.
>
---
#### [new 066] Multi-modal Representations for Fine-grained Multi-label Critical View of Safety Recognition
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于多模态手术安全识别任务，旨在解决CVS评估困难的问题。通过引入文本提示提升模型性能，提出CVS-AdaptNet方法，实现更准确的多标签分类。**

- **链接: [http://arxiv.org/pdf/2507.05007v1](http://arxiv.org/pdf/2507.05007v1)**

> **作者:** Britty Baby; Vinkle Srivastav; Pooja P. Jain; Kun Yuan; Pietro Mascagni; Nicolas Padoy
>
> **摘要:** The Critical View of Safety (CVS) is crucial for safe laparoscopic cholecystectomy, yet assessing CVS criteria remains a complex and challenging task, even for experts. Traditional models for CVS recognition depend on vision-only models learning with costly, labor-intensive spatial annotations. This study investigates how text can be harnessed as a powerful tool for both training and inference in multi-modal surgical foundation models to automate CVS recognition. Unlike many existing multi-modal models, which are primarily adapted for multi-class classification, CVS recognition requires a multi-label framework. Zero-shot evaluation of existing multi-modal surgical models shows a significant performance gap for this task. To address this, we propose CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained, binary classification across multiple labels by aligning image embeddings with textual descriptions of each CVS criterion using positive and negative prompts. By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the Endoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that CVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts, boosts CVS recognition over image-only methods. We also propose text-specific inference methods, that helps in analysing the image-text alignment. While further work is needed to match state-of-the-art spatial annotation-based methods, this approach highlights the potential of adapting generalist models to specialized surgical tasks. Code: https://github.com/CAMMA-public/CVS-AdaptNet
>
---
#### [new 067] MPQ-DMv2: Flexible Residual Mixed Precision Quantization for Low-Bit Diffusion Models with Temporal Distillation
- **分类: cs.CV**

- **简介: 该论文属于视觉生成任务，解决低比特扩散模型量化难题。提出MPQ-DMv2框架，通过混合精度量化和时间蒸馏提升性能。**

- **链接: [http://arxiv.org/pdf/2507.04290v1](http://arxiv.org/pdf/2507.04290v1)**

> **作者:** Weilun Feng; Chuanguang Yang; Haotong Qin; Yuqi Li; Xiangqi Li; Zhulin An; Libo Huang; Boyu Diao; Fuzhen Zhuang; Michele Magno; Yongjun Xu; Yingli Tian; Tingwen Huang
>
> **摘要:** Diffusion models have demonstrated remarkable performance on vision generation tasks. However, the high computational complexity hinders its wide application on edge devices. Quantization has emerged as a promising technique for inference acceleration and memory reduction. However, existing quantization methods do not generalize well under extremely low-bit (2-4 bit) quantization. Directly applying these methods will cause severe performance degradation. We identify that the existing quantization framework suffers from the outlier-unfriendly quantizer design, suboptimal initialization, and optimization strategy. We present MPQ-DMv2, an improved \textbf{M}ixed \textbf{P}recision \textbf{Q}uantization framework for extremely low-bit \textbf{D}iffusion \textbf{M}odels. For the quantization perspective, the imbalanced distribution caused by salient outliers is quantization-unfriendly for uniform quantizer. We propose \textit{Flexible Z-Order Residual Mixed Quantization} that utilizes an efficient binary residual branch for flexible quant steps to handle salient error. For the optimization framework, we theoretically analyzed the convergence and optimality of the LoRA module and propose \textit{Object-Oriented Low-Rank Initialization} to use prior quantization error for informative initialization. We then propose \textit{Memory-based Temporal Relation Distillation} to construct an online time-aware pixel queue for long-term denoising temporal information distillation, which ensures the overall temporal consistency between quantized and full-precision model. Comprehensive experiments on various generation tasks show that our MPQ-DMv2 surpasses current SOTA methods by a great margin on different architectures, especially under extremely low-bit widths.
>
---
#### [new 068] Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking
- **分类: cs.CV; stat.AP; stat.ME**

- **简介: 该论文属于多目标跟踪任务，旨在减少跟踪中断并实现在线参数学习与分类。提出两种跟踪器GaPP-Class和GaPP-ReaCtion，结合高斯过程与泊松过程，提升跟踪鲁棒性与适应性。**

- **链接: [http://arxiv.org/pdf/2507.04116v1](http://arxiv.org/pdf/2507.04116v1)**

> **作者:** Fred Lydeard; Bashar I. Ahmad; Simon Godsill
>
> **备注:** 18 pages, 5 figures, submitted to IEEE Transactions on Aerospace and Electronic Systems
>
> **摘要:** This paper presents a computationally efficient multi-object tracking approach that can minimise track breaks (e.g., in challenging environments and against agile targets), learn the measurement model parameters on-line (e.g., in dynamically changing scenes) and infer the class of the tracked objects, if joint tracking and kinematic behaviour classification is sought. It capitalises on the flexibilities offered by the integrated Gaussian process as a motion model and the convenient statistical properties of non-homogeneous Poisson processes as a suitable observation model. This can be combined with the proposed effective track revival / stitching mechanism. We accordingly introduce the two robust and adaptive trackers, Gaussian and Poisson Process with Classification (GaPP-Class) and GaPP with Revival and Classification (GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme that efficiently integrates track management and hyperparameter learning (including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class with the addition of a Markov Chain Monte Carlo kernel applied to each particle permitting track revival and stitching (e.g., within a few time steps after deleting a trajectory). Performance evaluation and benchmarking using synthetic and real data show that GaPP-Class and GaPP-ReaCtion outperform other state-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly reduces track breaks (e.g., by around 30% from real radar data and markedly more from simulated data).
>
---
#### [new 069] Information-Bottleneck Driven Binary Neural Network for Change Detection
- **分类: cs.CV**

- **简介: 该论文属于变化检测任务，解决二值神经网络在该任务中性能下降的问题。通过引入信息瓶颈原理，提升模型表征能力和特征区分度，实现更准确的变化检测。**

- **链接: [http://arxiv.org/pdf/2507.03504v1](http://arxiv.org/pdf/2507.03504v1)**

> **作者:** Kaijie Yin; Zhiyuan Zhang; Shu Kong; Tian Gao; Chengzhong Xu; Hui Kong
>
> **备注:** ICCV 2025 Accepted
>
> **摘要:** In this paper, we propose Binarized Change Detection (BiCD), the first binary neural network (BNN) designed specifically for change detection. Conventional network binarization approaches, which directly quantize both weights and activations in change detection models, severely limit the network's ability to represent input data and distinguish between changed and unchanged regions. This results in significantly lower detection accuracy compared to real-valued networks. To overcome these challenges, BiCD enhances both the representational power and feature separability of BNNs, improving detection performance. Specifically, we introduce an auxiliary objective based on the Information Bottleneck (IB) principle, guiding the encoder to retain essential input information while promoting better feature discrimination. Since directly computing mutual information under the IB principle is intractable, we design a compact, learnable auxiliary module as an approximation target, leading to a simple yet effective optimization strategy that minimizes both reconstruction loss and standard change detection loss. Extensive experiments on street-view and remote sensing datasets demonstrate that BiCD establishes a new benchmark for BNN-based change detection, achieving state-of-the-art performance in this domain.
>
---
#### [new 070] BiVM: Accurate Binarized Neural Network for Efficient Video Matting
- **分类: cs.CV**

- **简介: 该论文属于视频抠图任务，解决边缘设备上实时视频抠图的计算效率问题。通过设计高效的二值化网络BiVM，提升精度并降低计算和存储成本。**

- **链接: [http://arxiv.org/pdf/2507.04456v1](http://arxiv.org/pdf/2507.04456v1)**

> **作者:** Haotong Qin; Xianglong Liu; Xudong Ma; Lei Ke; Yulun Zhang; Jie Luo; Michele Magno
>
> **摘要:** Deep neural networks for real-time video matting suffer significant computational limitations on edge devices, hindering their adoption in widespread applications such as online conferences and short-form video production. Binarization emerges as one of the most common compression approaches with compact 1-bit parameters and efficient bitwise operations. However, accuracy and efficiency limitations exist in the binarized video matting network due to its degenerated encoder and redundant decoder. Following a theoretical analysis based on the information bottleneck principle, the limitations are mainly caused by the degradation of prediction-relevant information in the intermediate features and the redundant computation in prediction-irrelevant areas. We present BiVM, an accurate and resource-efficient Binarized neural network for Video Matting. First, we present a series of binarized computation structures with elastic shortcuts and evolvable topologies, enabling the constructed encoder backbone to extract high-quality representation from input videos for accurate prediction. Second, we sparse the intermediate feature of the binarized decoder by masking homogeneous parts, allowing the decoder to focus on representation with diverse details while alleviating the computation burden for efficient inference. Furthermore, we construct a localized binarization-aware mimicking framework with the information-guided strategy, prompting matting-related representation in full-precision counterparts to be accurately and fully utilized. Comprehensive experiments show that the proposed BiVM surpasses alternative binarized video matting networks, including state-of-the-art (SOTA) binarization methods, by a substantial margin. Moreover, our BiVM achieves significant savings of 14.3x and 21.6x in computation and storage costs, respectively. We also evaluate BiVM on ARM CPU hardware.
>
---
#### [new 071] Personalized Image Generation from an Author Writing Style
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于跨模态生成任务，旨在将文本风格转化为视觉内容。通过构建作者写作档案，利用大语言模型生成图像提示，实现风格化图像生成。**

- **链接: [http://arxiv.org/pdf/2507.03313v1](http://arxiv.org/pdf/2507.03313v1)**

> **作者:** Sagar Gandhi; Vishal Gandhi
>
> **摘要:** Translating nuanced, textually-defined authorial writing styles into compelling visual representations presents a novel challenge in generative AI. This paper introduces a pipeline that leverages Author Writing Sheets (AWS) - structured summaries of an author's literary characteristics - as input to a Large Language Model (LLM, Claude 3.7 Sonnet). The LLM interprets the AWS to generate three distinct, descriptive text-to-image prompts, which are then rendered by a diffusion model (Stable Diffusion 3.5 Medium). We evaluated our approach using 49 author styles from Reddit data, with human evaluators assessing the stylistic match and visual distinctiveness of the generated images. Results indicate a good perceived alignment between the generated visuals and the textual authorial profiles (mean style match: $4.08/5$), with images rated as moderately distinctive. Qualitative analysis further highlighted the pipeline's ability to capture mood and atmosphere, while also identifying challenges in representing highly abstract narrative elements. This work contributes a novel end-to-end methodology for visual authorial style personalization and provides an initial empirical validation, opening avenues for applications in creative assistance and cross-modal understanding.
>
---
#### [new 072] Leveraging the Structure of Medical Data for Improved Representation Learning
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于医学AI领域，旨在解决数据稀缺且结构丰富的医疗数据预训练问题。通过自监督学习利用医学数据的内在结构，提升表示学习效果。**

- **链接: [http://arxiv.org/pdf/2507.02987v1](http://arxiv.org/pdf/2507.02987v1)**

> **作者:** Andrea Agostini; Sonia Laguna; Alain Ryser; Samuel Ruiperez-Campillo; Moritz Vandenhirtz; Nicolas Deperrois; Farhad Nooralahzadeh; Michael Krauthammer; Thomas M. Sutter; Julia E. Vogt
>
> **摘要:** Building generalizable medical AI systems requires pretraining strategies that are data-efficient and domain-aware. Unlike internet-scale corpora, clinical datasets such as MIMIC-CXR offer limited image counts and scarce annotations, but exhibit rich internal structure through multi-view imaging. We propose a self-supervised framework that leverages the inherent structure of medical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and lateral views) as natural positive pairs, learning to reconstruct each view from sparse patches while aligning their latent embeddings. Our method requires no textual supervision and produces informative representations. Evaluated on MIMIC-CXR, we show strong performance compared to supervised objectives and baselines being trained without leveraging structure. This work provides a lightweight, modality-agnostic blueprint for domain-specific pretraining where data is structured but scarce
>
---
#### [new 073] HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival Prediction
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于癌症生存预测任务，解决WSI标签稀疏和视觉语言对齐不足的问题。提出HiLa框架，通过多层级特征提取与语言提示对齐提升预测性能。**

- **链接: [http://arxiv.org/pdf/2507.04613v1](http://arxiv.org/pdf/2507.04613v1)**

> **作者:** Jiaqi Cui; Lu Wen; Yuchen Fei; Bo Liu; Luping Zhou; Dinggang Shen; Yan Wang
>
> **备注:** Accepted by MICCAI2025
>
> **摘要:** Survival prediction using whole-slide images (WSIs) is crucial in cancer re-search. Despite notable success, existing approaches are limited by their reliance on sparse slide-level labels, which hinders the learning of discriminative repre-sentations from gigapixel WSIs. Recently, vision language (VL) models, which incorporate additional language supervision, have emerged as a promising solu-tion. However, VL-based survival prediction remains largely unexplored due to two key challenges. First, current methods often rely on only one simple lan-guage prompt and basic cosine similarity, which fails to learn fine-grained associ-ations between multi-faceted linguistic information and visual features within WSI, resulting in inadequate vision-language alignment. Second, these methods primarily exploit patch-level information, overlooking the intrinsic hierarchy of WSIs and their interactions, causing ineffective modeling of hierarchical interac-tions. To tackle these problems, we propose a novel Hierarchical vision-Language collaboration (HiLa) framework for improved survival prediction. Specifically, HiLa employs pretrained feature extractors to generate hierarchical visual features from WSIs at both patch and region levels. At each level, a series of language prompts describing various survival-related attributes are constructed and aligned with visual features via Optimal Prompt Learning (OPL). This ap-proach enables the comprehensive learning of discriminative visual features cor-responding to different survival-related attributes from prompts, thereby improv-ing vision-language alignment. Furthermore, we introduce two modules, i.e., Cross-Level Propagation (CLP) and Mutual Contrastive Learning (MCL) to maximize hierarchical cooperation by promoting interactions and consistency be-tween patch and region levels. Experiments on three TCGA datasets demonstrate our SOTA performance.
>
---
#### [new 074] Look-Back: Implicit Visual Re-focusing in MLLM Reasoning
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于多模态推理任务，旨在解决MLLM过度依赖文本而忽视视觉信息的问题。提出Look-Back方法，使模型自主回顾视觉信息，提升推理能力。**

- **链接: [http://arxiv.org/pdf/2507.03019v1](http://arxiv.org/pdf/2507.03019v1)**

> **作者:** Shuo Yang; Yuwei Niu; Yuyang Liu; Yang Ye; Bin Lin; Li Yuan
>
> **摘要:** Multimodal Large Language Models (MLLMs) have achieved remarkable progress in multimodal reasoning. However, they often excessively rely on textual information during the later stages of inference, neglecting the crucial integration of visual input. Current methods typically address this by explicitly injecting visual information to guide the reasoning process. In this work, through an analysis of MLLM attention patterns, we made an intriguing observation: with appropriate guidance, MLLMs can spontaneously re-focus their attention on visual inputs during the later stages of reasoning, even without explicit visual information injection. This spontaneous shift in focus suggests that MLLMs are intrinsically capable of performing visual fusion reasoning. Building on this insight, we introduce Look-Back, an implicit approach designed to guide MLLMs to ``look back" at visual information in a self-directed manner during reasoning. Look-Back empowers the model to autonomously determine when, where, and how to re-focus on visual inputs, eliminating the need for explicit model-structure constraints or additional input. We demonstrate that Look-Back significantly enhances the model's reasoning and perception capabilities, as evidenced by extensive empirical evaluations on multiple multimodal benchmarks.
>
---
#### [new 075] LTMSformer: A Local Trend-Aware Attention and Motion State Encoding Transformer for Multi-Agent Trajectory Prediction
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于多智能体轨迹预测任务，旨在解决时空依赖建模不足和高阶运动状态利用有限的问题。提出LTMSformer框架，结合局部趋势注意和运动状态编码，提升预测精度。**

- **链接: [http://arxiv.org/pdf/2507.04634v1](http://arxiv.org/pdf/2507.04634v1)**

> **作者:** Yixin Yan; Yang Li; Yuanfan Wang; Xiaozhou Zhou; Beihao Xia; Manjiang Hu; Hongmao Qin
>
> **摘要:** It has been challenging to model the complex temporal-spatial dependencies between agents for trajectory prediction. As each state of an agent is closely related to the states of adjacent time steps, capturing the local temporal dependency is beneficial for prediction, while most studies often overlook it. Besides, learning the high-order motion state attributes is expected to enhance spatial interaction modeling, but it is rarely seen in previous works. To address this, we propose a lightweight framework, LTMSformer, to extract temporal-spatial interaction features for multi-modal trajectory prediction. Specifically, we introduce a Local Trend-Aware Attention mechanism to capture the local temporal dependency by leveraging a convolutional attention mechanism with hierarchical local time boxes. Next, to model the spatial interaction dependency, we build a Motion State Encoder to incorporate high-order motion state attributes, such as acceleration, jerk, heading, etc. To further refine the trajectory prediction, we propose a Lightweight Proposal Refinement Module that leverages Multi-Layer Perceptrons for trajectory embedding and generates the refined trajectories with fewer model parameters. Experiment results on the Argoverse 1 dataset demonstrate that our method outperforms the baseline HiVT-64, reducing the minADE by approximately 4.35%, the minFDE by 8.74%, and the MR by 20%. We also achieve higher accuracy than HiVT-128 with a 68% reduction in model size.
>
---
#### [new 076] DMAT: An End-to-End Framework for Joint Atmospheric Turbulence Mitigation and Object Detection
- **分类: cs.CV**

- **简介: 该论文属于图像处理与目标检测任务，解决大气湍流导致的图像模糊和检测困难问题。提出DMAT框架，联合优化图像复原与目标检测。**

- **链接: [http://arxiv.org/pdf/2507.04323v1](http://arxiv.org/pdf/2507.04323v1)**

> **作者:** Paul Hill; Alin Achim; Dave Bull; Nantheera Anantrasirichai
>
> **摘要:** Atmospheric Turbulence (AT) degrades the clarity and accuracy of surveillance imagery, posing challenges not only for visualization quality but also for object classification and scene tracking. Deep learning-based methods have been proposed to improve visual quality, but spatio-temporal distortions remain a significant issue. Although deep learning-based object detection performs well under normal conditions, it struggles to operate effectively on sequences distorted by atmospheric turbulence. In this paper, we propose a novel framework that learns to compensate for distorted features while simultaneously improving visualization and object detection. This end-to-end framework leverages and exchanges knowledge of low-level distorted features in the AT mitigator with semantic features extracted in the object detector. Specifically, in the AT mitigator a 3D Mamba-based structure is used to handle the spatio-temporal displacements and blurring caused by turbulence. Features are extracted in a pyramid manner during the mitigation stage and passed to the detector. Optimization is achieved through back-propagation in both the AT mitigator and object detector. Our proposed DMAT outperforms state-of-the-art AT mitigation and object detection systems up to a 15% improvement on datasets corrupted by generated turbulence.
>
---
#### [new 077] S$^2$Edit: Text-Guided Image Editing with Precise Semantic and Spatial Control
- **分类: cs.CV**

- **简介: 该论文属于图像编辑任务，解决细粒度控制问题。通过引入可学习文本标记和对象掩码，实现精准语义与空间控制的个性化图像编辑。**

- **链接: [http://arxiv.org/pdf/2507.04584v1](http://arxiv.org/pdf/2507.04584v1)**

> **作者:** Xudong Liu; Zikun Chen; Ruowei Jiang; Ziyi Wu; Kejia Yin; Han Zhao; Parham Aarabi; Igor Gilitschenski
>
> **摘要:** Recent advances in diffusion models have enabled high-quality generation and manipulation of images guided by texts, as well as concept learning from images. However, naive applications of existing methods to editing tasks that require fine-grained control, e.g., face editing, often lead to suboptimal solutions with identity information and high-frequency details lost during the editing process, or irrelevant image regions altered due to entangled concepts. In this work, we propose S$^2$Edit, a novel method based on a pre-trained text-to-image diffusion model that enables personalized editing with precise semantic and spatial control. We first fine-tune our model to embed the identity information into a learnable text token. During fine-tuning, we disentangle the learned identity token from attributes to be edited by enforcing an orthogonality constraint in the textual feature space. To ensure that the identity token only affects regions of interest, we apply object masks to guide the cross-attention maps. At inference time, our method performs localized editing while faithfully preserving the original identity with semantically disentangled and spatially focused identity token learned. Extensive experiments demonstrate the superiority of S$^2$Edit over state-of-the-art methods both quantitatively and qualitatively. Additionally, we showcase several compositional image editing applications of S$^2$Edit such as makeup transfer.
>
---
#### [new 078] A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields
- **分类: cs.CV**

- **简介: 该论文属于3D场景重建任务，旨在解决NeRF在真实数据中深度估计不准确的问题。通过引入视图一致性分布和深度推动损失，实现更鲁棒的训练效果。**

- **链接: [http://arxiv.org/pdf/2507.04408v1](http://arxiv.org/pdf/2507.04408v1)**

> **作者:** Aoxiang Fan; Corentin Dumery; Nicolas Talabot; Pascal Fua
>
> **备注:** ICCV 2025 accepted
>
> **摘要:** Neural Radiance Fields (NeRF) has emerged as a compelling framework for scene representation and 3D recovery. To improve its performance on real-world data, depth regularizations have proven to be the most effective ones. However, depth estimation models not only require expensive 3D supervision in training, but also suffer from generalization issues. As a result, the depth estimations can be erroneous in practice, especially for outdoor unbounded scenes. In this paper, we propose to employ view-consistent distributions instead of fixed depth value estimations to regularize NeRF training. Specifically, the distribution is computed by utilizing both low-level color features and high-level distilled features from foundation models at the projected 2D pixel-locations from per-ray sampled 3D points. By sampling from the view-consistency distributions, an implicit regularization is imposed on the training of NeRF. We also utilize a depth-pushing loss that works in conjunction with the sampling technique to jointly provide effective regularizations for eliminating the failure modes. Extensive experiments conducted on various scenes from public datasets demonstrate that our proposed method can generate significantly better novel view synthesis results than state-of-the-art NeRF variants as well as different depth regularization methods.
>
---
#### [new 079] Efficient Training of Deep Networks using Guided Spectral Data Selection: A Step Toward Learning What You Need
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于数据选择任务，旨在提高深度网络训练效率。通过谱分析选择关键数据点，减少计算资源消耗，同时保持模型性能。**

- **链接: [http://arxiv.org/pdf/2507.04269v1](http://arxiv.org/pdf/2507.04269v1)**

> **作者:** Mohammadreza Sharifi; Ahad Harati
>
> **备注:** 19 pages, 10 figures, UnderReview in the Data Mining and Knowledge Discovery journal of Springer, Submitted Apr 2025
>
> **摘要:** Effective data curation is essential for optimizing neural network training. In this paper, we present the Guided Spectrally Tuned Data Selection (GSTDS) algorithm, which dynamically adjusts the subset of data points used for training using an off-the-shelf pre-trained reference model. Based on a pre-scheduled filtering ratio, GSTDS effectively reduces the number of data points processed per batch. The proposed method ensures an efficient selection of the most informative data points for training while avoiding redundant or less beneficial computations. Preserving data points in each batch is performed based on spectral analysis. A Fiedler vector-based scoring mechanism removes the filtered portion of the batch, lightening the resource requirements of the learning. The proposed data selection approach not only streamlines the training process but also promotes improved generalization and accuracy. Extensive experiments on standard image classification benchmarks, including CIFAR-10, Oxford-IIIT Pet, and Oxford-Flowers, demonstrate that GSTDS outperforms standard training scenarios and JEST, a recent state-of-the-art data curation method, on several key factors. It is shown that GSTDS achieves notable reductions in computational requirements, up to four times, without compromising performance. GSTDS exhibits a considerable growth in terms of accuracy under the limited computational resource usage, in contrast to other methodologies. These promising results underscore the potential of spectral-based data selection as a scalable solution for resource-efficient deep learning and motivate further exploration into adaptive data curation strategies. You can find the code at https://github.com/rezasharifi82/GSTDS.
>
---
#### [new 080] Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于单域泛化任务，旨在解决领域偏移和数据多样性不足的问题。提出LEAwareSGD方法，通过Lyapunov指数调整学习率，提升模型泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.04302v1](http://arxiv.org/pdf/2507.04302v1)**

> **作者:** Zuyu Zhang; Ning Chen; Yongshan Liu; Qinghua Zhang; Xu Zhang
>
> **备注:** Accepted to ICCV 2025
>
> **摘要:** Single Domain Generalization (SDG) aims to develop models capable of generalizing to unseen target domains using only one source domain, a task complicated by substantial domain shifts and limited data diversity. Existing SDG approaches primarily rely on data augmentation techniques, which struggle to effectively adapt training dynamics to accommodate large domain shifts. To address this, we propose LEAwareSGD, a novel Lyapunov Exponent (LE)-guided optimization approach inspired by dynamical systems theory. By leveraging LE measurements to modulate the learning rate, LEAwareSGD encourages model training near the edge of chaos, a critical state that optimally balances stability and adaptability. This dynamic adjustment allows the model to explore a wider parameter space and capture more generalizable features, ultimately enhancing the model's generalization capability. Extensive experiments on PACS, OfficeHome, and DomainNet demonstrate that LEAwareSGD yields substantial generalization gains, achieving up to 9.47\% improvement on PACS in low-data regimes. These results underscore the effectiveness of training near the edge of chaos for enhancing model generalization capability in SDG tasks.
>
---
#### [new 081] Intelligent Histology for Tumor Neurosurgery
- **分类: cs.CV**

- **简介: 该论文属于医学影像分析任务，旨在解决术中快速准确诊断肿瘤的问题。通过结合AI与拉曼显微成像技术，实现实时肿瘤分析与分类。**

- **链接: [http://arxiv.org/pdf/2507.03037v1](http://arxiv.org/pdf/2507.03037v1)**

> **作者:** Xinhai Hou; Akhil Kondepudi; Cheng Jiang; Yiwei Lyu; Samir Harake; Asadur Chowdury; Anna-Katharina Meißner; Volker Neuschmelting; David Reinecke; Gina Furtjes; Georg Widhalm; Lisa Irina Koerner; Jakob Straehle; Nicolas Neidert; Pierre Scheffler; Juergen Beck; Michael Ivan; Ashish Shah; Aditya Pandey; Sandra Camelo-Piragua; Dieter Henrik Heiland; Oliver Schnell; Chris Freudiger; Jacob Young; Melike Pekmezci; Katie Scotford; Shawn Hervey-Jumper; Daniel Orringer; Mitchel Berger; Todd Hollon
>
> **摘要:** The importance of rapid and accurate histologic analysis of surgical tissue in the operating room has been recognized for over a century. Our standard-of-care intraoperative pathology workflow is based on light microscopy and H\&E histology, which is slow, resource-intensive, and lacks real-time digital imaging capabilities. Here, we present an emerging and innovative method for intraoperative histologic analysis, called Intelligent Histology, that integrates artificial intelligence (AI) with stimulated Raman histology (SRH). SRH is a rapid, label-free, digital imaging method for real-time microscopic tumor tissue analysis. SRH generates high-resolution digital images of surgical specimens within seconds, enabling AI-driven tumor histologic analysis, molecular classification, and tumor infiltration detection. We review the scientific background, clinical translation, and future applications of intelligent histology in tumor neurosurgery. We focus on the major scientific and clinical studies that have demonstrated the transformative potential of intelligent histology across multiple neurosurgical specialties, including neurosurgical oncology, skull base, spine oncology, pediatric tumors, and periperal nerve tumors. Future directions include the development of AI foundation models through multi-institutional datasets, incorporating clinical and radiologic data for multimodal learning, and predicting patient outcomes. Intelligent histology represents a transformative intraoperative workflow that can reinvent real-time tumor analysis for 21st century neurosurgery.
>
---
#### [new 082] A Training-Free Style-Personalization via Scale-wise Autoregressive Model
- **分类: cs.CV**

- **简介: 该论文属于图像生成任务，解决风格个性化问题。提出无需训练的框架，通过多路径设计和注意力机制控制内容与风格，实现高效灵活的图像生成。**

- **链接: [http://arxiv.org/pdf/2507.04482v1](http://arxiv.org/pdf/2507.04482v1)**

> **作者:** Kyoungmin Lee; Jihun Park; Jongmin Gim; Wonhyeok Choi; Kyumin Hwang; Jaeyeul Kim; Sunghoon Im
>
> **备注:** 13 pages, 10 figures
>
> **摘要:** We present a training-free framework for style-personalized image generation that controls content and style information during inference using a scale-wise autoregressive model. Our method employs a three-path design--content, style, and generation--each guided by a corresponding text prompt, enabling flexible and efficient control over image semantics without any additional training. A central contribution of this work is a step-wise and attention-wise intervention analysis. Through systematic prompt and feature injection, we find that early-to-middle generation steps play a pivotal role in shaping both content and style, and that query features predominantly encode content-specific information. Guided by these insights, we introduce two targeted mechanisms: Key Stage Attention Sharing, which aligns content and style during the semantically critical steps, and Adaptive Query Sharing, which reinforces content semantics in later steps through similarity-aware query blending. Extensive experiments demonstrate that our method achieves competitive style fidelity and prompt fidelity compared to fine-tuned baselines, while offering faster inference and greater deployment flexibility.
>
---
#### [new 083] Robust Low-light Scene Restoration via Illumination Transition
- **分类: cs.CV**

- **简介: 该论文属于低光场景修复任务，旨在解决多视角低光图像合成正常光照视图的问题。通过构建光照过渡场和低秩去噪模块，提升渲染质量和多视角一致性。**

- **链接: [http://arxiv.org/pdf/2507.03976v1](http://arxiv.org/pdf/2507.03976v1)**

> **作者:** Ze Li; Feng Zhang; Xiatian Zhu; Meng Zhang; Yanghong Zhou; P. Y. Mok
>
> **备注:** 10 pages, 5 figures
>
> **摘要:** Synthesizing normal-light novel views from low-light multiview images is an important yet challenging task, given the low visibility and high ISO noise present in the input images. Existing low-light enhancement methods often struggle to effectively preprocess such low-light inputs, as they fail to consider correlations among multiple views. Although other state-of-the-art methods have introduced illumination-related components offering alternative solutions to the problem, they often result in drawbacks such as color distortions and artifacts, and they provide limited denoising effectiveness. In this paper, we propose a novel Robust Low-light Scene Restoration framework (RoSe), which enables effective synthesis of novel views in normal lighting conditions from low-light multiview image inputs, by formulating the task as an illuminance transition estimation problem in 3D space, conceptualizing it as a specialized rendering task. This multiview-consistent illuminance transition field establishes a robust connection between low-light and normal-light conditions. By further exploiting the inherent low-rank property of illumination to constrain the transition representation, we achieve more effective denoising without complex 2D techniques or explicit noise modeling. To implement RoSe, we design a concise dual-branch architecture and introduce a low-rank denoising module. Experiments demonstrate that RoSe significantly outperforms state-of-the-art models in both rendering quality and multiview consistency on standard benchmarks. The codes and data are available at https://pegasus2004.github.io/RoSe.
>
---
#### [new 084] AI-driven Web Application for Early Detection of Sudden Death Syndrome (SDS) in Soybean Leaves Using Hyperspectral Images and Genetic Algorithm
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于植物病害早期检测任务，旨在解决大豆猝死综合征的早期诊断问题。通过高光谱成像和遗传算法筛选关键波长，结合深度学习进行分类，并开发了在线诊断系统。**

- **链接: [http://arxiv.org/pdf/2507.03198v1](http://arxiv.org/pdf/2507.03198v1)**

> **作者:** Pappu Kumar Yadav; Rishik Aggarwal; Supriya Paudel; Amee Parmar; Hasan Mirzakhaninafchi; Zain Ul Abideen Usmani; Dhe Yeong Tchalla; Shyam Solanki; Ravi Mural; Sachin Sharma; Thomas F. Burks; Jianwei Qin; Moon S. Kim
>
> **备注:** 8 pages
>
> **摘要:** Sudden Death Syndrome (SDS), caused by Fusarium virguliforme, poses a significant threat to soybean production. This study presents an AI-driven web application for early detection of SDS on soybean leaves using hyperspectral imaging, enabling diagnosis prior to visible symptom onset. Leaf samples from healthy and inoculated plants were scanned using a portable hyperspectral imaging system (398-1011 nm), and a Genetic Algorithm was employed to select five informative wavelengths (505.4, 563.7, 712.2, 812.9, and 908.4 nm) critical for discriminating infection status. These selected bands were fed into a lightweight Convolutional Neural Network (CNN) to extract spatial-spectral features, which were subsequently classified using ten classical machine learning models. Ensemble classifiers (Random Forest, AdaBoost), Linear SVM, and Neural Net achieved the highest accuracy (>98%) and minimal error across all folds, as confirmed by confusion matrices and cross-validation metrics. Poor performance by Gaussian Process and QDA highlighted their unsuitability for this dataset. The trained models were deployed within a web application that enables users to upload hyperspectral leaf images, visualize spectral profiles, and receive real-time classification results. This system supports rapid and accessible plant disease diagnostics, contributing to precision agriculture practices. Future work will expand the training dataset to encompass diverse genotypes, field conditions, and disease stages, and will extend the system for multiclass disease classification and broader crop applicability.
>
---
#### [new 085] PhenoBench: A Comprehensive Benchmark for Cell Phenotyping
- **分类: cs.CV**

- **简介: 该论文属于细胞表型分析任务，旨在解决现有模型在H&E图像上表现不佳的问题。提出PhenoBench基准和PhenoCell数据集，评估模型在不同场景下的泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.03532v1](http://arxiv.org/pdf/2507.03532v1)**

> **作者:** Jerome Luescher; Nora Koreuber; Jannik Franzen; Fabian H. Reith; Claudia Winklmayr; Christian M. Schuerch; Dagmar Kainmueller; Josef Lorenz Rumberger
>
> **备注:** accepted for presentation at MICCAI 2025
>
> **摘要:** Digital pathology has seen the advent of a wealth of foundational models (FM), yet to date their performance on cell phenotyping has not been benchmarked in a unified manner. We therefore propose PhenoBench: A comprehensive benchmark for cell phenotyping on Hematoxylin and Eosin (H&E) stained histopathology images. We provide both PhenoCell, a new H&E dataset featuring 14 granular cell types identified by using multiplexed imaging, and ready-to-use fine-tuning and benchmarking code that allows the systematic evaluation of multiple prominent pathology FMs in terms of dense cell phenotype predictions in different generalization scenarios. We perform extensive benchmarking of existing FMs, providing insights into their generalization behavior under technical vs. medical domain shifts. Furthermore, while FMs achieve macro F1 scores > 0.70 on previously established benchmarks such as Lizard and PanNuke, on PhenoCell, we observe scores as low as 0.20. This indicates a much more challenging task not captured by previous benchmarks, establishing PhenoCell as a prime asset for future benchmarking of FMs and supervised models alike. Code and data are available on GitHub.
>
---
#### [new 086] MoDiT: Learning Highly Consistent 3D Motion Coefficients with Diffusion Transformer for Talking Head Generation
- **分类: cs.CV**

- **简介: 该论文属于语音驱动的说话人头像生成任务，解决面部动画不一致、身份漂移和眨眼不自然的问题。提出MoDiT框架，结合3DMM与扩散Transformer，提升运动一致性与真实性。**

- **链接: [http://arxiv.org/pdf/2507.05092v1](http://arxiv.org/pdf/2507.05092v1)**

> **作者:** Yucheng Wang; Dan Xu
>
> **摘要:** Audio-driven talking head generation is critical for applications such as virtual assistants, video games, and films, where natural lip movements are essential. Despite progress in this field, challenges remain in producing both consistent and realistic facial animations. Existing methods, often based on GANs or UNet-based diffusion models, face three major limitations: (i) temporal jittering caused by weak temporal constraints, resulting in frame inconsistencies; (ii) identity drift due to insufficient 3D information extraction, leading to poor preservation of facial identity; and (iii) unnatural blinking behavior due to inadequate modeling of realistic blink dynamics. To address these issues, we propose MoDiT, a novel framework that combines the 3D Morphable Model (3DMM) with a Diffusion-based Transformer. Our contributions include: (i) A hierarchical denoising strategy with revised temporal attention and biased self/cross-attention mechanisms, enabling the model to refine lip synchronization and progressively enhance full-face coherence, effectively mitigating temporal jittering. (ii) The integration of 3DMM coefficients to provide explicit spatial constraints, ensuring accurate 3D-informed optical flow prediction and improved lip synchronization using Wav2Lip results, thereby preserving identity consistency. (iii) A refined blinking strategy to model natural eye movements, with smoother and more realistic blinking behaviors.
>
---
#### [new 087] Towards a Psychoanalytic Perspective on VLM Behaviour: A First-step Interpretation with Intriguing Observations
- **分类: cs.CV; cs.CL; cs.LG**

- **简介: 该论文属于自然语言处理任务，旨在解决VLMs的幻觉问题。通过引入心理分类和基准测试，分析模型行为与认知偏差的关系。**

- **链接: [http://arxiv.org/pdf/2507.03123v1](http://arxiv.org/pdf/2507.03123v1)**

> **作者:** Xiangrui Liu; Man Luo; Agneet Chatterjee; Hua Wei; Yezhou Yang
>
> **摘要:** Hallucination is a long-standing problem that has been actively investigated in Vision-Language Models (VLMs). Existing research commonly attributes hallucinations to technical limitations or sycophancy bias, where the latter means the models tend to generate incorrect answers to align with user expectations. However, these explanations primarily focus on technical or externally driven factors, may have neglected the possibility that hallucination behaviours might mirror cognitive biases observed in human psychology. In this work, we introduce a psychological taxonomy, categorizing VLMs' hallucination behaviours, including sycophancy, logical inconsistency, and a newly identified VLMs behaviour: authority bias. To systematically analyze these behaviours, we design AIpsych, a scalable benchmark that reveals psychological tendencies in model response patterns. Leveraging this benchmark, we investigate how variations in model architecture and parameter size influence model behaviour when responding to strategically manipulated questions. Our experiments reveal that as model size increases, VLMs exhibit stronger sycophantic tendencies but reduced authority bias, suggesting increasing competence but a potential erosion of response integrity. A human subject study further validates our hypotheses and highlights key behavioural differences between VLMs and human respondents. This work suggests a new perspective for understanding hallucination in VLMs and highlights the importance of integrating psychological principles into model evaluation.The benchmark is available at https://github.com/lxrswdd/AIpsych.
>
---
#### [new 088] Rectifying Adversarial Sample with Low Entropy Prior for Test-Time Defense
- **分类: cs.CV**

- **简介: 该论文属于对抗样本防御任务，解决未知攻击下鲁棒性不足的问题。通过引入低熵先验，提出REAL方法提升测试时的对抗样本修复效果。**

- **链接: [http://arxiv.org/pdf/2507.03427v1](http://arxiv.org/pdf/2507.03427v1)**

> **作者:** Lina Ma; Xiaowei Fu; Fuxiang Huang; Xinbo Gao; Lei Zhang
>
> **备注:** To appear in IEEEE Transactions on Multimedia
>
> **摘要:** Existing defense methods fail to defend against unknown attacks and thus raise generalization issue of adversarial robustness. To remedy this problem, we attempt to delve into some underlying common characteristics among various attacks for generality. In this work, we reveal the commonly overlooked low entropy prior (LE) implied in various adversarial samples, and shed light on the universal robustness against unseen attacks in inference phase. LE prior is elaborated as two properties across various attacks as shown in Fig. 1 and Fig. 2: 1) low entropy misclassification for adversarial samples and 2) lower entropy prediction for higher attack intensity. This phenomenon stands in stark contrast to the naturally distributed samples. The LE prior can instruct existing test-time defense methods, thus we propose a two-stage REAL approach: Rectify Adversarial sample based on LE prior for test-time adversarial rectification. Specifically, to align adversarial samples more closely with clean samples, we propose to first rectify adversarial samples misclassified with low entropy by reverse maximizing prediction entropy, thereby eliminating their adversarial nature. To ensure the rectified samples can be correctly classified with low entropy, we carry out secondary rectification by forward minimizing prediction entropy, thus creating a Max-Min entropy optimization scheme. Further, based on the second property, we propose an attack-aware weighting mechanism to adaptively adjust the strengths of Max-Min entropy objectives. Experiments on several datasets show that REAL can greatly improve the performance of existing sample rectification models.
>
---
#### [new 089] Grounded Gesture Generation: Language, Motion, and Space
- **分类: cs.CV; cs.AI; cs.RO; 68T07, 68T42; I.2.7; I.2.6; H.5.2**

- **简介: 该论文属于手势生成任务，解决空间语境下手势生成问题。通过构建多模态数据集和框架，融合动作、语言与空间信息，推动具身交互研究。**

- **链接: [http://arxiv.org/pdf/2507.04522v1](http://arxiv.org/pdf/2507.04522v1)**

> **作者:** Anna Deichler; Jim O'Regan; Teo Guichoux; David Johansson; Jonas Beskow
>
> **备注:** Accepted as a non-archival paper at the CVPR 2025 Humanoid Agents Workshop. Project page: https://groundedgestures.github.io
>
> **摘要:** Human motion generation has advanced rapidly in recent years, yet the critical problem of creating spatially grounded, context-aware gestures has been largely overlooked. Existing models typically specialize either in descriptive motion generation, such as locomotion and object interaction, or in isolated co-speech gesture synthesis aligned with utterance semantics. However, both lines of work often treat motion and environmental grounding separately, limiting advances toward embodied, communicative agents. To address this gap, our work introduces a multimodal dataset and framework for grounded gesture generation, combining two key resources: (1) a synthetic dataset of spatially grounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing two-party dialogues. Together, they provide over 7.7 hours of synchronized motion, speech, and 3D scene information, standardized in the HumanML3D format. Our framework further connects to a physics-based simulator, enabling synthetic data generation and situated evaluation. By bridging gesture modeling and spatial grounding, our contribution establishes a foundation for advancing research in situated gesture generation and grounded multimodal interaction. Project page: https://groundedgestures.github.io/
>
---
#### [new 090] Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文属于历史文档修复任务，旨在解决传统方法在多模态和大规模修复上的不足。作者构建了FPHDR数据集并提出AutoHDR模型，提升OCR准确率至94.25%。**

- **链接: [http://arxiv.org/pdf/2507.05108v1](http://arxiv.org/pdf/2507.05108v1)**

> **作者:** Yuyi Zhang; Peirong Zhang; Zhenhua Yang; Pengyu Yan; Yongxin Shi; Pengwei Liu; Fengjun Guo; Lianwen Jin
>
> **摘要:** Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians' restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83\% to 84.05\%, with further enhancement to 94.25\% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.
>
---
#### [new 091] SAMed-2: Selective Memory Enhanced Medical Segment Anything Model
- **分类: cs.CV**

- **简介: 该论文属于医学图像分割任务，旨在解决医疗数据复杂、标注噪声及持续学习问题。提出SAMed-2模型，引入时间适配器和置信度记忆机制，提升分割性能。**

- **链接: [http://arxiv.org/pdf/2507.03698v1](http://arxiv.org/pdf/2507.03698v1)**

> **作者:** Zhiling Yan; Sifan Song; Dingjie Song; Yiwei Li; Rong Zhou; Weixiang Sun; Zhennong Chen; Sekeun Kim; Hui Ren; Tianming Liu; Quanzheng Li; Xiang Li; Lifang He; Lichao Sun
>
> **备注:** Accepted by MICCAI 2025
>
> **摘要:** Recent "segment anything" efforts show promise by learning from large-scale data, but adapting such models directly to medical images remains challenging due to the complexity of medical data, noisy annotations, and continual learning requirements across diverse modalities and anatomical structures. In this work, we propose SAMed-2, a new foundation model for medical image segmentation built upon the SAM-2 architecture. Specifically, we introduce a temporal adapter into the image encoder to capture image correlations and a confidence-driven memory mechanism to store high-certainty features for later retrieval. This memory-based strategy counters the pervasive noise in large-scale medical datasets and mitigates catastrophic forgetting when encountering new tasks or modalities. To train and evaluate SAMed-2, we curate MedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21 medical segmentation tasks. Our experiments on both internal benchmarks and 10 external datasets demonstrate superior performance over state-of-the-art baselines in multi-task scenarios. The code is available at: https://github.com/ZhilingYan/Medical-SAM-Bench.
>
---
#### [new 092] Losing Control: Data Poisoning Attack on Guided Diffusion via ControlNet
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于安全任务，研究ControlNet在数据中毒攻击下的漏洞，提出一种方法使模型在无文本触发时生成特定内容，揭示了开放源代码模型的潜在风险。**

- **链接: [http://arxiv.org/pdf/2507.04726v1](http://arxiv.org/pdf/2507.04726v1)**

> **作者:** Raz Lapid; Almog Dubin
>
> **摘要:** Text-to-image diffusion models have achieved remarkable success in translating textual prompts into high-fidelity images. ControlNets further extend these models by allowing precise, image-based conditioning (e.g., edge maps, depth, pose), enabling fine-grained control over structure and style. However, their dependence on large, publicly scraped datasets -- and the increasing use of community-shared data for fine-tuning -- exposes them to stealthy data poisoning attacks. In this work, we introduce a novel data poisoning method that manipulates ControlNets to generate images containing specific content without any text triggers. By injecting poisoned samples -- each pairing a subtly triggered input with an NSFW target -- the model retains clean-prompt fidelity yet reliably produces NSFW outputs when the trigger is present. On large-scale, high-quality datasets, our backdoor achieves high attack success rate while remaining imperceptible in raw inputs. These results reveal a critical vulnerability in open-source ControlNets pipelines and underscore the need for robust data sanitization and defense mechanisms.
>
---
#### [new 093] CVFusion: Cross-View Fusion of 4D Radar and Camera for 3D Object Detection
- **分类: cs.CV**

- **简介: 该论文属于3D目标检测任务，旨在解决4D雷达与相机融合不足的问题。提出CVFusion网络，通过两阶段融合提升检测性能。**

- **链接: [http://arxiv.org/pdf/2507.04587v1](http://arxiv.org/pdf/2507.04587v1)**

> **作者:** Hanzhi Zhong; Zhiyu Xiang; Ruoyu Xu; Jingyun Fu; Peng Xu; Shaohong Wang; Zhihao Yang; Tianyu Pu; Eryun Liu
>
> **摘要:** 4D radar has received significant attention in autonomous driving thanks to its robustness under adverse weathers. Due to the sparse points and noisy measurements of the 4D radar, most of the research finish the 3D object detection task by integrating images from camera and perform modality fusion in BEV space. However, the potential of the radar and the fusion mechanism is still largely unexplored, hindering the performance improvement. In this study, we propose a cross-view two-stage fusion network called CVFusion. In the first stage, we design a radar guided iterative (RGIter) BEV fusion module to generate high-recall 3D proposal boxes. In the second stage, we aggregate features from multiple heterogeneous views including points, image, and BEV for each proposal. These comprehensive instance level features greatly help refine the proposals and generate high-quality predictions. Extensive experiments on public datasets show that our method outperforms the previous state-of-the-art methods by a large margin, with 9.10% and 3.68% mAP improvements on View-of-Delft (VoD) and TJ4DRadSet, respectively. Our code will be made publicly available.
>
---
#### [new 094] U-ViLAR: Uncertainty-Aware Visual Localization for Autonomous Driving via Differentiable Association and Registration
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于视觉定位任务，解决GNSS信号差环境下自动驾驶的定位问题。提出U-ViLAR框架，结合不确定性引导的关联与配准，提升定位精度与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.04503v1](http://arxiv.org/pdf/2507.04503v1)**

> **作者:** Xiaofan Li; Zhihao Xu; Chenming Wu; Zhao Yang; Yumeng Zhang; Jiang-Jiang Liu; Haibao Yu; Fan Duan; Xiaoqing Ye; Yuan Wang; Shirui Li; Xun Sun; Ji Wan; Jun Wang
>
> **备注:** Vision Localization, Autonomous Driving, Bird's-Eye-View
>
> **摘要:** Accurate localization using visual information is a critical yet challenging task, especially in urban environments where nearby buildings and construction sites significantly degrade GNSS (Global Navigation Satellite System) signal quality. This issue underscores the importance of visual localization techniques in scenarios where GNSS signals are unreliable. This paper proposes U-ViLAR, a novel uncertainty-aware visual localization framework designed to address these challenges while enabling adaptive localization using high-definition (HD) maps or navigation maps. Specifically, our method first extracts features from the input visual data and maps them into Bird's-Eye-View (BEV) space to enhance spatial consistency with the map input. Subsequently, we introduce: a) Perceptual Uncertainty-guided Association, which mitigates errors caused by perception uncertainty, and b) Localization Uncertainty-guided Registration, which reduces errors introduced by localization uncertainty. By effectively balancing the coarse-grained large-scale localization capability of association with the fine-grained precise localization capability of registration, our approach achieves robust and accurate localization. Experimental results demonstrate that our method achieves state-of-the-art performance across multiple localization tasks. Furthermore, our model has undergone rigorous testing on large-scale autonomous driving fleets and has demonstrated stable performance in various challenging urban scenarios.
>
---
#### [new 095] Pose-Star: Anatomy-Aware Editing for Open-World Fashion Images
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于时尚图像编辑任务，解决现有方法在用户灵活性和姿态鲁棒性上的不足。提出Pose-Star框架，实现结构化、姿态鲁棒的编辑。**

- **链接: [http://arxiv.org/pdf/2507.03402v1](http://arxiv.org/pdf/2507.03402v1)**

> **作者:** Yuran Dong; Mang Ye
>
> **备注:** 18 pages, 17 figures, ICCV25
>
> **摘要:** To advance real-world fashion image editing, we analyze existing two-stage pipelines(mask generation followed by diffusion-based editing)which overly prioritize generator optimization while neglecting mask controllability. This results in two critical limitations: I) poor user-defined flexibility (coarse-grained human masks restrict edits to predefined regions like upper torso; fine-grained clothes masks preserve poses but forbid style/length customization). II) weak pose robustness (mask generators fail due to articulated poses and miss rare regions like waist, while human parsers remain limited by predefined categories). To address these gaps, we propose Pose-Star, a framework that dynamically recomposes body structures (e.g., neck, chest, etc.) into anatomy-aware masks (e.g., chest-length) for user-defined edits. In Pose-Star, we calibrate diffusion-derived attention (Star tokens) via skeletal keypoints to enhance rare structure localization in complex poses, suppress noise through phase-aware analysis of attention dynamics (Convergence,Stabilization,Divergence) with threshold masking and sliding-window fusion, and refine edges via cross-self attention merging and Canny alignment. This work bridges controlled benchmarks and open-world demands, pioneering anatomy-aware, pose-robust editing and laying the foundation for industrial fashion image editing.
>
---
#### [new 096] FA: Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection
- **分类: cs.CV**

- **简介: 该论文属于视觉-语言模型的分布外检测任务，旨在提升模型对未知数据的识别能力。通过强制提示学习，利用分布内知识增强检测效果，无需外部数据。**

- **链接: [http://arxiv.org/pdf/2507.04511v1](http://arxiv.org/pdf/2507.04511v1)**

> **作者:** Xinhua Lu; Runhe Lai; Yanqi Wu; Kanghao Chen; Wei-Shi Zheng; Ruixuan Wang
>
> **摘要:** Pre-trained vision-language models (VLMs) have advanced out-of-distribution (OOD) detection recently. However, existing CLIP-based methods often focus on learning OOD-related knowledge to improve OOD detection, showing limited generalization or reliance on external large-scale auxiliary datasets. In this study, instead of delving into the intricate OOD-related knowledge, we propose an innovative CLIP-based framework based on Forced prompt leArning (FA), designed to make full use of the In-Distribution (ID) knowledge and ultimately boost the effectiveness of OOD detection. Our key insight is to learn a prompt (i.e., forced prompt) that contains more diversified and richer descriptions of the ID classes beyond the textual semantics of class labels. Specifically, it promotes better discernment for ID images, by forcing more notable semantic similarity between ID images and the learnable forced prompt. Moreover, we introduce a forced coefficient, encouraging the forced prompt to learn more comprehensive and nuanced descriptions of the ID classes. In this way, FA is capable of achieving notable improvements in OOD detection, even when trained without any external auxiliary datasets, while maintaining an identical number of trainable parameters as CoOp. Extensive empirical evaluations confirm our method consistently outperforms current state-of-the-art methods. Code is available at https://github.com/0xFAFA/FA.
>
---
#### [new 097] Towards Lightest Low-Light Image Enhancement Architecture for Mobile Devices
- **分类: cs.CV**

- **简介: 该论文属于低光图像增强任务，旨在解决移动设备上实时处理的需求。提出LiteIE框架，采用轻量结构和无监督训练，实现高效准确的低光增强。**

- **链接: [http://arxiv.org/pdf/2507.04277v1](http://arxiv.org/pdf/2507.04277v1)**

> **作者:** Guangrui Bai; Hailong Yan; Wenhai Liu; Yahui Deng; Erbao Dong
>
> **备注:** Submitted to ESWA
>
> **摘要:** Real-time low-light image enhancement on mobile and embedded devices requires models that balance visual quality and computational efficiency. Existing deep learning methods often rely on large networks and labeled datasets, limiting their deployment on resource-constrained platforms. In this paper, we propose LiteIE, an ultra-lightweight unsupervised enhancement framework that eliminates dependence on large-scale supervision and generalizes well across diverse conditions. We design a backbone-agnostic feature extractor with only two convolutional layers to produce compact image features enhancement tensors. In addition, we develop a parameter-free Iterative Restoration Module, which reuses the extracted features to progressively recover fine details lost in earlier enhancement steps, without introducing any additional learnable parameters. We further propose an unsupervised training objective that integrates exposure control, edge-aware smoothness, and multi-scale color consistency losses. Experiments on the LOL dataset, LiteIE achieves 19.04 dB PSNR, surpassing SOTA by 1.4 dB while using only 0.07\% of its parameters. On a Snapdragon 8 Gen 3 mobile processor, LiteIE runs at 30 FPS for 4K images with just 58 parameters, enabling real-time deployment on edge devices. These results establish LiteIE as an efficient and practical solution for low-light enhancement on resource-limited platforms.
>
---
#### [new 098] Flux-Sculptor: Text-Driven Rich-Attribute Portrait Editing through Decomposed Spatial Flow Control
- **分类: cs.CV**

- **简介: 该论文属于文本驱动的人像编辑任务，旨在解决定位精度与修改灵活性之间的平衡问题。提出Flux-Sculptor框架，通过空间定位和结构到细节的控制策略实现更精确的编辑效果。**

- **链接: [http://arxiv.org/pdf/2507.03979v1](http://arxiv.org/pdf/2507.03979v1)**

> **作者:** Tianyao He; Runqi Wang; Yang Chen; Dejia Song; Nemo Chen; Xu Tang; Yao Hu
>
> **备注:** 17 pages, 17 figures
>
> **摘要:** Text-driven portrait editing holds significant potential for various applications but also presents considerable challenges. An ideal text-driven portrait editing approach should achieve precise localization and appropriate content modification, yet existing methods struggle to balance reconstruction fidelity and editing flexibility. To address this issue, we propose Flux-Sculptor, a flux-based framework designed for precise text-driven portrait editing. Our framework introduces a Prompt-Aligned Spatial Locator (PASL) to accurately identify relevant editing regions and a Structure-to-Detail Edit Control (S2D-EC) strategy to spatially guide the denoising process through sequential mask-guided fusion of latent representations and attention values. Extensive experiments demonstrate that Flux-Sculptor surpasses existing methods in rich-attribute editing and facial information preservation, making it a strong candidate for practical portrait editing applications. Project page is available at https://flux-sculptor.github.io/.
>
---
#### [new 099] Transferring Visual Explainability of Self-Explaining Models through Task Arithmetic
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于图像分类任务，解决自解释模型在不同领域间解释能力迁移的问题。通过任务算术框架，将源域的解释向量迁移到目标域，提升解释质量且不损失分类精度。**

- **链接: [http://arxiv.org/pdf/2507.04380v1](http://arxiv.org/pdf/2507.04380v1)**

> **作者:** Yuya Yoshikawa; Ryotaro Shimizu; Takahiro Kawashima; Yuki Saito
>
> **摘要:** In scenarios requiring both prediction and explanation efficiency for image classification, self-explaining models that perform both tasks in a single inference are effective. However, their training incurs substantial labeling and computational costs. This study aims to tackle the issue by proposing a method to transfer the visual explainability of self-explaining models, learned in a source domain, to a target domain based on a task arithmetic framework. Specifically, we construct a self-explaining model by extending image classifiers based on a vision-language pretrained model. We then define an \emph{explainability vector} as the difference between model parameters trained on the source domain with and without explanation supervision. Based on the task arithmetic framework, we impart explainability to a model trained only on the prediction task in the target domain by applying the explainability vector. Experimental results on various image classification datasets demonstrate that, except for transfers between some less-related domains, visual explainability can be successfully transferred from source to target domains, improving explanation quality in the target domain without sacrificing classification accuracy. Furthermore, we show that the explainability vector learned on a large and diverse dataset like ImageNet, extended with explanation supervision, exhibits universality and robustness, improving explanation quality on nine out of ten different target datasets. We also find that the explanation quality achieved with a single model inference is comparable to that of Kernel SHAP, which requires 150 model inferences.
>
---
#### [new 100] SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于视频模型跨领域评估任务，旨在解决视频基础模型在科学应用中的泛化能力问题。通过构建SciVid基准，验证ViFM在不同科学领域的有效性与局限性。**

- **链接: [http://arxiv.org/pdf/2507.03578v1](http://arxiv.org/pdf/2507.03578v1)**

> **作者:** Yana Hasson; Pauline Luc; Liliane Momeni; Maks Ovsjanikov; Guillaume Le Moing; Alina Kuznetsova; Ira Ktena; Jennifer J. Sun; Skanda Koppula; Dilara Gokay; Joseph Heyward; Etienne Pot; Andrew Zisserman
>
> **备注:** ICCV 2025, GitHub repo: https://github.com/google-deepmind/scivid
>
> **摘要:** In recent years, there has been a proliferation of spatiotemporal foundation models in different scientific disciplines. While promising, these models are often domain-specific and are only assessed within the particular applications for which they are designed. Given that many tasks can be represented as video modeling problems, video foundation models (ViFMs) hold considerable promise as general-purpose domain-agnostic approaches. However, it is not known whether the knowledge acquired on large-scale but potentially out-of-domain data can be effectively transferred across diverse scientific disciplines, and if a single, pretrained ViFM can be competitive with domain-specific baselines. To address this, we introduce SciVid, a comprehensive benchmark comprising five *Sci*entific *Vid*eo tasks, across medical computer vision, animal behavior, and weather forecasting. We adapt six leading ViFMs to SciVid using simple trainable readout modules, establishing strong baselines and demonstrating the potential for effective transfer learning. Specifically, we show that state-of-the-art results can be obtained in several applications by leveraging the general-purpose representations from ViFM backbones. Furthermore, our results reveal the limitations of existing ViFMs, and highlight opportunities for the development of generalizable models for high-impact scientific applications. We release our code at https://github.com/google-deepmind/scivid to facilitate further research in the development of ViFMs.
>
---
#### [new 101] Robust Incomplete-Modality Alignment for Ophthalmic Disease Grading and Diagnosis via Labeled Optimal Transport
- **分类: cs.CV**

- **简介: 该论文属于眼科疾病分级与诊断任务，解决多模态数据不完整导致的诊断准确性问题。提出一种鲁棒的多模态对齐与融合框架，利用最优传输实现跨模态特征对齐和有效融合。**

- **链接: [http://arxiv.org/pdf/2507.04999v1](http://arxiv.org/pdf/2507.04999v1)**

> **作者:** Qinkai Yu; Jianyang Xie; Yitian Zhao; Cheng Chen; Lijun Zhang; Liming Chen; Jun Cheng; Lu Liu; Yalin Zheng; Yanda Meng
>
> **备注:** MICCAI 2025
>
> **摘要:** Multimodal ophthalmic imaging-based diagnosis integrates color fundus image with optical coherence tomography (OCT) to provide a comprehensive view of ocular pathologies. However, the uneven global distribution of healthcare resources often results in real-world clinical scenarios encountering incomplete multimodal data, which significantly compromises diagnostic accuracy. Existing commonly used pipelines, such as modality imputation and distillation methods, face notable limitations: 1)Imputation methods struggle with accurately reconstructing key lesion features, since OCT lesions are localized, while fundus images vary in style. 2)distillation methods rely heavily on fully paired multimodal training data. To address these challenges, we propose a novel multimodal alignment and fusion framework capable of robustly handling missing modalities in the task of ophthalmic diagnostics. By considering the distinctive feature characteristics of OCT and fundus images, we emphasize the alignment of semantic features within the same category and explicitly learn soft matching between modalities, allowing the missing modality to utilize existing modality information, achieving robust cross-modal feature alignment under the missing modality. Specifically, we leverage the Optimal Transport for multi-scale modality feature alignment: class-wise alignment through predicted class prototypes and feature-wise alignment via cross-modal shared feature transport. Furthermore, we propose an asymmetric fusion strategy that effectively exploits the distinct characteristics of OCT and fundus modalities. Extensive evaluations on three large ophthalmic multimodal datasets demonstrate our model's superior performance under various modality-incomplete scenarios, achieving Sota performance in both complete modality and inter-modality incompleteness conditions. Code is available at https://github.com/Qinkaiyu/RIMA
>
---
#### [new 102] Quick Bypass Mechanism of Zero-Shot Diffusion-Based Image Restoration
- **分类: cs.CV**

- **简介: 该论文属于图像恢复任务，解决扩散模型在零样本情况下迭代时间过长的问题。提出Quick Bypass Mechanism加速去噪过程，同时保持恢复质量。**

- **链接: [http://arxiv.org/pdf/2507.04207v1](http://arxiv.org/pdf/2507.04207v1)**

> **作者:** Yu-Shan Tai; An-Yeu; Wu
>
> **摘要:** Recent advancements in diffusion models have demonstrated remarkable success in various image generation tasks. Building upon these achievements, diffusion models have also been effectively adapted to image restoration tasks, e.g., super-resolution and deblurring, aiming to recover high-quality images from degraded inputs. Although existing zero-shot approaches enable pretrained diffusion models to perform restoration tasks without additional fine-tuning, these methods often suffer from prolonged iteration times in the denoising process. To address this limitation, we propose a Quick Bypass Mechanism (QBM), a strategy that significantly accelerates the denoising process by initializing from an intermediate approximation, effectively bypassing early denoising steps. Furthermore, recognizing that approximation may introduce inconsistencies, we introduce a Revised Reverse Process (RRP), which adjusts the weighting of random noise to enhance the stochasticity and mitigate potential disharmony. We validate proposed methods on ImageNet-1K and CelebA-HQ across multiple image restoration tasks, e.g., super-resolution, deblurring, and compressed sensing. Our experimental results show that the proposed methods can effectively accelerate existing methods while maintaining original performance.
>
---
#### [new 103] Gated Recursive Fusion: A Stateful Approach to Scalable Multimodal Transformers
- **分类: cs.CV; cs.AI; cs.CL; I.4; I.2**

- **简介: 该论文属于多模态学习任务，解决跨模态融合与计算扩展性之间的矛盾。提出Gated Recurrent Fusion架构，实现线性可扩展的多模态表示学习。**

- **链接: [http://arxiv.org/pdf/2507.02985v1](http://arxiv.org/pdf/2507.02985v1)**

> **作者:** Yusuf Shihata
>
> **备注:** 13 pages, 2 figures
>
> **摘要:** Multimodal learning faces a fundamental tension between deep, fine-grained fusion and computational scalability. While cross-attention models achieve strong performance through exhaustive pairwise fusion, their quadratic complexity is prohibitive for settings with many modalities. We address this challenge with Gated Recurrent Fusion (GRF), a novel architecture that captures the power of cross-modal attention within a linearly scalable, recurrent pipeline. Our method processes modalities sequentially, updating an evolving multimodal context vector at each step. The core of our approach is a fusion block built on Transformer Decoder layers that performs symmetric cross-attention, mutually enriching the shared context and the incoming modality. This enriched information is then integrated via a Gated Fusion Unit (GFU) a GRU-inspired mechanism that dynamically arbitrates information flow, enabling the model to selectively retain or discard features. This stateful, recurrent design scales linearly with the number of modalities, O(n), making it ideal for high-modality environments. Experiments on the CMU-MOSI benchmark demonstrate that GRF achieves competitive performance compared to more complex baselines. Visualizations of the embedding space further illustrate that GRF creates structured, class-separable representations through its progressive fusion mechanism. Our work presents a robust and efficient paradigm for powerful, scalable multimodal representation learning.
>
---
#### [new 104] Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations
- **分类: cs.CV**

- **简介: 该论文属于多模态领域泛化任务，解决单模态方法难以有效迁移至多模态场景的问题，通过统一表示和监督解耦框架提升模型在未见领域的泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.03304v1](http://arxiv.org/pdf/2507.03304v1)**

> **作者:** Hai Huang; Yan Xia; Sashuai Zhou; Hanting Wang; Shulei Wang; Zhou Zhao
>
> **备注:** Accepted by ICCV 2025
>
> **摘要:** Domain Generalization (DG) aims to enhance model robustness in unseen or distributionally shifted target domains through training exclusively on source domains. Although existing DG techniques, such as data manipulation, learning strategies, and representation learning, have shown significant progress, they predominantly address single-modal data. With the emergence of numerous multi-modal datasets and increasing demand for multi-modal tasks, a key challenge in Multi-modal Domain Generalization (MMDG) has emerged: enabling models trained on multi-modal sources to generalize to unseen target distributions within the same modality set. Due to the inherent differences between modalities, directly transferring methods from single-modal DG to MMDG typically yields sub-optimal results. These methods often exhibit randomness during generalization due to the invisibility of target domains and fail to consider inter-modal consistency. Applying these methods independently to each modality in the MMDG setting before combining them can lead to divergent generalization directions across different modalities, resulting in degraded generalization capabilities. To address these challenges, we propose a novel approach that leverages Unified Representations to map different paired modalities together, effectively adapting DG methods to MMDG by enabling synchronized multi-modal improvements within the unified space. Additionally, we introduce a supervised disentanglement framework that separates modal-general and modal-specific information, further enhancing the alignment of unified representations. Extensive experiments on benchmark datasets, including EPIC-Kitchens and Human-Animal-Cartoon, demonstrate the effectiveness and superiority of our method in enhancing multi-modal domain generalization.
>
---
#### [new 105] VERITAS: Verification and Explanation of Realness in Images for Transparency in AI Systems
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于图像真实性验证任务，旨在解决AI生成图像与真实图像难以区分的问题，提出VERITAS框架实现小图像的检测与解释。**

- **链接: [http://arxiv.org/pdf/2507.05146v1](http://arxiv.org/pdf/2507.05146v1)**

> **作者:** Aadi Srivastava; Vignesh Natarajkumar; Utkarsh Bheemanaboyna; Devisree Akashapu; Nagraj Gaonkar; Archit Joshi
>
> **摘要:** The widespread and rapid adoption of AI-generated content, created by models such as Generative Adversarial Networks (GANs) and Diffusion Models, has revolutionized the digital media landscape by allowing efficient and creative content generation. However, these models also blur the difference between real images and AI-generated synthetic images, raising concerns regarding content authenticity and integrity. While many existing solutions to detect fake images focus solely on classification and higher-resolution images, they often lack transparency in their decision-making, making it difficult for users to understand why an image is classified as fake. In this paper, we present VERITAS, a comprehensive framework that not only accurately detects whether a small (32x32) image is AI-generated but also explains why it was classified that way through artifact localization and semantic reasoning. VERITAS produces human-readable explanations that describe key artifacts in synthetic images. We show that this architecture offers clear explanations of the basis of zero-shot synthetic image detection tasks. Code and relevant prompts can be found at https://github.com/V-i-g-n-e-s-h-N/VERITAS .
>
---
#### [new 106] Breaking Imitation Bottlenecks: Reinforced Diffusion Powers Diverse Trajectory Generation
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于自动驾驶轨迹生成任务，解决模仿学习导致的轨迹单一问题。通过结合强化学习与扩散模型，生成多样且安全的行驶路径。**

- **链接: [http://arxiv.org/pdf/2507.04049v1](http://arxiv.org/pdf/2507.04049v1)**

> **作者:** Ziying Song; Lin Liu; Hongyu Pan; Bencheng Liao; Mingzhe Guo; Lei Yang; Yongchang Zhang; Shaoqing Xu; Caiyan Jia; Yadan Luo
>
> **备注:** 16 pages, 6 figures
>
> **摘要:** Most end-to-end autonomous driving methods rely on imitation learning from single expert demonstrations, often leading to conservative and homogeneous behaviors that limit generalization in complex real-world scenarios. In this work, we propose DIVER, an end-to-end driving framework that integrates reinforcement learning with diffusion-based generation to produce diverse and feasible trajectories. At the core of DIVER lies a reinforced diffusion-based generation mechanism. First, the model conditions on map elements and surrounding agents to generate multiple reference trajectories from a single ground-truth trajectory, alleviating the limitations of imitation learning that arise from relying solely on single expert demonstrations. Second, reinforcement learning is employed to guide the diffusion process, where reward-based supervision enforces safety and diversity constraints on the generated trajectories, thereby enhancing their practicality and generalization capability. Furthermore, to address the limitations of L2-based open-loop metrics in capturing trajectory diversity, we propose a novel Diversity metric to evaluate the diversity of multi-mode predictions.Extensive experiments on the closed-loop NAVSIM and Bench2Drive benchmarks, as well as the open-loop nuScenes dataset, demonstrate that DIVER significantly improves trajectory diversity, effectively addressing the mode collapse problem inherent in imitation learning.
>
---
#### [new 107] Self-Supervised Real-Time Tracking of Military Vehicles in Low-FPS UAV Footage
- **分类: cs.CV**

- **简介: 该论文属于多目标跟踪任务，解决低帧率无人机视频中军事车辆的实时跟踪问题。通过单帧标注学习实例关联，提升跟踪鲁棒性与效率。**

- **链接: [http://arxiv.org/pdf/2507.05229v1](http://arxiv.org/pdf/2507.05229v1)**

> **作者:** Markiyan Kostiv; Anatolii Adamovskyi; Yevhen Cherniavskyi; Mykyta Varenyk; Ostap Viniavskyi; Igor Krashenyi; Oles Dobosevych
>
> **摘要:** Multi-object tracking (MOT) aims to maintain consistent identities of objects across video frames. Associating objects in low-frame-rate videos captured by moving unmanned aerial vehicles (UAVs) in actual combat scenarios is complex due to rapid changes in object appearance and position within the frame. The task becomes even more challenging due to image degradation caused by cloud video streaming and compression algorithms. We present how instance association learning from single-frame annotations can overcome these challenges. We show that global features of the scene provide crucial context for low-FPS instance association, allowing our solution to be robust to distractors and gaps in detections. We also demonstrate that such a tracking approach maintains high association quality even when reducing the input image resolution and latent representation size for faster inference. Finally, we present a benchmark dataset of annotated military vehicles collected from publicly available data sources. This paper was initially presented at the NATO Science and Technology Organization Symposium (ICMCIS) organized by the Information Systems Technology (IST)Scientific and Technical Committee, IST-209-RSY - the ICMCIS, held in Oeiras, Portugal, 13-14 May 2025.
>
---
#### [new 108] Group-wise Scaling and Orthogonal Decomposition for Domain-Invariant Feature Extraction in Face Anti-Spoofing
- **分类: cs.CV**

- **简介: 该论文属于人脸识别中的反欺骗任务，解决领域不变特征提取问题。通过FOD和GS-RM方法对齐权重和偏置，提升模型在未知领域的泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.04006v1](http://arxiv.org/pdf/2507.04006v1)**

> **作者:** Seungjin Jung; Kanghee Lee; Yonghyun Jeong; Haeun Noh; Jungmin Lee; Jongwon Choi
>
> **备注:** Published at ICCV 2025. code is will be available at https://github.com/SeungjinJung/GD-FAS
>
> **摘要:** Domain Generalizable Face Anti-Spoofing (DGFAS) methods effectively capture domain-invariant features by aligning the directions (weights) of local decision boundaries across domains. However, the bias terms associated with these boundaries remain misaligned, leading to inconsistent classification thresholds and degraded performance on unseen target domains. To address this issue, we propose a novel DGFAS framework that jointly aligns weights and biases through Feature Orthogonal Decomposition (FOD) and Group-wise Scaling Risk Minimization (GS-RM). Specifically, GS-RM facilitates bias alignment by balancing group-wise losses across multiple domains. FOD employs the Gram-Schmidt orthogonalization process to decompose the feature space explicitly into domain-invariant and domain-specific subspaces. By enforcing orthogonality between domain-specific and domain-invariant features during training using domain labels, FOD ensures effective weight alignment across domains without negatively impacting bias alignment. Additionally, we introduce Expected Calibration Error (ECE) as a novel evaluation metric for quantitatively assessing the effectiveness of our method in aligning bias terms across domains. Extensive experiments on benchmark datasets demonstrate that our approach achieves state-of-the-art performance, consistently improving accuracy, reducing bias misalignment, and enhancing generalization stability on unseen target domains.
>
---
#### [new 109] Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视频理解任务，解决时序视频定位问题。提出Tempo-R0模型，通过多模态时间感知强化学习提升模型对视频中事件边界的识别能力。**

- **链接: [http://arxiv.org/pdf/2507.04702v1](http://arxiv.org/pdf/2507.04702v1)**

> **作者:** Feng Yue; Zhaoxing Zhang; Junming Jiao; Zhengyu Liang; Shiwen Cao; Feifei Zhang; Rong Shen
>
> **摘要:** Temporal Video Grounding (TVG), which requires pinpointing relevant temporal segments from video based on language query, has always been a highly challenging task in the field of video understanding. Videos often have a larger volume of information and redundancy than texts or images. Models should present comprehensive understanding of the whole video to accurately retrieve query-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large Language Model (Video-MLLM) for the temporal video grounding task via multimodal temporal sensing reinforcement. Specifically, during the preprocessing stage of our pipeline, we employ Self-adaptive Attention Allocation (SAA) method based on frame content variation to efficiently use the MLLM's limited attention. The Explicit Timestamp-modal Aligned (ETA) method is also utilized to strengthen our model's capability to perceive the boundaries of events in the video. In the fine-tuning part of our pipeline, we creatively apply Partial Irrelevance Refusing-based Group Relative Policy Optimization (PIR-GRPO) in TVG area to foster model's temporal reasoning from not only accepting relevant video-query pairs but also refusing irrelevant ones. Experiments demonstrate that our method accomplishes a notable advantage over SOTA solutions by around 3.5% on both the original QVHighlights testbench and its corrected version with more reasonable ground truth annotations.
>
---
#### [new 110] ConceptMix++: Leveling the Playing Field in Text-to-Image Benchmarking via Iterative Prompt Optimization
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于文本到图像生成任务，旨在解决基准测试中因提示敏感性导致的模型能力低估问题。通过迭代优化提示，提升生成性能并实现公平比较。**

- **链接: [http://arxiv.org/pdf/2507.03275v1](http://arxiv.org/pdf/2507.03275v1)**

> **作者:** Haosheng Gan; Berk Tinaz; Mohammad Shahab Sepehri; Zalan Fabian; Mahdi Soltanolkotabi
>
> **备注:** An earlier version appeared in the CVPR 2025 Workshop on Generative Models for Computer Vision
>
> **摘要:** Current text-to-image (T2I) benchmarks evaluate models on rigid prompts, potentially underestimating true generative capabilities due to prompt sensitivity and creating biases that favor certain models while disadvantaging others. We introduce ConceptMix++, a framework that disentangles prompt phrasing from visual generation capabilities by applying iterative prompt optimization. Building on ConceptMix, our approach incorporates a multimodal optimization pipeline that leverages vision-language model feedback to refine prompts systematically. Through extensive experiments across multiple diffusion models, we show that optimized prompts significantly improve compositional generation performance, revealing previously hidden model capabilities and enabling fairer comparisons across T2I models. Our analysis reveals that certain visual concepts -- such as spatial relationships and shapes -- benefit more from optimization than others, suggesting that existing benchmarks systematically underestimate model performance in these categories. Additionally, we find strong cross-model transferability of optimized prompts, indicating shared preferences for effective prompt phrasing across models. These findings demonstrate that rigid benchmarking approaches may significantly underrepresent true model capabilities, while our framework provides more accurate assessment and insights for future development.
>
---
#### [new 111] CS-VLM: Compressed Sensing Attention for Efficient Vision-Language Representation Learning
- **分类: cs.CV**

- **简介: 该论文属于多模态学习任务，旨在解决视觉语言模型中注意力机制的高计算复杂度问题。提出CSAT架构，通过压缩感知降低计算成本，提升效率。**

- **链接: [http://arxiv.org/pdf/2507.02957v1](http://arxiv.org/pdf/2507.02957v1)**

> **作者:** Andrew Kiruluta; Preethi Raju; Priscilla Burity
>
> **摘要:** Vision-Language Models (vLLMs) have emerged as powerful architectures for joint reasoning over visual and textual inputs, enabling breakthroughs in image captioning, cross modal retrieval, and multimodal dialogue. However, as these models scale to longer video sequences and richer language descriptions, the quadratic complexity of the standard attention mechanism presents a fundamental computational bottleneck. This challenge is exacerbated in vLLMs, where attention must be computed not only within modalities but also across them, leading to prohibitive memory and latency costs. In this work, we introduce the Compressed Sensing Attention Transformer (CSAT), a novel architecture that reimagines attention computation through the lens of compressed sensing. By projecting high dimensional key and value representations into a lower-dimensional subspace via random measurement matrices and reconstructing the attention outputs using sparse recovery algorithms, CSAT significantly reduces attention complexity while maintaining semantic fidelity. Applied to vLLMs, CSAT exploits the inherent compressibility of both visual and textual representations especially evident in video, where temporal redundancy is high, and in language, where cross-modal grounding is often sparse. In contrast to LLMs, which must often model entangled symbolic dependencies, vLLMs benefit from structured sparsity in alignment and scene composition, making them particularly well-suited to compressed attention. We provide a formal mathematical treatment of CSAT, demonstrate its integration into vision language pipelines, and validate its performance on standard benchmarks, highlighting its promise as a scalable, interpretable, and resource efficient solution for next generation multimodal transformers.
>
---
#### [new 112] Model Compression using Progressive Channel Pruning
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于模型压缩任务，旨在加速CNN。提出PCP框架，通过迭代剪枝多层通道，减少精度损失，提升剪枝效果。**

- **链接: [http://arxiv.org/pdf/2507.04792v1](http://arxiv.org/pdf/2507.04792v1)**

> **作者:** Jinyang Guo; Weichen Zhang; Wanli Ouyang; Dong Xu
>
> **摘要:** In this work, we propose a simple but effective channel pruning framework called Progressive Channel Pruning (PCP) to accelerate Convolutional Neural Networks (CNNs). In contrast to the existing channel pruning methods that prune channels only once per layer in a layer-by-layer fashion, our new progressive framework iteratively prunes a small number of channels from several selected layers, which consists of a three-step attempting-selecting-pruning pipeline in each iteration. In the attempting step, we attempt to prune a pre-defined number of channels from one layer by using any existing channel pruning methods and estimate the accuracy drop for this layer based on the labelled samples in the validation set. In the selecting step, based on the estimated accuracy drops for all layers, we propose a greedy strategy to automatically select a set of layers that will lead to less overall accuracy drop after pruning these layers. In the pruning step, we prune a small number of channels from these selected layers. We further extend our PCP framework to prune channels for the deep transfer learning methods like Domain Adversarial Neural Network (DANN), in which we effectively reduce the data distribution mismatch in the channel pruning process by using both labelled samples from the source domain and pseudo-labelled samples from the target domain. Our comprehensive experiments on two benchmark datasets demonstrate that our PCP framework outperforms the existing channel pruning approaches under both supervised learning and transfer learning settings.
>
---
#### [new 113] Leveraging Out-of-Distribution Unlabeled Images: Semi-Supervised Semantic Segmentation with an Open-Vocabulary Model
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于半监督语义分割任务，旨在解决利用分布不同的未标注图像提升分割性能的问题。提出SemiOVS框架，有效利用开放词汇模型进行伪标签生成，显著提升性能。**

- **链接: [http://arxiv.org/pdf/2507.03302v1](http://arxiv.org/pdf/2507.03302v1)**

> **作者:** Wooseok Shin; Jisu Kang; Hyeonki Jeong; Jin Sob Kim; Sung Won Han
>
> **备注:** 19pages, 8 figures
>
> **摘要:** In semi-supervised semantic segmentation, existing studies have shown promising results in academic settings with controlled splits of benchmark datasets. However, the potential benefits of leveraging significantly larger sets of unlabeled images remain unexplored. In real-world scenarios, abundant unlabeled images are often available from online sources (web-scraped images) or large-scale datasets. However, these images may have different distributions from those of the target dataset, a situation known as out-of-distribution (OOD). Using these images as unlabeled data in semi-supervised learning can lead to inaccurate pseudo-labels, potentially misguiding network training. In this paper, we propose a new semi-supervised semantic segmentation framework with an open-vocabulary segmentation model (SemiOVS) to effectively utilize unlabeled OOD images. Extensive experiments on Pascal VOC and Context datasets demonstrate two key findings: (1) using additional unlabeled images improves the performance of semi-supervised learners in scenarios with few labels, and (2) using the open-vocabulary segmentation (OVS) model to pseudo-label OOD images leads to substantial performance gains. In particular, SemiOVS outperforms existing PrevMatch and SemiVL methods by +3.5 and +3.0 mIoU, respectively, on Pascal VOC with a 92-label setting, achieving state-of-the-art performance. These findings demonstrate that our approach effectively utilizes abundant unlabeled OOD images for semantic segmentation tasks. We hope this work can inspire future research and real-world applications. The code is available at https://github.com/wooseok-shin/SemiOVS
>
---
#### [new 114] Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for Long-range Haze Removal
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于图像去雾任务，解决长距离雾霾去除问题。通过融合可见光与近红外图像的语义和视觉信息，提升远距离场景的清晰度和细节。**

- **链接: [http://arxiv.org/pdf/2507.03893v1](http://arxiv.org/pdf/2507.03893v1)**

> **作者:** Yi Li; Xiaoxiong Wang; Jiawei Wang; Yi Chang; Kai Cao; Luxin Yan
>
> **备注:** This work has been accepted by IEEE Transactions on Multimedia for publication
>
> **摘要:** While image dehazing has advanced substantially in the past decade, most efforts have focused on short-range scenarios, leaving long-range haze removal under-explored. As distance increases, intensified scattering leads to severe haze and signal loss, making it impractical to recover distant details solely from visible images. Near-infrared, with superior fog penetration, offers critical complementary cues through multimodal fusion. However, existing methods focus on content integration while often neglecting haze embedded in visible images, leading to results with residual haze. In this work, we argue that the infrared and visible modalities not only provide complementary low-level visual features, but also share high-level semantic consistency. Motivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF) framework, comprising a semantic stream to reconstruct haze-free scenes and a visual stream to incorporate structural details from the near-infrared modality. The semantic stream first acquires haze-robust semantic prediction by aligning modality-invariant intrinsic representations. Then the shared semantics act as strong priors to restore clear and high-contrast distant scenes under severe haze degradation. In parallel, the visual stream focuses on recovering lost structural details from near-infrared by fusing complementary cues from both visible and near-infrared images. Through the cooperation of dual streams, HSVF produces results that exhibit both high-contrast scenes and rich texture details. Moreover, we introduce a novel pixel-aligned visible-infrared haze dataset with semantic labels to facilitate benchmarking. Extensive experiments demonstrate the superiority of our method over state-of-the-art approaches in real-world long-range haze removal.
>
---
#### [new 115] Spatio-Temporal LLM: Reasoning about Environments and Actions
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于多模态语言模型任务，解决环境与动作的时空理解问题。通过构建数据集并提出ST-LLM模型提升时空推理能力。**

- **链接: [http://arxiv.org/pdf/2507.05258v1](http://arxiv.org/pdf/2507.05258v1)**

> **作者:** Haozhen Zheng; Beitong Tian; Mingyuan Wu; Zhenggang Tang; Klara Nahrstedt; Alex Schwing
>
> **备注:** Code and data are available at https://zoezheng126.github.io/STLLM-website/
>
> **摘要:** Despite the significant recent progress of Multimodal Large Language Models (MLLMs), MLLMs still struggle to correctly answer prompts that require a holistic spatio-temporal understanding. Specifically, it is challenging to address prompts that refer to 1) the entirety of an environment that an agent equipped with an MLLM can operate in; and simultaneously also refer to 2) recent actions that just happened and are encoded in a video clip. However, such a holistic spatio-temporal understanding is important for agents operating in the real world. To address this issue, we first develop a framework to collect a large-scale dataset. Using the collected "Reasoning about Environments and Actions" (REA) dataset, we show that recent methods indeed struggle to correctly answer the prompts. To improve, we develop a "spatio-temporal LLM" (ST-LLM), a model equipped with projectors to improve both spatial understanding of an environment and temporal understanding of recent observations. On the collected REA data, we show that the proposed method significantly improves results compared to prior work. Code and data are available at https://zoezheng126.github.io/STLLM-website/.
>
---
#### [new 116] Iterative Zoom-In: Temporal Interval Exploration for Long Video Understanding
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视频理解任务，解决长视频中时间区间感知效率低的问题。提出Temporal Search框架，通过迭代探索提升模型对长视频的理解能力。**

- **链接: [http://arxiv.org/pdf/2507.02946v1](http://arxiv.org/pdf/2507.02946v1)**

> **作者:** Chenglin Li; Qianglong Chen; fengtao; Yin Zhang
>
> **摘要:** Multimodal Large Language Models (MLLMs) have shown strong performance in video understanding tasks. However, they continue to struggle with long-form videos because of an inefficient perception of temporal intervals. Unlike humans, who can dynamically adjust their temporal focus to locate query-relevant moments, current MLLMs often rely on dense, uniform sampling across the video timeline, leading to high memory consumption and a risk of missing crucial information. To address this challenge, we introduce Temporal Search, a training-free framework that enables MLLMs to explore temporal regions for improved long video understanding iteratively. TS is based on a key observation: the model's generation confidence across different temporal intervals is highly correlated with prediction accuracy. TS operates through two main iterative stages. First, the MLLM proposes a temporal interval that is likely to contain task-relevant information. Then, it samples a fixed number of frames from the interval, regardless of length, and feeds them into the model to produce a refined response and confidence score. TS refines the focus of the model by iteratively shifting attention to more fine-grained temporal intervals, improving its understanding of long videos. Additionally, keyframe-level descriptions are collected to facilitate cross-interval perception throughout the video. To further improve efficiency, we introduce TS-BFS, a best-first search strategy over a tree. Each node represents a candidate interval and is expanded via two methods: self-driven proposals and uniform partitioning. Nodes are scored based on confidence and self-evaluation, and the most promising one is selected for continued exploration.
>
---
#### [new 117] QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation
- **分类: cs.CV**

- **简介: 该论文属于文本到图像生成任务，解决多LoRA模型融合时的特征纠缠问题。通过QR分解实现参数高效且解耦的微调。**

- **链接: [http://arxiv.org/pdf/2507.04599v1](http://arxiv.org/pdf/2507.04599v1)**

> **作者:** Jiahui Yang; Yongjia Ma; Donglin Di; Hao Li; Wei Chen; Yan Xie; Jianxun Cui; Xun Yang; Wangmeng Zuo
>
> **备注:** ICCV 2025, 30 pages, 26 figures
>
> **摘要:** Existing text-to-image models often rely on parameter fine-tuning techniques such as Low-Rank Adaptation (LoRA) to customize visual attributes. However, when combining multiple LoRA models for content-style fusion tasks, unstructured modifications of weight matrices often lead to undesired feature entanglement between content and style attributes. We propose QR-LoRA, a novel fine-tuning framework leveraging QR decomposition for structured parameter updates that effectively separate visual attributes. Our key insight is that the orthogonal Q matrix naturally minimizes interference between different visual features, while the upper triangular R matrix efficiently encodes attribute-specific transformations. Our approach fixes both Q and R matrices while only training an additional task-specific $\Delta R$ matrix. This structured design reduces trainable parameters to half of conventional LoRA methods and supports effective merging of multiple adaptations without cross-contamination due to the strong disentanglement properties between $\Delta R$ matrices. Experiments demonstrate that QR-LoRA achieves superior disentanglement in content-style fusion tasks, establishing a new paradigm for parameter-efficient, disentangled fine-tuning in generative models.
>
---
#### [new 118] Vision-Language Models Can't See the Obvious
- **分类: cs.CV**

- **简介: 该论文属于视觉-语言模型评估任务，旨在解决LVLM在识别明显视觉特征上的不足。提出SalBench基准，通过三个任务测试模型的感知能力，发现其表现不佳。**

- **链接: [http://arxiv.org/pdf/2507.04741v1](http://arxiv.org/pdf/2507.04741v1)**

> **作者:** Yasser Dahou; Ngoc Dung Huynh; Phuc H. Le-Khac; Wamiq Reyaz Para; Ankit Singh; Sanath Narayan
>
> **摘要:** We present Saliency Benchmark (SalBench), a novel benchmark designed to assess the capability of Large Vision-Language Models (LVLM) in detecting visually salient features that are readily apparent to humans, such as a large circle amidst a grid of smaller ones. This benchmark focuses on low-level features including color, intensity, and orientation, which are fundamental to human visual processing. Our SalBench consists of images that highlight rare, unusual, or unexpected elements within scenes, and naturally draw human attention. It comprises three novel tasks for evaluating the perceptual capabilities of LVLM: Odd-One-Out Detection, Referring Odd-One-Out, and Visual Referring Odd-One-Out. We perform a comprehensive evaluation of state-of-the-art LVLM using SalBench and our findings reveal a surprising limitation: LVLM struggle to identify seemingly obvious visual anomalies, with even the advanced GPT-4o achieving only 47.6\% accuracy on such a simple task. SalBench will be an important step in measuring the capabilities of LVLM that align with the subtle definition of human attention.
>
---
#### [new 119] Learn 3D VQA Better with Active Selection and Reannotation
- **分类: cs.CV**

- **简介: 该论文属于3D视觉问答任务，旨在解决数据标注错误影响模型训练的问题。通过多轮主动学习和重新标注，提升模型性能并降低训练成本。**

- **链接: [http://arxiv.org/pdf/2507.04630v1](http://arxiv.org/pdf/2507.04630v1)**

> **作者:** Shengli Zhou; Yang Liu; Feng Zheng
>
> **备注:** Accepted by ACM MM 2025
>
> **摘要:** 3D Visual Question Answering (3D VQA) is crucial for enabling models to perceive the physical world and perform spatial reasoning. In 3D VQA, the free-form nature of answers often leads to improper annotations that can confuse or mislead models when training on the entire dataset. While other text generation tasks can mitigate this issue by learning on large-scale datasets, the scarcity of 3D scene data enlarges the negative effect of misleading annotations. Although active learning strategies can select valuable instances for training, they fail to identify and resolve misleading labels, which the oracle inevitably provides in practice. To address this issue, we propose a multi-turn interactive active learning strategy. This strategy selects data based on models' semantic uncertainty to form a solid knowledge foundation more effectively and actively requests reannotation from an oracle to resolve potentially misleading labels. For uncertainty assessment, we utilize a variance-based metric that takes semantic relationships between terms into consideration, thus avoiding the uniform inter-class similarity assumption of previous assessment metrics. Extensive experiments exhibit better model performance and a substantial reduction in training costs, with a halving of training costs for achieving relatively high accuracy. The code is available at https://github.com/fz-zsl/AQuA.
>
---
#### [new 120] DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于文本到图像生成任务，解决传统AR模型在质量和效率上的不足。提出DC-HT压缩分层编码器，提升生成效果与效率。**

- **链接: [http://arxiv.org/pdf/2507.04947v1](http://arxiv.org/pdf/2507.04947v1)**

> **作者:** Yecheng Wu; Junyu Chen; Zhuoyang Zhang; Enze Xie; Jincheng Yu; Junsong Chen; Jinyi Hu; Yao Lu; Song Han; Han Cai
>
> **备注:** ICCV 2025
>
> **摘要:** We introduce DC-AR, a novel masked autoregressive (AR) text-to-image generation framework that delivers superior image generation quality with exceptional computational efficiency. Due to the tokenizers' limitations, prior masked AR models have lagged behind diffusion models in terms of quality or efficiency. We overcome this limitation by introducing DC-HT - a deep compression hybrid tokenizer for AR models that achieves a 32x spatial compression ratio while maintaining high reconstruction fidelity and cross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT and create a new hybrid masked autoregressive image generation framework that first produces the structural elements through discrete tokens and then applies refinements via residual tokens. DC-AR achieves state-of-the-art results with a gFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while offering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to prior leading diffusion and autoregressive models.
>
---
#### [new 121] Consistent and Invariant Generalization Learning for Short-video Misinformation Detection
- **分类: cs.CV; cs.MM**

- **简介: 该论文属于短视频虚假信息检测任务，旨在解决模型在不同领域表现不佳的问题。通过一致性与不变性学习，提升多模态融合的鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.04061v1](http://arxiv.org/pdf/2507.04061v1)**

> **作者:** Hanghui Guo; Weijie Shi; Mengze Li; Juncheng Li; Hao Chen; Yue Cui; Jiajie Xu; Jia Zhu; Jiawei Shen; Zhangze Chen; Sirui Han
>
> **备注:** Accepted to ACM MM 2025,15 pages, 16figures
>
> **摘要:** Short-video misinformation detection has attracted wide attention in the multi-modal domain, aiming to accurately identify the misinformation in the video format accompanied by the corresponding audio. Despite significant advancements, current models in this field, trained on particular domains (source domains), often exhibit unsatisfactory performance on unseen domains (target domains) due to domain gaps. To effectively realize such domain generalization on the short-video misinformation detection task, we propose deep insights into the characteristics of different domains: (1) The detection on various domains may mainly rely on different modalities (i.e., mainly focusing on videos or audios). To enhance domain generalization, it is crucial to achieve optimal model performance on all modalities simultaneously. (2) For some domains focusing on cross-modal joint fraud, a comprehensive analysis relying on cross-modal fusion is necessary. However, domain biases located in each modality (especially in each frame of videos) will be accumulated in this fusion process, which may seriously damage the final identification of misinformation. To address these issues, we propose a new DOmain generalization model via ConsisTency and invariance learning for shORt-video misinformation detection (named DOCTOR), which contains two characteristic modules: (1) We involve the cross-modal feature interpolation to map multiple modalities into a shared space and the interpolation distillation to synchronize multi-modal learning; (2) We design the diffusion model to add noise to retain core features of multi modal and enhance domain invariant features through cross-modal guided denoising. Extensive experiments demonstrate the effectiveness of our proposed DOCTOR model. Our code is public available at https://github.com/ghh1125/DOCTOR.
>
---
#### [new 122] Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于自动驾驶中的3D目标检测任务，旨在解决低延迟与高精度的矛盾。提出EMC2系统，融合多传感器数据并优化计算效率。**

- **链接: [http://arxiv.org/pdf/2507.04123v1](http://arxiv.org/pdf/2507.04123v1)**

> **作者:** Linshen Liu; Boyan Su; Junyue Jiang; Guanlin Wu; Cong Guo; Ceyu Xu; Hao Frank Yang
>
> **备注:** Accepted at ICCV 2025
>
> **摘要:** This paper presents Edge-based Mixture of Experts (MoE) Collaborative Computing (EMC2), an optimal computing system designed for autonomous vehicles (AVs) that simultaneously achieves low-latency and high-accuracy 3D object detection. Unlike conventional approaches, EMC2 incorporates a scenario-aware MoE architecture specifically optimized for edge platforms. By effectively fusing LiDAR and camera data, the system leverages the complementary strengths of sparse 3D point clouds and dense 2D images to generate robust multimodal representations. To enable this, EMC2 employs an adaptive multimodal data bridge that performs multi-scale preprocessing on sensor inputs, followed by a scenario-aware routing mechanism that dynamically dispatches features to dedicated expert models based on object visibility and distance. In addition, EMC2 integrates joint hardware-software optimizations, including hardware resource utilization optimization and computational graph simplification, to ensure efficient and real-time inference on resource-constrained edge devices. Experiments on open-source benchmarks clearly show the EMC2 advancements as a end-to-end system. On the KITTI dataset, it achieves an average accuracy improvement of 3.58% and a 159.06% inference speedup compared to 15 baseline methods on Jetson platforms, with similar performance gains on the nuScenes dataset, highlighting its capability to advance reliable, real-time 3D object detection tasks for AVs.
>
---
#### [new 123] Radar Velocity Transformer: Single-scan Moving Object Segmentation in Noisy Radar Point Clouds
- **分类: cs.CV**

- **简介: 该论文属于雷达点云中移动目标分割任务，解决单次扫描下噪声雷达数据的移动物体识别问题。提出基于Transformer的方法，利用速度信息实现精准分割。**

- **链接: [http://arxiv.org/pdf/2507.03463v1](http://arxiv.org/pdf/2507.03463v1)**

> **作者:** Matthias Zeller; Vardeep S. Sandhu; Benedikt Mersch; Jens Behley; Michael Heidingsfeld; Cyrill Stachniss
>
> **备注:** Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA)
>
> **摘要:** The awareness about moving objects in the surroundings of a self-driving vehicle is essential for safe and reliable autonomous navigation. The interpretation of LiDAR and camera data achieves exceptional results but typically requires to accumulate and process temporal sequences of data in order to extract motion information. In contrast, radar sensors, which are already installed in most recent vehicles, can overcome this limitation as they directly provide the Doppler velocity of the detections and, hence incorporate instantaneous motion information within a single measurement. % In this paper, we tackle the problem of moving object segmentation in noisy radar point clouds. We also consider differentiating parked from moving cars, to enhance scene understanding. Instead of exploiting temporal dependencies to identify moving objects, we develop a novel transformer-based approach to perform single-scan moving object segmentation in sparse radar scans accurately. The key to our Radar Velocity Transformer is to incorporate the valuable velocity information throughout each module of the network, thereby enabling the precise segmentation of moving and non-moving objects. Additionally, we propose a transformer-based upsampling, which enhances the performance by adaptively combining information and overcoming the limitation of interpolation of sparse point clouds. Finally, we create a new radar moving object segmentation benchmark based on the RadarScenes dataset and compare our approach to other state-of-the-art methods. Our network runs faster than the frame rate of the sensor and shows superior segmentation results using only single-scan radar data.
>
---
#### [new 124] Modeling Urban Food Insecurity with Google Street View Images
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于城市食物不安全建模任务，旨在利用街景图像辅助识别食物不安全问题，通过特征提取与注意力机制进行图像聚合分析。**

- **链接: [http://arxiv.org/pdf/2507.02924v1](http://arxiv.org/pdf/2507.02924v1)**

> **作者:** David Li
>
> **摘要:** Food insecurity is a significant social and public health issue that plagues many urban metropolitan areas around the world. Existing approaches to identifying food insecurity rely primarily on qualitative and quantitative survey data, which is difficult to scale. This project seeks to explore the effectiveness of using street-level images in modeling food insecurity at the census tract level. To do so, we propose a two-step process of feature extraction and gated attention for image aggregation. We evaluate the effectiveness of our model by comparing against other model architectures, interpreting our learned weights, and performing a case study. While our model falls slightly short in terms of its predictive power, we believe our approach still has the potential to supplement existing methods of identifying food insecurity for urban planners and policymakers.
>
---
#### [new 125] Semantic Frame Interpolation
- **分类: cs.CV**

- **简介: 该论文提出语义帧插值任务，解决基于文本控制生成不同长度视频的问题。构建SemFi模型并建立SFI-300K数据集，提升生成内容的一致性与多样性。**

- **链接: [http://arxiv.org/pdf/2507.05173v1](http://arxiv.org/pdf/2507.05173v1)**

> **作者:** Yijia Hong; Jiangning Zhang; Ran Yi; Yuji Wang; Weijian Cao; Xiaobin Hu; Zhucun Xue; Yabiao Wang; Chengjie Wang; Lizhuang Ma
>
> **备注:** https://github.com/hyj542682306/Semantic-Frame-Interpolation
>
> **摘要:** Generating intermediate video content of varying lengths based on given first and last frames, along with text prompt information, offers significant research and application potential. However, traditional frame interpolation tasks primarily focus on scenarios with a small number of frames, no text control, and minimal differences between the first and last frames. Recent community developers have utilized large video models represented by Wan to endow frame-to-frame capabilities. However, these models can only generate a fixed number of frames and often fail to produce satisfactory results for certain frame lengths, while this setting lacks a clear official definition and a well-established benchmark. In this paper, we first propose a new practical Semantic Frame Interpolation (SFI) task from the perspective of academic definition, which covers the above two settings and supports inference at multiple frame rates. To achieve this goal, we propose a novel SemFi model building upon Wan2.1, which incorporates a Mixture-of-LoRA module to ensure the generation of high-consistency content that aligns with control conditions across various frame length limitations. Furthermore, we propose SFI-300K, the first general-purpose dataset and benchmark specifically designed for SFI. To support this, we collect and process data from the perspective of SFI, carefully designing evaluation metrics and methods to assess the model's performance across multiple dimensions, encompassing image and video, and various aspects, including consistency and diversity. Through extensive experiments on SFI-300K, we demonstrate that our method is particularly well-suited to meet the requirements of the SFI task.
>
---
#### [new 126] Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于隐私保护任务，旨在解决扩散模型在图像生成中可能引发的隐私泄露问题。通过评估八种扰动保护方法，在不同预算下测试其效果与隐蔽性。**

- **链接: [http://arxiv.org/pdf/2507.03953v1](http://arxiv.org/pdf/2507.03953v1)**

> **作者:** Kai Ye; Tianyi Chen; Zhen Wang
>
> **备注:** Accepted to the 2nd Workshop on Reliable and Responsible Foundation Models (R2-FM 2025) at ICML. 8 pages, 3 figures
>
> **摘要:** With the increasing adoption of diffusion models for image generation and personalization, concerns regarding privacy breaches and content misuse have become more pressing. In this study, we conduct a comprehensive comparison of eight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak, Mist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains. These methods are evaluated under varying perturbation budgets, using a range of metrics to assess visual imperceptibility and protective efficacy. Our results offer practical guidance for method selection. Code is available at: https://github.com/vkeilo/DiffAdvPerturbationBench.
>
---
#### [new 127] Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于多任务手术计算机视觉领域，解决传统模型难以处理多任务及部分标注问题。工作是提出MML-SurgAdapt框架，利用SPML学习实现高效多任务学习。**

- **链接: [http://arxiv.org/pdf/2507.05020v1](http://arxiv.org/pdf/2507.05020v1)**

> **作者:** Soham Walimbe; Britty Baby; Vinkle Srivastav; Nicolas Padoy
>
> **摘要:** Surgical AI often involves multiple tasks within a single procedure, like phase recognition or assessing the Critical View of Safety in laparoscopic cholecystectomy. Traditional models, built for one task at a time, lack flexibility, requiring a separate model for each. To address this, we introduce MML-SurgAdapt, a unified multi-task framework with Vision-Language Models (VLMs), specifically CLIP, to handle diverse surgical tasks through natural language supervision. A key challenge in multi-task learning is the presence of partial annotations when integrating different tasks. To overcome this, we employ Single Positive Multi-Label (SPML) learning, which traditionally reduces annotation burden by training models with only one positive label per instance. Our framework extends this approach to integrate data from multiple surgical tasks within a single procedure, enabling effective learning despite incomplete or noisy annotations. We demonstrate the effectiveness of our model on a combined dataset consisting of Cholec80, Endoscapes2023, and CholecT50, utilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt performs comparably to task-specific benchmarks, with the added advantage of handling noisy annotations. It also outperforms the existing SPML frameworks for the task. By reducing the required labels by 23%, our approach proposes a more scalable and efficient labeling process, significantly easing the annotation burden on clinicians. To our knowledge, this is the first application of SPML to integrate data from multiple surgical tasks, presenting a novel and generalizable solution for multi-task learning in surgical computer vision. Implementation is available at: https://github.com/CAMMA-public/MML-SurgAdapt
>
---
#### [new 128] Sat2City: 3D City Generation from A Single Satellite Image with Cascaded Latent Diffusion
- **分类: cs.CV**

- **简介: 该论文属于3D城市生成任务，旨在解决从单张卫星图像生成高精度3D城市结构的问题。提出Sat2City框架，结合稀疏体素和扩散模型，提升生成质量与细节。**

- **链接: [http://arxiv.org/pdf/2507.04403v1](http://arxiv.org/pdf/2507.04403v1)**

> **作者:** Tongyan Hua; Lutao Jiang; Ying-Cong Chen; Wufan Zhao
>
> **备注:** ICCV 2025
>
> **摘要:** Recent advancements in generative models have enabled 3D urban scene generation from satellite imagery, unlocking promising applications in gaming, digital twins, and beyond. However, most existing methods rely heavily on neural rendering techniques, which hinder their ability to produce detailed 3D structures on a broader scale, largely due to the inherent structural ambiguity derived from relatively limited 2D observations. To address this challenge, we propose Sat2City, a novel framework that synergizes the representational capacity of sparse voxel grids with latent diffusion models, tailored specifically for our novel 3D city dataset. Our approach is enabled by three key components: (1) A cascaded latent diffusion framework that progressively recovers 3D city structures from satellite imagery, (2) a Re-Hash operation at its Variational Autoencoder (VAE) bottleneck to compute multi-scale feature grids for stable appearance optimization and (3) an inverse sampling strategy enabling implicit supervision for smooth appearance transitioning.To overcome the challenge of collecting real-world city-scale 3D models with high-quality geometry and appearance, we introduce a dataset of synthesized large-scale 3D cities paired with satellite-view height maps. Validated on this dataset, our framework generates detailed 3D structures from a single satellite image, achieving superior fidelity compared to existing city generation models.
>
---
#### [new 129] Exploring Remote Physiological Signal Measurement under Dynamic Lighting Conditions at Night: Dataset, Experiment, and Analysis
- **分类: cs.CV**

- **简介: 该论文属于远程生理信号测量任务，旨在解决夜间动态光照下rPPG算法效果不佳的问题，通过构建DLCN数据集并进行实验分析。**

- **链接: [http://arxiv.org/pdf/2507.04306v1](http://arxiv.org/pdf/2507.04306v1)**

> **作者:** Zhipeng Li; Kegang Wang; Hanguang Xiao; Xingyue Liu; Feizhong Zhou; Jiaxin Jiang; Tianqi Liu
>
> **摘要:** Remote photoplethysmography (rPPG) is a non-contact technique for measuring human physiological signals. Due to its convenience and non-invasiveness, it has demonstrated broad application potential in areas such as health monitoring and emotion recognition. In recent years, the release of numerous public datasets has significantly advanced the performance of rPPG algorithms under ideal lighting conditions. However, the effectiveness of current rPPG methods in realistic nighttime scenarios with dynamic lighting variations remains largely unknown. Moreover, there is a severe lack of datasets specifically designed for such challenging environments, which has substantially hindered progress in this area of research. To address this gap, we present and release a large-scale rPPG dataset collected under dynamic lighting conditions at night, named DLCN. The dataset comprises approximately 13 hours of video data and corresponding synchronized physiological signals from 98 participants, covering four representative nighttime lighting scenarios. DLCN offers high diversity and realism, making it a valuable resource for evaluating algorithm robustness in complex conditions. Built upon the proposed Happy-rPPG Toolkit, we conduct extensive experiments and provide a comprehensive analysis of the challenges faced by state-of-the-art rPPG methods when applied to DLCN. The dataset and code are publicly available at https://github.com/dalaoplan/Happp-rPPG-Toolkit.
>
---
#### [new 130] Unlocking Compositional Control: Self-Supervision for LVLM-Based Image Generation
- **分类: cs.CV**

- **简介: 该论文属于文本到图像生成任务，解决复杂提示下的控制不足问题。提出Hi-SSLVLM模型，通过两阶段自监督学习提升生成精度与语义一致性。**

- **链接: [http://arxiv.org/pdf/2507.04151v1](http://arxiv.org/pdf/2507.04151v1)**

> **作者:** Fernando Gabriela Garcia; Spencer Burns; Ryan Shaw; Hunter Young
>
> **摘要:** This paper introduces Hierarchical Self-Supervised LVLM (Hi-SSLVLM), a novel generative model designed to significantly advance text-to-image synthesis, particularly for complex and compositionally challenging prompts. Traditional methods often grapple with the high cost of meticulously curated paired image-text datasets and struggle with precise control over fine-grained visual attributes and intricate spatial relationships. Our Hi-SSLVLM addresses these limitations through a unique two-stage self-supervised learning strategy. The first stage, Multi-Granularity Visual-Language Grounding, enables the Large Vision-Language Model (LVLM) backbone to autonomously generate and align hierarchical captions (global and local) to images, cultivating a deep internal semantic understanding without reliance on extensive human annotation. The second stage, Self-Refinement and Guided Image Generation, leverages this acquired knowledge by an Internal Compositional Planning (ICP) mechanism, where the LVLM first formulates detailed textual sub-prompts to guide the image generation process, complemented by a novel Semantic Consistency Loss for precise output alignment. Comprehensive experiments against leading baselines, including Janus-Pro-1B, Stable Diffusion XL 1.0, DeepFloyd IF v1.0, and ControlNet-XL, on multi-dimensional benchmarks such as Gemini-2.0-Flash and InternVL3-78B, demonstrate Hi-SSLVLM's superior performance across all fine-grained metrics. An in-depth ablation study confirms the critical role of each proposed component. Furthermore, human evaluations corroborate our quantitative findings, highlighting Hi-SSLVLM's enhanced fidelity to prompt, compositional accuracy, and overall aesthetic quality, marking a significant step towards more controllable and semantically consistent open-ended text-to-image generation.
>
---
#### [new 131] Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models
- **分类: cs.CV; cs.AI; cs.IR; I.2.10**

- **简介: 该论文属于多媒体验证任务，旨在解决虚假多媒体信息检测问题。通过多智能体系统结合多模态大语言模型，实现内容真实性验证与来源追踪。**

- **链接: [http://arxiv.org/pdf/2507.04410v1](http://arxiv.org/pdf/2507.04410v1)**

> **作者:** Huy Hoan Le; Van Sy Thinh Nguyen; Thi Le Chi Dang; Vo Thanh Khang Nguyen; Truong Thanh Hung Nguyen; Hung Cao
>
> **备注:** 33rd ACM International Conference on Multimedia (MM'25) Grand Challenge on Multimedia Verification
>
> **摘要:** This paper presents our submission to the ACMMM25 - Grand Challenge on Multimedia Verification. We developed a multi-agent verification system that combines Multimodal Large Language Models (MLLMs) with specialized verification tools to detect multimedia misinformation. Our system operates through six stages: raw data processing, planning, information extraction, deep research, evidence collection, and report generation. The core Deep Researcher Agent employs four tools: reverse image search, metadata analysis, fact-checking databases, and verified news processing that extracts spatial, temporal, attribution, and motivational context. We demonstrate our approach on a challenge dataset sample involving complex multimedia content. Our system successfully verified content authenticity, extracted precise geolocation and timing information, and traced source attribution across multiple platforms, effectively addressing real-world multimedia verification scenarios.
>
---
#### [new 132] Unlearning the Noisy Correspondence Makes CLIP More Robust
- **分类: cs.CV; cs.MM**

- **简介: 该论文属于视觉-语言模型任务，解决噪声对应样本影响模型性能的问题。提出NCU框架，通过遗忘机制提升CLIP的鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.03434v1](http://arxiv.org/pdf/2507.03434v1)**

> **作者:** Haochen Han; Alex Jinpeng Wang; Peijun Ye; Fangming Liu
>
> **备注:** ICCV 2025
>
> **摘要:** The data appetite for Vision-Language Models (VLMs) has continuously scaled up from the early millions to billions today, which faces an untenable trade-off with data quality and inevitably introduces Noisy Correspondence (NC) samples. Undoubtedly, such semantically unrelated data significantly impairs the performance of VLMs. Previous efforts mainly address this challenge by estimating refined alignment for more precise guidance. However, such resource-intensive pipelines that train VLMs from scratch struggle to meet realistic data demands. In this paper, we present a brand new perspective that seeks to directly eliminate the harmful effects of NC in pre-trained VLMs. Specifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning framework that efficiently enhances VLMs' robustness by forgetting learned noisy knowledge. The key to NCU is learning the hardest negative information, which can provide explicit unlearning direction for both false positives and false negatives. Such twin goals unlearning process can be formalized into one unified optimal transport objective for fast fine-tuning. We validate our approach with the prevailing CLIP model over various downstream tasks. Remarkably, NCU surpasses the robust pre-trained method on zero-shot transfer while with lower computational overhead. The code will be released upon acceptance.
>
---
#### [new 133] Text-Guided Multi-Instance Learning for Scoliosis Screening via Gait Video Analysis
- **分类: cs.CV**

- **简介: 该论文属于医学图像分析任务，旨在解决青少年脊柱侧弯早期检测难题。通过分析步态视频，提出TG-MILNet模型，提升检测准确性和可解释性。**

- **链接: [http://arxiv.org/pdf/2507.02996v1](http://arxiv.org/pdf/2507.02996v1)**

> **作者:** Haiqing Li; Yuzhi Guo; Feng Jiang; Thao M. Dang; Hehuan Ma; Qifeng Zhou; Jean Gao; Junzhou Huang
>
> **备注:** 10.5 pages, 4 figures, MICCAI conference
>
> **摘要:** Early-stage scoliosis is often difficult to detect, particularly in adolescents, where delayed diagnosis can lead to serious health issues. Traditional X-ray-based methods carry radiation risks and rely heavily on clinical expertise, limiting their use in large-scale screenings. To overcome these challenges, we propose a Text-Guided Multi-Instance Learning Network (TG-MILNet) for non-invasive scoliosis detection using gait videos. To handle temporal misalignment in gait sequences, we employ Dynamic Time Warping (DTW) clustering to segment videos into key gait phases. To focus on the most relevant diagnostic features, we introduce an Inter-Bag Temporal Attention (IBTA) mechanism that highlights critical gait phases. Recognizing the difficulty in identifying borderline cases, we design a Boundary-Aware Model (BAM) to improve sensitivity to subtle spinal deviations. Additionally, we incorporate textual guidance from domain experts and large language models (LLM) to enhance feature representation and improve model interpretability. Experiments on the large-scale Scoliosis1K gait dataset show that TG-MILNet achieves state-of-the-art performance, particularly excelling in handling class imbalance and accurately detecting challenging borderline cases. The code is available at https://github.com/lhqqq/TG-MILNet
>
---
#### [new 134] YOLO-Based Pipeline Monitoring in Challenging Visual Environments
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于水下管道检测任务，旨在解决低可见性环境下的图像识别难题。通过对比YOLOv8和YOLOv11模型，提升管道结构识别与缺陷检测能力。**

- **链接: [http://arxiv.org/pdf/2507.02967v1](http://arxiv.org/pdf/2507.02967v1)**

> **作者:** Pragya Dhungana; Matteo Fresta; Niraj Tamrakar; Hariom Dhungana
>
> **摘要:** Condition monitoring subsea pipelines in low-visibility underwater environments poses significant challenges due to turbidity, light distortion, and image degradation. Traditional visual-based inspection systems often fail to provide reliable data for mapping, object recognition, or defect detection in such conditions. This study explores the integration of advanced artificial intelligence (AI) techniques to enhance image quality, detect pipeline structures, and support autonomous fault diagnosis. This study conducts a comparative analysis of two most robust versions of YOLOv8 and Yolov11 and their three variants tailored for image segmentation tasks in complex and low-visibility subsea environments. Using pipeline inspection datasets captured beneath the seabed, it evaluates model performance in accurately delineating target structures under challenging visual conditions. The results indicated that YOLOv11 outperformed YOLOv8 in overall performance.
>
---
#### [new 135] LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection
- **分类: cs.CV; cs.AI; I.2.10; I.4.8; I.5**

- **简介: 该论文属于图像生成检测任务，旨在区分真实与生成图像。提出LATTE方法，通过建模潜在空间轨迹来捕捉生成图像的特征，提升检测效果。**

- **链接: [http://arxiv.org/pdf/2507.03054v1](http://arxiv.org/pdf/2507.03054v1)**

> **作者:** Ana Vasilcoiu; Ivona Najdenkoska; Zeno Geradts; Marcel Worring
>
> **备注:** 10 pages, 6 figures, submitted to NeurIPS 2025, includes benchmark evaluations on GenImage and Diffusion Forensics
>
> **摘要:** The rapid advancement of diffusion-based image generators has made it increasingly difficult to distinguish generated from real images. This can erode trust in digital media, making it critical to develop generalizable detectors for generated images. Recent methods leverage diffusion denoising cues, but mainly focus on single-step reconstruction errors, ignoring the inherent sequential nature of the denoising process. In this work, we propose LATTE - Latent Trajectory Embedding - a novel approach that models the evolution of latent embeddings across several denoising timesteps. By modeling the trajectory of such embeddings rather than single-step errors, LATTE captures subtle, discriminative patterns that distinguish real from generated images. Each latent is refined by employing our latent-visual feature refinement module and aggregated into a unified representation. Afterwards, it is fused with the visual features and finally passed into a lightweight classifier. Our experiments demonstrate that LATTE surpasses the baselines on several established benchmarks, such as GenImage and DiffusionFake. Moreover, it demonstrates strong performance in cross-generator and cross-datasets settings, highlighting the potential of using the trajectory of latent embeddings for generated image detection. The code is available on the following link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.
>
---
#### [new 136] An analysis of vision-language models for fabric retrieval
- **分类: cs.CV**

- **简介: 该论文属于跨模态检索任务，解决工业领域中文本到图像的零样本检索问题。通过生成结构化描述并评估多个VLM模型性能，提升布料检索准确性。**

- **链接: [http://arxiv.org/pdf/2507.04735v1](http://arxiv.org/pdf/2507.04735v1)**

> **作者:** Francesco Giuliari; Asif Khan Pattan; Mohamed Lamine Mekhalfi; Fabio Poiesi
>
> **备注:** Accepted at Ital-IA 2025
>
> **摘要:** Effective cross-modal retrieval is essential for applications like information retrieval and recommendation systems, particularly in specialized domains such as manufacturing, where product information often consists of visual samples paired with a textual description. This paper investigates the use of Vision Language Models(VLMs) for zero-shot text-to-image retrieval on fabric samples. We address the lack of publicly available datasets by introducing an automated annotation pipeline that uses Multimodal Large Language Models (MLLMs) to generate two types of textual descriptions: freeform natural language and structured attribute-based descriptions. We produce these descriptions to evaluate retrieval performance across three Vision-Language Models: CLIP, LAION-CLIP, and Meta's Perception Encoder. Our experiments demonstrate that structured, attribute-rich descriptions significantly enhance retrieval accuracy, particularly for visually complex fabric classes, with the Perception Encoder outperforming other models due to its robust feature alignment capabilities. However, zero-shot retrieval remains challenging in this fine-grained domain, underscoring the need for domain-adapted approaches. Our findings highlight the importance of combining technical textual descriptions with advanced VLMs to optimize cross-modal retrieval in industrial applications.
>
---
#### [new 137] GraphBrep: Learning B-Rep in Graph Structure for Efficient CAD Generation
- **分类: cs.CV**

- **简介: 该论文属于CAD生成任务，解决B-Rep生成中的冗余与计算成本问题。提出GraphBrep模型，通过图结构显式表示拓扑，提升效率并保持生成质量。**

- **链接: [http://arxiv.org/pdf/2507.04765v1](http://arxiv.org/pdf/2507.04765v1)**

> **作者:** Weilin Lai; Tie Xu; Hu Wang
>
> **摘要:** Direct B-Rep generation is increasingly important in CAD workflows, eliminating costly modeling sequence data and supporting complex features. A key challenge is modeling joint distribution of the misaligned geometry and topology. Existing methods tend to implicitly embed topology into the geometric features of edges. Although this integration ensures feature alignment, it also causes edge geometry to carry more redundant structural information compared to the original B-Rep, leading to significantly higher computational cost. To reduce redundancy, we propose GraphBrep, a B-Rep generation model that explicitly represents and learns compact topology. Following the original structure of B-Rep, we construct an undirected weighted graph to represent surface topology. A graph diffusion model is employed to learn topology conditioned on surface features, serving as the basis for determining connectivity between primitive surfaces. The explicit representation ensures a compact data structure, effectively reducing computational cost during both training and inference. Experiments on two large-scale unconditional datasets and one category-conditional dataset demonstrate the proposed method significantly reduces training and inference times (up to 31.3% and 56.3% for given datasets, respectively) while maintaining high-quality CAD generation compared with SOTA.
>
---
#### [new 138] Source-Free Domain Adaptation via Multi-view Contrastive Learning
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于无监督域适应任务，解决SFUDA中原型样本质量低和伪标签错误的问题，通过RSM、MVCL和噪声过滤提升模型性能。**

- **链接: [http://arxiv.org/pdf/2507.03321v1](http://arxiv.org/pdf/2507.03321v1)**

> **作者:** Amirfarhad Farhadi; Naser Mozayani; Azadeh Zamanifar
>
> **摘要:** Domain adaptation has become a widely adopted approach in machine learning due to the high costs associated with labeling data. It is typically applied when access to a labeled source domain is available. However, in real-world scenarios, privacy concerns often restrict access to sensitive information, such as fingerprints, bank account details, and facial images. A promising solution to this issue is Source-Free Unsupervised Domain Adaptation (SFUDA), which enables domain adaptation without requiring access to labeled target domain data. Recent research demonstrates that SFUDA can effectively address domain discrepancies; however, two key challenges remain: (1) the low quality of prototype samples, and (2) the incorrect assignment of pseudo-labels. To tackle these challenges, we propose a method consisting of three main phases. In the first phase, we introduce a Reliable Sample Memory (RSM) module to improve the quality of prototypes by selecting more representative samples. In the second phase, we employ a Multi-View Contrastive Learning (MVCL) approach to enhance pseudo-label quality by leveraging multiple data augmentations. In the final phase, we apply a noisy label filtering technique to further refine the pseudo-labels. Our experiments on three benchmark datasets - VisDA 2017, Office-Home, and Office-31 - demonstrate that our method achieves approximately 2 percent and 6 percent improvements in classification accuracy over the second-best method and the average of 13 well-known state-of-the-art approaches, respectively.
>
---
#### [new 139] From Video to EEG: Adapting Joint Embedding Predictive Architecture to Uncover Visual Concepts in Brain Signal Analysis
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于EEG分类任务，旨在解决数据少、特征复杂的问题。提出EEG-VJEPA模型，通过视频序列思路学习时空特征，提升分类性能与可解释性。**

- **链接: [http://arxiv.org/pdf/2507.03633v1](http://arxiv.org/pdf/2507.03633v1)**

> **作者:** Amir Hojjati; Lu Li; Ibrahim Hameed; Anis Yazidi; Pedro G. Lind; Rabindra Khadka
>
> **摘要:** EEG signals capture brain activity with high temporal and low spatial resolution, supporting applications such as neurological diagnosis, cognitive monitoring, and brain-computer interfaces. However, effective analysis is hindered by limited labeled data, high dimensionality, and the absence of scalable models that fully capture spatiotemporal dependencies. Existing self-supervised learning (SSL) methods often focus on either spatial or temporal features, leading to suboptimal representations. To this end, we propose EEG-VJEPA, a novel adaptation of the Video Joint Embedding Predictive Architecture (V-JEPA) for EEG classification. By treating EEG as video-like sequences, EEG-VJEPA learns semantically meaningful spatiotemporal representations using joint embeddings and adaptive masking. To our knowledge, this is the first work that exploits V-JEPA for EEG classification and explores the visual concepts learned by the model. Evaluations on the publicly available Temple University Hospital (TUH) Abnormal EEG dataset show that EEG-VJEPA outperforms existing state-of-the-art models in classification accuracy.Beyond classification accuracy, EEG-VJEPA captures physiologically relevant spatial and temporal signal patterns, offering interpretable embeddings that may support human-AI collaboration in diagnostic workflows. These findings position EEG-VJEPA as a promising framework for scalable, trustworthy EEG analysis in real-world clinical settings.
>
---
#### [new 140] Physics-Guided Dual Implicit Neural Representations for Source Separation
- **分类: cs.CV; cond-mat.str-el; cs.LG; physics.data-an**

- **简介: 该论文属于源分离任务，解决信号中背景和失真干扰的问题。通过双隐式神经表示框架，无需标签数据即可分离物理信号。**

- **链接: [http://arxiv.org/pdf/2507.05249v1](http://arxiv.org/pdf/2507.05249v1)**

> **作者:** Yuan Ni; Zhantao Chen; Alexander N. Petsch; Edmund Xu; Cheng Peng; Alexander I. Kolesnikov; Sugata Chowdhury; Arun Bansil; Jana B. Thayer; Joshua J. Turner
>
> **摘要:** Significant challenges exist in efficient data analysis of most advanced experimental and observational techniques because the collected signals often include unwanted contributions--such as background and signal distortions--that can obscure the physically relevant information of interest. To address this, we have developed a self-supervised machine-learning approach for source separation using a dual implicit neural representation framework that jointly trains two neural networks: one for approximating distortions of the physical signal of interest and the other for learning the effective background contribution. Our method learns directly from the raw data by minimizing a reconstruction-based loss function without requiring labeled data or pre-defined dictionaries. We demonstrate the effectiveness of our framework by considering a challenging case study involving large-scale simulated as well as experimental momentum-energy-dependent inelastic neutron scattering data in a four-dimensional parameter space, characterized by heterogeneous background contributions and unknown distortions to the target signal. The method is found to successfully separate physically meaningful signals from a complex or structured background even when the signal characteristics vary across all four dimensions of the parameter space. An analytical approach that informs the choice of the regularization parameter is presented. Our method offers a versatile framework for addressing source separation problems across diverse domains, ranging from superimposed signals in astronomical measurements to structural features in biomedical image reconstructions.
>
---
#### [new 141] LVLM-Composer's Explicit Planning for Image Generation
- **分类: cs.CV**

- **简介: 该论文属于文本到图像生成任务，旨在解决复杂描述中多对象准确布局与姿态生成的问题。提出LVLM-Composer模型，通过语义规划和特征对齐提升生成质量。**

- **链接: [http://arxiv.org/pdf/2507.04152v1](http://arxiv.org/pdf/2507.04152v1)**

> **作者:** Spencer Ramsey; Jeffrey Lee; Amina Grant
>
> **摘要:** The burgeoning field of generative artificial intelligence has fundamentally reshaped our approach to content creation, with Large Vision-Language Models (LVLMs) standing at its forefront. While current LVLMs have demonstrated impressive capabilities in text-to-image generation, they often falter when confronted with complex textual descriptions demanding precise compositional understanding and visual planning. This limitation particularly impacts the accurate rendering of multiple objects, their attributes, spatial relationships, and specific poses within intricate scenes, as evidenced by benchmarks like LongBench-T2I. To address these challenges, we introduce LVLM-Composer, a novel 10-billion parameter scale LVLM specifically engineered for enhanced compositional image synthesis. Our method incorporates a Hierarchical Semantic Planning Module for structured prompt decomposition and a Fine-Grained Feature Alignment Mechanism for precise visual guidance during generation. We propose a multi-stage training paradigm, featuring Hierarchical Semantic-Visual Grounding Pre-training and Compositional Planning Reinforcement Learning with Self-Correction, to instill robust compositional reasoning. Extensive experiments on the LongBench-T2I benchmark, utilizing automatic evaluation by Gemini-2.0-Flash and InternVL3-78B, demonstrate LVLM-Composer's superior performance across critical compositional dimensions including object accuracy, composition fidelity, and pose accuracy, significantly outperforming state-of-the-art baselines. An in-depth ablation study further validates the indispensable contribution of our proposed modules, while human evaluations confirm the perceptual superiority of our generated images. LVLM-Composer represents a significant step towards truly controllable and compositionally accurate open-ended text-to-image generation.
>
---
#### [new 142] GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation
- **分类: cs.CV; cs.AI; cs.CL; cs.MM**

- **简介: 该论文提出GameTileNet，一个用于低分辨率游戏艺术的语义数据集，解决生成内容与叙事不一致及风格单一问题，支持程序化内容生成和视觉-语言对齐任务。**

- **链接: [http://arxiv.org/pdf/2507.02941v1](http://arxiv.org/pdf/2507.02941v1)**

> **作者:** Yi-Chun Chen; Arnav Jhala
>
> **备注:** Note: This is a preprint version of a paper submitted to AIIDE 2025. It includes additional discussion of limitations and future directions that were omitted from the conference version due to space constraints
>
> **摘要:** GameTileNet is a dataset designed to provide semantic labels for low-resolution digital game art, advancing procedural content generation (PCG) and related AI research as a vision-language alignment task. Large Language Models (LLMs) and image-generative AI models have enabled indie developers to create visual assets, such as sprites, for game interactions. However, generating visuals that align with game narratives remains challenging due to inconsistent AI outputs, requiring manual adjustments by human artists. The diversity of visual representations in automatically generated game content is also limited because of the imbalance in distributions across styles for training data. GameTileNet addresses this by collecting artist-created game tiles from OpenGameArt.org under Creative Commons licenses and providing semantic annotations to support narrative-driven content generation. The dataset introduces a pipeline for object detection in low-resolution tile-based game art (e.g., 32x32 pixels) and annotates semantics, connectivity, and object classifications. GameTileNet is a valuable resource for improving PCG methods, supporting narrative-rich game content, and establishing a baseline for object detection in low-resolution, non-photorealistic images. TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles designed to support narrative-driven procedural content generation through visual-language alignment.
>
---
#### [new 143] VISC: mmWave Radar Scene Flow Estimation using Pervasive Visual-Inertial Supervision
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于场景流估计任务，解决mmWave雷达在缺乏LiDAR监督下的训练难题。通过融合VI数据与神经网络，提升场景流估计精度。**

- **链接: [http://arxiv.org/pdf/2507.03938v1](http://arxiv.org/pdf/2507.03938v1)**

> **作者:** Kezhong Liu; Yiwen Zhou; Mozi Chen; Jianhua He; Jingao Xu; Zheng Yang; Chris Xiaoxuan Lu; Shengkai Zhang
>
> **摘要:** This work proposes a mmWave radar's scene flow estimation framework supervised by data from a widespread visual-inertial (VI) sensor suite, allowing crowdsourced training data from smart vehicles. Current scene flow estimation methods for mmWave radar are typically supervised by dense point clouds from 3D LiDARs, which are expensive and not widely available in smart vehicles. While VI data are more accessible, visual images alone cannot capture the 3D motions of moving objects, making it difficult to supervise their scene flow. Moreover, the temporal drift of VI rigid transformation also degenerates the scene flow estimation of static points. To address these challenges, we propose a drift-free rigid transformation estimator that fuses kinematic model-based ego-motions with neural network-learned results. It provides strong supervision signals to radar-based rigid transformation and infers the scene flow of static points. Then, we develop an optical-mmWave supervision extraction module that extracts the supervision signals of radar rigid transformation and scene flow. It strengthens the supervision by learning the scene flow of dynamic points with the joint constraints of optical and mmWave radar measurements. Extensive experiments demonstrate that, in smoke-filled environments, our method even outperforms state-of-the-art (SOTA) approaches using costly LiDARs.
>
---
#### [new 144] Be the Change You Want to See: Revisiting Remote Sensing Change Detection Practices
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于遥感变化检测任务，旨在解决模型性能提升依赖复杂架构的问题。通过系统分析基础设计选择，提出优化方案，证明简单模型也能达到先进水平。**

- **链接: [http://arxiv.org/pdf/2507.03367v1](http://arxiv.org/pdf/2507.03367v1)**

> **作者:** Blaž Rolih; Matic Fučka; Filip Wolf; Luka Čehovin Zajc
>
> **备注:** Accepted by IEEE TGRS: https://doi.org/10.1109/TGRS.2025.3585342
>
> **摘要:** Remote sensing change detection aims to localize semantic changes between images of the same location captured at different times. In the past few years, newer methods have attributed enhanced performance to the additions of new and complex components to existing architectures. Most fail to measure the performance contribution of fundamental design choices such as backbone selection, pre-training strategies, and training configurations. We claim that such fundamental design choices often improve performance even more significantly than the addition of new architectural components. Due to that, we systematically revisit the design space of change detection models and analyse the full potential of a well-optimised baseline. We identify a set of fundamental design choices that benefit both new and existing architectures. Leveraging this insight, we demonstrate that when carefully designed, even an architecturally simple model can match or surpass state-of-the-art performance on six challenging change detection datasets. Our best practices generalise beyond our architecture and also offer performance improvements when applied to related methods, indicating that the space of fundamental design choices has been underexplored. Our guidelines and architecture provide a strong foundation for future methods, emphasizing that optimizing core components is just as important as architectural novelty in advancing change detection performance. Code: https://github.com/blaz-r/BTC-change-detection
>
---
#### [new 145] InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D Geometry-Consistent Attention Prior
- **分类: cs.CV**

- **简介: 该论文属于3D场景编辑任务，解决多视角编辑中的局部不一致和用户控制不足问题。通过交互选择关键视图和引入3D几何一致性注意力机制，提升编辑质量和用户体验。**

- **链接: [http://arxiv.org/pdf/2507.04961v1](http://arxiv.org/pdf/2507.04961v1)**

> **作者:** Minghao Wen; Shengjie Wu; Kangkan Wang; Dong Liang
>
> **摘要:** 3D Gaussian Splatting based 3D editing has demonstrated impressive performance in recent years. However, the multi-view editing often exhibits significant local inconsistency, especially in areas of non-rigid deformation, which lead to local artifacts, texture blurring, or semantic variations in edited 3D scenes. We also found that the existing editing methods, which rely entirely on text prompts make the editing process a "one-shot deal", making it difficult for users to control the editing degree flexibly. In response to these challenges, we present InterGSEdit, a novel framework for high-quality 3DGS editing via interactively selecting key views with users' preferences. We propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to adaptively screen a group of semantically consistent reference views for each user-selected key view. Then, the cross-attention maps derived from the reference views are used in a weighted Gaussian Splatting unprojection to construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project $GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive attention strategy that prioritizes 3D-constrained attention for geometric consistency during early inference, and gradually prioritizes 2D cross-attention maps in diffusion for fine-grained features during the later inference. Extensive experiments demonstrate that InterGSEdit achieves state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing with improved user experience.
>
---
#### [new 146] PresentAgent: Multimodal Agent for Presentation Video Generation
- **分类: cs.CV**

- **简介: 该论文属于多媒体生成任务，旨在将文本文档转化为同步的视频演示。工作包括构建多模态系统PresentAgent，实现视觉与语音内容的整合与评估。**

- **链接: [http://arxiv.org/pdf/2507.04036v1](http://arxiv.org/pdf/2507.04036v1)**

> **作者:** Jingwei Shi; Zeyu Zhang; Biao Wu; Yanjie Liang; Meng Fang; Ling Chen; Yang Zhao
>
> **摘要:** We present PresentAgent, a multimodal agent that transforms long-form documents into narrated presentation videos. While existing approaches are limited to generating static slides or text summaries, our method advances beyond these limitations by producing fully synchronized visual and spoken content that closely mimics human-style presentations. To achieve this integration, PresentAgent employs a modular pipeline that systematically segments the input document, plans and renders slide-style visual frames, generates contextual spoken narration with large language models and Text-to-Speech models, and seamlessly composes the final video with precise audio-visual alignment. Given the complexity of evaluating such multimodal outputs, we introduce PresentEval, a unified assessment framework powered by Vision-Language Models that comprehensively scores videos across three critical dimensions: content fidelity, visual clarity, and audience comprehension through prompt-based evaluation. Our experimental validation on a curated dataset of 30 document-presentation pairs demonstrates that PresentAgent approaches human-level quality across all evaluation metrics. These results highlight the significant potential of controllable multimodal agents in transforming static textual materials into dynamic, effective, and accessible presentation formats. Code will be available at https://github.com/AIGeeksGroup/PresentAgent.
>
---
#### [new 147] ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic Urban Environments
- **分类: cs.CV**

- **简介: 该论文属于自动驾驶场景建模任务，解决动态城市环境的高保真实时渲染问题。提出ArmGS方法，通过多粒度外观优化提升场景重建质量。**

- **链接: [http://arxiv.org/pdf/2507.03886v1](http://arxiv.org/pdf/2507.03886v1)**

> **作者:** Guile Wu; Dongfeng Bai; Bingbing Liu
>
> **备注:** Technical report
>
> **摘要:** This work focuses on modeling dynamic urban environments for autonomous driving simulation. Contemporary data-driven methods using neural radiance fields have achieved photorealistic driving scene modeling, but they suffer from low rendering efficacy. Recently, some approaches have explored 3D Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity reconstruction and real-time rendering. However, these approaches often neglect to model fine-grained variations between frames and camera viewpoints, leading to suboptimal results. In this work, we propose a new approach named ArmGS that exploits composite driving Gaussian splatting with multi-granularity appearance refinement for autonomous driving scene modeling. The core idea of our approach is devising a multi-level appearance modeling scheme to optimize a set of transformation parameters for composite Gaussian refinement from multiple granularities, ranging from local Gaussian level to global image level and dynamic actor level. This not only models global scene appearance variations between frames and camera viewpoints, but also models local fine-grained changes of background and objects. Extensive experiments on multiple challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and VKITTI2, demonstrate the superiority of our approach over the state-of-the-art methods.
>
---
#### [new 148] Query-Based Adaptive Aggregation for Multi-Dataset Joint Training Toward Universal Visual Place Recognition
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于视觉定位任务，解决多数据集训练中的泛化问题。提出QAA方法，通过查询自适应聚合提升模型性能。**

- **链接: [http://arxiv.org/pdf/2507.03831v1](http://arxiv.org/pdf/2507.03831v1)**

> **作者:** Jiuhong Xiao; Yang Zhou; Giuseppe Loianno
>
> **备注:** 9 pages, 4 figures
>
> **摘要:** Deep learning methods for Visual Place Recognition (VPR) have advanced significantly, largely driven by large-scale datasets. However, most existing approaches are trained on a single dataset, which can introduce dataset-specific inductive biases and limit model generalization. While multi-dataset joint training offers a promising solution for developing universal VPR models, divergences among training datasets can saturate limited information capacity in feature aggregation layers, leading to suboptimal performance. To address these challenges, we propose Query-based Adaptive Aggregation (QAA), a novel feature aggregation technique that leverages learned queries as reference codebooks to effectively enhance information capacity without significant computational or parameter complexity. We show that computing the Cross-query Similarity (CS) between query-level image features and reference codebooks provides a simple yet effective way to generate robust descriptors. Our results demonstrate that QAA outperforms state-of-the-art models, achieving balanced generalization across diverse datasets while maintaining peak performance comparable to dataset-specific models. Ablation studies further explore QAA's mechanisms and scalability. Visualizations reveal that the learned queries exhibit diverse attention patterns across datasets. Code will be publicly released.
>
---
#### [new 149] PromptSR: Cascade Prompting for Lightweight Image Super-Resolution
- **分类: cs.CV**

- **简介: 该论文属于图像超分辨率任务，旨在解决轻量级Vision Transformer接收域受限的问题。通过提出级联提示块，提升全局信息与局部细节的结合，增强模型性能。**

- **链接: [http://arxiv.org/pdf/2507.04118v1](http://arxiv.org/pdf/2507.04118v1)**

> **作者:** Wenyang Liu; Chen Cai; Jianjun Gao; Kejun Wu; Yi Wang; Kim-Hui Yap; Lap-Pui Chau
>
> **备注:** Accepted in TMM
>
> **摘要:** Although the lightweight Vision Transformer has significantly advanced image super-resolution (SR), it faces the inherent challenge of a limited receptive field due to the window-based self-attention modeling. The quadratic computational complexity relative to window size restricts its ability to use a large window size for expanding the receptive field while maintaining low computational costs. To address this challenge, we propose PromptSR, a novel prompt-empowered lightweight image SR method. The core component is the proposed cascade prompting block (CPB), which enhances global information access and local refinement via three cascaded prompting layers: a global anchor prompting layer (GAPL) and two local prompting layers (LPLs). The GAPL leverages downscaled features as anchors to construct low-dimensional anchor prompts (APs) through cross-scale attention, significantly reducing computational costs. These APs, with enhanced global perception, are then used to provide global prompts, efficiently facilitating long-range token connections. The two LPLs subsequently combine category-based self-attention and window-based self-attention to refine the representation in a coarse-to-fine manner. They leverage attention maps from the GAPL as additional global prompts, enabling them to perceive features globally at different granularities for adaptive local refinement. In this way, the proposed CPB effectively combines global priors and local details, significantly enlarging the receptive field while maintaining the low computational costs of our PromptSR. The experimental results demonstrate the superiority of our method, which outperforms state-of-the-art lightweight SR methods in quantitative, qualitative, and complexity evaluations. Our code will be released at https://github.com/wenyang001/PromptSR.
>
---
#### [new 150] Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing
- **分类: cs.CV**

- **简介: 该论文属于图像编辑任务，解决复杂指令理解与身份保留问题。提出X-Planner系统，通过多模态语言模型分解指令并自动生成编辑方案。**

- **链接: [http://arxiv.org/pdf/2507.05259v1](http://arxiv.org/pdf/2507.05259v1)**

> **作者:** Chun-Hsiao Yeh; Yilin Wang; Nanxuan Zhao; Richard Zhang; Yuheng Li; Yi Ma; Krishna Kumar Singh
>
> **备注:** Project page: https://danielchyeh.github.io/x-planner/
>
> **摘要:** Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark.
>
---
#### [new 151] A Simulator Dataset to Support the Study of Impaired Driving
- **分类: cs.CV; cs.LG; cs.RO**

- **简介: 该论文属于自动驾驶研究任务，旨在解决 impaired driving 问题。通过构建包含23.7小时模拟驾驶的数据集，研究酒精和分心对驾驶行为的影响。**

- **链接: [http://arxiv.org/pdf/2507.02867v1](http://arxiv.org/pdf/2507.02867v1)**

> **作者:** John Gideon; Kimimasa Tamura; Emily Sumner; Laporsha Dees; Patricio Reyes Gomez; Bassamul Haq; Todd Rowell; Avinash Balachandran; Simon Stent; Guy Rosman
>
> **备注:** 8 pages, 6 figures, 4 tables
>
> **摘要:** Despite recent advances in automated driving technology, impaired driving continues to incur a high cost to society. In this paper, we present a driving dataset designed to support the study of two common forms of driver impairment: alcohol intoxication and cognitive distraction. Our dataset spans 23.7 hours of simulated urban driving, with 52 human subjects under normal and impaired conditions, and includes both vehicle data (ground truth perception, vehicle pose, controls) and driver-facing data (gaze, audio, surveys). It supports analysis of changes in driver behavior due to alcohol intoxication (0.10\% blood alcohol content), two forms of cognitive distraction (audio n-back and sentence parsing tasks), and combinations thereof, as well as responses to a set of eight controlled road hazards, such as vehicle cut-ins. The dataset will be made available at https://toyotaresearchinstitute.github.io/IDD/.
>
---
#### [new 152] MoReMouse: Monocular Reconstruction of Laboratory Mouse
- **分类: cs.CV**

- **简介: 该论文属于3D重建任务，解决实验室小鼠表面运动重建难题。通过构建合成数据集、使用Transformer架构和几何嵌入，提升重建精度与稳定性。**

- **链接: [http://arxiv.org/pdf/2507.04258v1](http://arxiv.org/pdf/2507.04258v1)**

> **作者:** Yuan Zhong; Jingxiang Sun; Liang An; Yebin Liu
>
> **摘要:** Laboratory mice play a crucial role in biomedical research, yet accurate 3D mouse surface motion reconstruction remains challenging due to their complex non-rigid geometric deformations and textureless appearance. Moreover, the absence of structured 3D datasets severely hinders the progress beyond sparse keypoint tracking. To narrow the gap, we present MoReMouse, the first monocular dense 3D reconstruction network tailored for laboratory mice. To achieve this goal, we highlight three key designs. First, we construct the first high-fidelity dense-view synthetic dataset for mice, by rendering our self-designed realistic Gaussian mouse avatar. Second, MoReMouse adopts a transformer-based feedforward architecture with triplane representation, achieving high-quality 3D surface generation from a single image. Third, we create geodesic-based continuous correspondence embeddings on mouse surface, which serve as strong semantic priors to improve reconstruction stability and surface consistency. Extensive quantitative and qualitative experiments demonstrate that MoReMouse significantly outperforms existing open-source methods in accuracy and robustness. Video results are available at https://zyyw-eric.github.io/MoreMouse-webpage/.
>
---
#### [new 153] All in One: Visual-Description-Guided Unified Point Cloud Segmentation
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于3D点云分割任务，旨在解决细粒度分类和实例区分难题。通过融合视觉-语言模型与多模态信息，提出新框架VDG-Uni3DSeg，提升分割性能。**

- **链接: [http://arxiv.org/pdf/2507.05211v1](http://arxiv.org/pdf/2507.05211v1)**

> **作者:** Zongyan Han; Mohamed El Amine Boudjoghra; Jiahua Dong; Jinhong Wang; Rao Muhammad Anwer
>
> **备注:** Accepted by ICCV2025
>
> **摘要:** Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at https://github.com/Hanzy1996/VDG-Uni3DSeg.
>
---
#### [new 154] Dual-frequency Selected Knowledge Distillation with Statistical-based Sample Rectification for PolSAR Image Classification
- **分类: cs.CV**

- **简介: 该论文属于PolSAR图像分类任务，旨在解决双频数据协同分类中区域一致性影响和数据利用不足的问题。提出SKDNet-SSR模型，结合统计样本修正和双频知识蒸馏，提升分类性能。**

- **链接: [http://arxiv.org/pdf/2507.03268v1](http://arxiv.org/pdf/2507.03268v1)**

> **作者:** Xinyue Xin; Ming Li; Yan Wu; Xiang Li; Peng Zhang; Dazhi Xu
>
> **摘要:** The collaborative classification of dual-frequency PolSAR images is a meaningful but also challenging research. The effect of regional consistency on classification information learning and the rational use of dual-frequency data are two main difficulties for dual-frequency collaborative classification. To tackle these problems, a selected knowledge distillation network with statistical-based sample rectification (SKDNet-SSR) is proposed in this article. First, in addition to applying CNN and ViT as local and global feature extractors, a statistical-based dynamic sample rectification (SDSR) module is designed to avoid the impact of poor regional consistency on spatial information learning process. Specifically, based on the fact that the PolSAR covariance matrix conforms to the complex Wishart distribution, SDSR first dynamically evaluates the sample purity, and then performs pixel selection and pixel generation to remove noisy pixels, thereby avoiding the feature interaction between informative pixels and noisy pixels and improving the classification feature extraction process. Next, a dual-frequency gate-selected distillation (DGSD) module is constructed to emphasize the advantages of different frequency bands and perform complementary learning on dual-frequency data. It uses the dominant single-frequency branch on each sample as teacher model to train the dual-frequency student model, enabling the student model to learn the optimal results and realizing complementary utilization of dual-frequency data on different terrain objects. Comprehensive experiments on four measured dual-frequency PolSAR data demonstrate that the proposed SKDNet-SSR outperforms other related methods.
>
---
#### [new 155] Unleashing the Power of Neural Collapse: Consistent Supervised-Unsupervised Alignment for Generalized Category Discovery
- **分类: cs.CV**

- **简介: 该论文属于通用类别发现任务，旨在解决类别混淆和特征重叠问题。提出NC-GCD框架，通过ETF原型和一致性损失提升新类别识别性能。**

- **链接: [http://arxiv.org/pdf/2507.04725v1](http://arxiv.org/pdf/2507.04725v1)**

> **作者:** Jizhou Han; Shaokun Wang; Yuhang He; Chenhao Ding; Qiang Wang; Xinyuan Gao; SongLin Dong; Yihong Gong
>
> **摘要:** Generalized Category Discovery (GCD) focuses on classifying known categories while simultaneously discovering novel categories from unlabeled data. However, previous GCD methods face challenges due to inconsistent optimization objectives and category confusion. This leads to feature overlap and ultimately hinders performance on novel categories. To address these issues, we propose the Neural Collapse-inspired Generalized Category Discovery (NC-GCD) framework. By pre-assigning and fixing Equiangular Tight Frame (ETF) prototypes, our method ensures an optimal geometric structure and a consistent optimization objective for both known and novel categories. We introduce a Consistent ETF Alignment Loss that unifies supervised and unsupervised ETF alignment and enhances category separability. Additionally, a Semantic Consistency Matcher (SCM) is designed to maintain stable and consistent label assignments across clustering iterations. Our method achieves strong performance on multiple GCD benchmarks, significantly enhancing novel category accuracy and demonstrating its effectiveness.
>
---
#### [new 156] Colorectal Cancer Tumor Grade Segmentation in Digital Histopathology Images: From Giga to Mini Challenge
- **分类: cs.CV**

- **简介: 该论文聚焦于结直肠癌肿瘤分级分割任务，旨在解决病理诊断主观性强、专家短缺的问题，通过组织竞赛和公开数据集推动自动化解决方案。**

- **链接: [http://arxiv.org/pdf/2507.04681v1](http://arxiv.org/pdf/2507.04681v1)**

> **作者:** Alper Bahcekapili; Duygu Arslan; Umut Ozdemir; Berkay Ozkirli; Emre Akbas; Ahmet Acar; Gozde B. Akar; Bingdou He; Shuoyu Xu; Umit Mert Caglar; Alptekin Temizel; Guillaume Picaud; Marc Chaumont; Gérard Subsol; Luc Téot; Fahad Alsharekh; Shahad Alghannam; Hexiang Mao; Wenhua Zhang
>
> **备注:** Accepted Grand Challenge Paper ICIP 2025
>
> **摘要:** Colorectal cancer (CRC) is the third most diagnosed cancer and the second leading cause of cancer-related death worldwide. Accurate histopathological grading of CRC is essential for prognosis and treatment planning but remains a subjective process prone to observer variability and limited by global shortages of trained pathologists. To promote automated and standardized solutions, we organized the ICIP Grand Challenge on Colorectal Cancer Tumor Grading and Segmentation using the publicly available METU CCTGS dataset. The dataset comprises 103 whole-slide images with expert pixel-level annotations for five tissue classes. Participants submitted segmentation masks via Codalab, evaluated using metrics such as macro F-score and mIoU. Among 39 participating teams, six outperformed the Swin Transformer baseline (62.92 F-score). This paper presents an overview of the challenge, dataset, and the top-performing methods
>
---
#### [new 157] Computationally efficient non-Intrusive pre-impact fall detection system
- **分类: cs.CV**

- **简介: 该论文属于跌倒检测任务，旨在解决现有系统侵入性强或计算成本高的问题。工作包括使用视频数据和简化模型实现非侵入且高效的预跌倒检测。**

- **链接: [http://arxiv.org/pdf/2507.03705v1](http://arxiv.org/pdf/2507.03705v1)**

> **作者:** Praveen Jesudhas; Raghuveera T; Shiney Jeyaraj
>
> **摘要:** Existing pre-impact fall detection systems have high accuracy, however they are either intrusive to the subject or require heavy computational resources for fall detection, resulting in prohibitive deployment costs. These factors limit the global adoption of existing fall detection systems. In this work we present a Pre-impact fall detection system that is both non-intrusive and computationally efficient at deployment. Our system utilizes video data of the locality available through cameras, thereby requiring no specialized equipment to be worn by the subject. Further, the fall detection system utilizes minimal fall specific features and simplistic neural network models, designed to reduce the computational cost of the system. A minimal set of fall specific features are derived from the skeletal data, post observing the relative position of human skeleton during fall. These features are shown to have different distributions for Fall and non-fall scenarios proving their discriminative capability. A Long Short Term Memory (LSTM) based network is selected and the network architecture and training parameters are designed after evaluation of performance on standard datasets. In the Pre-impact fall detection system the computation requirement is about 18 times lesser than existing modules with a comparable accuracy of 88%. Given the low computation requirements and higher accuracy levels, the proposed system is suitable for wider adoption in engineering systems related to industrial and residential safety.
>
---
#### [new 158] From Imitation to Innovation: The Emergence of AI Unique Artistic Styles and the Challenge of Copyright Protection
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于AI艺术版权研究任务，旨在解决AI生成作品的版权认定问题，提出ArtBulb框架和AICD数据集以提升判断准确性。**

- **链接: [http://arxiv.org/pdf/2507.04769v1](http://arxiv.org/pdf/2507.04769v1)**

> **作者:** Zexi Jia; Chuanwei Huang; Yeshuang Zhu; Hongyan Fei; Ying Deng; Zhiqiang Yuan; Jiapei Zhang; Jinchao Zhang; Jie Zhou
>
> **摘要:** Current legal frameworks consider AI-generated works eligible for copyright protection when they meet originality requirements and involve substantial human intellectual input. However, systematic legal standards and reliable evaluation methods for AI art copyrights are lacking. Through comprehensive analysis of legal precedents, we establish three essential criteria for determining distinctive artistic style: stylistic consistency, creative uniqueness, and expressive accuracy. To address these challenges, we introduce ArtBulb, an interpretable and quantifiable framework for AI art copyright judgment that combines a novel style description-based multimodal clustering method with multimodal large language models (MLLMs). We also present AICD, the first benchmark dataset for AI art copyright annotated by artists and legal experts. Experimental results demonstrate that ArtBulb outperforms existing models in both quantitative and qualitative evaluations. Our work aims to bridge the gap between the legal and technological communities and bring greater attention to the societal issue of AI art copyrights.
>
---
#### [new 159] Task-Specific Generative Dataset Distillation with Difficulty-Guided Sampling
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于图像分类任务，解决生成数据集在保持性能的同时减少数据量的问题。通过引入难度引导的采样策略提升下游任务效果。**

- **链接: [http://arxiv.org/pdf/2507.03331v1](http://arxiv.org/pdf/2507.03331v1)**

> **作者:** Mingzhuo Li; Guang Li; Jiafeng Mao; Linfeng Ye; Takahiro Ogawa; Miki Haseyama
>
> **摘要:** To alleviate the reliance of deep neural networks on large-scale datasets, dataset distillation aims to generate compact, high-quality synthetic datasets that can achieve comparable performance to the original dataset. The integration of generative models has significantly advanced this field. However, existing approaches primarily focus on aligning the distilled dataset with the original one, often overlooking task-specific information that can be critical for optimal downstream performance. In this paper, focusing on the downstream task of classification, we propose a task-specific sampling strategy for generative dataset distillation that incorporates the concept of difficulty to consider the requirements of the target task better. The final dataset is sampled from a larger image pool with a sampling distribution obtained by matching the difficulty distribution of the original dataset. A logarithmic transformation is applied as a pre-processing step to correct for distributional bias. The results of extensive experiments demonstrate the effectiveness of our method and suggest its potential for enhancing performance on other downstream tasks.
>
---
#### [new 160] UDF-GMA: Uncertainty Disentanglement and Fusion for General Movement Assessment
- **分类: cs.CV; cs.LG; eess.IV**

- **简介: 该论文属于自动化运动评估任务，旨在解决姿态估计中的不确定性问题。提出UDF-GMA模型，分离并融合认知与数据噪声不确定性，提升分类效果。**

- **链接: [http://arxiv.org/pdf/2507.04814v1](http://arxiv.org/pdf/2507.04814v1)**

> **作者:** Zeqi Luo; Ali Gooya; Edmond S. L. Ho
>
> **备注:** This work has been accepted for publication in IEEE Journal of Biomedical and Health Informatics (J-BHI)
>
> **摘要:** General movement assessment (GMA) is a non-invasive tool for the early detection of brain dysfunction through the qualitative assessment of general movements, and the development of automated methods can broaden its application. However, mainstream pose-based automated GMA methods are prone to uncertainty due to limited high-quality data and noisy pose estimation, hindering clinical reliability without reliable uncertainty measures. In this work, we introduce UDF-GMA which explicitly models epistemic uncertainty in model parameters and aleatoric uncertainty from data noise for pose-based automated GMA. UDF-GMA effectively disentangles uncertainties by directly modelling aleatoric uncertainty and estimating epistemic uncertainty through Bayesian approximation. We further propose fusing these uncertainties with the embedded motion representation to enhance class separation. Extensive experiments on the Pmi-GMA benchmark dataset demonstrate the effectiveness and generalisability of the proposed approach in predicting poor repertoire.
>
---
#### [new 161] Beyond One Shot, Beyond One Perspective: Cross-View and Long-Horizon Distillation for Better LiDAR Representations
- **分类: cs.CV; cs.LG; cs.RO**

- **简介: 该论文属于LiDAR表示学习任务，旨在提升语义分割和3D目标检测效果。针对现有方法忽视时空信息的问题，提出LiMA框架，通过跨视角、长时序聚合增强表示能力。**

- **链接: [http://arxiv.org/pdf/2507.05260v1](http://arxiv.org/pdf/2507.05260v1)**

> **作者:** Xiang Xu; Lingdong Kong; Song Wang; Chuanwei Zhou; Qingshan Liu
>
> **备注:** ICCV 2025; 26 pages, 12 figures, 10 tables; Code at http://github.com/Xiangxu-0103/LiMA
>
> **摘要:** LiDAR representation learning aims to extract rich structural and semantic information from large-scale, readily available datasets, reducing reliance on costly human annotations. However, existing LiDAR representation strategies often overlook the inherent spatiotemporal cues in LiDAR sequences, limiting their effectiveness. In this work, we propose LiMA, a novel long-term image-to-LiDAR Memory Aggregation framework that explicitly captures longer range temporal correlations to enhance LiDAR representation learning. LiMA comprises three key components: 1) a Cross-View Aggregation module that aligns and fuses overlapping regions across neighboring camera views, constructing a more unified and redundancy-free memory bank; 2) a Long-Term Feature Propagation mechanism that efficiently aligns and integrates multi-frame image features, reinforcing temporal coherence during LiDAR representation learning; and 3) a Cross-Sequence Memory Alignment strategy that enforces consistency across driving sequences, improving generalization to unseen environments. LiMA maintains high pretraining efficiency and incurs no additional computational overhead during downstream tasks. Extensive experiments on mainstream LiDAR-based perception benchmarks demonstrate that LiMA significantly improves both LiDAR semantic segmentation and 3D object detection. We hope this work inspires more effective pretraining paradigms for autonomous driving. The code has be made publicly accessible for future research.
>
---
#### [new 162] Boosting Temporal Sentence Grounding via Causal Inference
- **分类: cs.CV; cs.MM**

- **简介: 该论文属于时间句子定位任务，解决视频与文本间虚假相关性问题，通过因果推断和反事实推理提升模型鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.04958v1](http://arxiv.org/pdf/2507.04958v1)**

> **作者:** Kefan Tang; Lihuo He; Jisheng Dang; Xinbo Gao
>
> **备注:** Accepted by ACM MM 2025
>
> **摘要:** Temporal Sentence Grounding (TSG) aims to identify relevant moments in an untrimmed video that semantically correspond to a given textual query. Despite existing studies having made substantial progress, they often overlook the issue of spurious correlations between video and textual queries. These spurious correlations arise from two primary factors: (1) inherent biases in the textual data, such as frequent co-occurrences of specific verbs or phrases, and (2) the model's tendency to overfit to salient or repetitive patterns in video content. Such biases mislead the model into associating textual cues with incorrect visual moments, resulting in unreliable predictions and poor generalization to out-of-distribution examples. To overcome these limitations, we propose a novel TSG framework, causal intervention and counterfactual reasoning that utilizes causal inference to eliminate spurious correlations and enhance the model's robustness. Specifically, we first formulate the TSG task from a causal perspective with a structural causal model. Then, to address unobserved confounders reflecting textual biases toward specific verbs or phrases, a textual causal intervention is proposed, utilizing do-calculus to estimate the causal effects. Furthermore, visual counterfactual reasoning is performed by constructing a counterfactual scenario that focuses solely on video features, excluding the query and fused multi-modal features. This allows us to debias the model by isolating and removing the influence of the video from the overall effect. Experiments on public datasets demonstrate the superiority of the proposed method. The code is available at https://github.com/Tangkfan/CICR.
>
---
#### [new 163] ZERO: Multi-modal Prompt-based Visual Grounding
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于目标检测任务，旨在解决工业场景下少样本、跨域检测问题。提出ZERO模型，结合多模态提示与高效训练策略，提升检测性能与适应性。**

- **链接: [http://arxiv.org/pdf/2507.04270v1](http://arxiv.org/pdf/2507.04270v1)**

> **作者:** Sangbum Choi; Kyeongryeol Go
>
> **备注:** A solution report for CVPR2025 Foundational FSOD Challenge
>
> **摘要:** Recent advances in artificial intelligence have led to the emergence of foundation models, large-scale pre-trained neural networks that serve as versatile starting points for a wide range of downstream tasks. In this work, we present ZERO, a zero-shot multi-prompt object detection model specifically designed for robust, production-ready deployment across diverse industrial domains. ZERO integrates direct image input with multiple user-defined prompts, which can include both textual and visual cues, and processes them through dedicated encoders to generate accurate detection outputs. The model architecture is optimized for scalability, with a total of 1.033 TFLOPS and 622.346 million parameters, and is trained using a domain-specific image database exceeding one billion images. For the CVPR 2025 Foundational Few-Shot Object Detection (FSOD) Challenge, we introduce a domain-specific fine-tuning strategy that emphasizes prompt diversity and conservative pseudo-labeling, enabling effective adaptation to new domains with minimal supervision. Our approach demonstrates practical advantages in flexibility, efficiency, and real-world applicability, achieving strong performance on the RF20VL-fsod benchmark despite limited annotation budgets. The results highlight the potential of prompt-driven, data-centric AI for scalable and adaptive object detection in dynamic industrial environments.
>
---
#### [new 164] Beyond Accuracy: Metrics that Uncover What Makes a `Good' Visual Descriptor
- **分类: cs.CV**

- **简介: 该论文属于视觉描述符研究任务，旨在解决描述符质量评估问题。通过分析表示能力和与预训练数据的关系，提出新度量方法以超越准确率评估。**

- **链接: [http://arxiv.org/pdf/2507.03542v1](http://arxiv.org/pdf/2507.03542v1)**

> **作者:** Ethan Lin; Linxi Zhao; Atharva Sehgal; Jennifer J. Sun
>
> **备注:** VisCon @ CVPR 2025
>
> **摘要:** Text-based visual descriptors-ranging from simple class names to more descriptive phrases-are widely used in visual concept discovery and image classification with vision-language models (VLMs). Their effectiveness, however, depends on a complex interplay of factors, including semantic clarity, presence in the VLM's pre-training data, and how well the descriptors serve as a meaningful representation space. In this work, we systematically analyze descriptor quality along two key dimensions: (1) representational capacity, and (2) relationship with VLM pre-training data. We evaluate a spectrum of descriptor generation methods, from zero-shot LLM-generated prompts to iteratively refined descriptors. Motivated by ideas from representation alignment and language understanding, we introduce two alignment-based metrics-Global Alignment and CLIP Similarity-that move beyond accuracy. These metrics allow us to shed light on how different descriptor generation strategies interact with foundation model properties, offering insights into ways of studying descriptor effectiveness beyond accuracy evaluations.
>
---
#### [new 165] CTA: Cross-Task Alignment for Better Test Time Training
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于计算机视觉领域，解决模型在分布变化下的性能下降问题。通过引入CTA方法，提升测试时训练的鲁棒性与泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.05221v1](http://arxiv.org/pdf/2507.05221v1)**

> **作者:** Samuel Barbeau; Pedram Fekri; David Osowiechi; Ali Bahri; Moslem YazdanpanahMasih Aminbeidokhti; Christian Desrosiers
>
> **备注:** Preprint, under review
>
> **摘要:** Deep learning models have demonstrated exceptional performance across a wide range of computer vision tasks. However, their performance often degrades significantly when faced with distribution shifts, such as domain or dataset changes. Test-Time Training (TTT) has emerged as an effective method to enhance model robustness by incorporating an auxiliary unsupervised task during training and leveraging it for model updates at test time. In this work, we introduce CTA (Cross-Task Alignment), a novel approach for improving TTT. Unlike existing TTT methods, CTA does not require a specialized model architecture and instead takes inspiration from the success of multi-modal contrastive learning to align a supervised encoder with a self-supervised one. This process enforces alignment between the learned representations of both models, thereby mitigating the risk of gradient interference, preserving the intrinsic robustness of self-supervised learning and enabling more semantically meaningful updates at test-time. Experimental results demonstrate substantial improvements in robustness and generalization over the state-of-the-art on several benchmark datasets.
>
---
#### [new 166] Enhancing Sports Strategy with Video Analytics and Data Mining: Automated Video-Based Analytics Framework for Tennis Doubles
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于体育视频分析任务，旨在解决网球双打自动化分析不足的问题。通过构建标注框架和融合深度学习模型，提升球员位置、击球类型等的识别精度。**

- **链接: [http://arxiv.org/pdf/2507.02906v1](http://arxiv.org/pdf/2507.02906v1)**

> **作者:** Jia Wei Chen
>
> **备注:** B.Sc. thesis 59 pages, 26 figures
>
> **摘要:** We present a comprehensive video-based analytics framework for tennis doubles that addresses the lack of automated analysis tools for this strategically complex sport. Our approach introduces a standardised annotation methodology encompassing player positioning, shot types, court formations, and match outcomes, coupled with a specialised annotation tool designed to meet the unique requirements of tennis video labelling. The framework integrates advanced machine learning techniques including GroundingDINO for precise player localisation through natural language grounding and YOLO-Pose for robust pose estimation. This combination significantly reduces manual annotation effort whilst improving data consistency and quality. We evaluate our approach on doubles tennis match data and demonstrate that CNN-based models with transfer learning substantially outperform pose-based methods for predicting shot types, player positioning, and formations. The CNN models effectively capture complex visual and contextual features essential for doubles tennis analysis. Our integrated system bridges advanced analytical capabilities with the strategic complexities of tennis doubles, providing a foundation for automated tactical analysis, performance evaluation, and strategic modelling in professional tennis.
>
---
#### [new 167] T-SYNTH: A Knowledge-Based Dataset of Synthetic Breast Images
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于医学图像处理任务，旨在解决真实数据不足的问题。通过物理模拟生成合成乳腺影像及其像素级标注，构建了T-SYNTH数据集，用于提升检测任务性能。**

- **链接: [http://arxiv.org/pdf/2507.04038v1](http://arxiv.org/pdf/2507.04038v1)**

> **作者:** Christopher Wiedeman; Anastasiia Sarmakeeva; Elena Sizikova; Daniil Filienko; Miguel Lago; Jana G. Delfino; Aldo Badano
>
> **摘要:** One of the key impediments for developing and assessing robust medical imaging algorithms is limited access to large-scale datasets with suitable annotations. Synthetic data generated with plausible physical and biological constraints may address some of these data limitations. We propose the use of physics simulations to generate synthetic images with pixel-level segmentation annotations, which are notoriously difficult to obtain. Specifically, we apply this approach to breast imaging analysis and release T-SYNTH, a large-scale open-source dataset of paired 2D digital mammography (DM) and 3D digital breast tomosynthesis (DBT) images. Our initial experimental results indicate that T-SYNTH images show promise for augmenting limited real patient datasets for detection tasks in DM and DBT. Our data and code are publicly available at https://github.com/DIDSR/tsynth-release.
>
---
#### [new 168] Satellite-based Rabi rice paddy field mapping in India: a case study on Telangana state
- **分类: cs.CV**

- **简介: 该论文属于农业遥感任务，解决小农地区水稻种植区精准监测问题。通过构建基于物候的分类框架，提升区域农业监测精度。**

- **链接: [http://arxiv.org/pdf/2507.05189v1](http://arxiv.org/pdf/2507.05189v1)**

> **作者:** Prashanth Reddy Putta; Fabio Dell'Acqua
>
> **备注:** 60 pages, 17 figures. Intended for submission to Remote Sensing Applications: Society and Environment (RSASE). Funded by the European Union - NextGenerationEU, Mission 4 Component 1.5
>
> **摘要:** Accurate rice area monitoring is critical for food security and agricultural policy in smallholder farming regions, yet conventional remote sensing approaches struggle with the spatiotemporal heterogeneity characteristic of fragmented agricultural landscapes. This study developed a phenology-driven classification framework that systematically adapts to local agro-ecological variations across 32 districts in Telangana, India during the 2018-19 Rabi rice season. The research reveals significant spatiotemporal diversity, with phenological timing varying by up to 50 days between districts and field sizes ranging from 0.01 to 2.94 hectares. Our district-specific calibration approach achieved 93.3% overall accuracy, an 8.0 percentage point improvement over conventional regional clustering methods, with strong validation against official government statistics (R^2 = 0.981) demonstrating excellent agreement between remotely sensed and ground truth data. The framework successfully mapped 732,345 hectares by adapting to agro-climatic variations, with Northern districts requiring extended land preparation phases (up to 55 days) while Southern districts showed compressed cultivation cycles. Field size analysis revealed accuracy declining 6.8 percentage points from medium to tiny fields, providing insights for operational monitoring in fragmented landscapes. These findings demonstrate that remote sensing frameworks must embrace rather than simplify landscape complexity, advancing region-specific agricultural monitoring approaches that maintain scientific rigor while serving practical policy and food security applications.
>
---
#### [new 169] Efficient Event-Based Semantic Segmentation via Exploiting Frame-Event Fusion: A Hybrid Neural Network Approach
- **分类: cs.CV**

- **简介: 该论文属于图像语义分割任务，旨在解决事件相机与帧数据融合不足的问题。提出一种混合神经网络框架，通过三个模块提升分割精度并降低能耗。**

- **链接: [http://arxiv.org/pdf/2507.03765v1](http://arxiv.org/pdf/2507.03765v1)**

> **作者:** Hebei Li; Yansong Peng; Jiahui Yuan; Peixi Wu; Jin Wang; Yueyi Zhang; Xiaoyan Sun
>
> **摘要:** Event cameras have recently been introduced into image semantic segmentation, owing to their high temporal resolution and other advantageous properties. However, existing event-based semantic segmentation methods often fail to fully exploit the complementary information provided by frames and events, resulting in complex training strategies and increased computational costs. To address these challenges, we propose an efficient hybrid framework for image semantic segmentation, comprising a Spiking Neural Network branch for events and an Artificial Neural Network branch for frames. Specifically, we introduce three specialized modules to facilitate the interaction between these two branches: the Adaptive Temporal Weighting (ATW) Injector, the Event-Driven Sparse (EDS) Injector, and the Channel Selection Fusion (CSF) module. The ATW Injector dynamically integrates temporal features from event data into frame features, enhancing segmentation accuracy by leveraging critical dynamic temporal information. The EDS Injector effectively combines sparse event data with rich frame features, ensuring precise temporal and spatial information alignment. The CSF module selectively merges these features to optimize segmentation performance. Experimental results demonstrate that our framework not only achieves state-of-the-art accuracy across the DDD17-Seg, DSEC-Semantic, and M3ED-Semantic datasets but also significantly reduces energy consumption, achieving a 65\% reduction on the DSEC-Semantic dataset.
>
---
#### [new 170] Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles
- **分类: cs.CV; cs.AI; cs.ET; cs.LG; cs.RO**

- **简介: 该论文属于驾驶员状态评估任务，解决自动化车辆中驾驶员接管准备度的准确评估问题。通过多摄像头融合与深度学习模型提升预测准确性。**

- **链接: [http://arxiv.org/pdf/2507.04139v1](http://arxiv.org/pdf/2507.04139v1)**

> **作者:** Mahdi Rezaei; Mohsen Azarmi
>
> **备注:** 8 pages, 4 Figures, 4 Tables. Accepted at IEEE IV 2025
>
> **摘要:** Ensuring safe transition of control in automated vehicles requires an accurate and timely assessment of driver readiness. This paper introduces Driver-Net, a novel deep learning framework that fuses multi-camera inputs to estimate driver take-over readiness. Unlike conventional vision-based driver monitoring systems that focus on head pose or eye gaze, Driver-Net captures synchronised visual cues from the driver's head, hands, and body posture through a triple-camera setup. The model integrates spatio-temporal data using a dual-path architecture, comprising a Context Block and a Feature Block, followed by a cross-modal fusion strategy to enhance prediction accuracy. Evaluated on a diverse dataset collected from the University of Leeds Driving Simulator, the proposed method achieves an accuracy of up to 95.8% in driver readiness classification. This performance significantly enhances existing approaches and highlights the importance of multimodal and multi-view fusion. As a real-time, non-intrusive solution, Driver-Net contributes meaningfully to the development of safer and more reliable automated vehicles and aligns with new regulatory mandates and upcoming safety standards.
>
---
#### [new 171] Learning Adaptive Node Selection with External Attention for Human Interaction Recognition
- **分类: cs.CV**

- **简介: 该论文属于人体交互识别任务，解决传统方法无法动态捕捉交互关系的问题。提出ASEA网络，结合自适应节点选择和外部注意力机制，提升交互建模效果。**

- **链接: [http://arxiv.org/pdf/2507.03936v1](http://arxiv.org/pdf/2507.03936v1)**

> **作者:** Chen Pang; Xuequan Lu; Qianyu Zhou; Lei Lyu
>
> **备注:** Accepted by ACM MM25
>
> **摘要:** Most GCN-based methods model interacting individuals as independent graphs, neglecting their inherent inter-dependencies. Although recent approaches utilize predefined interaction adjacency matrices to integrate participants, these matrices fail to adaptively capture the dynamic and context-specific joint interactions across different actions. In this paper, we propose the Active Node Selection with External Attention Network (ASEA), an innovative approach that dynamically captures interaction relationships without predefined assumptions. Our method models each participant individually using a GCN to capture intra-personal relationships, facilitating a detailed representation of their actions. To identify the most relevant nodes for interaction modeling, we introduce the Adaptive Temporal Node Amplitude Calculation (AT-NAC) module, which estimates global node activity by combining spatial motion magnitude with adaptive temporal weighting, thereby highlighting salient motion patterns while reducing irrelevant or redundant information. A learnable threshold, regularized to prevent extreme variations, is defined to selectively identify the most informative nodes for interaction modeling. To capture interactions, we design the External Attention (EA) module to operate on active nodes, effectively modeling the interaction dynamics and semantic relationships between individuals. Extensive evaluations show that our method captures interaction relationships more effectively and flexibly, achieving state-of-the-art performance.
>
---
#### [new 172] DNF-Intrinsic: Deterministic Noise-Free Diffusion for Indoor Inverse Rendering
- **分类: cs.CV**

- **简介: 该论文属于室内逆渲染任务，解决噪声影响下的结构与外观信息丢失问题。通过直接使用源图像进行确定性内在属性预测，提升渲染质量。**

- **链接: [http://arxiv.org/pdf/2507.03924v1](http://arxiv.org/pdf/2507.03924v1)**

> **作者:** Rongjia Zheng; Qing Zhang; Chengjiang Long; Wei-Shi Zheng
>
> **摘要:** Recent methods have shown that pre-trained diffusion models can be fine-tuned to enable generative inverse rendering by learning image-conditioned noise-to-intrinsic mapping. Despite their remarkable progress, they struggle to robustly produce high-quality results as the noise-to-intrinsic paradigm essentially utilizes noisy images with deteriorated structure and appearance for intrinsic prediction, while it is common knowledge that structure and appearance information in an image are crucial for inverse rendering. To address this issue, we present DNF-Intrinsic, a robust yet efficient inverse rendering approach fine-tuned from a pre-trained diffusion model, where we propose to take the source image rather than Gaussian noise as input to directly predict deterministic intrinsic properties via flow matching. Moreover, we design a generative renderer to constrain that the predicted intrinsic properties are physically faithful to the source image. Experiments on both synthetic and real-world datasets show that our method clearly outperforms existing state-of-the-art methods.
>
---
#### [new 173] Hear-Your-Click: Interactive Video-to-Audio Generation via Object-aware Contrastive Audio-Visual Fine-tuning
- **分类: cs.CV; cs.AI; cs.SD; eess.AS**

- **简介: 该论文属于视频到音频生成任务，解决现有方法难以精准生成特定物体声音的问题。通过交互式点击和对象感知的音频视觉对齐技术实现更精确的音频生成。**

- **链接: [http://arxiv.org/pdf/2507.04959v1](http://arxiv.org/pdf/2507.04959v1)**

> **作者:** Yingshan Liang; Keyu Fan; Zhicheng Du; Yiran Wang; Qingyang Shi; Xinyu Zhang; Jiasheng Lu; Peiwu Qin
>
> **摘要:** Video-to-audio (V2A) generation shows great potential in fields such as film production. Despite significant advances, current V2A methods, which rely on global video information, struggle with complex scenes and often fail to generate audio tailored to specific objects or regions in the videos. To address these limitations, we introduce Hear-Your-Click, an interactive V2A framework that enables users to generate sounds for specific objects in the videos by simply clicking on the frame. To achieve this, we propose Object-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided Visual Encoder (MVE) to obtain object-level visual features aligned with corresponding audio segments. Furthermore, we tailor two data augmentation strategies: Random Video Stitching (RVS) and Mask-guided Loudness Modulation (MLM), aimed at enhancing the model's sensitivity to the segmented objects. To effectively measure the audio-visual correspondence, we design a new evaluation metric, the CAV score, for evaluation. Extensive experiments demonstrate that our framework offers more precise control and improved generation performance across various metrics. Project Page: https://github.com/SynapGrid/Hear-Your-Click
>
---
#### [new 174] CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via Chain-of-Thought Reasoning
- **分类: cs.CV**

- **简介: 该论文属于语义分割任务，旨在解决道路场景中的OOD检测问题。针对密集、远距离和大物体等挑战场景，提出基于CoT的框架，提升检测效果。**

- **链接: [http://arxiv.org/pdf/2507.03984v1](http://arxiv.org/pdf/2507.03984v1)**

> **作者:** Jeonghyo Song; Kimin Yun; DaeUng Jo; Jinyoung Kim; Youngjoon Yoo
>
> **备注:** 6 pages, 3 figures
>
> **摘要:** Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the reliability of semantic segmentation models, particularly in complex road environments where safety and accuracy are paramount. Despite recent advancements in large language models (LLMs), notably GPT-4, which significantly enhanced multimodal reasoning through Chain-of-Thought (CoT) prompting, the application of CoT-based visual reasoning for OOD semantic segmentation remains largely unexplored. In this paper, through extensive analyses of the road scene anomalies, we identify three challenging scenarios where current state-of-the-art OOD segmentation methods consistently struggle: (1) densely packed and overlapping objects, (2) distant scenes with small objects, and (3) large foreground-dominant objects. To address the presented challenges, we propose a novel CoT-based framework targeting OOD detection in road anomaly scenes. Our method leverages the extensive knowledge and reasoning capabilities of foundation models, such as GPT-4, to enhance OOD detection through improved image understanding and prompt-based reasoning aligned with observed problematic scene attributes. Extensive experiments show that our framework consistently outperforms state-of-the-art methods on both standard benchmarks and our newly defined challenging subset of the RoadAnomaly dataset, offering a robust and interpretable solution for OOD semantic segmentation in complex driving environments.
>
---
#### [new 175] Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation
- **分类: cs.CV**

- **简介: 该论文属于具身导航任务，解决如何让智能体主动感知和理解3D环境的问题。提出MTU3D框架，融合视觉语言与探索，提升导航效率和泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.04047v1](http://arxiv.org/pdf/2507.04047v1)**

> **作者:** Ziyu Zhu; Xilin Wang; Yixuan Li; Zhuofan Zhang; Xiaojian Ma; Yixin Chen; Baoxiong Jia; Wei Liang; Qian Yu; Zhidong Deng; Siyuan Huang; Qing Li
>
> **备注:** Embodied AI; 3D Vision Language Understanding
>
> **摘要:** Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce \underline{\textbf{M}}ove \underline{\textbf{t}}o \underline{\textbf{U}}nderstand (\textbf{\model}), a unified framework that integrates active perception with \underline{\textbf{3D}} vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations: 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploring, which represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines \textbf{V}ision-\textbf{L}anguage-\textbf{E}xploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14\%, 23\%, 9\%, and 2\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. \model's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.
>
---
#### [new 176] Habitat Classification from Ground-Level Imagery Using Deep Neural Networks
- **分类: cs.CV; I.2.10; I.4.8; I.5.4; I.2.1**

- **简介: 该论文属于图像分类任务，旨在解决地面栖息地自动识别问题。通过深度学习模型提升分类精度，实现高效生态评估。**

- **链接: [http://arxiv.org/pdf/2507.04017v1](http://arxiv.org/pdf/2507.04017v1)**

> **作者:** Hongrui Shi; Lisa Norton; Lucy Ridding; Simon Rolph; Tom August; Claire M Wood; Lan Qie; Petra Bosilj; James M Brown
>
> **备注:** 26 pages, 12 figures, 6 tables
>
> **摘要:** Habitat assessment at local scales -- critical for enhancing biodiversity and guiding conservation priorities -- often relies on expert field survey that can be costly, motivating the exploration of AI-driven tools to automate and refine this process. While most AI-driven habitat mapping depends on remote sensing, it is often constrained by sensor availability, weather, and coarse resolution. In contrast, ground-level imagery captures essential structural and compositional cues invisible from above and remains underexplored for robust, fine-grained habitat classification. This study addresses this gap by applying state-of-the-art deep neural network architectures to ground-level habitat imagery. Leveraging data from the UK Countryside Survey covering 18 broad habitat types, we evaluate two families of models -- convolutional neural networks (CNNs) and vision transformers (ViTs) -- under both supervised and supervised contrastive learning paradigms. Our results demonstrate that ViTs consistently outperform state-of-the-art CNN baselines on key classification metrics (Top-3 accuracy = 91\%, MCC = 0.66) and offer more interpretable scene understanding tailored to ground-level images. Moreover, supervised contrastive learning significantly reduces misclassification rates among visually similar habitats (e.g., Improved vs. Neutral Grassland), driven by a more discriminative embedding space. Finally, our best model performs on par with experienced ecological experts in habitat classification from images, underscoring the promise of expert-level automated assessment. By integrating advanced AI with ecological expertise, this research establishes a scalable, cost-effective framework for ground-level habitat monitoring to accelerate biodiversity conservation and inform land-use decisions at the national scale.
>
---
#### [new 177] Robustifying 3D Perception through Least-Squares Multi-Agent Graphs Object Tracking
- **分类: cs.CV**

- **简介: 该论文属于多智能体3D目标跟踪任务，旨在解决对抗噪声下的感知鲁棒性问题。通过最小二乘图方法融合多车检测，提升跟踪精度与抗干扰能力。**

- **链接: [http://arxiv.org/pdf/2507.04762v1](http://arxiv.org/pdf/2507.04762v1)**

> **作者:** Maria Damanaki; Ioulia Kapsali; Nikos Piperigkos; Alexandros Gkillas; Aris S. Lalos
>
> **备注:** 6 pages, 3 figures, 4 tables
>
> **摘要:** The critical perception capabilities of EdgeAI systems, such as autonomous vehicles, are required to be resilient against adversarial threats, by enabling accurate identification and localization of multiple objects in the scene over time, mitigating their impact. Single-agent tracking offers resilience to adversarial attacks but lacks situational awareness, underscoring the need for multi-agent cooperation to enhance context understanding and robustness. This paper proposes a novel mitigation framework on 3D LiDAR scene against adversarial noise by tracking objects based on least-squares graph on multi-agent adversarial bounding boxes. Specifically, we employ the least-squares graph tool to reduce the induced positional error of each detection's centroid utilizing overlapped bounding boxes on a fully connected graph via differential coordinates and anchor points. Hence, the multi-vehicle detections are fused and refined mitigating the adversarial impact, and associated with existing tracks in two stages performing tracking to further suppress the adversarial threat. An extensive evaluation study on the real-world V2V4Real dataset demonstrates that the proposed method significantly outperforms both state-of-the-art single and multi-agent tracking frameworks by up to 23.3% under challenging adversarial conditions, operating as a resilient approach without relying on additional defense mechanisms.
>
---
#### [new 178] Iterative Misclassification Error Training (IMET): An Optimized Neural Network Training Technique for Image Classification
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于医学图像分类任务，旨在解决数据噪声、标签错误和过拟合问题。提出IMET方法，通过迭代识别误分类样本提升模型鲁棒性和准确性。**

- **链接: [http://arxiv.org/pdf/2507.02979v1](http://arxiv.org/pdf/2507.02979v1)**

> **作者:** Ruhaan Singh; Sreelekha Guggilam
>
> **摘要:** Deep learning models have proven to be effective on medical datasets for accurate diagnostic predictions from images. However, medical datasets often contain noisy, mislabeled, or poorly generalizable images, particularly for edge cases and anomalous outcomes. Additionally, high quality datasets are often small in sample size that can result in overfitting, where models memorize noise rather than learn generalizable patterns. This in particular, could pose serious risks in medical diagnostics where the risk associated with mis-classification can impact human life. Several data-efficient training strategies have emerged to address these constraints. In particular, coreset selection identifies compact subsets of the most representative samples, enabling training that approximates full-dataset performance while reducing computational overhead. On the other hand, curriculum learning relies on gradually increasing training difficulty and accelerating convergence. However, developing a generalizable difficulty ranking mechanism that works across diverse domains, datasets, and models while reducing the computational tasks and remains challenging. In this paper, we introduce Iterative Misclassification Error Training (IMET), a novel framework inspired by curriculum learning and coreset selection. The IMET approach is aimed to identify misclassified samples in order to streamline the training process, while prioritizing the model's attention to edge case senarious and rare outcomes. The paper evaluates IMET's performance on benchmark medical image classification datasets against state-of-the-art ResNet architectures. The results demonstrating IMET's potential for enhancing model robustness and accuracy in medical image analysis are also presented in the paper.
>
---
#### [new 179] SeqGrowGraph: Learning Lane Topology as a Chain of Graph Expansions
- **分类: cs.CV**

- **简介: 该论文属于自动驾驶中的车道拓扑建模任务，解决传统方法难以处理复杂结构的问题。提出SeqGrowGraph框架，通过图扩展序列学习车道结构。**

- **链接: [http://arxiv.org/pdf/2507.04822v1](http://arxiv.org/pdf/2507.04822v1)**

> **作者:** Mengwei Xie; Shuang Zeng; Xinyuan Chang; Xinran Liu; Zheng Pan; Mu Xu; Xing Wei
>
> **摘要:** Accurate lane topology is essential for autonomous driving, yet traditional methods struggle to model the complex, non-linear structures-such as loops and bidirectional lanes-prevalent in real-world road structure. We present SeqGrowGraph, a novel framework that learns lane topology as a chain of graph expansions, inspired by human map-drawing processes. Representing the lane graph as a directed graph $G=(V,E)$, with intersections ($V$) and centerlines ($E$), SeqGrowGraph incrementally constructs this graph by introducing one vertex at a time. At each step, an adjacency matrix ($A$) expands from $n \times n$ to $(n+1) \times (n+1)$ to encode connectivity, while a geometric matrix ($M$) captures centerline shapes as quadratic B\'ezier curves. The graph is serialized into sequences, enabling a transformer model to autoregressively predict the chain of expansions, guided by a depth-first search ordering. Evaluated on nuScenes and Argoverse 2 datasets, SeqGrowGraph achieves state-of-the-art performance.
>
---
#### [new 180] DreamPoster: A Unified Framework for Image-Conditioned Generative Poster Design
- **分类: cs.CV**

- **简介: 该论文属于图像生成任务，旨在解决从图像和文本生成高质量海报的问题。工作包括构建数据集、改进模型架构及训练策略，提升生成质量与灵活性。**

- **链接: [http://arxiv.org/pdf/2507.04218v1](http://arxiv.org/pdf/2507.04218v1)**

> **作者:** Xiwei Hu; Haokun Chen; Zhongqi Qi; Hui Zhang; Dexiang Hong; Jie Shao; Xinglong Wu
>
> **摘要:** We present DreamPoster, a Text-to-Image generation framework that intelligently synthesizes high-quality posters from user-provided images and text prompts while maintaining content fidelity and supporting flexible resolution and layout outputs. Specifically, DreamPoster is built upon our T2I model, Seedream3.0 to uniformly process different poster generating types. For dataset construction, we propose a systematic data annotation pipeline that precisely annotates textual content and typographic hierarchy information within poster images, while employing comprehensive methodologies to construct paired datasets comprising source materials (e.g., raw graphics/text) and their corresponding final poster outputs. Additionally, we implement a progressive training strategy that enables the model to hierarchically acquire multi-task generation capabilities while maintaining high-quality generation. Evaluations on our testing benchmarks demonstrate DreamPoster's superiority over existing methods, achieving a high usability rate of 88.55\%, compared to GPT-4o (47.56\%) and SeedEdit3.0 (25.96\%). DreamPoster will be online in Jimeng and other Bytedance Apps.
>
---
#### [new 181] From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach
- **分类: cs.CV; cs.AI; cs.CL**

- **简介: 该论文属于视频描述任务，旨在解决生成复杂自然语言描述的问题。通过构建时空事件图，提出可解释的自监督方法，实现视觉与语言的深度融合。**

- **链接: [http://arxiv.org/pdf/2507.04815v1](http://arxiv.org/pdf/2507.04815v1)**

> **作者:** Mihai Masala; Marius Leordeanu
>
> **备注:** arXiv admin note: text overlap with arXiv:2501.08460
>
> **摘要:** The task of describing video content in natural language is commonly referred to as video captioning. Unlike conventional video captions, which are typically brief and widely available, long-form paragraph descriptions in natural language are scarce. This limitation of current datasets is due to the expensive human manual annotation required and to the highly challenging task of explaining the language formation process from the perspective of the underlying story, as a complex system of interconnected events in space and time. Through a thorough analysis of recently published methods and available datasets, we identify a general lack of published resources dedicated to the problem of describing videos in complex language, beyond the level of descriptions in the form of enumerations of simple captions. Furthermore, while state-of-the-art methods produce impressive results on the task of generating shorter captions from videos by direct end-to-end learning between the videos and text, the problem of explaining the relationship between vision and language is still beyond our reach. In this work, we propose a shared representation between vision and language, based on graphs of events in space and time, which can be obtained in an explainable and analytical way, to integrate and connect multiple vision tasks to produce the final natural language description. Moreover, we also demonstrate how our automated and explainable video description generation process can function as a fully automatic teacher to effectively train direct, end-to-end neural student pathways, within a self-supervised neuro-analytical system. We validate that our explainable neuro-analytical approach generates coherent, rich and relevant textual descriptions on videos collected from multiple varied datasets, using both standard evaluation metrics, human annotations and consensus from ensembles of state-of-the-art VLMs.
>
---
#### [new 182] LEHA-CVQAD: Dataset To Enable Generalized Video Quality Assessment of Compression Artifacts
- **分类: cs.CV**

- **简介: 该论文属于视频质量评估任务，旨在解决压缩伪影评估问题。提出LEHA-CVQAD数据集和RDAE指标，以提升VQA模型的准确性与实用性。**

- **链接: [http://arxiv.org/pdf/2507.03990v1](http://arxiv.org/pdf/2507.03990v1)**

> **作者:** Aleksandr Gushchin; Maksim Smirnov; Dmitriy Vatolin; Anastasia Antsiferova
>
> **摘要:** We propose the LEHA-CVQAD (Large-scale Enriched Human-Annotated) dataset, which comprises 6,240 clips for compression-oriented video quality assessment. 59 source videos are encoded with 186 codec-preset variants, 1.8M pairwise, and 1.5k MOS ratings are fused into a single quality scale; part of the videos remains hidden for blind evaluation. We also propose Rate-Distortion Alignment Error (RDAE), a novel evaluation metric that quantifies how well VQA models preserve bitrate-quality ordering, directly supporting codec parameter tuning. Testing IQA/VQA methods reveals that popular VQA metrics exhibit high RDAE and lower correlations, underscoring the dataset challenges and utility. The open part and the results of LEHA-CVQAD are available at https://aleksandrgushchin.github$.io/lcvqad/
>
---
#### [new 183] SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillation
- **分类: cs.CV**

- **简介: 该论文属于文本到3D生成任务，解决一致性模型中的自一致性与跨一致性不平衡问题，提出SegmentDreamer框架提升生成质量。**

- **链接: [http://arxiv.org/pdf/2507.05256v1](http://arxiv.org/pdf/2507.05256v1)**

> **作者:** Jiahao Zhu; Zixuan Chen; Guangcong Wang; Xiaohua Xie; Yi Zhou
>
> **备注:** Accepted by ICCV 2025, project page: https://zjhjojo.github.io/
>
> **摘要:** Recent advancements in text-to-3D generation improve the visual quality of Score Distillation Sampling (SDS) and its variants by directly connecting Consistency Distillation (CD) to score distillation. However, due to the imbalance between self-consistency and cross-consistency, these CD-based methods inherently suffer from improper conditional guidance, leading to sub-optimal generation results. To address this issue, we present SegmentDreamer, a novel framework designed to fully unleash the potential of consistency models for high-fidelity text-to-3D generation. Specifically, we reformulate SDS through the proposed Segmented Consistency Trajectory Distillation (SCTD), effectively mitigating the imbalance issues by explicitly defining the relationship between self- and cross-consistency. Moreover, SCTD partitions the Probability Flow Ordinary Differential Equation (PF-ODE) trajectory into multiple sub-trajectories and ensures consistency within each segment, which can theoretically provide a significantly tighter upper bound on distillation error. Additionally, we propose a distillation pipeline for a more swift and stable generation. Extensive experiments demonstrate that our SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).
>
---
#### [new 184] SFOOD: A Multimodal Benchmark for Comprehensive Food Attribute Analysis Beyond RGB with Spectral Insights
- **分类: cs.CV**

- **简介: 该论文属于食品属性分析任务，旨在解决现有研究仅关注分类而忽视其他属性的问题。通过构建首个大规模光谱食品基准SFOOD，融合光谱数据与实验测量，提升对食物多维度属性的准确感知。**

- **链接: [http://arxiv.org/pdf/2507.04412v1](http://arxiv.org/pdf/2507.04412v1)**

> **作者:** Zhenbo Xu; Jinghan Yang; Gong Huang; Jiqing Feng; Liu Liu; Ruihan Sun; Ajin Meng; Zhuo Zhang; Zhaofeng He
>
> **摘要:** With the rise and development of computer vision and LLMs, intelligence is everywhere, especially for people and cars. However, for tremendous food attributes (such as origin, quantity, weight, quality, sweetness, etc.), existing research still mainly focuses on the study of categories. The reason is the lack of a large and comprehensive benchmark for food. Besides, many food attributes (such as sweetness, weight, and fine-grained categories) are challenging to accurately percept solely through RGB cameras. To fulfill this gap and promote the development of intelligent food analysis, in this paper, we built the first large-scale spectral food (SFOOD) benchmark suite. We spent a lot of manpower and equipment costs to organize existing food datasets and collect hyperspectral images of hundreds of foods, and we used instruments to experimentally determine food attributes such as sweetness and weight. The resulting benchmark consists of 3,266 food categories and 2,351 k data points for 17 main food categories. Extensive evaluations find that: (i) Large-scale models are still poor at digitizing food. Compared to people and cars, food has gradually become one of the most difficult objects to study; (ii) Spectrum data are crucial for analyzing food properties (such as sweetness). Our benchmark will be open source and continuously iterated for different food analysis tasks.
>
---
#### [new 185] Dynamic Multimodal Prototype Learning in Vision-Language Models
- **分类: cs.CV**

- **简介: 该论文属于视觉-语言模型任务，解决文本原型不足导致的性能限制问题。提出ProtoMM框架，构建多模态原型并动态更新，提升模型泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.03657v1](http://arxiv.org/pdf/2507.03657v1)**

> **作者:** Xingyu Zhu; Shuo Wang; Beier Zhu; Miaoge Li; Yunfan Li; Junfeng Fang; Zhicai Wang; Dongsheng Wang; Hanwang Zhang
>
> **摘要:** With the increasing attention to pre-trained vision-language models (VLMs), \eg, CLIP, substantial efforts have been devoted to many downstream tasks, especially in test-time adaptation (TTA). However, previous works focus on learning prototypes only in the textual modality while overlooking the ambiguous semantics in class names. These ambiguities lead to textual prototypes that are insufficient to capture visual concepts, resulting in limited performance. To address this issue, we introduce \textbf{ProtoMM}, a training-free framework that constructs multimodal prototypes to adapt VLMs during the test time. By viewing the prototype as a discrete distribution over the textual descriptions and visual particles, ProtoMM has the ability to combine the multimodal features for comprehensive prototype learning. More importantly, the visual particles are dynamically updated as the testing stream flows. This allows our multimodal prototypes to continually learn from the data, enhancing their generalizability in unseen scenarios. In addition, we quantify the importance of the prototypes and test images by formulating their semantic distance as an optimal transport problem. Extensive experiments on 15 zero-shot benchmarks demonstrate the effectiveness of our method, achieving a 1.03\% average accuracy improvement over state-of-the-art methods on ImageNet and its variant datasets.
>
---
#### [new 186] Foundation versus Domain-specific Models: Performance Comparison, Fusion, and Explainability in Face Recognition
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于人脸识别任务，比较基础模型与领域模型的性能，探讨融合与可解释性，旨在提升识别准确性和透明度。**

- **链接: [http://arxiv.org/pdf/2507.03541v1](http://arxiv.org/pdf/2507.03541v1)**

> **作者:** Redwan Sony; Parisa Farmanifard; Arun Ross; Anil K. Jain
>
> **摘要:** In this paper, we address the following question: How do generic foundation models (e.g., CLIP, BLIP, LLaVa, DINO) compare against a domain-specific face recognition model (viz., AdaFace or ArcFace) on the face recognition task? Through a series of experiments involving several foundation models and benchmark datasets, we are able to report the following findings: (a) In all datasets considered, domain-specific models outperformed zero-shot foundation models. (b) The performance of zero-shot generic foundation models improves on over-segmented face images than tightly cropped faces thereby suggesting the importance of contextual clues. For example, at a False Match Rate (FMR) of 0.01%, the True Match Rate (TMR) of OpenCLIP improved from 64.97% to 81.73% on the LFW dataset as the face crop increased from 112x112 to 250x250 while the TMR of domain-specific AdaFace dropped from 99.09% to 77.31%. (c) A simple score-level fusion of a foundation model with a domain-specific FR model improved the accuracy at low FMRs. For example, the TMR of AdaFace when fused with BLIP improved from 72.64% to 83.31% at an FMR of 0.0001% on the IJB-B dataset and from 73.17% to 85.81% on the IJB-C dataset. (d) Foundation models, such as ChatGPT, can be used to impart explainability to the FR pipeline (e.g., ``Despite minor lighting and head tilt differences, the two left-profile images show high consistency in forehead slope, nose shape, chin contour...''). In some instances, foundation models are even able to resolve low-confidence decisions made by AdaFace (e.g., ``Although AdaFace assigns a low similarity score of 0.21, both images exhibit visual similarity...and the pair is likely of the same person''), thereby reiterating the importance of combining domain-specific FR models with generic foundation models in a judicious manner.
>
---
#### [new 187] MVNet: Hyperspectral Remote Sensing Image Classification Based on Hybrid Mamba-Transformer Vision Backbone Architecture
- **分类: cs.CV**

- **简介: 该论文属于高光谱图像分类任务，旨在解决数据高维、样本少和冗余导致的过拟合问题。提出MVNet网络，融合3D-CNN、Transformer和Mamba优势，提升分类精度与效率。**

- **链接: [http://arxiv.org/pdf/2507.04409v1](http://arxiv.org/pdf/2507.04409v1)**

> **作者:** Guandong Li; Mengxia Ye
>
> **备注:** arXiv admin note: substantial text overlap with arXiv:2506.08324, arXiv:2504.15155, arXiv:2504.13045, arXiv:2503.23472
>
> **摘要:** Hyperspectral image (HSI) classification faces challenges such as high-dimensional data, limited training samples, and spectral redundancy, which often lead to overfitting and insufficient generalization capability. This paper proposes a novel MVNet network architecture that integrates 3D-CNN's local feature extraction, Transformer's global modeling, and Mamba's linear complexity sequence modeling capabilities, achieving efficient spatial-spectral feature extraction and fusion. MVNet features a redesigned dual-branch Mamba module, including a State Space Model (SSM) branch and a non-SSM branch employing 1D convolution with SiLU activation, enhancing modeling of both short-range and long-range dependencies while reducing computational latency in traditional Mamba. The optimized HSI-MambaVision Mixer module overcomes the unidirectional limitation of causal convolution, capturing bidirectional spatial-spectral dependencies in a single forward pass through decoupled attention that focuses on high-value features, alleviating parameter redundancy and the curse of dimensionality. On IN, UP, and KSC datasets, MVNet outperforms mainstream hyperspectral image classification methods in both classification accuracy and computational efficiency, demonstrating robust capability in processing complex HSI data.
>
---
#### [new 188] Domain Generalizable Portrait Style Transfer
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于图像风格迁移任务，旨在实现跨域的高质量肖像风格迁移。通过建立语义对应和AdaIN-Wavelet变换，提升风格迁移的准确性和可控性。**

- **链接: [http://arxiv.org/pdf/2507.04243v1](http://arxiv.org/pdf/2507.04243v1)**

> **作者:** Xinbo Wang; Wenju Xu; Qing Zhang; Wei-Shi Zheng
>
> **备注:** Accepted to ICCV2025
>
> **摘要:** This paper presents a portrait style transfer method that generalizes well to various different domains while enabling high-quality semantic-aligned stylization on regions including hair, eyes, eyelashes, skins, lips, and background. To this end, we propose to establish dense semantic correspondence between the given input and reference portraits based on a pre-trained model and a semantic adapter, with which we obtain a warped reference semantically aligned with the input. To ensure effective yet controllable style transfer, we devise an AdaIN-Wavelet transform to balance content preservation and stylization by blending low-frequency information of the warped reference with high-frequency information of the input in the latent space. A style adapter is also designed to provide style guidance from the warped reference. With the stylized latent from AdaIN-Wavelet transform, we employ a dual-conditional diffusion model that integrates a ControlNet recording high-frequency information and the style guidance to generate the final result. Extensive experiments demonstrate the superiority of our method. Our code and trained model are available at https://github.com/wangxb29/DGPST.
>
---
#### [new 189] MambaVideo for Discrete Video Tokenization with Channel-Split Quantization
- **分类: cs.CV**

- **简介: 该论文属于视频生成任务，解决视频数据高维带来的效率问题。提出Mamba架构和通道分割量化方法，提升离散视频分词效果。**

- **链接: [http://arxiv.org/pdf/2507.04559v1](http://arxiv.org/pdf/2507.04559v1)**

> **作者:** Dawit Mureja Argaw; Xian Liu; Joon Son Chung; Ming-Yu Liu; Fitsum Reda
>
> **备注:** Project website: https://research.nvidia.com/labs/dir/mamba-tokenizer/
>
> **摘要:** Discrete video tokenization is essential for efficient autoregressive generative modeling due to the high dimensionality of video data. This work introduces a state-of-the-art discrete video tokenizer with two key contributions. First, we propose a novel Mamba-based encoder-decoder architecture that overcomes the limitations of previous sequencebased tokenizers. Second, we introduce a new quantization scheme, channel-split quantization, which significantly enhances the representational power of quantized latents while preserving the token count. Our model sets a new state-of-the-art, outperforming both causal 3D convolutionbased and Transformer-based approaches across multiple datasets. Experimental results further demonstrate its robustness as a tokenizer for autoregressive video generation.
>
---
#### [new 190] Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于人类运动预测任务，解决动作过渡不自然和动作特征难学习的问题。提出两个记忆库和自适应注意力策略，提升预测准确性。**

- **链接: [http://arxiv.org/pdf/2507.04062v1](http://arxiv.org/pdf/2507.04062v1)**

> **作者:** Jianwei Tang; Hong Yang; Tengyue Chen; Jian-Fang Hu
>
> **备注:** accepted by CVPR2025
>
> **摘要:** Action-driven stochastic human motion prediction aims to generate future motion sequences of a pre-defined target action based on given past observed sequences performing non-target actions. This task primarily presents two challenges. Firstly, generating smooth transition motions is hard due to the varying transition speeds of different actions. Secondly, the action characteristic is difficult to be learned because of the similarity of some actions. These issues cause the predicted results to be unreasonable and inconsistent. As a result, we propose two memory banks, the Soft-transition Action Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems above. The STAB stores the action transition information. It is equipped with the novel soft searching approach, which encourages the model to focus on multiple possible action categories of observed motions. The ACB records action characteristic, which produces more prior information for predicting certain actions. To fuse the features retrieved from the two banks better, we further propose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments on four motion prediction datasets demonstrate that our approach consistently outperforms the previous state-of-the-art. The demo and code are available at https://hyqlat.github.io/STABACB.github.io/.
>
---
#### [new 191] StreamDiT: Real-Time Streaming Text-to-Video Generation
- **分类: cs.CV; cs.AI; cs.LG; eess.IV**

- **简介: 该论文属于文本到视频生成任务，解决现有模型生成短片段、无法实时应用的问题。提出StreamDiT模型，实现高质量实时视频流生成。**

- **链接: [http://arxiv.org/pdf/2507.03745v1](http://arxiv.org/pdf/2507.03745v1)**

> **作者:** Akio Kodaira; Tingbo Hou; Ji Hou; Masayoshi Tomizuka; Yue Zhao
>
> **摘要:** Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: <a href="https://cumulo-autumn.github.io/StreamDiT/">this https URL.</a>
>
---
#### [new 192] CPKD: Clinical Prior Knowledge-Constrained Diffusion Models for Surgical Phase Recognition in Endoscopic Submucosal Dissection
- **分类: cs.CV**

- **简介: 该论文属于手术阶段识别任务，解决复杂内镜流程中阶段识别不准确的问题。提出CPKD模型，结合临床知识与扩散生成方法提升识别性能。**

- **链接: [http://arxiv.org/pdf/2507.03295v1](http://arxiv.org/pdf/2507.03295v1)**

> **作者:** Xiangning Zhang; Jinnan Chen; Qingwei Zhang; Yaqi Wang; Chengfeng Zhou; Xiaobo Li; Dahong Qian
>
> **摘要:** Gastrointestinal malignancies constitute a leading cause of cancer-related mortality worldwide, with advanced-stage prognosis remaining particularly dismal. Originating as a groundbreaking technique for early gastric cancer treatment, Endoscopic Submucosal Dissection has evolved into a versatile intervention for diverse gastrointestinal lesions. While computer-assisted systems significantly enhance procedural precision and safety in ESD, their clinical adoption faces a critical bottleneck: reliable surgical phase recognition within complex endoscopic workflows. Current state-of-the-art approaches predominantly rely on multi-stage refinement architectures that iteratively optimize temporal predictions. In this paper, we present Clinical Prior Knowledge-Constrained Diffusion (CPKD), a novel generative framework that reimagines phase recognition through denoising diffusion principles while preserving the core iterative refinement philosophy. This architecture progressively reconstructs phase sequences starting from random noise and conditioned on visual-temporal features. To better capture three domain-specific characteristics, including positional priors, boundary ambiguity, and relation dependency, we design a conditional masking strategy. Furthermore, we incorporate clinical prior knowledge into the model training to improve its ability to correct phase logical errors. Comprehensive evaluations on ESD820, Cholec80, and external multi-center demonstrate that our proposed CPKD achieves superior or comparable performance to state-of-the-art approaches, validating the effectiveness of diffusion-based generative paradigms for surgical phase recognition.
>
---
#### [new 193] UGG-ReID: Uncertainty-Guided Graph Model for Multi-Modal Object Re-Identification
- **分类: cs.CV**

- **简介: 该论文属于多模态目标重识别任务，旨在解决因模态噪声和冲突导致的不确定性问题。通过引入不确定性引导的图模型，提升模型鲁棒性与多模态融合效果。**

- **链接: [http://arxiv.org/pdf/2507.04638v1](http://arxiv.org/pdf/2507.04638v1)**

> **作者:** Xixi Wan; Aihua Zheng; Bo Jiang; Beibei Wang; Chenglong Li; Jin Tang
>
> **摘要:** Multi-modal object Re-IDentification (ReID) has gained considerable attention with the goal of retrieving specific targets across cameras using heterogeneous visual data sources. Existing methods primarily aim to improve identification performance, but often overlook the uncertainty arising from inherent defects, such as intra-modal noise and inter-modal conflicts. This uncertainty is particularly significant in the case of fine-grained local occlusion and frame loss, which becomes a challenge in multi-modal learning. To address the above challenge, we propose a robust approach named Uncertainty-Guided Graph model for multi-modal object ReID (UGG-ReID). UGG-ReID is designed to mitigate noise interference and facilitate effective multi-modal fusion by estimating both local and sample-level aleatoric uncertainty and explicitly modeling their dependencies. Specifically, we first propose the Gaussian patch-graph representation model that leverages uncertainty to quantify fine-grained local cues and capture their structural relationships. This process boosts the expressiveness of modal-specific information, ensuring that the generated embeddings are both more informative and robust. Subsequently, we design an uncertainty-guided mixture of experts strategy that dynamically routes samples to experts exhibiting low uncertainty. This strategy effectively suppresses noise-induced instability, leading to enhanced robustness. Meanwhile, we design an uncertainty-guided routing to strengthen the multi-modal interaction, improving the performance. UGG-ReID is comprehensively evaluated on five representative multi-modal object ReID datasets, encompassing diverse spectral modalities. Experimental results show that the proposed method achieves excellent performance on all datasets and is significantly better than current methods in terms of noise immunity. Our code will be made public upon acceptance.
>
---
#### [new 194] Generate, Refine, and Encode: Leveraging Synthesized Novel Samples for On-the-Fly Fine-Grained Category Discovery
- **分类: cs.CV**

- **简介: 该论文研究在线细粒度类别发现任务，解决已知与未知类别识别问题。通过生成、精炼和编码合成样本，提升模型性能。**

- **链接: [http://arxiv.org/pdf/2507.04051v1](http://arxiv.org/pdf/2507.04051v1)**

> **作者:** Xiao Liu; Nan Pu; Haiyang Zheng; Wenjing Li; Nicu Sebe; Zhun Zhong
>
> **备注:** ICCV 2025
>
> **摘要:** In this paper, we investigate a practical yet challenging task: On-the-fly Category Discovery (OCD). This task focuses on the online identification of newly arriving stream data that may belong to both known and unknown categories, utilizing the category knowledge from only labeled data. Existing OCD methods are devoted to fully mining transferable knowledge from only labeled data. However, the transferability learned by these methods is limited because the knowledge contained in known categories is often insufficient, especially when few annotated data/categories are available in fine-grained recognition. To mitigate this limitation, we propose a diffusion-based OCD framework, dubbed DiffGRE, which integrates Generation, Refinement, and Encoding in a multi-stage fashion. Specifically, we first design an attribute-composition generation method based on cross-image interpolation in the diffusion latent space to synthesize novel samples. Then, we propose a diversity-driven refinement approach to select the synthesized images that differ from known categories for subsequent OCD model training. Finally, we leverage a semi-supervised leader encoding to inject additional category knowledge contained in synthesized data into the OCD models, which can benefit the discovery of both known and unknown categories during the on-the-fly inference process. Extensive experiments demonstrate the superiority of our DiffGRE over previous methods on six fine-grained datasets.
>
---
#### [new 195] VectorLLM: Human-like Extraction of Structured Building Contours vis Multimodal LLMs
- **分类: cs.CV**

- **简介: 该论文属于遥感图像中建筑物轮廓提取任务，旨在解决传统方法复杂且泛化能力差的问题。提出VectorLLM模型，通过多模态大语言模型直接回归角点，提升准确性和通用性。**

- **链接: [http://arxiv.org/pdf/2507.04664v1](http://arxiv.org/pdf/2507.04664v1)**

> **作者:** Tao Zhang; Shiqing Wei; Shihao Chen; Wenling Yu; Muying Luo; Shunping Ji
>
> **摘要:** Automatically extracting vectorized building contours from remote sensing imagery is crucial for urban planning, population estimation, and disaster assessment. Current state-of-the-art methods rely on complex multi-stage pipelines involving pixel segmentation, vectorization, and polygon refinement, which limits their scalability and real-world applicability. Inspired by the remarkable reasoning capabilities of Large Language Models (LLMs), we introduce VectorLLM, the first Multi-modal Large Language Model (MLLM) designed for regular building contour extraction from remote sensing images. Unlike existing approaches, VectorLLM performs corner-point by corner-point regression of building contours directly, mimicking human annotators' labeling process. Our architecture consists of a vision foundation backbone, an MLP connector, and an LLM, enhanced with learnable position embeddings to improve spatial understanding capability. Through comprehensive exploration of training strategies including pretraining, supervised fine-tuning, and preference optimization across WHU, WHU-Mix, and CrowdAI datasets, VectorLLM significantly outperformed the previous SOTA methods by 5.6 AP, 7.1 AP, 13.6 AP, respectively in the three datasets. Remarkably, VectorLLM exhibits strong zero-shot performance on unseen objects including aircraft, water bodies, and oil tanks, highlighting its potential for unified modeling of diverse remote sensing object contour extraction tasks. Overall, this work establishes a new paradigm for vector extraction in remote sensing, leveraging the topological reasoning capabilities of LLMs to achieve both high accuracy and exceptional generalization. All the codes and weights will be published for promoting community development.
>
---
#### [new 196] OBSER: Object-Based Sub-Environment Recognition for Zero-Shot Environmental Inference
- **分类: cs.CV; cs.AI; cs.LG; stat.ML**

- **简介: 该论文属于环境识别任务，解决零样本环境推理问题。提出OBSER框架，通过贝叶斯方法分析对象与子环境的关系，提升开放世界中的环境理解能力。**

- **链接: [http://arxiv.org/pdf/2507.02929v1](http://arxiv.org/pdf/2507.02929v1)**

> **作者:** Won-Seok Choi; Dong-Sig Han; Suhyung Choi; Hyeonseo Yang; Byoung-Tak Zhang
>
> **备注:** This manuscript was initially submitted to ICCV 2025 and is now made available as a preprint
>
> **摘要:** We present the Object-Based Sub-Environment Recognition (OBSER) framework, a novel Bayesian framework that infers three fundamental relationships between sub-environments and their constituent objects. In the OBSER framework, metric and self-supervised learning models estimate the object distributions of sub-environments on the latent space to compute these measures. Both theoretically and empirically, we validate the proposed framework by introducing the ($\epsilon,\delta$) statistically separable (EDS) function which indicates the alignment of the representation. Our framework reliably performs inference in open-world and photorealistic environments and outperforms scene-based methods in chained retrieval tasks. The OBSER framework enables zero-shot recognition of environments to achieve autonomous environment understanding.
>
---
#### [new 197] Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于多模态语言模型研究，解决多视觉编码器冗余问题。通过分析与实验，提出度量方法评估编码器贡献，揭示冗余现象。**

- **链接: [http://arxiv.org/pdf/2507.03262v1](http://arxiv.org/pdf/2507.03262v1)**

> **作者:** Song Mao; Yang Chen; Pinglong Cai; Ding Wang; Guohang Yan; Zhi Yu; Botian Shi
>
> **备注:** Wrok in Process
>
> **摘要:** Multimodal Large Language Models (MLLMs) increasingly adopt multiple vision encoders to capture diverse visual information, ranging from coarse semantics to fine grained details. While this approach is intended to enhance visual understanding capability, we observe that the performance gains from adding encoders often diminish and can even lead to performance degradation, a phenomenon we term encoder redundancy. This paper presents a systematic investigation into this issue. Through comprehensive ablation studies on state of the art multi encoder MLLMs, we empirically demonstrate that significant redundancy exists. To quantify each encoder's unique contribution, we propose a principled metric: the Conditional Utilization Rate (CUR). Building on CUR, we introduce the Information Gap (IG) to capture the overall disparity in encoder utility within a model.Our experiments reveal that certain vision encoders contribute little, or even negatively, to overall performance, confirming substantial redundancy. Our experiments reveal that certain vision encoders contribute minimally, or even negatively, to the model's performance, confirming the prevalence of redundancy. These findings highlight critical inefficiencies in current multi encoder designs and establish that our proposed metrics can serve as valuable diagnostic tools for developing more efficient and effective multimodal architectures.
>
---
#### [new 198] NRSeg: Noise-Resilient Learning for BEV Semantic Segmentation via Driving World Models
- **分类: cs.CV; cs.RO; eess.IV**

- **简介: 该论文属于BEV语义分割任务，旨在解决合成数据噪声影响模型性能的问题。提出NRSeg框架，结合PGCM、BiDPP和HLSE模块，提升分割鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.04002v1](http://arxiv.org/pdf/2507.04002v1)**

> **作者:** Siyu Li; Fei Teng; Yihong Cao; Kailun Yang; Zhiyong Li; Yaonan Wang
>
> **备注:** The source code will be made publicly available at https://github.com/lynn-yu/NRSeg
>
> **摘要:** Birds' Eye View (BEV) semantic segmentation is an indispensable perception task in end-to-end autonomous driving systems. Unsupervised and semi-supervised learning for BEV tasks, as pivotal for real-world applications, underperform due to the homogeneous distribution of the labeled data. In this work, we explore the potential of synthetic data from driving world models to enhance the diversity of labeled data for robustifying BEV segmentation. Yet, our preliminary findings reveal that generation noise in synthetic data compromises efficient BEV model learning. To fully harness the potential of synthetic data from world models, this paper proposes NRSeg, a noise-resilient learning framework for BEV semantic segmentation. Specifically, a Perspective-Geometry Consistency Metric (PGCM) is proposed to quantitatively evaluate the guidance capability of generated data for model learning. This metric originates from the alignment measure between the perspective road mask of generated data and the mask projected from the BEV labels. Moreover, a Bi-Distribution Parallel Prediction (BiDPP) is designed to enhance the inherent robustness of the model, where the learning process is constrained through parallel prediction of multinomial and Dirichlet distributions. The former efficiently predicts semantic probabilities, whereas the latter adopts evidential deep learning to realize uncertainty quantification. Furthermore, a Hierarchical Local Semantic Exclusion (HLSE) module is designed to address the non-mutual exclusivity inherent in BEV semantic segmentation tasks. Experimental results demonstrate that NRSeg achieves state-of-the-art performance, yielding the highest improvements in mIoU of 13.8% and 11.4% in unsupervised and semi-supervised BEV segmentation tasks, respectively. The source code will be made publicly available at https://github.com/lynn-yu/NRSeg.
>
---
#### [new 199] VR-YOLO: Enhancing PCB Defect Detection with Viewpoint Robustness Based on YOLO
- **分类: cs.CV; eess.IV**

- **简介: 该论文属于PCB缺陷检测任务，旨在提升模型在不同视角下的鲁棒性。通过改进YOLOv8，提出DSE和KOF方法，显著提高检测精度。**

- **链接: [http://arxiv.org/pdf/2507.02963v1](http://arxiv.org/pdf/2507.02963v1)**

> **作者:** Hengyi Zhu; Linye Wei; He Li
>
> **摘要:** The integration of large-scale circuits and systems emphasizes the importance of automated defect detection of electronic components. The YOLO image detection model has been used to detect PCB defects and it has become a typical AI-assisted case of traditional industrial production. However, conventional detection algorithms have stringent requirements for the angle, orientation, and clarity of target images. In this paper, we propose an enhanced PCB defect detection algorithm, named VR-YOLO, based on the YOLOv8 model. This algorithm aims to improve the model's generalization performance and enhance viewpoint robustness in practical application scenarios. We first propose a diversified scene enhancement (DSE) method by expanding the PCB defect dataset by incorporating diverse scenarios and segmenting samples to improve target diversity. A novel key object focus (KOF) scheme is then presented by considering angular loss and introducing an additional attention mechanism to enhance fine-grained learning of small target features. Experimental results demonstrate that our improved PCB defect detection approach achieves a mean average precision (mAP) of 98.9% for the original test images, and 94.7% for the test images with viewpoint shifts (horizontal and vertical shear coefficients of $\pm 0.06$ and rotation angle of $\pm 10$ degrees), showing significant improvements compared to the baseline YOLO model with negligible additional computational cost.
>
---
#### [new 200] Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps
- **分类: cs.CV**

- **简介: 该论文属于SLAM任务，解决户外场景中3DGS方法的尺度漂移和几何先验缺失问题，提出S3PO-GS方法提升跟踪精度与重建质量。**

- **链接: [http://arxiv.org/pdf/2507.03737v1](http://arxiv.org/pdf/2507.03737v1)**

> **作者:** Chong Cheng; Sicheng Yu; Zijian Wang; Yifan Zhou; Hao Wang
>
> **备注:** Accepted by ICCV2025
>
> **摘要:** 3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its high-fidelity and real-time novel view synthesis performance. However, some previous 3DGS SLAM methods employ a differentiable rendering pipeline for tracking, \textbf{lack geometric priors} in outdoor scenes. Other approaches introduce separate tracking modules, but they accumulate errors with significant camera movement, leading to \textbf{scale drift}. To address these challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a self-consistent tracking module anchored in the 3DGS pointmap, which avoids cumulative scale drift and achieves more precise and robust tracking with fewer iterations. Additionally, we design a patch-based pointmap dynamic mapping module, which introduces geometric priors while avoiding scale ambiguity. This significantly enhances tracking accuracy and the quality of scene reconstruction, making it particularly suitable for complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project page: https://3dagentworld.github.io/S3PO-GS/.
>
---
#### [new 201] Radar Tracker: Moving Instance Tracking in Sparse and Noisy Radar Point Clouds
- **分类: cs.CV**

- **简介: 该论文属于移动目标跟踪任务，解决稀疏噪声雷达点云中的实例跟踪问题。通过结合时序偏移预测和注意力机制，提升跟踪准确性。**

- **链接: [http://arxiv.org/pdf/2507.03441v1](http://arxiv.org/pdf/2507.03441v1)**

> **作者:** Matthias Zeller; Daniel Casado Herraez; Jens Behley; Michael Heidingsfeld; Cyrill Stachniss
>
> **备注:** Proc. of the IEEE Intl. Conf. on Robotics & Automation (ICRA)
>
> **摘要:** Robots and autonomous vehicles should be aware of what happens in their surroundings. The segmentation and tracking of moving objects are essential for reliable path planning, including collision avoidance. We investigate this estimation task for vehicles using radar sensing. We address moving instance tracking in sparse radar point clouds to enhance scene interpretation. We propose a learning-based radar tracker incorporating temporal offset predictions to enable direct center-based association and enhance segmentation performance by including additional motion cues. We implement attention-based tracking for sparse radar scans to include appearance features and enhance performance. The final association combines geometric and appearance features to overcome the limitations of center-based tracking to associate instances reliably. Our approach shows an improved performance on the moving instance tracking benchmark of the RadarScenes dataset compared to the current state of the art.
>
---
#### [new 202] ReLoop: "Seeing Twice and Thinking Backwards" via Closed-loop Training to Mitigate Hallucinations in Multimodal understanding
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于多模态理解任务，旨在解决MLLMs中的幻觉问题。通过提出ReLoop框架，利用闭环训练增强跨模态一致性，提升模型输出的准确性和可靠性。**

- **链接: [http://arxiv.org/pdf/2507.04943v1](http://arxiv.org/pdf/2507.04943v1)**

> **作者:** Jianjiang Yang; Ziyan Huang; Yanshu Li
>
> **备注:** 8 pages,6 figures,5 tables
>
> **摘要:** While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in open-ended visual question answering, they remain vulnerable to hallucinations. These are outputs that contradict or misrepresent input semantics, posing a critical challenge to the reliability and factual consistency. Existing methods often rely on external verification or post-hoc correction, lacking an internal mechanism to validate outputs directly during training. To bridge this gap, we propose ReLoop, a unified closed-loop training framework that encourages multimodal consistency for cross-modal understanding in MLLMs. ReLoop adopts a ring-shaped structure that integrates three complementary consistency feedback mechanisms, obliging MLLMs to "seeing twice and thinking backwards". Specifically, ReLoop employs the frozen Consistency Feedback Plugin (CFP), comprising semantic reconstruction, visual description, and an attention supervision module for attention alignment. These components collectively enforce semantic reversibility, visual consistency, and interpretable attention, enabling the model to correct its outputs during training. Extensive evaluations and analyses demonstrate the effectiveness of ReLoop in reducing hallucination rates across multiple benchmarks, establishing a robust method for hallucination mitigation in MLLMs. We will release our source code and data in the camera-ready version.
>
---
#### [new 203] VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting
- **分类: cs.CV; cs.RO**

- **简介: 该论文属于机器人操作任务，解决VLA模型在新物体或环境中的泛化问题。提出VOTE框架，通过高效动作预测和集成投票策略提升性能与速度。**

- **链接: [http://arxiv.org/pdf/2507.05116v1](http://arxiv.org/pdf/2507.05116v1)**

> **作者:** Juyi Lin; Amir Taherin; Arash Akbari; Arman Akbari; Lei Lu; Guangyu Chen; Taskin Padir; Xiaomeng Yang; Weiwei Chen; Yiqian Li; Xue Lin; David Kaeli; Pu Zhao; Yanzhi Wang
>
> **摘要:** Recent large-scale Vision Language Action (VLA) models have shown superior performance in robotic manipulation tasks guided by natural language. However, their generalization remains limited when applied to novel objects or unfamiliar environments that lie outside the training distribution. To address this, many existing approaches integrate additional components such as depth estimation, segmentation, or even diffusion to improve generalization, at the cost of adding significant computation overhead, resulting in low efficiency. This motivates the exploration of efficient action prediction methods, which are independent of additional high-level visual representations or diffusion techniques. In this work, we propose VOTE, an efficient and general framework for the optimization and acceleration of VLA models. In details, we propose a novel tokenizer-free fine-tuning approach for parallel accurate action prediction, which reduces computational overhead and accelerates inference speed. Additionally, we adopt an ensemble voting strategy for the action sampling, which significantly improves model performance and enhances generalization. Experimental results show that our method achieves state-of-the-art performance with 35$\times$ faster inference and 145 Hz throughput. All the details and codes will be open-sourced.
>
---
#### [new 204] Temporal Continual Learning with Prior Compensation for Human Motion Prediction
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于人体运动预测任务，解决短时预测受长时预测影响及先验信息利用不足的问题。提出TCL框架与PCF机制，提升预测效果。**

- **链接: [http://arxiv.org/pdf/2507.04060v1](http://arxiv.org/pdf/2507.04060v1)**

> **作者:** Jianwei Tang; Jiangxin Sun; Xiaotong Lin; Lifang Zhang; Wei-Shi Zheng; Jian-Fang Hu
>
> **备注:** Advances in Neural Information Processing Systems 2023
>
> **摘要:** Human Motion Prediction (HMP) aims to predict future poses at different moments according to past motion sequences. Previous approaches have treated the prediction of various moments equally, resulting in two main limitations: the learning of short-term predictions is hindered by the focus on long-term predictions, and the incorporation of prior information from past predictions into subsequent predictions is limited. In this paper, we introduce a novel multi-stage training framework called Temporal Continual Learning (TCL) to address the above challenges. To better preserve prior information, we introduce the Prior Compensation Factor (PCF). We incorporate it into the model training to compensate for the lost prior information. Furthermore, we derive a more reasonable optimization objective through theoretical derivation. It is important to note that our TCL framework can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on four HMP benchmark datasets demonstrate the effectiveness and flexibility of TCL. The code is available at https://github.com/hyqlat/TCL.
>
---
#### [new 205] CoT-lized Diffusion: Let's Reinforce T2I Generation Step-by-step
- **分类: cs.CV**

- **简介: 该论文属于文本生成图像任务，解决复杂场景空间对齐问题。通过整合多模态大模型与扩散过程，实现逐步推理与布局优化，提升生成精度。**

- **链接: [http://arxiv.org/pdf/2507.04451v1](http://arxiv.org/pdf/2507.04451v1)**

> **作者:** Zheyuan Liu; Munan Ning; Qihui Zhang; Shuo Yang; Zhongrui Wang; Yiwei Yang; Xianzhe Xu; Yibing Song; Weihua Chen; Fan Wang; Li Yuan
>
> **摘要:** Current text-to-image (T2I) generation models struggle to align spatial composition with the input text, especially in complex scenes. Even layout-based approaches yield suboptimal spatial control, as their generation process is decoupled from layout planning, making it difficult to refine the layout during synthesis. We present CoT-Diff, a framework that brings step-by-step CoT-style reasoning into T2I generation by tightly integrating Multimodal Large Language Model (MLLM)-driven 3D layout planning with the diffusion process. CoT-Diff enables layout-aware reasoning inline within a single diffusion round: at each denoising step, the MLLM evaluates intermediate predictions, dynamically updates the 3D scene layout, and continuously guides the generation process. The updated layout is converted into semantic conditions and depth maps, which are fused into the diffusion model via a condition-aware attention mechanism, enabling precise spatial control and semantic injection. Experiments on 3D Scene benchmarks show that CoT-Diff significantly improves spatial alignment and compositional fidelity, and outperforms the state-of-the-art method by 34.7% in complex scene spatial accuracy, thereby validating the effectiveness of this entangled generation paradigm.
>
---
#### [new 206] Ascending the Infinite Ladder: Benchmarking Spatial Deformation Reasoning in Vision-Language Models
- **分类: cs.CV**

- **简介: 该论文属于视觉语言模型的空间推理任务，旨在评估模型在空间变形推理上的能力。通过构建2D到3D的基准测试，发现现有模型在此任务上表现不佳。**

- **链接: [http://arxiv.org/pdf/2507.02978v1](http://arxiv.org/pdf/2507.02978v1)**

> **作者:** Jiahuan Zhang; Shunwen Bai; Tianheng Wang; Kaiwen Guo; Kai Han; Guozheng Rao; Kaicheng Yu
>
> **摘要:** Humans naturally possess the spatial reasoning ability to form and manipulate images and structures of objects in space. There is an increasing effort to endow Vision-Language Models (VLMs) with similar spatial reasoning capabilities. However, it remains unclear whether these models truly understand and manipulate spatial objects or not. To address this question, we propose a new evaluation framework aimed at assessing the performance of VLMs in spatial deformation reasoning tasks. Specifically, we construct a benchmark for spatial deformation reasoning from 2D to 3D. Leveraging our data engine, we can generate unlimited evaluation problem pairs with infinite steps, without any data leakage. We explore whether the model can effectively perform spatial deformation reasoning from two directions: forward reasoning (given the operations, find the final state) and reverse reasoning (given the final state, determine the operations). We adopt a ladder competition format, using the number of deformation steps as the level classification criterion, with the goal of exploring the boundaries of the model's deformation reasoning capabilities. Interestingly, the benchmarking results reveal that almost no model demonstrates plausible spatial deformation reasoning abilities. Furthermore, even after applying targeted training and mainstream reasoning enhancement methods, the models are still unable to perform well on 3D spatial deformation reasoning.
>
---
#### [new 207] Mirror in the Model: Ad Banner Image Generation via Reflective Multi-LLM and Multi-modal Agents
- **分类: cs.CV**

- **简介: 该论文属于广告设计任务，解决自动生成高质量广告横幅的问题。提出MIMO框架，结合多模态代理和协调循环，提升设计质量与一致性。**

- **链接: [http://arxiv.org/pdf/2507.03326v1](http://arxiv.org/pdf/2507.03326v1)**

> **作者:** Zhao Wang; Bowen Chen; Yotaro Shimose; Sota Moriyama; Heng Wang; Shingo Takamatsu
>
> **摘要:** Recent generative models such as GPT-4o have shown strong capabilities in producing high-quality images with accurate text rendering. However, commercial design tasks like advertising banners demand more than visual fidelity -- they require structured layouts, precise typography, consistent branding, and more. In this paper, we introduce MIMO (Mirror In-the-Model), an agentic refinement framework for automatic ad banner generation. MIMO combines a hierarchical multi-modal agent system (MIMO-Core) with a coordination loop (MIMO-Loop) that explores multiple stylistic directions and iteratively improves design quality. Requiring only a simple natural language based prompt and logo image as input, MIMO automatically detects and corrects multiple types of errors during generation. Experiments show that MIMO significantly outperforms existing diffusion and LLM-based baselines in real-world banner design scenarios.
>
---
#### [new 208] TeethGenerator: A two-stage framework for paired pre- and post-orthodontic 3D dental data generation
- **分类: cs.CV**

- **简介: 该论文属于医学图像生成任务，旨在解决3D牙齿模型数据不足的问题。提出TeethGenerator框架，生成配对的正畸前后牙齿模型，提升牙齿排列网络训练效果。**

- **链接: [http://arxiv.org/pdf/2507.04685v1](http://arxiv.org/pdf/2507.04685v1)**

> **作者:** Changsong Lei; Yaqian Liang; Shaofeng Wang; Jiajia Dai; Yong-Jin Liu
>
> **备注:** Accepted by ICCV 2025
>
> **摘要:** Digital orthodontics represents a prominent and critical application of computer vision technology in the medical field. So far, the labor-intensive process of collecting clinical data, particularly in acquiring paired 3D orthodontic teeth models, constitutes a crucial bottleneck for developing tooth arrangement neural networks. Although numerous general 3D shape generation methods have been proposed, most of them focus on single-object generation and are insufficient for generating anatomically structured teeth models, each comprising 24-32 segmented teeth. In this paper, we propose TeethGenerator, a novel two-stage framework designed to synthesize paired 3D teeth models pre- and post-orthodontic, aiming to facilitate the training of downstream tooth arrangement networks. Specifically, our approach consists of two key modules: (1) a teeth shape generation module that leverages a diffusion model to learn the distribution of morphological characteristics of teeth, enabling the generation of diverse post-orthodontic teeth models; and (2) a teeth style generation module that synthesizes corresponding pre-orthodontic teeth models by incorporating desired styles as conditional inputs. Extensive qualitative and quantitative experiments demonstrate that our synthetic dataset aligns closely with the distribution of real orthodontic data, and promotes tooth alignment performance significantly when combined with real data for training. The code and dataset are available at https://github.com/lcshhh/teeth_generator.
>
---
#### [new 209] Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation Model
- **分类: cs.CV; cs.AI; cs.LG**

- **简介: 该论文属于口腔影像分析任务，旨在解决CBCT中牙科标志点检测的少样本问题。通过引入GeoSapiens框架提升检测精度。**

- **链接: [http://arxiv.org/pdf/2507.04710v1](http://arxiv.org/pdf/2507.04710v1)**

> **作者:** Anbang Wang; Marawan Elbatel; Keyuan Liu; Lizhuo Lin; Meng Lan; Yanqi Yang; Xiaomeng Li
>
> **备注:** MICCAI 2025
>
> **摘要:** Accurate detection of anatomic landmarks is essential for assessing alveolar bone and root conditions, thereby optimizing clinical outcomes in orthodontics, periodontics, and implant dentistry. Manual annotation of landmarks on cone-beam computed tomography (CBCT) by dentists is time-consuming, labor-intensive, and subject to inter-observer variability. Deep learning-based automated methods present a promising approach to streamline this process efficiently. However, the scarcity of training data and the high cost of expert annotations hinder the adoption of conventional deep learning techniques. To overcome these challenges, we introduce GeoSapiens, a novel few-shot learning framework designed for robust dental landmark detection using limited annotated CBCT of anterior teeth. Our GeoSapiens framework comprises two key components: (1) a robust baseline adapted from Sapiens, a foundational model that has achieved state-of-the-art performance in human-centric vision tasks, and (2) a novel geometric loss function that improves the model's capacity to capture critical geometric relationships among anatomical structures. Experiments conducted on our collected dataset of anterior teeth landmarks revealed that GeoSapiens surpassed existing landmark detection methods, outperforming the leading approach by an 8.18% higher success detection rate at a strict 0.5 mm threshold-a standard widely recognized in dental diagnostics. Code is available at: https://github.com/xmed-lab/GeoSapiens.
>
---
#### [new 210] NOVO: Unlearning-Compliant Vision Transformers
- **分类: cs.CV**

- **简介: 该论文属于机器学习中的遗忘任务，旨在解决模型在不损失性能的情况下删除特定数据的影响。工作是提出一种无需微调的视觉Transformer架构，通过训练过程模拟遗忘。**

- **链接: [http://arxiv.org/pdf/2507.03281v1](http://arxiv.org/pdf/2507.03281v1)**

> **作者:** Soumya Roy; Soumya Banerjee; Vinay Verma; Soumik Dasgupta; Deepak Gupta; Piyush Rai
>
> **摘要:** Machine unlearning (MUL) refers to the problem of making a pre-trained model selectively forget some training instances or class(es) while retaining performance on the remaining dataset. Existing MUL research involves fine-tuning using a forget and/or retain set, making it expensive and/or impractical, and often causing performance degradation in the unlearned model. We introduce {\pname}, an unlearning-aware vision transformer-based architecture that can directly perform unlearning for future unlearning requests without any fine-tuning over the requested set. The proposed model is trained by simulating unlearning during the training process itself. It involves randomly separating class(es)/sub-class(es) present in each mini-batch into two disjoint sets: a proxy forget-set and a retain-set, and the model is optimized so that it is unable to predict the forget-set. Forgetting is achieved by withdrawing keys, making unlearning on-the-fly and avoiding performance degradation. The model is trained jointly with learnable keys and original weights, ensuring withholding a key irreversibly erases information, validated by membership inference attack scores. Extensive experiments on various datasets, architectures, and resolutions confirm {\pname}'s superiority over both fine-tuning-free and fine-tuning-based methods.
>
---
#### [new 211] An Advanced Deep Learning Framework for Ischemic and Hemorrhagic Brain Stroke Diagnosis Using Computed Tomography (CT) Images
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于脑卒中分类任务，旨在提高CT图像中缺血性和出血性脑卒中的诊断准确性。通过结合预训练模型与优化算法，实现高效精准的早期诊断。**

- **链接: [http://arxiv.org/pdf/2507.03558v1](http://arxiv.org/pdf/2507.03558v1)**

> **作者:** Md. Sabbir Hossen; Eshat Ahmed Shuvo; Shibbir Ahmed Arif; Pabon Shaha; Md. Saiduzzaman; Mostofa Kamal Nasir
>
> **备注:** Preprint version. Submitted for peer review
>
> **摘要:** Brain stroke is one of the leading causes of mortality and long-term disability worldwide, highlighting the need for precise and fast prediction techniques. Computed Tomography (CT) scan is considered one of the most effective methods for diagnosing brain strokes. The majority of stroke classification techniques rely on a single slice-level prediction mechanism, allowing the radiologist to manually choose the most critical CT slice from the original CT volume. Although clinical evaluations are often used in traditional diagnostic procedures, machine learning (ML) has opened up new avenues for improving stroke diagnosis. To supplement traditional diagnostic techniques, this study investigates the use of machine learning models, specifically concerning the prediction of brain stroke at an early stage utilizing CT scan images. In this research, we proposed a novel approach to brain stroke detection leveraging machine learning techniques, focusing on optimizing classification performance with pre-trained deep learning models and advanced optimization strategies. Pre-trained models, including DenseNet201, InceptionV3, MobileNetV2, ResNet50, and Xception, are utilized for feature extraction. Additionally, we employed feature engineering techniques, including BFO, PCA, and LDA, to enhance models' performance further. These features are subsequently classified using machine learning algorithms such as SVC, RF, XGB, DT, LR, KNN, and GNB. Our experiments demonstrate that the combination of MobileNetV2, LDA, and SVC achieved the highest classification accuracy of 97.93%, significantly outperforming other model-optimizer-classifier combinations. The results underline the effectiveness of integrating lightweight pre-trained models with robust optimization and classification techniques for brain stroke diagnosis.
>
---
#### [new 212] Farm-Level, In-Season Crop Identification for India
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于农业监测任务，旨在解决印度境内实时、精准的作物识别问题。通过深度学习结合卫星数据，实现全国范围内的农田级多作物识别与季节性分析。**

- **链接: [http://arxiv.org/pdf/2507.02972v1](http://arxiv.org/pdf/2507.02972v1)**

> **作者:** Ishan Deshpande; Amandeep Kaur Reehal; Chandan Nath; Renu Singh; Aayush Patel; Aishwarya Jayagopal; Gaurav Singh; Gaurav Aggarwal; Amit Agarwal; Prathmesh Bele; Sridhar Reddy; Tanya Warrier; Kinjal Singh; Ashish Tendulkar; Luis Pazos Outon; Nikita Saxena; Agata Dondzik; Dinesh Tewari; Shruti Garg; Avneet Singh; Harsh Dhand; Vaibhav Rajan; Alok Talekar
>
> **摘要:** Accurate, timely, and farm-level crop type information is paramount for national food security, agricultural policy formulation, and economic planning, particularly in agriculturally significant nations like India. While remote sensing and machine learning have become vital tools for crop monitoring, existing approaches often grapple with challenges such as limited geographical scalability, restricted crop type coverage, the complexities of mixed-pixel and heterogeneous landscapes, and crucially, the robust in-season identification essential for proactive decision-making. We present a framework designed to address the critical data gaps for targeted data driven decision making which generates farm-level, in-season, multi-crop identification at national scale (India) using deep learning. Our methodology leverages the strengths of Sentinel-1 and Sentinel-2 satellite imagery, integrated with national-scale farm boundary data. The model successfully identifies 12 major crops (which collectively account for nearly 90% of India's total cultivated area showing an agreement with national crop census 2023-24 of 94% in winter, and 75% in monsoon season). Our approach incorporates an automated season detection algorithm, which estimates crop sowing and harvest periods. This allows for reliable crop identification as early as two months into the growing season and facilitates rigorous in-season performance evaluation. Furthermore, we have engineered a highly scalable inference pipeline, culminating in what is, to our knowledge, the first pan-India, in-season, farm-level crop type data product. The system's effectiveness and scalability are demonstrated through robust validation against national agricultural statistics, showcasing its potential to deliver actionable, data-driven insights for transformative agricultural monitoring and management across India.
>
---
#### [new 213] CLOT: Closed Loop Optimal Transport for Unsupervised Action Segmentation
- **分类: cs.CV**

- **简介: 该论文属于动作分割任务，解决无监督环境下分割效果受限的问题。提出CLOT框架，通过多级循环学习提升分割性能。**

- **链接: [http://arxiv.org/pdf/2507.03539v1](http://arxiv.org/pdf/2507.03539v1)**

> **作者:** Elena Bueno-Benito; Mariella Dimiccoli
>
> **备注:** Accepted to ICCV2025
>
> **摘要:** Unsupervised action segmentation has recently pushed its limits with ASOT, an optimal transport (OT)-based method that simultaneously learns action representations and performs clustering using pseudo-labels. Unlike other OT-based approaches, ASOT makes no assumptions on the action ordering, and it is able to decode a temporally consistent segmentation from a noisy cost matrix between video frames and action labels. However, the resulting segmentation lacks segment-level supervision, which limits the effectiveness of the feedback between frames and action representations. To address this limitation, we propose Closed Loop Optimal Transport (CLOT), a novel OT-based framework that introduces a multi-level cyclic feature learning mechanism. Leveraging its encoder-decoder architecture, CLOT learns pseudo-labels alongside frame and segment embeddings by solving two separate OT problems. It then refines both frame embeddings and pseudo-labels through cross-attention between the learned frame and segment embeddings, integrating a third OT problem. Experimental results on four benchmark datasets demonstrate the benefits of cyclical learning for unsupervised action segmentation.
>
---
#### [new 214] On the rankability of visual embeddings
- **分类: cs.CV**

- **简介: 该论文研究视觉嵌入模型是否能捕捉连续有序属性，定义为“rank axes”。任务是判断模型是否可排序，通过少量样本即可恢复有效排序轴，提升图像排序应用。**

- **链接: [http://arxiv.org/pdf/2507.03683v1](http://arxiv.org/pdf/2507.03683v1)**

> **作者:** Ankit Sonthalia; Arnas Uselis; Seong Joon Oh
>
> **摘要:** We study whether visual embedding models capture continuous, ordinal attributes along linear directions, which we term _rank axes_. We define a model as _rankable_ for an attribute if projecting embeddings onto such an axis preserves the attribute's order. Across 7 popular encoders and 9 datasets with attributes like age, crowd count, head pose, aesthetics, and recency, we find that many embeddings are inherently rankable. Surprisingly, a small number of samples, or even just two extreme examples, often suffice to recover meaningful rank axes, without full-scale supervision. These findings open up new use cases for image ranking in vector databases and motivate further study into the structure and learning of rankable embeddings. Our code is available at https://github.com/aktsonthalia/rankable-vision-embeddings.
>
---
#### [new 215] Multimodal Alignment with Cross-Attentive GRUs for Fine-Grained Video Understanding
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于视频理解任务，解决多模态信息融合问题。通过GRU和交叉注意力机制融合视频、图像和文本，提升细粒度视频分类性能。**

- **链接: [http://arxiv.org/pdf/2507.03531v1](http://arxiv.org/pdf/2507.03531v1)**

> **作者:** Namho Kim; Junhwa Kim
>
> **摘要:** Fine-grained video classification requires understanding complex spatio-temporal and semantic cues that often exceed the capacity of a single modality. In this paper, we propose a multimodal framework that fuses video, image, and text representations using GRU-based sequence encoders and cross-modal attention mechanisms. The model is trained using a combination of classification or regression loss, depending on the task, and is further regularized through feature-level augmentation and autoencoding techniques. To evaluate the generality of our framework, we conduct experiments on two challenging benchmarks: the DVD dataset for real-world violence detection and the Aff-Wild2 dataset for valence-arousal estimation. Our results demonstrate that the proposed fusion strategy significantly outperforms unimodal baselines, with cross-attention and feature augmentation contributing notably to robustness and performance.
>
---
#### [new 216] Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations
- **分类: cs.CV**

- **简介: 该论文属于文本生成视频任务，解决身份一致性与时空协调的矛盾。通过分解空间与时间表示，提升视频质量与身份保真度。**

- **链接: [http://arxiv.org/pdf/2507.04705v1](http://arxiv.org/pdf/2507.04705v1)**

> **作者:** Yuji Wang; Moran Li; Xiaobin Hu; Ran Yi; Jiangning Zhang; Han Feng; Weijian Cao; Yabiao Wang; Chengjie Wang; Lizhuang Ma
>
> **摘要:** Identity-preserving text-to-video (IPT2V) generation, which aims to create high-fidelity videos with consistent human identity, has become crucial for downstream applications. However, current end-to-end frameworks suffer a critical spatial-temporal trade-off: optimizing for spatially coherent layouts of key elements (e.g., character identity preservation) often compromises instruction-compliant temporal smoothness, while prioritizing dynamic realism risks disrupting the spatial coherence of visual structures. To tackle this issue, we propose a simple yet effective spatial-temporal decoupled framework that decomposes representations into spatial features for layouts and temporal features for motion dynamics. Specifically, our paper proposes a semantic prompt optimization mechanism and stage-wise decoupled generation paradigm. The former module decouples the prompt into spatial and temporal components. Aligned with the subsequent stage-wise decoupled approach, the spatial prompts guide the text-to-image (T2I) stage to generate coherent spatial features, while the temporal prompts direct the sequential image-to-video (I2V) stage to ensure motion consistency. Experimental results validate that our approach achieves excellent spatiotemporal consistency, demonstrating outstanding performance in identity preservation, text relevance, and video quality. By leveraging this simple yet robust mechanism, our algorithm secures the runner-up position in 2025 ACM MultiMedia Challenge.
>
---
#### [new 217] Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts
- **分类: cs.CV; cs.AI; cs.RO**

- **简介: 该论文属于立体匹配任务，解决跨域泛化不足的问题。通过融合LoRA与MoE模块，提升模型鲁棒性与效率。**

- **链接: [http://arxiv.org/pdf/2507.04631v1](http://arxiv.org/pdf/2507.04631v1)**

> **作者:** Yun Wang; Longguang Wang; Chenghao Zhang; Yongjian Zhang; Zhanjie Zhang; Ao Ma; Chenyou Fan; Tin Lun Lam; Junjie Hu
>
> **摘要:** Recently, learning-based stereo matching networks have advanced significantly. However, they often lack robustness and struggle to achieve impressive cross-domain performance due to domain shifts and imbalanced disparity distributions among diverse datasets. Leveraging Vision Foundation Models (VFMs) can intuitively enhance the model's robustness, but integrating such a model into stereo matching cost-effectively to fully realize their robustness remains a key challenge. To address this, we propose SMoEStereo, a novel framework that adapts VFMs for stereo matching through a tailored, scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal experts within MoE to adapt varying scenes across domains, while the latter injects inductive bias into frozen VFMs to improve geometric feature extraction. Importantly, to mitigate computational overhead, we further propose a lightweight decision network that selectively activates MoE modules based on input complexity, balancing efficiency with accuracy. Extensive experiments demonstrate that our method exhibits state-of-the-art cross-domain and joint generalization across multiple benchmarks without dataset-specific adaptation. The code is available at \textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}.
>
---
#### [new 218] Estimating Object Physical Properties from RGB-D Vision and Depth Robot Sensors Using Deep Learning
- **分类: cs.CV**

- **简介: 该论文属于物体质量估计任务，解决如何通过RGB-D视觉和深度传感器数据准确估算物体质量的问题。工作包括融合点云与RGB图像，构建合成数据集并训练深度与质量估计模型。**

- **链接: [http://arxiv.org/pdf/2507.05029v1](http://arxiv.org/pdf/2507.05029v1)**

> **作者:** Ricardo Cardoso; Plinio Moreno
>
> **摘要:** Inertial mass plays a crucial role in robotic applications such as object grasping, manipulation, and simulation, providing a strong prior for planning and control. Accurately estimating an object's mass before interaction can significantly enhance the performance of various robotic tasks. However, mass estimation using only vision sensors is a relatively underexplored area. This paper proposes a novel approach combining sparse point-cloud data from depth images with RGB images to estimate the mass of objects. We evaluate a range of point-cloud processing architectures, alongside RGB-only methods. To overcome the limited availability of training data, we create a synthetic dataset using ShapeNetSem 3D models, simulating RGBD images via a Kinect camera. This synthetic data is used to train an image generation model for estimating dense depth maps, which we then use to augment an existing dataset of images paired with mass values. Our approach significantly outperforms existing benchmarks across all evaluated metrics. The data generation (https://github.com/RavineWindteer/ShapenetSem-to-RGBD) as well as the training of the depth estimator (https://github.com/RavineWindteer/GLPDepth-Edited) and the mass estimator (https://github.com/RavineWindteer/Depth-mass-estimator) are available online.
>
---
#### [new 219] Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and Control for Text-to-Image Generation
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于文本到图像生成任务，解决模型生成内容与提示语义不一致的“幻觉”问题。提出Hallucination Tri-Space和ARC机制，实现对生成过程的动态控制与优化。**

- **链接: [http://arxiv.org/pdf/2507.04946v1](http://arxiv.org/pdf/2507.04946v1)**

> **作者:** Jianjiang Yang; Ziyan Huang
>
> **备注:** 12 pages, 6 figures, 4 tables
>
> **摘要:** Despite remarkable progress in image quality and prompt fidelity, text-to-image (T2I) diffusion models continue to exhibit persistent "hallucinations", where generated content subtly or significantly diverges from the intended prompt semantics. While often regarded as unpredictable artifacts, we argue that these failures reflect deeper, structured misalignments within the generative process. In this work, we propose a cognitively inspired perspective that reinterprets hallucinations as trajectory drift within a latent alignment space. Empirical observations reveal that generation unfolds within a multiaxial cognitive tension field, where the model must continuously negotiate competing demands across three key critical axes: semantic coherence, structural alignment, and knowledge grounding. We then formalize this three-axis space as the \textbf{Hallucination Tri-Space} and introduce the Alignment Risk Code (ARC): a dynamic vector representation that quantifies real-time alignment tension during generation. The magnitude of ARC captures overall misalignment, its direction identifies the dominant failure axis, and its imbalance reflects tension asymmetry. Based on this formulation, we develop the TensionModulator (TM-ARC): a lightweight controller that operates entirely in latent space. TM-ARC monitors ARC signals and applies targeted, axis-specific interventions during the sampling process. Extensive experiments on standard T2I benchmarks demonstrate that our approach significantly reduces hallucination without compromising image quality or diversity. This framework offers a unified and interpretable approach for understanding and mitigating generative failures in diffusion-based T2I systems.
>
---
#### [new 220] A Vision-Based Closed-Form Solution for Measuring the Rotation Rate of an Object by Tracking One Point
- **分类: cs.CV**

- **简介: 该论文属于视觉测量任务，解决物体旋转速率测量问题。通过跟踪一点，利用解析方法计算旋转率，无需已知物体形状或场景信息。**

- **链接: [http://arxiv.org/pdf/2507.03237v1](http://arxiv.org/pdf/2507.03237v1)**

> **作者:** Daniel Raviv; Juan D. Yepes; Eiki M. Martinson
>
> **摘要:** We demonstrate that, under orthographic projection and with a camera fixated on a point located on a rigid body, the rotation of that body can be analytically obtained by tracking only one other feature in the image. With some exceptions, any tracked point, regardless of its location on the body, yields the same value of the instantaneous rotation rate. The proposed method is independent of the shape of the 3D object and does not require a priori knowledge about the scene. This algorithm is suited for parallel processing and can achieve segmentation of the scene by distinguishing points that do not belong to the same rigid body, simply because they do not produce the same value of the rotation. This paper presents an analytical derivation, simulation results, and results from real video data.
>
---
#### [new 221] A Data-Driven Novelty Score for Diverse In-Vehicle Data Recording
- **分类: cs.CV**

- **简介: 该论文属于自动驾驶数据集构建任务，旨在解决真实数据中罕见场景不足的问题。通过动态均值漂移算法计算新颖性得分，实现高效数据筛选与平衡。**

- **链接: [http://arxiv.org/pdf/2507.04529v1](http://arxiv.org/pdf/2507.04529v1)**

> **作者:** Philipp Reis; Joshua Ransiek; David Petri; Jacob Langner; Eric Sax
>
> **备注:** 8 pages, accepted at the IEEE ITSC 2025
>
> **摘要:** High-quality datasets are essential for training robust perception systems in autonomous driving. However, real-world data collection is often biased toward common scenes and objects, leaving novel cases underrepresented. This imbalance hinders model generalization and compromises safety. The core issue is the curse of rarity. Over time, novel events occur infrequently, and standard logging methods fail to capture them effectively. As a result, large volumes of redundant data are stored, while critical novel cases are diluted, leading to biased datasets. This work presents a real-time data selection method focused on object-level novelty detection to build more balanced and diverse datasets. The method assigns a data-driven novelty score to image frames using a novel dynamic Mean Shift algorithm. It models normal content based on mean and covariance statistics to identify frames with novel objects, discarding those with redundant elements. The main findings show that reducing the training dataset size with this method can improve model performance, whereas higher redundancy tends to degrade it. Moreover, as data redundancy increases, more aggressive filtering becomes both possible and beneficial. While random sampling can offer some gains, it often leads to overfitting and unpredictability in outcomes. The proposed method supports real-time deployment with 32 frames per second and is constant over time. By continuously updating the definition of normal content, it enables efficient detection of novelties in a continuous data stream.
>
---
#### [new 222] SeqTex: Generate Mesh Textures in Video Sequence
- **分类: cs.CV; cs.AI; cs.GR**

- **简介: 该论文属于3D纹理生成任务，解决缺乏高质量3D纹理数据的问题。通过引入SeqTex框架，直接生成UV纹理图，提升3D一致性与真实感。**

- **链接: [http://arxiv.org/pdf/2507.04285v1](http://arxiv.org/pdf/2507.04285v1)**

> **作者:** Ze Yuan; Xin Yu; Yangtian Sun; Yuan-Chen Guo; Yan-Pei Cao; Ding Liang; Xiaojuan Qi
>
> **摘要:** Training native 3D texture generative models remains a fundamental yet challenging problem, largely due to the limited availability of large-scale, high-quality 3D texture datasets. This scarcity hinders generalization to real-world scenarios. To address this, most existing methods finetune foundation image generative models to exploit their learned visual priors. However, these approaches typically generate only multi-view images and rely on post-processing to produce UV texture maps -- an essential representation in modern graphics pipelines. Such two-stage pipelines often suffer from error accumulation and spatial inconsistencies across the 3D surface. In this paper, we introduce SeqTex, a novel end-to-end framework that leverages the visual knowledge encoded in pretrained video foundation models to directly generate complete UV texture maps. Unlike previous methods that model the distribution of UV textures in isolation, SeqTex reformulates the task as a sequence generation problem, enabling the model to learn the joint distribution of multi-view renderings and UV textures. This design effectively transfers the consistent image-space priors from video foundation models into the UV domain. To further enhance performance, we propose several architectural innovations: a decoupled multi-view and UV branch design, geometry-informed attention to guide cross-domain feature alignment, and adaptive token resolution to preserve fine texture details while maintaining computational efficiency. Together, these components allow SeqTex to fully utilize pretrained video priors and synthesize high-fidelity UV texture maps without the need for post-processing. Extensive experiments show that SeqTex achieves state-of-the-art performance on both image-conditioned and text-conditioned 3D texture generation tasks, with superior 3D consistency, texture-geometry alignment, and real-world generalization.
>
---
#### [new 223] Towards Spatially-Varying Gain and Binning
- **分类: cs.CV; eess.IV**

- **简介: 该论文属于图像传感器优化任务，旨在提升噪声性能和动态范围。通过空间变化的增益和分 binning 技术，改善成像质量。**

- **链接: [http://arxiv.org/pdf/2507.04190v1](http://arxiv.org/pdf/2507.04190v1)**

> **作者:** Anqi Yang; Eunhee Kang; Wei Chen; Hyong-Euk Lee; Aswin C. Sankaranarayanan
>
> **摘要:** Pixels in image sensors have progressively become smaller, driven by the goal of producing higher-resolution imagery. However, ceteris paribus, a smaller pixel accumulates less light, making image quality worse. This interplay of resolution, noise, and the dynamic range of the sensor and their impact on the eventual quality of acquired imagery is a fundamental concept in photography. In this paper, we propose spatially-varying gain and binning to enhance the noise performance and dynamic range of image sensors. First, we show that by varying gain spatially to local scene brightness, the read noise can be made negligible, and the dynamic range of a sensor is expanded by an order of magnitude. Second, we propose a simple analysis to find a binning size that best balances resolution and noise for a given light level; this analysis predicts a spatially-varying binning strategy, again based on local scene brightness, to effectively increase the overall signal-to-noise ratio. % without sacrificing resolution. We discuss analog and digital binning modes and, perhaps surprisingly, show that digital binning outperforms its analog counterparts when a larger gain is allowed. Finally, we demonstrate that combining spatially-varying gain and binning in various applications, including high dynamic range imaging, vignetting, and lens distortion.
>
---
#### [new 224] Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis
- **分类: cs.CV; cs.RO; eess.IV; I.4.9**

- **简介: 该论文属于视觉导航验证任务，解决传统方法设置复杂、运行慢的问题，提出实时合成视图增强数据集，并引入新的相机位姿距离度量。**

- **链接: [http://arxiv.org/pdf/2507.02993v1](http://arxiv.org/pdf/2507.02993v1)**

> **作者:** Marius Neuhalfen; Jonathan Grzymisch; Manuel Sanchez-Gestido
>
> **备注:** Published at the EUCASS2025 conference in Rome. Source code is public, please see link in paper
>
> **摘要:** This work introduces VISY-REVE: a novel pipeline to validate image processing algorithms for Vision-Based Navigation. Traditional validation methods such as synthetic rendering or robotic testbed acquisition suffer from difficult setup and slow runtime. Instead, we propose augmenting image datasets in real-time with synthesized views at novel poses. This approach creates continuous trajectories from sparse, pre-existing datasets in open or closed-loop. In addition, we introduce a new distance metric between camera poses, the Boresight Deviation Distance, which is better suited for view synthesis than existing metrics. Using it, a method for increasing the density of image datasets is developed.
>
---
#### [new 225] Taming Anomalies with Down-Up Sampling Networks: Group Center Preserving Reconstruction for 3D Anomaly Detection
- **分类: cs.CV; 68T10; I.4; I.5; J.6**

- **简介: 该论文属于3D异常检测任务，旨在解决高精度点云重建难题。提出DUS-Net网络，通过保留组中心结构实现更精确的点云重建。**

- **链接: [http://arxiv.org/pdf/2507.03903v1](http://arxiv.org/pdf/2507.03903v1)**

> **作者:** Hanzhe Liang; Jie Zhang; Tao Dai; Linlin Shen; Jinbao Wang; Can Gao
>
> **备注:** ACM MM25 Accepted
>
> **摘要:** Reconstruction-based methods have demonstrated very promising results for 3D anomaly detection. However, these methods face great challenges in handling high-precision point clouds due to the large scale and complex structure. In this study, a Down-Up Sampling Network (DUS-Net) is proposed to reconstruct high-precision point clouds for 3D anomaly detection by preserving the group center geometric structure. The DUS-Net first introduces a Noise Generation module to generate noisy patches, which facilitates the diversity of training data and strengthens the feature representation for reconstruction. Then, a Down-sampling Network~(Down-Net) is developed to learn an anomaly-free center point cloud from patches with noise injection. Subsequently, an Up-sampling Network (Up-Net) is designed to reconstruct high-precision point clouds by fusing multi-scale up-sampling features. Our method leverages group centers for construction, enabling the preservation of geometric structure and providing a more precise point cloud. Extensive experiments demonstrate the effectiveness of our proposed method, achieving state-of-the-art (SOTA) performance with an Object-level AUROC of 79.9% and 79.5%, and a Point-level AUROC of 71.2% and 84.7% on the Real3D-AD and Anomaly-ShapeNet datasets, respectively.
>
---
#### [new 226] Topological Signatures vs. Gradient Histograms: A Comparative Study for Medical Image Classification
- **分类: cs.CV; cs.LG**

- **简介: 该论文属于医学图像分类任务，比较HOG与TDA在糖尿病视网膜病变检测中的效果，通过实验验证两种方法的性能与特点。**

- **链接: [http://arxiv.org/pdf/2507.03006v1](http://arxiv.org/pdf/2507.03006v1)**

> **作者:** Faisal Ahmed; Mohammad Alfrad Nobel Bhuiyan
>
> **备注:** 18 pages, 12 figures
>
> **摘要:** We present the first comparative study of two fundamentally distinct feature extraction techniques: Histogram of Oriented Gradients (HOG) and Topological Data Analysis (TDA), for medical image classification using retinal fundus images. HOG captures local texture and edge patterns through gradient orientation histograms, while TDA, using cubical persistent homology, extracts high-level topological signatures that reflect the global structure of pixel intensities. We evaluate both methods on the large APTOS dataset for two classification tasks: binary detection (normal versus diabetic retinopathy) and five-class diabetic retinopathy severity grading. From each image, we extract 26244 HOG features and 800 TDA features, using them independently to train seven classical machine learning models with 10-fold cross-validation. XGBoost achieved the best performance in both cases: 94.29 percent accuracy (HOG) and 94.18 percent (TDA) on the binary task; 74.41 percent (HOG) and 74.69 percent (TDA) on the multi-class task. Our results show that both methods offer competitive performance but encode different structural aspects of the images. This is the first work to benchmark gradient-based and topological features on retinal imagery. The techniques are interpretable, applicable to other medical imaging domains, and suitable for integration into deep learning pipelines.
>
---
#### [new 227] Less is More: Empowering GUI Agent with Context-Aware Simplification
- **分类: cs.CV; cs.AI; cs.HC; cs.LG**

- **简介: 该论文属于GUI代理任务，解决元素与历史上下文建模问题。提出SimpAgent框架，通过元素掩码和历史压缩提升效率与性能。**

- **链接: [http://arxiv.org/pdf/2507.03730v1](http://arxiv.org/pdf/2507.03730v1)**

> **作者:** Gongwei Chen; Xurui Zhou; Rui Shao; Yibo Lyu; Kaiwen Zhou; Shuai Wang; Wentao Li; Yinchuan Li; Zhongang Qi; Liqiang Nie
>
> **备注:** Accepted to ICCV 2025
>
> **摘要:** The research focus of GUI agents is shifting from text-dependent to pure-vision-based approaches, which, though promising, prioritize comprehensive pre-training data collection while neglecting contextual modeling challenges. We probe the characteristics of element and history contextual modeling in GUI agent and summarize: 1) the high-density and loose-relation of element context highlight the existence of many unrelated elements and their negative influence; 2) the high redundancy of history context reveals the inefficient history modeling in current GUI agents. In this work, we propose a context-aware simplification framework for building an efficient and effective GUI Agent, termed SimpAgent. To mitigate potential interference from numerous unrelated elements, we introduce a masking-based element pruning method that circumvents the intractable relation modeling through an efficient masking mechanism. To reduce the redundancy in historical information, we devise a consistency-guided history compression module, which enhances implicit LLM-based compression through innovative explicit guidance, achieving an optimal balance between performance and efficiency. With the above components, SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances. Comprehensive navigation experiments across diverse web and mobile environments demonstrate the effectiveness and potential of our agent.
>
---
#### [new 228] Transcribing Spanish Texts from the Past: Experiments with Transkribus, Tesseract and Granite
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于历史文本OCR任务，旨在提高西班牙文古籍的转录准确性。研究对比了三种OCR方法，尝试在普通硬件上实现有效转录。**

- **链接: [http://arxiv.org/pdf/2507.04878v1](http://arxiv.org/pdf/2507.04878v1)**

> **作者:** Yanco Amor Torterolo-Orta; Jaione Macicior-Mitxelena; Marina Miguez-Lamanuzzi; Ana García-Serrano
>
> **备注:** This paper was written as part of a shared task organized within the 2025 edition of the Iberian Languages Evaluation Forum (IberLEF 2025), held at SEPLN 2025 in Zaragoza. This paper describes the joint participation of two teams in said competition, GRESEL1 and GRESEL2, each with an individual paper that will be published in CEUR
>
> **摘要:** This article presents the experiments and results obtained by the GRESEL team in the IberLEF 2025 shared task PastReader: Transcribing Texts from the Past. Three types of experiments were conducted with the dual aim of participating in the task and enabling comparisons across different approaches. These included the use of a web-based OCR service, a traditional OCR engine, and a compact multimodal model. All experiments were run on consumer-grade hardware, which, despite lacking high-performance computing capacity, provided sufficient storage and stability. The results, while satisfactory, leave room for further improvement. Future work will focus on exploring new techniques and ideas using the Spanish-language dataset provided by the shared task, in collaboration with Biblioteca Nacional de Espa\~na (BNE).
>
---
#### [new 229] Enhancing Sports Strategy with Video Analytics and Data Mining: Assessing the effectiveness of Multimodal LLMs in tennis video analysis
- **分类: cs.CV; cs.AI; I.2.7; I.2.10; I.4**

- **简介: 该论文属于体育视频分析任务，旨在解决网球比赛中动作序列识别的问题，通过评估多模态大语言模型的性能并探索提升方法。**

- **链接: [http://arxiv.org/pdf/2507.02904v1](http://arxiv.org/pdf/2507.02904v1)**

> **作者:** Charlton Teo
>
> **备注:** B.Comp. dissertation
>
> **摘要:** The use of Large Language Models (LLMs) in recent years has also given rise to the development of Multimodal LLMs (MLLMs). These new MLLMs allow us to process images, videos and even audio alongside textual inputs. In this project, we aim to assess the effectiveness of MLLMs in analysing sports videos, focusing mainly on tennis videos. Despite research done on tennis analysis, there remains a gap in models that are able to understand and identify the sequence of events in a tennis rally, which would be useful in other fields of sports analytics. As such, we will mainly assess the MLLMs on their ability to fill this gap - to classify tennis actions, as well as their ability to identify these actions in a sequence of tennis actions in a rally. We further looked into ways we can improve the MLLMs' performance, including different training methods and even using them together with other traditional models.
>
---
#### [new 230] Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions
- **分类: cs.CV; cs.CL; cs.MM**

- **简介: 该论文属于文物解读任务，旨在解决 tombstone 信息解析难题。通过多模态框架结合视觉语言模型和检索增强生成，提升解析准确性和鲁棒性。**

- **链接: [http://arxiv.org/pdf/2507.04377v1](http://arxiv.org/pdf/2507.04377v1)**

> **作者:** Xiao Zhang; Johan Bos
>
> **备注:** Accepted by ACMMM 2025
>
> **摘要:** Tombstones are historically and culturally rich artifacts, encapsulating individual lives, community memory, historical narratives and artistic expression. Yet, many tombstones today face significant preservation challenges, including physical erosion, vandalism, environmental degradation, and political shifts. In this paper, we introduce a novel multi-modal framework for tombstones digitization, aiming to improve the interpretation, organization and retrieval of tombstone content. Our approach leverages vision-language models (VLMs) to translate tombstone images into structured Tombstone Meaning Representations (TMRs), capturing both image and text information. To further enrich semantic parsing, we incorporate retrieval-augmented generation (RAG) for integrate externally dependent elements such as toponyms, occupation codes, and ontological concepts. Compared to traditional OCR-based pipelines, our method improves parsing accuracy from an F1 score of 36.1 to 89.5. We additionally evaluate the model's robustness across diverse linguistic and cultural inscriptions, and simulate physical degradation through image fusion to assess performance under noisy or damaged conditions. Our work represents the first attempt to formalize tombstone understanding using large vision-language models, presenting implications for heritage preservation.
>
---
#### [new 231] MRC-DETR: An Adaptive Multi-Residual Coupled Transformer for Bare Board PCB Defect Detection
- **分类: cs.CV**

- **简介: 该论文属于PCB缺陷检测任务，解决特征表示不足、计算冗余和数据匮乏问题。提出MRC-DETR框架，包含MRDCB和ASPN模块，并构建新数据集。**

- **链接: [http://arxiv.org/pdf/2507.03386v1](http://arxiv.org/pdf/2507.03386v1)**

> **作者:** Jiangzhong Cao; Huanqi Wu; Xu Zhang; Lianghong Tan; Huan Zhang
>
> **摘要:** In modern electronic manufacturing, defect detection on Printed Circuit Boards (PCBs) plays a critical role in ensuring product yield and maintaining the reliability of downstream assembly processes. However, existing methods often suffer from limited feature representation, computational redundancy, and insufficient availability of high-quality training data -- challenges that hinder their ability to meet industrial demands for both accuracy and efficiency. To address these limitations, we propose MRC-DETR, a novel and efficient detection framework tailored for bare PCB defect inspection, built upon the foundation of RT-DETR. Firstly, to enhance feature representation capability, we design a Multi-Residual Directional Coupled Block (MRDCB). This module improves channel-wise feature interaction through a multi-residual structure. Moreover, a cross-spatial learning strategy is integrated to capture fine-grained pixel-level relationships, further enriching the representational power of the extracted features. Secondly, to reduce computational redundancy caused by inefficient cross-layer information fusion, we introduce an Adaptive Screening Pyramid Network (ASPN). This component dynamically filters and aggregates salient low-level features, selectively fusing them with high-level semantic features. By focusing on informative regions and suppressing redundant computations, ASPN significantly improves both efficiency and detection accuracy. Finally, to tackle the issue of insufficient training data, particularly in the context of bare PCBs, we construct a new, high-quality dataset that fills a critical gap in current public resources. Our dataset not only supports the training and evaluation of our proposed framework but also serves as a valuable benchmark for future research in this domain.
>
---
#### [new 232] Bridging Vision and Language: Optimal Transport-Driven Radiology Report Generation via LLMs
- **分类: cs.CV**

- **简介: 该论文属于医学影像报告生成任务，旨在解决LLMs在临床有效性上的不足。通过引入最优传输方法对齐图像与文本特征，并设计疾病预测模块，提升报告的临床准确性。**

- **链接: [http://arxiv.org/pdf/2507.03908v1](http://arxiv.org/pdf/2507.03908v1)**

> **作者:** Haifeng Zhao; Yufei Zhang; Leilei Ma; Shuo Xu; Dengdi Sun
>
> **摘要:** Radiology report generation represents a significant application within medical AI, and has achieved impressive results. Concurrently, large language models (LLMs) have demonstrated remarkable performance across various domains. However, empirical validation indicates that general LLMs tend to focus more on linguistic fluency rather than clinical effectiveness, and lack the ability to effectively capture the relationship between X-ray images and their corresponding texts, thus resulting in poor clinical practicability. To address these challenges, we propose Optimal Transport-Driven Radiology Report Generation (OTDRG), a novel framework that leverages Optimal Transport (OT) to align image features with disease labels extracted from reports, effectively bridging the cross-modal gap. The core component of OTDRG is Alignment \& Fine-Tuning, where OT utilizes results from the encoding of label features and image visual features to minimize cross-modal distances, then integrating image and text features for LLMs fine-tuning. Additionally, we design a novel disease prediction module to predict disease labels contained in X-ray images during validation and testing. Evaluated on the MIMIC-CXR and IU X-Ray datasets, OTDRG achieves state-of-the-art performance in both natural language generation (NLG) and clinical efficacy (CE) metrics, delivering reports that are not only linguistically coherent but also clinically accurate.
>
---
#### [new 233] Mimesis, Poiesis, and Imagination: Exploring Text-to-Image Generation of Biblical Narratives
- **分类: cs.CV**

- **简介: 该论文属于文本生成图像任务，探讨AI如何再现圣经故事。研究分析AI生成图像的风格与局限，评估其创造性与宗教深度。**

- **链接: [http://arxiv.org/pdf/2507.02973v1](http://arxiv.org/pdf/2507.02973v1)**

> **作者:** Willem Th. van Peursen; Samuel E. Entsua-Mensah
>
> **摘要:** This study explores the intersection of artificial intelligence and the visualization of Biblical narratives by analyzing AI-generated images of Exodus 2:5-9 (Moses found in River Nile) using MidJourney. Drawing on the classical concepts of mimesis (imitation) and poiesis (creative generation), the authors investigate how text-to-image (T2I) models reproduce or reimagine sacred narratives. Through comparative visual analysis, including Google image results and classical paintings, the research evaluates the stylistic, theological, and cultural dimensions of AI-generated depictions. Findings show that while AI excels in producing aesthetically rich and imaginative visuals, it also reflects the biases and limitations of its training data. The study highlights AI's potential to augment human imagination but questions its capacity for genuine creativity, authorial intent, and theological depth. It concludes by suggesting that AI can serve as a creative partner in reinterpreting biblical texts, though its role in sacred art remains complex and contested.
>
---
#### [new 234] Deconfounding Causal Inference through Two-Branch Framework with Early-Forking for Sensor-Based Cross-Domain Activity Recognition
- **分类: cs.CV**

- **简介: 该论文属于跨域活动识别任务，解决传感器数据分布差异导致的性能下降问题。提出双分支框架，分离因果与非因果特征，提升模型泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.03898v1](http://arxiv.org/pdf/2507.03898v1)**

> **作者:** Di Xiong; Lei Zhang; Shuoyuan Wang; Dongzhou Cheng; Wenbo Huang
>
> **备注:** Accepted by Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)
>
> **摘要:** Recently, domain generalization (DG) has emerged as a promising solution to mitigate distribution-shift issue in sensor-based human activity recognition (HAR) scenario. However, most existing DG-based works have merely focused on modeling statistical dependence between sensor data and activity labels, neglecting the importance of intrinsic casual mechanism. Intuitively, every sensor input can be viewed as a mixture of causal (category-aware) and non-causal factors (domain-specific), where only the former affects activity classification judgment. In this paper, by casting such DG-based HAR as a casual inference problem, we propose a causality-inspired representation learning algorithm for cross-domain activity recognition. To this end, an early-forking two-branch framework is designed, where two separate branches are respectively responsible for learning casual and non-causal features, while an independence-based Hilbert-Schmidt Information Criterion is employed to implicitly disentangling them. Additionally, an inhomogeneous domain sampling strategy is designed to enhance disentanglement, while a category-aware domain perturbation layer is performed to prevent representation collapse. Extensive experiments on several public HAR benchmarks demonstrate that our causality-inspired approach significantly outperforms eleven related state-of-the-art baselines under cross-person, cross-dataset, and cross-position settings. Detailed ablation and visualizations analyses reveal underlying casual mechanism, indicating its effectiveness, efficiency, and universality in cross-domain activity recognition scenario.
>
---
#### [new 235] Differential Attention for Multimodal Crisis Event Analysis
- **分类: cs.CV**

- **简介: 该论文属于多模态危机事件分析任务，旨在提升危机数据分类性能。通过融合视觉语言模型和注意力机制，增强多模态信息对齐与特征提取。**

- **链接: [http://arxiv.org/pdf/2507.05165v1](http://arxiv.org/pdf/2507.05165v1)**

> **作者:** Nusrat Munia; Junfeng Zhu; Olfa Nasraoui; Abdullah-Al-Zubaer Imran
>
> **备注:** Presented at CVPRw 2025, MMFM3
>
> **摘要:** Social networks can be a valuable source of information during crisis events. In particular, users can post a stream of multimodal data that can be critical for real-time humanitarian response. However, effectively extracting meaningful information from this large and noisy data stream and effectively integrating heterogeneous data remains a formidable challenge. In this work, we explore vision language models (VLMs) and advanced fusion strategies to enhance the classification of crisis data in three different tasks. We incorporate LLaVA-generated text to improve text-image alignment. Additionally, we leverage Contrastive Language-Image Pretraining (CLIP)-based vision and text embeddings, which, without task-specific fine-tuning, outperform traditional models. To further refine multimodal fusion, we employ Guided Cross Attention (Guided CA) and combine it with the Differential Attention mechanism to enhance feature alignment by emphasizing critical information while filtering out irrelevant content. Our results show that while Differential Attention improves classification performance, Guided CA remains highly effective in aligning multimodal features. Extensive experiments on the CrisisMMD benchmark data set demonstrate that the combination of pretrained VLMs, enriched textual descriptions, and adaptive fusion strategies consistently outperforms state-of-the-art models in classification accuracy, contributing to more reliable and interpretable models for three different tasks that are crucial for disaster response. Our code is available at https://github.com/Munia03/Multimodal_Crisis_Event.
>
---
#### [new 236] VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于多模态嵌入任务，解决现有模型对视频和视觉文档支持不足的问题。提出VLM2Vec-V2框架，支持多种视觉输入并提升性能。**

- **链接: [http://arxiv.org/pdf/2507.04590v1](http://arxiv.org/pdf/2507.04590v1)**

> **作者:** Rui Meng; Ziyan Jiang; Ye Liu; Mingyi Su; Xinyi Yang; Yuepeng Fu; Can Qin; Zeyuan Chen; Ran Xu; Caiming Xiong; Yingbo Zhou; Wenhu Chen; Semih Yavuz
>
> **备注:** Technical Report
>
> **摘要:** Multimodal embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering over different modalities. However, existing multimodal embeddings like VLM2Vec, E5-V, GME are predominantly focused on natural images, with limited support for other visual forms such as videos and visual documents. This restricts their applicability in real-world scenarios, including AI agents, multi-modal search and recommendation, and retrieval-augmented generation (RAG). To close this gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark that extends MMEB with five new task types: visual document retrieval, video retrieval, temporal grounding, video classification and video question answering - spanning text, image, video, and visual document inputs. Next, we train VLM2Vec-V2, a general-purpose embedding model that supports text, image, video, and visual document inputs. Extensive experiments show that VLM2Vec-V2 achieves strong performance not only on the newly introduced video and document retrieval tasks, but also improves over prior baselines on the original image benchmarks. Through extensive evaluation, our study offers insights into the generalizability of various multimodal embedding models and highlights effective strategies for unified embedding learning, laying the groundwork for more scalable and adaptable representation learning in both research and real-world settings.
>
---
#### [new 237] De-Fake: Style based Anomaly Deepfake Detection
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于深度伪造检测任务，旨在解决 face-swap 深度伪造的检测问题。通过分析风格差异，在不访问真实人脸图像的情况下实现隐私保护的高效检测。**

- **链接: [http://arxiv.org/pdf/2507.03334v1](http://arxiv.org/pdf/2507.03334v1)**

> **作者:** Sudev Kumar Padhi; Harshit Kumar; Umesh Kashyap; Sk. Subidh Ali
>
> **摘要:** Detecting deepfakes involving face-swaps presents a significant challenge, particularly in real-world scenarios where anyone can perform face-swapping with freely available tools and apps without any technical knowledge. Existing deepfake detection methods rely on facial landmarks or inconsistencies in pixel-level features and often struggle with face-swap deepfakes, where the source face is seamlessly blended into the target image or video. The prevalence of face-swap is evident in everyday life, where it is used to spread false information, damage reputations, manipulate political opinions, create non-consensual intimate deepfakes (NCID), and exploit children by enabling the creation of child sexual abuse material (CSAM). Even prominent public figures are not immune to its impact, with numerous deepfakes of them circulating widely across social media platforms. Another challenge faced by deepfake detection methods is the creation of datasets that encompass a wide range of variations, as training models require substantial amounts of data. This raises privacy concerns, particularly regarding the processing and storage of personal facial data, which could lead to unauthorized access or misuse. Our key idea is to identify these style discrepancies to detect face-swapped images effectively without accessing the real facial image. We perform comprehensive evaluations using multiple datasets and face-swapping methods, which showcases the effectiveness of SafeVision in detecting face-swap deepfakes across diverse scenarios. SafeVision offers a reliable and scalable solution for detecting face-swaps in a privacy preserving manner, making it particularly effective in challenging real-world applications. To the best of our knowledge, SafeVision is the first deepfake detection using style features while providing inherent privacy protection.
>
---
#### [new 238] Zero Memory Overhead Approach for Protecting Vision Transformer Parameters
- **分类: cs.CV**

- **简介: 该论文属于模型可靠性任务，解决ViT参数在内存中受位翻转故障影响的问题。通过替换最低有效位为奇偶校验位实现零内存开销的容错保护。**

- **链接: [http://arxiv.org/pdf/2507.03816v1](http://arxiv.org/pdf/2507.03816v1)**

> **作者:** Fereshteh Baradaran; Mohsen Raji; Azadeh Baradaran; Arezoo Baradaran; Reihaneh Akbarifard
>
> **摘要:** Vision Transformers (ViTs) have demonstrated superior performance over Convolutional Neural Networks (CNNs) in various vision-related tasks such as classification, object detection, and segmentation due to their use of self-attention mechanisms. As ViTs become more popular in safety-critical applications like autonomous driving, ensuring their correct functionality becomes essential, especially in the presence of bit-flip faults in their parameters stored in memory. In this paper, a fault tolerance technique is introduced to protect ViT parameters against bit-flip faults with zero memory overhead. Since the least significant bits of parameters are not critical for model accuracy, replacing the LSB with a parity bit provides an error detection mechanism without imposing any overhead on the model. When faults are detected, affected parameters are masked by zeroing out, as most parameters in ViT models are near zero, effectively preventing accuracy degradation. This approach enhances reliability across ViT models, improving the robustness of parameters to bit-flips by up to three orders of magnitude, making it an effective zero-overhead solution for fault tolerance in critical applications.
>
---
#### [new 239] HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding
- **分类: cs.CV; cs.AI**

- **简介: 该论文属于多模态大模型在以人为中心的视频理解任务。针对现有基准不足，提出HV-MMBench，涵盖多维度、多类型、多场景和长时间视频的评估体系，以全面评测模型能力。**

- **链接: [http://arxiv.org/pdf/2507.04909v1](http://arxiv.org/pdf/2507.04909v1)**

> **作者:** Yuxuan Cai; Jiangning Zhang; Zhenye Gan; Qingdong He; Xiaobin Hu; Junwei Zhu; Yabiao Wang; Chengjie Wang; Zhucun Xue; Xinwei He; Xiang Bai
>
> **备注:** Under review
>
> **摘要:** Multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks involving both images and videos. However, their capacity to comprehend human-centric video data remains underexplored, primarily due to the absence of comprehensive and high-quality evaluation benchmarks. Existing human-centric benchmarks predominantly emphasize video generation quality and action recognition, while overlooking essential perceptual and cognitive abilities required in human-centered scenarios. Furthermore, they are often limited by single-question paradigms and overly simplistic evaluation metrics. To address above limitations, we propose a modern HV-MMBench, a rigorously curated benchmark designed to provide a more holistic evaluation of MLLMs in human-centric video understanding. Compared to existing human-centric video benchmarks, our work offers the following key features: (1) Diverse evaluation dimensions: HV-MMBench encompasses 15 tasks, ranging from basic attribute perception (e.g., age estimation, emotion recognition) to advanced cognitive reasoning (e.g., social relationship prediction, intention prediction), enabling comprehensive assessment of model capabilities; (2) Varied data types: The benchmark includes multiple-choice, fill-in-blank, true/false, and open-ended question formats, combined with diverse evaluation metrics, to more accurately and robustly reflect model performance; (3) Multi-domain video coverage: The benchmark spans 50 distinct visual scenarios, enabling comprehensive evaluation across fine-grained scene variations; (4) Temporal coverage: The benchmark covers videos from short-term (10 seconds) to long-term (up to 30min) durations, supporting systematic analysis of models temporal reasoning abilities across diverse contextual lengths.
>
---
#### [new 240] Computed Tomography Visual Question Answering with Cross-modal Feature Graphing
- **分类: cs.CV; cs.CL**

- **简介: 该论文属于医学影像VQA任务，解决CT数据空间连续性不足导致回答不准确的问题。通过构建跨模态图结构融合视觉与文本信息，提升模型推理能力。**

- **链接: [http://arxiv.org/pdf/2507.04333v1](http://arxiv.org/pdf/2507.04333v1)**

> **作者:** Yuanhe Tian; Chen Su; Junwen Duan; Yan Song
>
> **备注:** 9 pages, 3 figures
>
> **摘要:** Visual question answering (VQA) in medical imaging aims to support clinical diagnosis by automatically interpreting complex imaging data in response to natural language queries. Existing studies typically rely on distinct visual and textual encoders to independently extract features from medical images and clinical questions, which are subsequently combined to generate answers. Specifically, in computed tomography (CT), such approaches are similar to the conventional practices in medical image analysis. However, these approaches pay less attention to the spatial continuity and inter-slice correlations in the volumetric CT data, leading to fragmented and imprecise responses. In this paper, we propose a novel large language model (LLM)-based framework enhanced by a graph representation of salient features. Different from conventional multimodal encoding strategies, our approach constructs a cross-modal graph integrating both visual and textual features, treating individual CT slices and question tokens as nodes within the graph. We further leverage an attentive graph convolutional network to dynamically fuse information within this structure. The resulting aggregated graph features then serve as a soft prompt to guide a large language model in generating accurate answers. Extensive experiments on the M3D-VQA benchmark demonstrate that our approach consistently outperforms baselines across multiple evaluation metrics, offering more robust reasoning capabilities.
>
---
#### [new 241] Visual Hand Gesture Recognition with Deep Learning: A Comprehensive Review of Methods, Datasets, Challenges and Future Research Directions
- **分类: cs.CV**

- **简介: 该论文属于视觉手部手势识别任务，旨在系统梳理方法、数据集与挑战，为研究者提供指导。**

- **链接: [http://arxiv.org/pdf/2507.04465v1](http://arxiv.org/pdf/2507.04465v1)**

> **作者:** Konstantinos Foteinos; Jorgen Cani; Manousos Linardakis; Panagiotis Radoglou-Grammatikis; Vasileios Argyriou; Panagiotis Sarigiannidis; Iraklis Varlamis; Georgios Th. Papadopoulos
>
> **摘要:** The rapid evolution of deep learning (DL) models and the ever-increasing size of available datasets have raised the interest of the research community in the always important field of vision-based hand gesture recognition (VHGR), and delivered a wide range of applications, such as sign language understanding and human-computer interaction using cameras. Despite the large volume of research works in the field, a structured and complete survey on VHGR is still missing, leaving researchers to navigate through hundreds of papers in order to find the right combination of data, model, and approach for each task. The current survey aims to fill this gap by presenting a comprehensive overview of this aspect of computer vision. With a systematic research methodology that identifies the state-of-the-art works and a structured presentation of the various methods, datasets, and evaluation metrics, this review aims to constitute a useful guideline for researchers, helping them to choose the right strategy for delving into a certain VHGR task. Starting with the methodology used for study selection, literature retrieval, and the analytical framing, the survey identifies and organizes key VHGR approaches using a taxonomy-based format in various dimensions such as input modality and application domain. The core of the survey provides an in-depth analysis of state-of-the-art techniques across three primary VHGR tasks: static gesture recognition, isolated dynamic gestures and continuous gesture recognition. For each task, the architectural trends and learning strategies are listed. Additionally, the study reviews commonly used datasets - emphasizing on annotation schemes - and evaluates standard performance metrics. It concludes by identifying major challenges in VHGR, including both general computer vision issues and domain-specific obstacles, and outlines promising directions for future research.
>
---
#### [new 242] Pedestrian Intention Prediction via Vision-Language Foundation Models
- **分类: cs.CV; cs.AI; cs.ET; cs.LG; cs.RO**

- **简介: 该论文属于自动驾驶中的行人意图预测任务，旨在解决传统方法在泛化性和上下文理解上的不足。通过引入视觉-语言基础模型和优化提示模板，提升预测准确性。**

- **链接: [http://arxiv.org/pdf/2507.04141v1](http://arxiv.org/pdf/2507.04141v1)**

> **作者:** Mohsen Azarmi; Mahdi Rezaei; He Wang
>
> **摘要:** Prediction of pedestrian crossing intention is a critical function in autonomous vehicles. Conventional vision-based methods of crossing intention prediction often struggle with generalizability, context understanding, and causal reasoning. This study explores the potential of vision-language foundation models (VLFMs) for predicting pedestrian crossing intentions by integrating multimodal data through hierarchical prompt templates. The methodology incorporates contextual information, including visual frames, physical cues observations, and ego-vehicle dynamics, into systematically refined prompts to guide VLFMs effectively in intention prediction. Experiments were conducted on three common datasets-JAAD, PIE, and FU-PIP. Results demonstrate that incorporating vehicle speed, its variations over time, and time-conscious prompts significantly enhances the prediction accuracy up to 19.8%. Additionally, optimised prompts generated via an automatic prompt engineering framework yielded 12.5% further accuracy gains. These findings highlight the superior performance of VLFMs compared to conventional vision-based models, offering enhanced generalisation and contextual understanding for autonomous driving applications.
>
---
#### [new 243] Segmentation of separated Lumens in 3D CTA images of Aortic Dissection
- **分类: eess.IV; cs.CV; q-bio.QM**

- **简介: 该论文属于医学图像分割任务，旨在解决主动脉夹层中真假腔分离问题。通过使用填充裂口的3D表面切割连接，实现腔体分离，辅助医生诊断。**

- **链接: [http://arxiv.org/pdf/2507.03655v1](http://arxiv.org/pdf/2507.03655v1)**

> **作者:** Christophe Lohou; Bruno Miguel
>
> **摘要:** Aortic dissection is a serious pathology and requires an emergency management. It is characterized by one or more tears of the intimal wall of the normal blood duct of the aorta (true lumen); the blood under pressure then creates a second blood lumen (false lumen) in the media tissue. The two lumens are separated by an intimal wall, called flap. From the segmentation of connected lumens (more precisely, blood inside lumens) of an aortic dissection 3D Computed Tomography Angiography (CTA) image, our previous studies allow us to retrieve the intimal flap by using Mathematical Morphology operators, and characterize intimal tears by 3d thin surfaces that fill them, these surfaces are obtained by operating the Aktouf et al. closing algorithm proposed in the framework of Digital Topology. Indeed, intimal tears are 3D holes in the intimal flap; although it is impossible to directly segment such non-concrete data, it is nevertheless possible to "materialize" them with these 3D filling surfaces that may be quantified or make easier the visualization of these holes. In this paper, we use these surfaces that fill tears to cut connections between lumens in order to separate them. This is the first time that surfaces filling tears are used as an image processing operator (to disconnect several parts of a 3D object). This lumen separation allows us to provide one of the first cartographies of an aortic dissection, that may better visually assist physicians during their diagnosis. Our method is able to disconnect lumens, that may also lead to enhance several current investigations (registration, segmentation, hemodynamics).
>
---
#### [new 244] EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling
- **分类: cs.RO; cs.AI; cs.CV**

- **简介: 该论文属于机器人政策训练任务，旨在解决Real2Sim2Real迁移难题。通过PhysAligner和VisAligner减少物理与视觉差异，提升仿真到现实的迁移效果。**

- **链接: [http://arxiv.org/pdf/2507.05198v1](http://arxiv.org/pdf/2507.05198v1)**

> **作者:** Boyuan Wang; Xinpan Meng; Xiaofeng Wang; Zheng Zhu; Angen Ye; Yang Wang; Zhiqin Yang; Chaojun Ni; Guan Huang; Xingang Wang
>
> **备注:** Project Page: https://embodiedreamer.github.io/
>
> **摘要:** The rapid advancement of Embodied AI has led to an increasing demand for large-scale, high-quality real-world data. However, collecting such embodied data remains costly and inefficient. As a result, simulation environments have become a crucial surrogate for training robot policies. Yet, the significant Real2Sim2Real gap remains a critical bottleneck, particularly in terms of physical dynamics and visual appearance. To address this challenge, we propose EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both the physics and appearance perspectives. Specifically, we propose PhysAligner, a differentiable physics module designed to reduce the Real2Sim physical gap. It jointly optimizes robot-specific parameters such as control gains and friction coefficients to better align simulated dynamics with real-world observations. In addition, we introduce VisAligner, which incorporates a conditional video diffusion model to bridge the Sim2Real appearance gap by translating low-fidelity simulated renderings into photorealistic videos conditioned on simulation states, enabling high-fidelity visual transfer. Extensive experiments validate the effectiveness of EmbodieDreamer. The proposed PhysAligner reduces physical parameter estimation error by 3.74% compared to simulated annealing methods while improving optimization speed by 89.91\%. Moreover, training robot policies in the generated photorealistic environment leads to a 29.17% improvement in the average task success rate across real-world tasks after reinforcement learning. Code, model and data will be publicly available.
>
---
#### [new 245] Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning
- **分类: cs.RO; cs.AI; cs.CV; cs.LG**

- **简介: 该论文属于自主机器人路径规划任务，旨在解决多源轨迹数据在目标领域应用时的适应性问题。提出IMMP方法，通过参数合并提升模型泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.04790v1](http://arxiv.org/pdf/2507.04790v1)**

> **作者:** Giwon Lee; Wooseong Jeong; Daehee Park; Jaewoo Jeong; Kuk-Jin Yoon
>
> **备注:** Accepted at ICCV 2025
>
> **摘要:** Motion planning is a crucial component of autonomous robot driving. While various trajectory datasets exist, effectively utilizing them for a target domain remains challenging due to differences in agent interactions and environmental characteristics. Conventional approaches, such as domain adaptation or ensemble learning, leverage multiple source datasets but suffer from domain imbalance, catastrophic forgetting, and high computational costs. To address these challenges, we propose Interaction-Merged Motion Planning (IMMP), a novel approach that leverages parameter checkpoints trained on different domains during adaptation to the target domain. IMMP follows a two-step process: pre-merging to capture agent behaviors and interactions, sufficiently extracting diverse information from the source domain, followed by merging to construct an adaptable model that efficiently transfers diverse interactions to the target domain. Our method is evaluated on various planning benchmarks and models, demonstrating superior performance compared to conventional approaches.
>
---
#### [new 246] Piggyback Camera: Easy-to-Deploy Visual Surveillance by Mobile Sensing on Commercial Robot Vacuums
- **分类: cs.RO; cs.CV**

- **简介: 该论文属于视觉监控任务，解决机器人定位与环境建模问题。通过在扫地机器人上安装手机，利用神经惯性导航和数据增强方法实现精准定位与物体映射。**

- **链接: [http://arxiv.org/pdf/2507.04910v1](http://arxiv.org/pdf/2507.04910v1)**

> **作者:** Ryo Yonetani
>
> **摘要:** This paper presents Piggyback Camera, an easy-to-deploy system for visual surveillance using commercial robot vacuums. Rather than requiring access to internal robot systems, our approach mounts a smartphone equipped with a camera and Inertial Measurement Unit (IMU) on the robot, making it applicable to any commercial robot without hardware modifications. The system estimates robot poses through neural inertial navigation and efficiently captures images at regular spatial intervals throughout the cleaning task. We develop a novel test-time data augmentation method called Rotation-Augmented Ensemble (RAE) to mitigate domain gaps in neural inertial navigation. A loop closure method that exploits robot cleaning patterns further refines these estimated poses. We demonstrate the system with an object mapping application that analyzes captured images to geo-localize objects in the environment. Experimental evaluation in retail environments shows that our approach achieves 0.83 m relative pose error for robot localization and 0.97 m positional error for object mapping of over 100 items.
>
---
#### [new 247] Cancer cytoplasm segmentation in hyperspectral cell image with data augmentation
- **分类: eess.IV; cs.CV; physics.med-ph**

- **简介: 该论文属于医学图像分割任务，旨在解决癌症细胞质准确识别的问题。通过数据增强方法提升深度学习模型在高光谱图像上的性能。**

- **链接: [http://arxiv.org/pdf/2507.03325v1](http://arxiv.org/pdf/2507.03325v1)**

> **作者:** Rebeka Sultana; Hibiki Horibe; Tomoaki Murakami; Ikuko Shimizu
>
> **摘要:** Hematoxylin and Eosin (H&E)-stained images are commonly used to detect nuclear or cancerous regions in cells from images captured by a microscope. Identifying cancer cytoplasm is crucial for determining the type of cancer; hence, obtaining accurate cancer cytoplasm regions in cell images is important. While CMOS images often lack detailed information necessary for diagnosis, hyperspectral images provide more comprehensive cell information. Using a deep learning model, we propose a method for detecting cancer cell cytoplasm in hyperspectral images. Deep learning models require large datasets for learning; however, capturing a large number of hyperspectral images is difficult. Additionally, hyperspectral images frequently contain instrumental noise, depending on the characteristics of the imaging devices. We propose a data augmentation method to account for instrumental noise. CMOS images were used for data augmentation owing to their visual clarity, which facilitates manual annotation compared to original hyperspectral images. Experimental results demonstrate the effectiveness of the proposed data augmentation method both quantitatively and qualitatively.
>
---
#### [new 248] 3D PixBrush: Image-Guided Local Texture Synthesis
- **分类: cs.GR; cs.CV**

- **简介: 该论文属于3D纹理合成任务，解决如何在3D网格上精准生成与参考图像一致的局部纹理问题。提出3D PixBrush方法，无需用户输入即可自动预测定位掩码和纹理。**

- **链接: [http://arxiv.org/pdf/2507.03731v1](http://arxiv.org/pdf/2507.03731v1)**

> **作者:** Dale Decatur; Itai Lang; Kfir Aberman; Rana Hanocka
>
> **摘要:** We present 3D PixBrush, a method for performing image-driven edits of local regions on 3D meshes. 3D PixBrush predicts a localization mask and a synthesized texture that faithfully portray the object in the reference image. Our predicted localizations are both globally coherent and locally precise. Globally - our method contextualizes the object in the reference image and automatically positions it onto the input mesh. Locally - our method produces masks that conform to the geometry of the reference image. Notably, our method does not require any user input (in the form of scribbles or bounding boxes) to achieve accurate localizations. Instead, our method predicts a localization mask on the 3D mesh from scratch. To achieve this, we propose a modification to the score distillation sampling technique which incorporates both the predicted localization and the reference image, referred to as localization-modulated image guidance. We demonstrate the effectiveness of our proposed technique on a wide variety of meshes and images.
>
---
#### [new 249] EvRWKV: A RWKV Framework for Effective Event-guided Low-Light Image Enhancement
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于低光图像增强任务，解决噪声大、细节丢失问题，提出EvRWKV框架融合事件流与图像，提升低光场景下图像质量。**

- **链接: [http://arxiv.org/pdf/2507.03184v1](http://arxiv.org/pdf/2507.03184v1)**

> **作者:** WenJie Cai; Qingguo Meng; Zhenyu Wang; Xingbo Dong; Zhe Jin
>
> **摘要:** Capturing high-quality visual content under low-light conditions remains a challenging problem due to severe noise, motion blur, and underexposure, which degrade the performance of downstream applications. Traditional frame-based low-light enhancement methods often amplify noise or fail to preserve structural details, especially in real-world scenarios. Event cameras, offering high dynamic range and microsecond temporal resolution by asynchronously capturing brightness changes, emerge as promising alternatives for low-light imaging. However, existing event-image fusion methods suffer from simplistic fusion strategies and inadequate handling of spatial-temporal misalignment and noise. To address these challenges, we propose EvRWKV, a novel framework that enables continuous cross-modal interaction through dual-domain processing. Our approach incorporates a Cross-RWKV module, leveraging the Receptance Weighted Key Value (RWKV) architecture for fine-grained temporal and cross-modal fusion, and an Event Image Spectral Fusion Enhancer (EISFE) module, which jointly performs adaptive frequency-domain noise suppression and spatial-domain deformable convolution alignment. Extensive qualitative and quantitative evaluations on real-world low-light datasets(SDE, SDSD, RELED) demonstrate that EvRWKV achieves state-of-the-art performance, effectively enhancing image quality by suppressing noise, restoring structural details, and improving visual clarity in challenging low-light conditions.
>
---
#### [new 250] Sequential Attention-based Sampling for Histopathological Analysis
- **分类: eess.IV; cs.AI; cs.CV**

- **简介: 该论文属于病理图像分析任务，解决高分辨率全切片图像计算成本高的问题。提出SASHA模型，通过注意力机制高效采样，实现快速准确诊断。**

- **链接: [http://arxiv.org/pdf/2507.05077v1](http://arxiv.org/pdf/2507.05077v1)**

> **作者:** Tarun G; Naman Malpani; Gugan Thoppe; Sridharan Devarajan
>
> **摘要:** Deep neural networks are increasingly applied for automated histopathology. Yet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering it computationally infeasible to analyze them entirely at high resolution. Diagnostic labels are largely available only at the slide-level, because expert annotation of images at a finer (patch) level is both laborious and expensive. Moreover, regions with diagnostic information typically occupy only a small fraction of the WSI, making it inefficient to examine the entire slide at full resolution. Here, we propose SASHA -- {\it S}equential {\it A}ttention-based {\it S}ampling for {\it H}istopathological {\it A}nalysis -- a deep reinforcement learning approach for efficient analysis of histopathological images. First, SASHA learns informative features with a lightweight hierarchical, attention-based multiple instance learning (MIL) model. Second, SASHA samples intelligently and zooms selectively into a small fraction (10-20\%) of high-resolution patches, to achieve reliable diagnosis. We show that SASHA matches state-of-the-art methods that analyze the WSI fully at high-resolution, albeit at a fraction of their computational and memory costs. In addition, it significantly outperforms competing, sparse sampling methods. We propose SASHA as an intelligent sampling model for medical imaging challenges that involve automated diagnosis with exceptionally large images containing sparsely informative features.
>
---
#### [new 251] MoDA: Multi-modal Diffusion Architecture for Talking Head Generation
- **分类: cs.GR; cs.CV**

- **简介: 该论文属于Talking Head生成任务，解决身份与语音驱动的面部动画生成问题。提出MoDA架构，通过多模态融合提升生成效果与效率。**

- **链接: [http://arxiv.org/pdf/2507.03256v1](http://arxiv.org/pdf/2507.03256v1)**

> **作者:** Xinyang Li; Gen Li; Zhihui Lin; Yichen Qian; GongXin Yao; Weinan Jia; Weihua Chen; Fan Wang
>
> **备注:** 12 pages, 7 figures
>
> **摘要:** Talking head generation with arbitrary identities and speech audio remains a crucial problem in the realm of digital humans and the virtual metaverse. Recently, diffusion models have become a popular generative technique in this field with their strong generation and generalization capabilities. However, several challenges remain for diffusion-based methods: 1) inefficient inference and visual artifacts, which arise from the implicit latent space of Variational Auto-Encoders (VAE), complicating the diffusion process; 2) authentic facial expressions and head movements, resulting from insufficient multi-modal information interaction. In this paper, MoDA handle these challenges by 1) defines a joint parameter space to bridge motion generation and neural rendering, and leverages flow matching to simplify the diffusion learning process; 2) introduces a multi-modal diffusion architecture to model the interaction among noisy motion, audio, and auxiliary conditions, ultimately enhancing overall facial expressiveness. Subsequently, a coarse-to-fine fusion strategy is adopted to progressively integrate different modalities, ensuring effective integration across feature spaces. Experimental results demonstrate that MoDA significantly improves video diversity, realism, and efficiency, making it suitable for real-world applications.
>
---
#### [new 252] ViTaL: A Multimodality Dataset and Benchmark for Multi-pathological Ovarian Tumor Recognition
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于多模态卵巢肿瘤分类任务，旨在解决数据不足和多病理分类问题。构建了ViTaL数据集并提出ViTaL-Net模型，实现高精度分类。**

- **链接: [http://arxiv.org/pdf/2507.04383v1](http://arxiv.org/pdf/2507.04383v1)**

> **作者:** You Zhou; Lijiang Chen; Guangxia Cui; Wenpei Bai; Yu Guo; Shuchang Lyu; Guangliang Cheng; Qi Zhao
>
> **摘要:** Ovarian tumor, as a common gynecological disease, can rapidly deteriorate into serious health crises when undetected early, thus posing significant threats to the health of women. Deep neural networks have the potential to identify ovarian tumors, thereby reducing mortality rates, but limited public datasets hinder its progress. To address this gap, we introduce a vital ovarian tumor pathological recognition dataset called \textbf{ViTaL} that contains \textbf{V}isual, \textbf{T}abular and \textbf{L}inguistic modality data of 496 patients across six pathological categories. The ViTaL dataset comprises three subsets corresponding to different patient data modalities: visual data from 2216 two-dimensional ultrasound images, tabular data from medical examinations of 496 patients, and linguistic data from ultrasound reports of 496 patients. It is insufficient to merely distinguish between benign and malignant ovarian tumors in clinical practice. To enable multi-pathology classification of ovarian tumor, we propose a ViTaL-Net based on the Triplet Hierarchical Offset Attention Mechanism (THOAM) to minimize the loss incurred during feature fusion of multi-modal data. This mechanism could effectively enhance the relevance and complementarity between information from different modalities. ViTaL-Net serves as a benchmark for the task of multi-pathology, multi-modality classification of ovarian tumors. In our comprehensive experiments, the proposed method exhibited satisfactory performance, achieving accuracies exceeding 90\% on the two most common pathological types of ovarian tumor and an overall performance of 85\%. Our dataset and code are available at https://github.com/GGbond-study/vitalnet.
>
---
#### [new 253] FB-Diff: Fourier Basis-guided Diffusion for Temporal Interpolation of 4D Medical Imaging
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于4D医学影像的时间插值任务，旨在解决呼吸运动的非线性与准周期性问题，提出FB-Diff模型，利用傅里叶基和生理先验提升插值效果。**

- **链接: [http://arxiv.org/pdf/2507.04547v1](http://arxiv.org/pdf/2507.04547v1)**

> **作者:** Xin You; Runze Yang; Chuyan Zhang; Zhongliang Jiang; Jie Yang; Nassir Navab
>
> **备注:** Accepted by ICCV 2025
>
> **摘要:** The temporal interpolation task for 4D medical imaging, plays a crucial role in clinical practice of respiratory motion modeling. Following the simplified linear-motion hypothesis, existing approaches adopt optical flow-based models to interpolate intermediate frames. However, realistic respiratory motions should be nonlinear and quasi-periodic with specific frequencies. Intuited by this property, we resolve the temporal interpolation task from the frequency perspective, and propose a Fourier basis-guided Diffusion model, termed FB-Diff. Specifically, due to the regular motion discipline of respiration, physiological motion priors are introduced to describe general characteristics of temporal data distributions. Then a Fourier motion operator is elaborately devised to extract Fourier bases by incorporating physiological motion priors and case-specific spectral information in the feature space of Variational Autoencoder. Well-learned Fourier bases can better simulate respiratory motions with motion patterns of specific frequencies. Conditioned on starting and ending frames, the diffusion model further leverages well-learned Fourier bases via the basis interaction operator, which promotes the temporal interpolation task in a generative manner. Extensive results demonstrate that FB-Diff achieves state-of-the-art (SOTA) perceptual performance with better temporal consistency while maintaining promising reconstruction metrics. Codes are available.
>
---
#### [new 254] SPATIA: Multimodal Model for Prediction and Generation of Spatial Cell Phenotypes
- **分类: q-bio.QM; cs.AI; cs.CV**

- **简介: 该论文提出SPATIA模型，解决空间转录组学中细胞表型预测与生成问题，整合细胞形态、基因表达和空间信息，提升多模态数据分析性能。**

- **链接: [http://arxiv.org/pdf/2507.04704v1](http://arxiv.org/pdf/2507.04704v1)**

> **作者:** Zhenglun Kong; Mufan Qiu; John Boesen; Xiang Lin; Sukwon Yun; Tianlong Chen; Manolis Kellis; Marinka Zitnik
>
> **摘要:** Understanding how cellular morphology, gene expression, and spatial organization jointly shape tissue function is a central challenge in biology. Image-based spatial transcriptomics technologies now provide high-resolution measurements of cell images and gene expression profiles, but machine learning methods typically analyze these modalities in isolation or at limited resolution. We address the problem of learning unified, spatially aware representations that integrate cell morphology, gene expression, and spatial context across biological scales. This requires models that can operate at single-cell resolution, reason across spatial neighborhoods, and generalize to whole-slide tissue organization. Here, we introduce SPATIA, a multi-scale generative and predictive model for spatial transcriptomics. SPATIA learns cell-level embeddings by fusing image-derived morphological tokens and transcriptomic vector tokens using cross-attention and then aggregates them at niche and tissue levels using transformer modules to capture spatial dependencies. SPATIA incorporates token merging in its generative diffusion decoder to synthesize high-resolution cell images conditioned on gene expression. We assembled a multi-scale dataset consisting of 17 million cell-gene pairs, 1 million niche-gene pairs, and 10,000 tissue-gene pairs across 49 donors, 17 tissue types, and 12 disease states. We benchmark SPATIA against 13 existing models across 12 individual tasks, which span several categories including cell annotation, cell clustering, gene imputation, cross-modal prediction, and image generation. SPATIA achieves improved performance over all baselines and generates realistic cell morphologies that reflect transcriptomic perturbations.
>
---
#### [new 255] Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via Self-Evolving Distillation
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文属于视觉语言模型任务，旨在解决 hallucinations 问题。通过自进化蒸馏方法 SEED，识别并净化模型中的幻觉知识，提升模型可靠性。**

- **链接: [http://arxiv.org/pdf/2507.04680v1](http://arxiv.org/pdf/2507.04680v1)**

> **作者:** Wenhao Li; Xiu Su; Jingyi Wu; Feng Yang; Yang Liu; Yi Chen; Shan You; Chang Xu
>
> **摘要:** Large Vision-Language Models (LVLMs) have demonstrated remarkable advancements in numerous areas such as multimedia. However, hallucination issues significantly limit their credibility and application potential. Existing mitigation methods typically rely on external tools or the comparison of multi-round inference, which significantly increase inference time. In this paper, we propose \textbf{SE}lf-\textbf{E}volving \textbf{D}istillation (\textbf{SEED}), which identifies hallucinations within the inner knowledge of LVLMs, isolates and purges them, and then distills the purified knowledge back into the model, enabling self-evolution. Furthermore, we identified that traditional distillation methods are prone to inducing void spaces in the output space of LVLMs. To address this issue, we propose a Mode-Seeking Evolving approach, which performs distillation to capture the dominant modes of the purified knowledge distribution, thereby avoiding the chaotic results that could emerge from void spaces. Moreover, we introduce a Hallucination Elimination Adapter, which corrects the dark knowledge of the original model by learning purified knowledge. Extensive experiments on multiple benchmarks validate the superiority of our SEED, demonstrating substantial improvements in mitigating hallucinations for representative LVLM models such as LLaVA-1.5 and InternVL2. Remarkably, the F1 score of LLaVA-1.5 on the hallucination evaluation metric POPE-Random improved from 81.3 to 88.3.
>
---
#### [new 256] Uncovering Neuroimaging Biomarkers of Brain Tumor Surgery with AI-Driven Methods
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于脑肿瘤手术预后预测任务，旨在通过AI方法识别神经影像生物标志物，提升手术决策的精准性与可解释性。**

- **链接: [http://arxiv.org/pdf/2507.04881v1](http://arxiv.org/pdf/2507.04881v1)**

> **作者:** Carmen Jimenez-Mesa; Yizhou Wan; Guilio Sansone; Francisco J. Martinez-Murcia; Javier Ramirez; Pietro Lio; Juan M. Gorriz; Stephen J. Price; John Suckling; Michail Mamalakis
>
> **摘要:** Brain tumor resection is a complex procedure with significant implications for patient survival and quality of life. Predictions of patient outcomes provide clinicians and patients the opportunity to select the most suitable onco-functional balance. In this study, global features derived from structural magnetic resonance imaging in a clinical dataset of 49 pre- and post-surgery patients identified potential biomarkers associated with survival outcomes. We propose a framework that integrates Explainable AI (XAI) with neuroimaging-based feature engineering for survival assessment, offering guidance for surgical decision-making. In this study, we introduce a global explanation optimizer that refines survival-related feature attribution in deep learning models, enhancing interpretability and reliability. Our findings suggest that survival is influenced by alterations in regions associated with cognitive and sensory functions, indicating the importance of preserving areas involved in decision-making and emotional regulation during surgery to improve outcomes. The global explanation optimizer improves both fidelity and comprehensibility of explanations compared to state-of-the-art XAI methods. It effectively identifies survival-related variability, underscoring its relevance in precision medicine for brain tumor treatment.
>
---
#### [new 257] SPIDER: Structure-Preferential Implicit Deep Network for Biplanar X-ray Reconstruction
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于医学影像重建任务，旨在解决从双平面X光图像重建准确CT体积的问题。通过引入结构先验和联合监督，提升重建精度与 anatomical 合理性。**

- **链接: [http://arxiv.org/pdf/2507.04684v1](http://arxiv.org/pdf/2507.04684v1)**

> **作者:** Tianqi Yu; Xuanyu Tian; Jiawen Yang; Dongming He; Jingyi Yu; Xudong Wang; Yuyao Zhang
>
> **摘要:** Biplanar X-ray imaging is widely used in health screening, postoperative rehabilitation evaluation of orthopedic diseases, and injury surgery due to its rapid acquisition, low radiation dose, and straightforward setup. However, 3D volume reconstruction from only two orthogonal projections represents a profoundly ill-posed inverse problem, owing to the intrinsic lack of depth information and irreducible ambiguities in soft-tissue visualization. Some existing methods can reconstruct skeletal structures and Computed Tomography (CT) volumes, they often yield incomplete bone geometry, imprecise tissue boundaries, and a lack of anatomical realism, thereby limiting their clinical utility in scenarios such as surgical planning and postoperative assessment. In this study, we introduce SPIDER, a novel supervised framework designed to reconstruct CT volumes from biplanar X-ray images. SPIDER incorporates tissue structure as prior (e.g., anatomical segmentation) into an implicit neural representation decoder in the form of joint supervision through a unified encoder-decoder architecture. This design enables the model to jointly learn image intensities and anatomical structures in a pixel-aligned fashion. To address the challenges posed by sparse input and structural ambiguity, SPIDER directly embeds anatomical constraints into the reconstruction process, thereby enhancing structural continuity and reducing soft-tissue artifacts. We conduct comprehensive experiments on clinical head CT datasets and show that SPIDER generates anatomically accurate reconstructions from only two projections. Furthermore, our approach demonstrates strong potential in downstream segmentation tasks, underscoring its utility in personalized treatment planning and image-guided surgical navigation.
>
---
#### [new 258] Information-Guided Diffusion Sampling for Dataset Distillation
- **分类: cs.LG; cs.AI; cs.CV; cs.IT; math.IT**

- **简介: 该论文属于数据集蒸馏任务，解决低样本每类（IPC）下生成样本多样性不足的问题。通过信息理论框架优化扩散采样过程，提升数据集质量。**

- **链接: [http://arxiv.org/pdf/2507.04619v1](http://arxiv.org/pdf/2507.04619v1)**

> **作者:** Linfeng Ye; Shayan Mohajer Hamidi; Guang Li; Takahiro Ogawa; Miki Haseyama; Konstantinos N. Plataniotis
>
> **摘要:** Dataset distillation aims to create a compact dataset that retains essential information while maintaining model performance. Diffusion models (DMs) have shown promise for this task but struggle in low images-per-class (IPC) settings, where generated samples lack diversity. In this paper, we address this issue from an information-theoretic perspective by identifying two key types of information that a distilled dataset must preserve: ($i$) prototype information $\mathrm{I}(X;Y)$, which captures label-relevant features; and ($ii$) contextual information $\mathrm{H}(X | Y)$, which preserves intra-class variability. Here, $(X,Y)$ represents the pair of random variables corresponding to the input data and its ground truth label, respectively. Observing that the required contextual information scales with IPC, we propose maximizing $\mathrm{I}(X;Y) + \beta \mathrm{H}(X | Y)$ during the DM sampling process, where $\beta$ is IPC-dependent. Since directly computing $\mathrm{I}(X;Y)$ and $\mathrm{H}(X | Y)$ is intractable, we develop variational estimations to tightly lower-bound these quantities via a data-driven approach. Our approach, information-guided diffusion sampling (IGDS), seamlessly integrates with diffusion models and improves dataset distillation across all IPC settings. Experiments on Tiny ImageNet and ImageNet subsets show that IGDS significantly outperforms existing methods, particularly in low-IPC regimes. The code will be released upon acceptance.
>
---
#### [new 259] Outcome prediction and individualized treatment effect estimation in patients with large vessel occlusion stroke
- **分类: eess.IV; cs.CV; cs.LG; 68; I.2; J.3**

- **简介: 该论文属于医疗预测任务，旨在预测大血管闭塞卒中患者的预后并估计个体治疗效果。通过整合临床数据和影像信息，构建深度学习模型进行功能结局预测与治疗效果评估。**

- **链接: [http://arxiv.org/pdf/2507.03046v1](http://arxiv.org/pdf/2507.03046v1)**

> **作者:** Lisa Herzog; Pascal Bühler; Ezequiel de la Rosa; Beate Sick; Susanne Wegener
>
> **备注:** Under review for SWITCH 2025 (MICCAI)
>
> **摘要:** Mechanical thrombectomy has become the standard of care in patients with stroke due to large vessel occlusion (LVO). However, only 50% of successfully treated patients show a favorable outcome. We developed and evaluated interpretable deep learning models to predict functional outcomes in terms of the modified Rankin Scale score alongside individualized treatment effects (ITEs) using data of 449 LVO stroke patients from a randomized clinical trial. Besides clinical variables, we considered non-contrast CT (NCCT) and angiography (CTA) scans which were integrated using novel foundation models to make use of advanced imaging information. Clinical variables had a good predictive power for binary functional outcome prediction (AUC of 0.719 [0.666, 0.774]) which could slightly be improved when adding CTA imaging (AUC of 0.737 [0.687, 0.795]). Adding NCCT scans or a combination of NCCT and CTA scans to clinical features yielded no improvement. The most important clinical predictor for functional outcome was pre-stroke disability. While estimated ITEs were well calibrated to the average treatment effect, discriminatory ability was limited indicated by a C-for-Benefit statistic of around 0.55 in all models. In summary, the models allowed us to jointly integrate CT imaging and clinical features while achieving state-of-the-art prediction performance and ITE estimates. Yet, further research is needed to particularly improve ITE estimation.
>
---
#### [new 260] Adopting a human developmental visual diet yields robust, shape-based AI vision
- **分类: cs.LG; cs.CV**

- **简介: 该论文属于计算机视觉任务，旨在解决AI与人类视觉的差异问题。通过模仿人类视觉发展过程，构建了发育视觉饮食，提升AI的鲁棒性和形状识别能力。**

- **链接: [http://arxiv.org/pdf/2507.03168v1](http://arxiv.org/pdf/2507.03168v1)**

> **作者:** Zejin Lu; Sushrut Thorat; Radoslaw M Cichy; Tim C Kietzmann
>
> **摘要:** Despite years of research and the dramatic scaling of artificial intelligence (AI) systems, a striking misalignment between artificial and human vision persists. Contrary to humans, AI heavily relies on texture-features rather than shape information, lacks robustness to image distortions, remains highly vulnerable to adversarial attacks, and struggles to recognise simple abstract shapes within complex backgrounds. To close this gap, we here introduce a solution that arises from a previously underexplored direction: rather than scaling up, we take inspiration from how human vision develops from early infancy into adulthood. We quantified the visual maturation by synthesising decades of psychophysical and neurophysiological research into a novel developmental visual diet (DVD) for AI vision. We show that guiding AI systems through this human-inspired curriculum produces models that closely align with human behaviour on every hallmark of robust vision tested yielding the strongest reported reliance on shape information to date, abstract shape recognition beyond the state of the art, higher robustness to image corruptions, and stronger resilience to adversarial attacks. By outperforming high parameter AI foundation models trained on orders of magnitude more data, we provide evidence that robust AI vision can be achieved by guiding the way how a model learns, not merely how much it learns, offering a resource-efficient route toward safer and more human-like artificial visual systems.
>
---
#### [new 261] Automated Workflow for the Detection of Vugs
- **分类: physics.geo-ph; cs.CV**

- **简介: 该论文属于地质图像分析任务，旨在解决vug自动检测问题。通过计算机视觉技术实现vug的自动化识别与统计分析，提升检测效率和准确性。**

- **链接: [http://arxiv.org/pdf/2507.02988v1](http://arxiv.org/pdf/2507.02988v1)**

> **作者:** M. Quamer Nasim; T. Maiti; N. Mosavat; P. V. Grech; T. Singh; P. Nath Singha Roy
>
> **备注:** 5 pages, 3 Figures
>
> **摘要:** Image logs are crucial in capturing high-quality geological information about subsurface formations. Among the various geological features that can be gleaned from Formation Micro Imager log, vugs are essential for reservoir evaluation. This paper introduces an automated Vug Detection Model, leveraging advanced computer vision techniques to streamline the vug identification process. Manual and semiautomated methods are limited by individual bias, labour-intensity and inflexibility in parameter finetuning. Our methodology also introduces statistical analysis on vug characteristics. Pre-processing steps, including logical file extraction and normalization, ensured standardized and usable data. The sixstep vug identification methodology encompasses top-k mode extraction, adaptive thresholding, contour identification, aggregation, advanced filtering, and optional filtering for low vuggy regions. The model's adaptability is evidenced by its ability to identify vugs missed by manual picking undertaken by experts. Results demonstrate the model's accuracy through validation against expert picks. Detailed metrics, such as count, mean, and standard deviation of vug areas within zones, were introduced, showcasing the model's capabilities compared to manual picking. The vug area distribution plot enhances understanding of vug types in the reservoir. This research focuses on the identification and characterization of vugs that in turn aids in the better understanding of reservoirs.
>
---
#### [new 262] NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving
- **分类: cs.RO; cs.CV; cs.LG; cs.MM; cs.SY; eess.SY**

- **简介: 该论文属于自动驾驶任务，旨在解决局部感知与全局导航信息融合不足的问题。提出NavigScene数据集及三种方法，提升模型在复杂环境中的推理与泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.05227v1](http://arxiv.org/pdf/2507.05227v1)**

> **作者:** Qucheng Peng; Chen Bai; Guoxiang Zhang; Bo Xu; Xiaotong Liu; Xiaoyin Zheng; Chen Chen; Cheng Lu
>
> **备注:** Accepted by ACM Multimedia 2025
>
> **摘要:** Autonomous driving systems have made significant advances in Q&A, perception, prediction, and planning based on local visual information, yet they struggle to incorporate broader navigational context that human drivers routinely utilize. We address this critical gap between local sensor data and global navigation information by proposing NavigScene, an auxiliary navigation-guided natural language dataset that simulates a human-like driving environment within autonomous driving systems. Moreover, we develop three complementary paradigms to leverage NavigScene: (1) Navigation-guided Reasoning, which enhances vision-language models by incorporating navigation context into the prompting approach; (2) Navigation-guided Preference Optimization, a reinforcement learning method that extends Direct Preference Optimization to improve vision-language model responses by establishing preferences for navigation-relevant summarized information; and (3) Navigation-guided Vision-Language-Action model, which integrates navigation guidance and vision-language models with conventional driving models through feature fusion. Extensive experiments demonstrate that our approaches significantly improve performance across perception, prediction, planning, and question-answering tasks by enabling reasoning capabilities beyond visual range and improving generalization to diverse driving scenarios. This work represents a significant step toward more comprehensive autonomous driving systems capable of navigating complex, unfamiliar environments with greater reliability and safety.
>
---
#### [new 263] Dynamic Frequency Feature Fusion Network for Multi-Source Remote Sensing Data Classification
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于多源遥感数据分类任务，旨在解决频率域特征建模适应性差的问题。提出DFFNet网络，通过动态滤波和跨模态融合提升分类性能。**

- **链接: [http://arxiv.org/pdf/2507.04510v1](http://arxiv.org/pdf/2507.04510v1)**

> **作者:** Yikang Zhao; Feng Gao; Xuepeng Jin; Junyu Dong; Qian Du
>
> **备注:** Accepted by IEEE GRSL
>
> **摘要:** Multi-source data classification is a critical yet challenging task for remote sensing image interpretation. Existing methods lack adaptability to diverse land cover types when modeling frequency domain features. To this end, we propose a Dynamic Frequency Feature Fusion Network (DFFNet) for hyperspectral image (HSI) and Synthetic Aperture Radar (SAR) / Light Detection and Ranging (LiDAR) data joint classification. Specifically, we design a dynamic filter block to dynamically learn the filter kernels in the frequency domain by aggregating the input features. The frequency contextual knowledge is injected into frequency filter kernels. Additionally, we propose spectral-spatial adaptive fusion block for cross-modal feature fusion. It enhances the spectral and spatial attention weight interactions via channel shuffle operation, thereby providing comprehensive cross-modal feature fusion. Experiments on two benchmark datasets show that our DFFNet outperforms state-of-the-art methods in multi-source data classification. The codes will be made publicly available at https://github.com/oucailab/DFFNet.
>
---
#### [new 264] Bridging KAN and MLP: MJKAN, a Hybrid Architecture with Both Efficiency and Expressiveness
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文属于神经网络架构设计任务，旨在解决KAN计算成本高和MLP表达力不足的问题，提出MJKAN混合结构以提升效率与表现。**

- **链接: [http://arxiv.org/pdf/2507.04690v1](http://arxiv.org/pdf/2507.04690v1)**

> **作者:** Hanseon Joo; Hayoung Choi; Ook Lee; Minjong Cheon
>
> **摘要:** Kolmogorov-Arnold Networks (KANs) have garnered attention for replacing fixed activation functions with learnable univariate functions, but they exhibit practical limitations, including high computational costs and performance deficits in general classification tasks. In this paper, we propose the Modulation Joint KAN (MJKAN), a novel neural network layer designed to overcome these challenges. MJKAN integrates a FiLM (Feature-wise Linear Modulation)-like mechanism with Radial Basis Function (RBF) activations, creating a hybrid architecture that combines the non-linear expressive power of KANs with the efficiency of Multilayer Perceptrons (MLPs). We empirically validated MJKAN's performance across a diverse set of benchmarks, including function regression, image classification (MNIST, CIFAR-10/100), and natural language processing (AG News, SMS Spam). The results demonstrate that MJKAN achieves superior approximation capabilities in function regression tasks, significantly outperforming MLPs, with performance improving as the number of basis functions increases. Conversely, in image and text classification, its performance was competitive with MLPs but revealed a critical dependency on the number of basis functions. We found that a smaller basis size was crucial for better generalization, highlighting that the model's capacity must be carefully tuned to the complexity of the data to prevent overfitting. In conclusion, MJKAN offers a flexible architecture that inherits the theoretical advantages of KANs while improving computational efficiency and practical viability.
>
---
#### [new 265] Hybrid-View Attention for csPCa Classification in TRUS
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于前列腺癌分类任务，旨在解决TRUS图像诊断困难的问题。提出混合视图注意力网络，融合多视角信息提升分类性能。**

- **链接: [http://arxiv.org/pdf/2507.03421v1](http://arxiv.org/pdf/2507.03421v1)**

> **作者:** Zetian Feng; Juan Fu; Xuebin Zou; Hongsheng Ye; Hong Wu; Jianhua Zhou; Yi Wang
>
> **摘要:** Prostate cancer (PCa) is a leading cause of cancer-related mortality in men, and accurate identification of clinically significant PCa (csPCa) is critical for timely intervention. Transrectal ultrasound (TRUS) is widely used for prostate biopsy; however, its low contrast and anisotropic spatial resolution pose diagnostic challenges. To address these limitations, we propose a novel hybrid-view attention (HVA) network for csPCa classification in 3D TRUS that leverages complementary information from transverse and sagittal views. Our approach integrates a CNN-transformer hybrid architecture, where convolutional layers extract fine-grained local features and transformer-based HVA models global dependencies. Specifically, the HVA comprises intra-view attention to refine features within a single view and cross-view attention to incorporate complementary information across views. Furthermore, a hybrid-view adaptive fusion module dynamically aggregates features along both channel and spatial dimensions, enhancing the overall representation. Experiments are conducted on an in-house dataset containing 590 subjects who underwent prostate biopsy. Comparative and ablation results prove the efficacy of our method. The code is available at https://github.com/mock1ngbrd/HVAN.
>
---
#### [new 266] An HTR-LLM Workflow for High-Accuracy Transcription and Analysis of Abbreviated Latin Court Hand
- **分类: cs.DL; cs.CL; cs.CV**

- **简介: 该论文属于历史文献处理任务，旨在解决中世纪法律文档转录难题。通过四阶段HTR-LLM工作流，提升转录准确性和分析质量。**

- **链接: [http://arxiv.org/pdf/2507.04132v1](http://arxiv.org/pdf/2507.04132v1)**

> **作者:** Joshua D. Isom
>
> **摘要:** This article presents and validates an ideal, four-stage workflow for the high-accuracy transcription and analysis of challenging medieval legal documents. The process begins with a specialized Handwritten Text Recognition (HTR) model, itself created using a novel "Clean Ground Truth" curation method where a Large Language Model (LLM) refines the training data. This HTR model provides a robust baseline transcription (Stage 1). In Stage 2, this baseline is fed, along with the original document image, to an LLM for multimodal post-correction, grounding the LLM's analysis and improving accuracy. The corrected, abbreviated text is then expanded into full, scholarly Latin using a prompt-guided LLM (Stage 3). A final LLM pass performs Named-Entity Correction (NEC), regularizing proper nouns and generating plausible alternatives for ambiguous readings (Stage 4). We validate this workflow through detailed case studies, achieving Word Error Rates (WER) in the range of 2-7% against scholarly ground truths. The results demonstrate that this hybrid, multi-stage approach effectively automates the most laborious aspects of transcription while producing a high-quality, analyzable output, representing a powerful and practical solution for the current technological landscape.
>
---
#### [new 267] When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need
- **分类: cs.LG; cs.AI; cs.CR; cs.CV**

- **简介: 该论文属于知识蒸馏任务，解决非可迁移教师导致的数据无关知识蒸馏鲁棒性问题。通过识别并过滤OOD样本提升蒸馏效果。**

- **链接: [http://arxiv.org/pdf/2507.04119v1](http://arxiv.org/pdf/2507.04119v1)**

> **作者:** Ziming Hong; Runnan Chen; Zengmao Wang; Bo Han; Bo Du; Tongliang Liu
>
> **备注:** Accepted by ICML 2025
>
> **摘要:** Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexplored. In this work, we conduct the first investigation into distilling non-transferable learning (NTL) teachers using DFKD, where the transferability from an ID domain to an out-of-distribution (OOD) domain is prohibited. We find that NTL teachers fool DFKD through divert the generator's attention from the useful ID knowledge to the misleading OOD knowledge. This hinders ID knowledge transfer but prioritizes OOD knowledge transfer. To mitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit DFKD by identifying and filtering out OOD-like synthetic samples. Specifically, inspired by the evidence that NTL teachers show stronger adversarial robustness on OOD samples than ID samples, we split synthetic samples into two groups according to their robustness. The fragile group is treated as ID-like data and used for normal knowledge distillation, while the robust group is seen as OOD-like data and utilized for forgetting OOD knowledge. Extensive experiments demonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers. Code is released at https://github.com/tmllab/2025_ICML_ATEsc.
>
---
#### [new 268] Inverse Synthetic Aperture Fourier Ptychography
- **分类: eess.IV; cs.CV; cs.LG**

- **简介: 该论文属于成像技术领域，解决FP系统中测量多样性获取成本高的问题，通过目标运动生成多样性并提出学习方法估计k空间坐标，实现无需已知目标旋转的合成孔径成像。**

- **链接: [http://arxiv.org/pdf/2507.03733v1](http://arxiv.org/pdf/2507.03733v1)**

> **作者:** Matthew A. Chan; Casey J. Pellizzari; Christopher A. Metzler
>
> **摘要:** Fourier ptychography (FP) is a powerful light-based synthetic aperture imaging technique that allows one to reconstruct a high-resolution, wide field-of-view image by computationally integrating a diverse collection of low-resolution, far-field measurements. Typically, FP measurement diversity is introduced by changing the angle of the illumination or the position of the camera; either approach results in sampling different portions of the target's spatial frequency content, but both approaches introduce substantial costs and complexity to the acquisition process. In this work, we introduce Inverse Synthetic Aperture Fourier Ptychography, a novel approach to FP that foregoes changing the illumination angle or camera position and instead generates measurement diversity through target motion. Critically, we also introduce a novel learning-based method for estimating k-space coordinates from dual plane intensity measurements, thereby enabling synthetic aperture imaging without knowing the rotation of the target. We experimentally validate our method in simulation and on a tabletop optical system.
>
---
#### [new 269] What to Do Next? Memorizing skills from Egocentric Instructional Video
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文属于交互式动作规划任务，旨在解决从第一视角视频中学习高目标导向动作的问题。通过结合拓扑可及性记忆与Transformer模型，提升动作选择与偏差检测能力。**

- **链接: [http://arxiv.org/pdf/2507.02997v1](http://arxiv.org/pdf/2507.02997v1)**

> **作者:** Jing Bi; Chenliang Xu
>
> **摘要:** Learning to perform activities through demonstration requires extracting meaningful information about the environment from observations. In this research, we investigate the challenge of planning high-level goal-oriented actions in a simulation setting from an egocentric perspective. We present a novel task, interactive action planning, and propose an approach that combines topological affordance memory with transformer architecture. The process of memorizing the environment's structure through extracting affordances facilitates selecting appropriate actions based on the context. Moreover, the memory model allows us to detect action deviations while accomplishing specific objectives. To assess the method's versatility, we evaluate it in a realistic interactive simulation environment. Our experimental results demonstrate that the proposed approach learns meaningful representations, resulting in improved performance and robust when action deviations occur.
>
---
#### [new 270] Attributing Data for Sharpness-Aware Minimization
- **分类: cs.LG; cs.AI; cs.CV; stat.ML**

- **简介: 该论文属于模型训练任务，解决SAM中数据贡献评估问题。提出两种数据估值方法，提升数据评价与模型优化效果。**

- **链接: [http://arxiv.org/pdf/2507.04059v1](http://arxiv.org/pdf/2507.04059v1)**

> **作者:** Chenyang Ren; Yifan Jia; Huanyi Xie; Zhaobin Xu; Tianxing Wei; Liangyu Wang; Lijie Hu; Di Wang
>
> **备注:** 25 pages
>
> **摘要:** Sharpness-aware Minimization (SAM) improves generalization in large-scale model training by linking loss landscape geometry to generalization. However, challenges such as mislabeled noisy data and privacy concerns have emerged as significant issues. Data attribution, which identifies the contributions of specific training samples, offers a promising solution. However, directly rendering existing data influence evaluation tools such as influence functions (IF) to SAM will be inapplicable or inaccurate as SAM utilizes an inner loop to find model perturbations that maximize loss, which the outer loop then minimizes, resulting in a doubled computational structure. Additionally, this bilevel structure complicates the modeling of data influence on the parameters. In this paper, based on the IF, we develop two innovative data valuation methods for SAM, each offering unique benefits in different scenarios: the Hessian-based IF and the Gradient Trajectory-based IF. The first one provides a comprehensive estimation of data influence using a closed-form measure that relies only on the trained model weights. In contrast, the other IF for SAM utilizes gradient trajectory information during training for more accurate and efficient data assessment. Extensive experiments demonstrate their effectiveness in data evaluation and parameter tuning, with applications in identifying mislabeled data, model editing, and enhancing interpretability.
>
---
#### [new 271] Clustering via Self-Supervised Diffusion
- **分类: cs.AI; cs.CV**

- **简介: 该论文属于无监督学习任务，旨在解决高维数据聚类问题。提出CLUDI框架，结合扩散模型与预训练视觉Transformer，实现更鲁棒的聚类效果。**

- **链接: [http://arxiv.org/pdf/2507.04283v1](http://arxiv.org/pdf/2507.04283v1)**

> **作者:** Roy Uziel; Irit Chelly; Oren Freifeld; Ari Pakman
>
> **摘要:** Diffusion models, widely recognized for their success in generative tasks, have not yet been applied to clustering. We introduce Clustering via Diffusion (CLUDI), a self-supervised framework that combines the generative power of diffusion models with pre-trained Vision Transformer features to achieve robust and accurate clustering. CLUDI is trained via a teacher-student paradigm: the teacher uses stochastic diffusion-based sampling to produce diverse cluster assignments, which the student refines into stable predictions. This stochasticity acts as a novel data augmentation strategy, enabling CLUDI to uncover intricate structures in high-dimensional data. Extensive evaluations on challenging datasets demonstrate that CLUDI achieves state-of-the-art performance in unsupervised classification, setting new benchmarks in clustering robustness and adaptability to complex data distributions.
>
---
#### [new 272] More than One Step at a Time: Designing Procedural Feedback for Non-visual Makeup Routines
- **分类: cs.HC; cs.CV**

- **简介: 该论文属于辅助技术领域，旨在解决视障人群在非视觉化妆流程中的操作困难。通过用户研究和专家访谈，提出反馈需求分类及设计建议。**

- **链接: [http://arxiv.org/pdf/2507.03942v1](http://arxiv.org/pdf/2507.03942v1)**

> **作者:** Franklin Mingzhe Li; Akihiko Oharazawa; Chloe Qingyu Zhu; Misty Fan; Daisuke Sato; Chieko Asakawa; Patrick Carrington
>
> **备注:** ASSETS 2025
>
> **摘要:** Makeup plays a vital role in self-expression, identity, and confidence - yet remains an underexplored domain for assistive technology, especially for people with vision impairments. While existing tools support isolated tasks such as color identification or product labeling, they rarely address the procedural complexity of makeup routines: coordinating step sequences, managing product placement, and assessing the final look with accessible feedback. To understand the real-world process, we conducted a contextual inquiry with 15 visually impaired makeup users, capturing real-time makeup application behaviors and their step-by-step information needs and assessment approaches. Our findings reveal embodied, tactile-first strategies; persistent challenges in blending, symmetry, and assessment; and a desire for honest, real-time, goal-aligned feedback. We also interviewed five professional makeup artists, who reviewed participant makeup videos and provided expert responses to participant-raised questions and assessment practices. We contribute a taxonomy of feedback needs in non-visual makeup, and outline design implications for future assistive systems - emphasizing hands-free, conversational interaction and context-aware, procedural support for expressive and independent beauty practices.
>
---
#### [new 273] F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding
- **分类: cs.GR; cs.CV**

- **简介: 该论文属于时间变化体积可视化任务，解决大规模数据训练慢的问题。提出F-Hash编码架构，提升收敛速度与渲染效率。**

- **链接: [http://arxiv.org/pdf/2507.03836v1](http://arxiv.org/pdf/2507.03836v1)**

> **作者:** Jianxin Sun; David Lenz; Hongfeng Yu; Tom Peterka
>
> **摘要:** Interactive time-varying volume visualization is challenging due to its complex spatiotemporal features and sheer size of the dataset. Recent works transform the original discrete time-varying volumetric data into continuous Implicit Neural Representations (INR) to address the issues of compression, rendering, and super-resolution in both spatial and temporal domains. However, training the INR takes a long time to converge, especially when handling large-scale time-varying volumetric datasets. In this work, we proposed F-Hash, a novel feature-based multi-resolution Tesseract encoding architecture to greatly enhance the convergence speed compared with existing input encoding methods for modeling time-varying volumetric data. The proposed design incorporates multi-level collision-free hash functions that map dynamic 4D multi-resolution embedding grids without bucket waste, achieving high encoding capacity with compact encoding parameters. Our encoding method is agnostic to time-varying feature detection methods, making it a unified encoding solution for feature tracking and evolution visualization. Experiments show the F-Hash achieves state-of-the-art convergence speed in training various time-varying volumetric datasets for diverse features. We also proposed an adaptive ray marching algorithm to optimize the sample streaming for faster rendering of the time-varying neural representation.
>
---
#### [new 274] Deep-Learning-Assisted Highly-Accurate COVID-19 Diagnosis on Lung Computed Tomography Images
- **分类: eess.IV; cs.AI; cs.CV**

- **简介: 该论文属于医学图像诊断任务，旨在提高新冠CT影像的准确诊断。通过数据质量控制和改进损失函数解决数据不平衡问题，模型在基准测试中表现优异。**

- **链接: [http://arxiv.org/pdf/2507.04252v1](http://arxiv.org/pdf/2507.04252v1)**

> **作者:** Yinuo Wang; Juhyun Bae; Ka Ho Chow; Shenyang Chen; Shreyash Gupta
>
> **摘要:** COVID-19 is a severe and acute viral disease that can cause symptoms consistent with pneumonia in which inflammation is caused in the alveolous regions of the lungs leading to a build-up of fluid and breathing difficulties. Thus, the diagnosis of COVID using CT scans has been effective in assisting with RT-PCR diagnosis and severity classifications. In this paper, we proposed a new data quality control pipeline to refine the quality of CT images based on GAN and sliding windows. Also, we use class-sensitive cost functions including Label Distribution Aware Loss(LDAM Loss) and Class-balanced(CB) Loss to solve the long-tail problem existing in datasets. Our model reaches more than 0.983 MCC in the benchmark test dataset.
>
---
#### [new 275] A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated Rendering in Virtual Reality
- **分类: cs.GR; cs.CV; cs.DC**

- **简介: 该论文属于虚拟现实图像渲染任务，解决实时渲染延迟问题。通过并行化眼动追踪与局部高分辨率渲染，提升效率并保持画质。**

- **链接: [http://arxiv.org/pdf/2507.04147v1](http://arxiv.org/pdf/2507.04147v1)**

> **作者:** Shuo Xin; Haiyu Wang; Sai Qian Zhang
>
> **备注:** ACM International Conference on Supercomputing 2025
>
> **摘要:** Virtual reality (VR) significantly transforms immersive digital interfaces, greatly enhancing education, professional practices, and entertainment by increasing user engagement and opening up new possibilities in various industries. Among its numerous applications, image rendering is crucial. Nevertheless, rendering methodologies like 3D Gaussian Splatting impose high computational demands, driven predominantly by user expectations for superior visual quality. This results in notable processing delays for real-time image rendering, which greatly affects the user experience. Additionally, VR devices such as head-mounted displays (HMDs) are intricately linked to human visual behavior, leveraging knowledge from perception and cognition to improve user experience. These insights have spurred the development of foveated rendering, a technique that dynamically adjusts rendering resolution based on the user's gaze direction. The resultant solution, known as gaze-tracked foveated rendering, significantly reduces the computational burden of the rendering process. Although gaze-tracked foveated rendering can reduce rendering costs, the computational overhead of the gaze tracking process itself can sometimes outweigh the rendering savings, leading to increased processing latency. To address this issue, we propose an efficient rendering framework called~\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated rendering via the parallelization of gaze tracking and foveated rendering processes. For the rendering algorithm, we utilize 3D Gaussian Splatting, a state-of-the-art neural rendering technique. Evaluation results demonstrate that A3FR can reduce end-to-end rendering latency by up to $2\times$ while maintaining visual quality.
>
---
#### [new 276] SecureT2I: No More Unauthorized Manipulation on AI Generated Images from Prompts
- **分类: cs.CR; cs.CV**

- **简介: 该论文属于图像生成安全任务，旨在防止未经授权的AI图像编辑。通过分类图像并设计不同损失函数，实现对授权与非授权图像的差异化处理。**

- **链接: [http://arxiv.org/pdf/2507.03636v1](http://arxiv.org/pdf/2507.03636v1)**

> **作者:** Xiaodong Wu; Xiangman Li; Qi Li; Jianbing Ni; Rongxing Lu
>
> **摘要:** Text-guided image manipulation with diffusion models enables flexible and precise editing based on prompts, but raises ethical and copyright concerns due to potential unauthorized modifications. To address this, we propose SecureT2I, a secure framework designed to prevent unauthorized editing in diffusion-based generative models. SecureT2I is compatible with both general-purpose and domain-specific models and can be integrated via lightweight fine-tuning without architectural changes. We categorize images into a permit set and a forbid set based on editing permissions. For the permit set, the model learns to perform high-quality manipulations as usual. For the forbid set, we introduce training objectives that encourage vague or semantically ambiguous outputs (e.g., blurred images), thereby suppressing meaningful edits. The core challenge is to block unauthorized editing while preserving editing quality for permitted inputs. To this end, we design separate loss functions that guide selective editing behavior. Extensive experiments across multiple datasets and models show that SecureT2I effectively degrades manipulation quality on forbidden images while maintaining performance on permitted ones. We also evaluate generalization to unseen inputs and find that SecureT2I consistently outperforms baselines. Additionally, we analyze different vagueness strategies and find that resize-based degradation offers the best trade-off for secure manipulation control.
>
---
#### [new 277] Consistency-Aware Padding for Incomplete Multi-Modal Alignment Clustering Based on Self-Repellent Greedy Anchor Search
- **分类: cs.LG; cs.CV; I.2.6; I.5.3**

- **简介: 该论文属于多模态数据对齐与融合任务，解决不完整、不对齐数据的填充问题。提出CAPIMAC方法，结合自排斥贪心锚点搜索和一致性感知填充模块，提升数据融合质量。**

- **链接: [http://arxiv.org/pdf/2507.03917v1](http://arxiv.org/pdf/2507.03917v1)**

> **作者:** Shubin Ma; Liang Zhao; Mingdong Lu; Yifan Guo; Bo Xu
>
> **备注:** Accepted at IJCAI 2025. 9 pages, 3 figures
>
> **摘要:** Multimodal representation is faithful and highly effective in describing real-world data samples' characteristics by describing their complementary information. However, the collected data often exhibits incomplete and misaligned characteristics due to factors such as inconsistent sensor frequencies and device malfunctions. Existing research has not effectively addressed the issue of filling missing data in scenarios where multiview data are both imbalanced and misaligned. Instead, it relies on class-level alignment of the available data. Thus, it results in some data samples not being well-matched, thereby affecting the quality of data fusion. In this paper, we propose the Consistency-Aware Padding for Incomplete Multimodal Alignment Clustering Based on Self-Repellent Greedy Anchor Search(CAPIMAC) to tackle the problem of filling imbalanced and misaligned data in multimodal datasets. Specifically, we propose a self-repellent greedy anchor search module(SRGASM), which employs a self-repellent random walk combined with a greedy algorithm to identify anchor points for re-representing incomplete and misaligned multimodal data. Subsequently, based on noise-contrastive learning, we design a consistency-aware padding module (CAPM) to effectively interpolate and align imbalanced and misaligned data, thereby improving the quality of multimodal data fusion. Experimental results demonstrate the superiority of our method over benchmark datasets. The code will be publicly released at https://github.com/Autism-mm/CAPIMAC.git.
>
---
#### [new 278] StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling
- **分类: cs.RO; cs.CV**

- **简介: 该论文属于视觉-语言导航任务，解决实时处理连续视觉流与语言指令的低延迟问题。提出StreamVLN框架，结合快慢上下文模型，提升多模态推理效率与长期一致性。**

- **链接: [http://arxiv.org/pdf/2507.05240v1](http://arxiv.org/pdf/2507.05240v1)**

> **作者:** Meng Wei; Chenyang Wan; Xiqian Yu; Tai Wang; Yuqiang Yang; Xiaohan Mao; Chenming Zhu; Wenzhe Cai; Hanqing Wang; Yilun Chen; Xihui Liu; Jiangmiao Pang
>
> **摘要:** Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{https://streamvln.github.io/}{https://streamvln.github.io/}.
>
---
#### [new 279] A Rigorous Behavior Assessment of CNNs Using a Data-Domain Sampling Regime
- **分类: cs.LG; cs.CV; cs.HC**

- **简介: 该论文属于计算机视觉任务，研究CNN在图表识别中的表现。通过数据域采样评估其感知行为，解决模型与人类表现差异问题，发现CNN性能受训练测试数据距离影响。**

- **链接: [http://arxiv.org/pdf/2507.03866v1](http://arxiv.org/pdf/2507.03866v1)**

> **作者:** Shuning Jiang; Wei-Lun Chao; Daniel Haehn; Hanspeter Pfister; Jian Chen
>
> **备注:** This is a preprint of a paper that has been conditionally accepted for publication at IEEE VIS 2025. The final version may be different upon publication. 9 pages main text, 11 pages supplementary contents, 37 figures
>
> **摘要:** We present a data-domain sampling regime for quantifying CNNs' graphic perception behaviors. This regime lets us evaluate CNNs' ratio estimation ability in bar charts from three perspectives: sensitivity to training-test distribution discrepancies, stability to limited samples, and relative expertise to human observers. After analyzing 16 million trials from 800 CNNs models and 6,825 trials from 113 human participants, we arrived at a simple and actionable conclusion: CNNs can outperform humans and their biases simply depend on the training-test distance. We show evidence of this simple, elegant behavior of the machines when they interpret visualization images. osf.io/gfqc3 provides registration, the code for our sampling regime, and experimental results.
>
---
#### [new 280] CLIP-RL: Surgical Scene Segmentation Using Contrastive Language-Vision Pretraining & Reinforcement Learning
- **分类: eess.IV; cs.AI; cs.CV; cs.LG**

- **简介: 该论文属于医学图像分割任务，解决手术场景中器械与组织区分问题。通过结合对比学习、强化学习和课程学习，提出CLIP-RL模型，提升分割精度。**

- **链接: [http://arxiv.org/pdf/2507.04317v1](http://arxiv.org/pdf/2507.04317v1)**

> **作者:** Fatmaelzahraa Ali Ahmed; Muhammad Arsalan; Abdulaziz Al-Ali; Khalid Al-Jalham; Shidin Balakrishnan
>
> **摘要:** Understanding surgical scenes can provide better healthcare quality for patients, especially with the vast amount of video data that is generated during MIS. Processing these videos generates valuable assets for training sophisticated models. In this paper, we introduce CLIP-RL, a novel contrastive language-image pre-training model tailored for semantic segmentation for surgical scenes. CLIP-RL presents a new segmentation approach which involves reinforcement learning and curriculum learning, enabling continuous refinement of the segmentation masks during the full training pipeline. Our model has shown robust performance in different optical settings, such as occlusions, texture variations, and dynamic lighting, presenting significant challenges. CLIP model serves as a powerful feature extractor, capturing rich semantic context that enhances the distinction between instruments and tissues. The RL module plays a pivotal role in dynamically refining predictions through iterative action-space adjustments. We evaluated CLIP-RL on the EndoVis 2018 and EndoVis 2017 datasets. CLIP-RL achieved a mean IoU of 81%, outperforming state-of-the-art models, and a mean IoU of 74.12% on EndoVis 2017. This superior performance was achieved due to the combination of contrastive learning with reinforcement learning and curriculum learning.
>
---
#### [new 281] Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models
- **分类: cs.AI; cs.CV; 68T01**

- **简介: 该论文属于视觉-语言模型任务，解决幻灯片动画生成问题。通过构建首个公开数据集并优化模型，提升动画理解与生成效果。**

- **链接: [http://arxiv.org/pdf/2507.03916v1](http://arxiv.org/pdf/2507.03916v1)**

> **作者:** Yifan Jiang; Yibo Xue; Yukun Kang; Pin Zheng; Jian Peng; Feiran Wu; Changliang Xu
>
> **备注:** Appendix at: https://github.com/PAMPAS-Lab/ANA-PPT-Anamation/blob/main/Appendix.pdf
>
> **摘要:** Slide animations, such as fade-ins, fly-ins, and wipes, are critical for audience engagement, efficient information delivery, and vivid visual expression. However, most AI-driven slide-generation tools still lack native animation support, and existing vision-language models (VLMs) struggle with animation tasks due to the absence of public datasets and limited temporal-reasoning capabilities. To address this gap, we release the first public dataset for slide-animation modeling: 12,000 triplets of natural-language descriptions, animation JSON files, and rendered videos, collectively covering every built-in PowerPoint effect. Using this resource, we fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our Coverage-Order-Detail Assessment (CODA) metric, which evaluates action coverage, temporal order, and detail fidelity. On a manually curated test set of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and shows significant improvements in CODA-detail. This demonstrates that low-rank adaptation enables reliable temporal reasoning and generalization beyond synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric provide a rigorous benchmark and foundation for future research on VLM-based dynamic slide generation.
>
---
#### [new 282] Towards Interpretable PolSAR Image Classification: Polarimetric Scattering Mechanism Informed Concept Bottleneck and Kolmogorov-Arnold Network
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于PolSAR图像分类任务，旨在解决深度学习模型的不可解释性问题。通过引入极化散射机制和新型网络结构，实现特征到可理解概念的转化与模型解释。**

- **链接: [http://arxiv.org/pdf/2507.03315v1](http://arxiv.org/pdf/2507.03315v1)**

> **作者:** Jinqi Zhang; Fangzhou Han; Di Zhuang; Lamei Zhang; Bin Zou; Li Yuan
>
> **摘要:** In recent years, Deep Learning (DL) based methods have received extensive and sufficient attention in the field of PolSAR image classification, which show excellent performance. However, due to the ``black-box" nature of DL methods, the interpretation of the high-dimensional features extracted and the backtracking of the decision-making process based on the features are still unresolved problems. In this study, we first highlight this issue and attempt to achieve the interpretability analysis of DL-based PolSAR image classification technology with the help of Polarimetric Target Decomposition (PTD), a feature extraction method related to the scattering mechanism unique to the PolSAR image processing field. In our work, by constructing the polarimetric conceptual labels and a novel structure named Parallel Concept Bottleneck Networks (PaCBM), the uninterpretable high-dimensional features are transformed into human-comprehensible concepts based on physically verifiable polarimetric scattering mechanisms. Then, the Kolmogorov-Arnold Network (KAN) is used to replace Multi-Layer Perceptron (MLP) for achieving a more concise and understandable mapping process between layers and further enhanced non-linear modeling ability. The experimental results on several PolSAR datasets show that the features could be conceptualization under the premise of achieving satisfactory accuracy through the proposed pipeline, and the analytical function for predicting category labels from conceptual labels can be obtained by combining spline functions, thus promoting the research on the interpretability of the DL-based PolSAR image classification model.
>
---
#### [new 283] Time2Agri: Temporal Pretext Tasks for Agricultural Monitoring
- **分类: cs.LG; cs.CV**

- **简介: 该论文属于农业监测领域，针对遥感数据的时间特性不足问题，提出三种新预训练任务，提升作物识别与产量预测效果。**

- **链接: [http://arxiv.org/pdf/2507.04366v1](http://arxiv.org/pdf/2507.04366v1)**

> **作者:** Moti Rattan Gupta; Anupam Sobti
>
> **摘要:** Self Supervised Learning(SSL) has emerged as a prominent paradigm for label-efficient learning, and has been widely utilized by remote sensing foundation models(RSFMs). Recent RSFMs including SatMAE, DoFA, primarily rely on masked autoencoding(MAE), contrastive learning or some combination of them. However, these pretext tasks often overlook the unique temporal characteristics of agricultural landscape, namely nature's cycle. Motivated by this gap, we propose three novel agriculture-specific pretext tasks, namely Time-Difference Prediction(TD), Temporal Frequency Prediction(FP), and Future-Frame Prediction(FF). Comprehensive evaluation on SICKLE dataset shows FF achieves 69.6% IoU on crop mapping and FP reduces yield prediction error to 30.7% MAPE, outperforming all baselines, and TD remains competitive on most tasks. Further, we also scale FF to the national scale of India, achieving 54.2% IoU outperforming all baselines on field boundary delineation on FTW India dataset.
>
---
#### [new 284] A Deep Unfolding Framework for Diffractive Snapshot Spectral Imaging
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于高光谱成像任务，解决DSSI系统中重建算法不兼容的问题。提出DDU框架，提升重建效率与稳定性。**

- **链接: [http://arxiv.org/pdf/2507.04622v1](http://arxiv.org/pdf/2507.04622v1)**

> **作者:** Zhengyue Zhuge; Jiahui Xu; Shiqi Chen; Hao Xu; Yueting Chen; Zhihai Xu; Huajun Feng
>
> **摘要:** Snapshot hyperspectral imaging systems acquire spectral data cubes through compressed sensing. Recently, diffractive snapshot spectral imaging (DSSI) methods have attracted significant attention. While various optical designs and improvements continue to emerge, research on reconstruction algorithms remains limited. Although numerous networks and deep unfolding methods have been applied on similar tasks, they are not fully compatible with DSSI systems because of their distinct optical encoding mechanism. In this paper, we propose an efficient deep unfolding framework for diffractive systems, termed diffractive deep unfolding (DDU). Specifically, we derive an analytical solution for the data fidelity term in DSSI, ensuring both the efficiency and the effectiveness during the iterative reconstruction process. Given the severely ill-posed nature of the problem, we employ a network-based initialization strategy rather than non-learning-based methods or linear layers, leading to enhanced stability and performance. Our framework demonstrates strong compatibility with existing state-of-the-art (SOTA) models, which effectively address the initialization and prior subproblem. Extensive experiments validate the superiority of the proposed DDU framework, showcasing improved performance while maintaining comparable parameter counts and computational complexity. These results suggest that DDU provides a solid foundation for future unfolding-based methods in DSSI.
>
---
#### [new 285] Evaluating the Evaluators: Trust in Adversarial Robustness Tests
- **分类: cs.CR; cs.AI; cs.CV; cs.LG**

- **简介: 该论文属于模型鲁棒性评估任务，解决现有测试方法不一致的问题，提出AttackBench框架以标准化、可靠地评估梯度攻击效果。**

- **链接: [http://arxiv.org/pdf/2507.03450v1](http://arxiv.org/pdf/2507.03450v1)**

> **作者:** Antonio Emanuele Cinà; Maura Pintor; Luca Demetrio; Ambra Demontis; Battista Biggio; Fabio Roli
>
> **摘要:** Despite significant progress in designing powerful adversarial evasion attacks for robustness verification, the evaluation of these methods often remains inconsistent and unreliable. Many assessments rely on mismatched models, unverified implementations, and uneven computational budgets, which can lead to biased results and a false sense of security. Consequently, robustness claims built on such flawed testing protocols may be misleading and give a false sense of security. As a concrete step toward improving evaluation reliability, we present AttackBench, a benchmark framework developed to assess the effectiveness of gradient-based attacks under standardized and reproducible conditions. AttackBench serves as an evaluation tool that ranks existing attack implementations based on a novel optimality metric, which enables researchers and practitioners to identify the most reliable and effective attack for use in subsequent robustness evaluations. The framework enforces consistent testing conditions and enables continuous updates, making it a reliable foundation for robustness verification.
>
---
#### [new 286] Attention-Guided Multi-Scale Local Reconstruction for Point Clouds via Masked Autoencoder Self-Supervised Learning
- **分类: cs.GR; cs.CV**

- **简介: 该论文属于点云处理任务，旨在解决低层局部特征利用不足的问题。提出PointAMaLR框架，通过多尺度局部重建和注意力机制提升特征表示与重建精度。**

- **链接: [http://arxiv.org/pdf/2507.04084v1](http://arxiv.org/pdf/2507.04084v1)**

> **作者:** Xin Cao; Haoyu Wang; Yuzhu Mao; Xinda Liu; Linzhi Su; Kang Li
>
> **备注:** 22 pages
>
> **摘要:** Self-supervised learning has emerged as a prominent research direction in point cloud processing. While existing models predominantly concentrate on reconstruction tasks at higher encoder layers, they often neglect the effective utilization of low-level local features, which are typically employed solely for activation computations rather than directly contributing to reconstruction tasks. To overcome this limitation, we introduce PointAMaLR, a novel self-supervised learning framework that enhances feature representation and processing accuracy through attention-guided multi-scale local reconstruction. PointAMaLR implements hierarchical reconstruction across multiple local regions, with lower layers focusing on fine-scale feature restoration while upper layers address coarse-scale feature reconstruction, thereby enabling complex inter-patch interactions. Furthermore, to augment feature representation capabilities, we incorporate a Local Attention (LA) module in the embedding layer to enhance semantic feature understanding. Comprehensive experiments on benchmark datasets ModelNet and ShapeNet demonstrate PointAMaLR's superior accuracy and quality in both classification and reconstruction tasks. Moreover, when evaluated on the real-world dataset ScanObjectNN and the 3D large scene segmentation dataset S3DIS, our model achieves highly competitive performance metrics. These results not only validate PointAMaLR's effectiveness in multi-scale semantic understanding but also underscore its practical applicability in real-world scenarios.
>
---
#### [new 287] PASC-Net:Plug-and-play Shape Self-learning Convolutions Network with Hierarchical Topology Constraints for Vessel Segmentation
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于医学图像分割任务，旨在解决血管结构复杂导致的分割不准确问题。通过引入SSL模块和HTC模块提升分割精度与拓扑连通性。**

- **链接: [http://arxiv.org/pdf/2507.04008v1](http://arxiv.org/pdf/2507.04008v1)**

> **作者:** Xiao Zhang; Zhuo Jin; Shaoxuan Wu; Fengyu Wang; Guansheng Peng; Xiang Zhang; Ying Huang; JingKun Chen; Jun Feng
>
> **摘要:** Accurate vessel segmentation is crucial to assist in clinical diagnosis by medical experts. However, the intricate tree-like tubular structure of blood vessels poses significant challenges for existing segmentation algorithms. Small vascular branches are often overlooked due to their low contrast compared to surrounding tissues, leading to incomplete vessel segmentation. Furthermore, the complex vascular topology prevents the model from accurately capturing and reconstructing vascular structure, resulting in incorrect topology, such as breakpoints at the bifurcation of the vascular tree. To overcome these challenges, we propose a novel vessel segmentation framework called PASC Net. It includes two key modules: a plug-and-play shape self-learning convolutional (SSL) module that optimizes convolution kernel design, and a hierarchical topological constraint (HTC) module that ensures vascular connectivity through topological constraints. Specifically, the SSL module enhances adaptability to vascular structures by optimizing conventional convolutions into learnable strip convolutions, which improves the network's ability to perceive fine-grained features of tubular anatomies. Furthermore, to better preserve the coherence and integrity of vascular topology, the HTC module incorporates hierarchical topological constraints-spanning linear, planar, and volumetric levels-which serve to regularize the network's representation of vascular continuity and structural consistency. We replaced the standard convolutional layers in U-Net, FCN, U-Mamba, and nnUNet with SSL convolutions, leading to consistent performance improvements across all architectures. Furthermore, when integrated into the nnUNet framework, our method outperformed other methods on multiple metrics, achieving state-of-the-art vascular segmentation performance.
>
---
#### [new 288] DANCE: Resource-Efficient Neural Architecture Search with Data-Aware and Continuous Adaptation
- **分类: cs.LG; cs.CV**

- **简介: 该论文属于神经网络架构搜索任务，解决NAS在实际部署中适应性差、成本高的问题。提出DANCE方法，通过连续进化实现高效架构搜索与适配。**

- **链接: [http://arxiv.org/pdf/2507.04671v1](http://arxiv.org/pdf/2507.04671v1)**

> **作者:** Maolin Wang; Tianshuo Wei; Sheng Zhang; Ruocheng Guo; Wanyu Wang; Shanshan Ye; Lixin Zou; Xuetao Wei; Xiangyu Zhao
>
> **备注:** Accepted by IJCAI 2025
>
> **摘要:** Neural Architecture Search (NAS) has emerged as a powerful approach for automating neural network design. However, existing NAS methods face critical limitations in real-world deployments: architectures lack adaptability across scenarios, each deployment context requires costly separate searches, and performance consistency across diverse platforms remains challenging. We propose DANCE (Dynamic Architectures with Neural Continuous Evolution), which reformulates architecture search as a continuous evolution problem through learning distributions over architectural components. DANCE introduces three key innovations: a continuous architecture distribution enabling smooth adaptation, a unified architecture space with learned selection gates for efficient sampling, and a multi-stage training strategy for effective deployment optimization. Extensive experiments across five datasets demonstrate DANCE's effectiveness. Our method consistently outperforms state-of-the-art NAS approaches in terms of accuracy while significantly reducing search costs. Under varying computational constraints, DANCE maintains robust performance while smoothly adapting architectures to different hardware requirements. The code and appendix can be found at https://github.com/Applied-Machine-Learning-Lab/DANCE.
>
---
#### [new 289] Dual-Alignment Knowledge Retention for Continual Medical Image Segmentation
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于医学图像分割任务，解决持续学习中的灾难性遗忘问题。提出双对齐策略，增强历史数据与当前任务的关联，提升模型泛化能力。**

- **链接: [http://arxiv.org/pdf/2507.03638v1](http://arxiv.org/pdf/2507.03638v1)**

> **作者:** Yuxin Ye; Yan Liu; Shujian Yu
>
> **摘要:** Continual learning in medical image segmentation involves sequential data acquisition across diverse domains (e.g., clinical sites), where task interference between past and current domains often leads to catastrophic forgetting. Existing continual learning methods fail to capture the complex dependencies between tasks. We introduce a novel framework that mitigates forgetting by establishing and enhancing complex dependencies between historical data and the network in the present task. Our framework features a dual-alignment strategy, the cross-network alignment (CNA) module aligns the features extracted from the bottleneck layers of the current and previous networks, respectively, while the cross-representation alignment (CRA) module aligns the features learned by the current network from historical buffered data and current input data, respectively. Implementing both types of alignment is a non-trivial task. To address this, we further analyze the linear and nonlinear forms of the well-established Hilbert-Schmidt Independence Criterion (HSIC) and deliberately design feature mapping and feature pairing blocks within the CRA module. Experiments on medical image segmentation task demonstrate our framework's effectiveness in mitigating catastrophic forgetting under domain shifts.
>
---
#### [new 290] Online Continual Learning via Spiking Neural Networks with Sleep Enhanced Latent Replay
- **分类: cs.NE; cs.CV; cs.LG**

- **简介: 该论文属于在线持续学习任务，解决动态环境中记忆占用高和新任务偏差问题。提出SESRL方法，结合脉冲神经网络和睡眠增强的潜在回放机制，降低内存消耗并提升模型性能。**

- **链接: [http://arxiv.org/pdf/2507.02901v1](http://arxiv.org/pdf/2507.02901v1)**

> **作者:** Erliang Lin; Wenbin Luo; Wei Jia; Yu Chen; Shaofu Yang
>
> **备注:** 9 pages, 4figures
>
> **摘要:** Edge computing scenarios necessitate the development of hardware-efficient online continual learning algorithms to be adaptive to dynamic environment. However, existing algorithms always suffer from high memory overhead and bias towards recently trained tasks. To tackle these issues, this paper proposes a novel online continual learning approach termed as SESLR, which incorporates a sleep enhanced latent replay scheme with spiking neural networks (SNNs). SESLR leverages SNNs' binary spike characteristics to store replay features in single bits, significantly reducing memory overhead. Furthermore, inspired by biological sleep-wake cycles, SESLR introduces a noise-enhanced sleep phase where the model exclusively trains on replay samples with controlled noise injection, effectively mitigating classification bias towards new classes. Extensive experiments on both conventional (MNIST, CIFAR10) and neuromorphic (NMNIST, CIFAR10-DVS) datasets demonstrate SESLR's effectiveness. On Split CIFAR10, SESLR achieves nearly 30% improvement in average accuracy with only one-third of the memory consumption compared to baseline methods. On Split CIFAR10-DVS, it improves accuracy by approximately 10% while reducing memory overhead by a factor of 32. These results validate SESLR as a promising solution for online continual learning in resource-constrained edge computing scenarios.
>
---
#### [new 291] When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning
- **分类: cs.AI; cs.CV**

- **简介: 该论文研究手术动作规划任务，比较模仿学习与强化学习的效果。结果表明模仿学习在手术动作预测中优于强化学习。**

- **链接: [http://arxiv.org/pdf/2507.05011v1](http://arxiv.org/pdf/2507.05011v1)**

> **作者:** Maxence Boels; Harry Robertshaw; Alejandro Granados; Prokar Dasgupta; Sebastien Ourselin
>
> **备注:** This manuscript has been submitted to a conference and is being peer reviewed
>
> **摘要:** Surgical action planning requires predicting future instrument-verb-target triplets for real-time assistance. While teleoperated robotic surgery provides natural expert demonstrations for imitation learning (IL), reinforcement learning (RL) could potentially discover superior strategies through exploration. We present the first comprehensive comparison of IL versus RL for surgical action planning on CholecT50. Our Dual-task Autoregressive Imitation Learning (DARIL) baseline achieves 34.6% action triplet recognition mAP and 33.6% next frame prediction mAP with smooth planning degradation to 29.2% at 10-second horizons. We evaluated three RL variants: world model-based RL, direct video RL, and inverse RL enhancement. Surprisingly, all RL approaches underperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while direct video RL achieved only 15.9%. Our analysis reveals that distribution matching on expert-annotated test sets systematically favors IL over potentially valid RL policies that differ from training demonstrations. This challenges assumptions about RL superiority in sequential decision making and provides crucial insights for surgical AI development.
>
---
#### [new 292] MurreNet: Modeling Holistic Multimodal Interactions Between Histopathology and Genomic Profiles for Survival Prediction
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于癌症生存预测任务，旨在解决多模态数据（病理图像与基因组）融合不足的问题。提出MurreNet模型，通过分解和融合多模态表示提升预测性能。**

- **链接: [http://arxiv.org/pdf/2507.04891v1](http://arxiv.org/pdf/2507.04891v1)**

> **作者:** Mingxin Liu; Chengfei Cai; Jun Li; Pengbo Xu; Jinze Li; Jiquan Ma; Jun Xu
>
> **备注:** 11 pages, 2 figures, Accepted by MICCAI 2025
>
> **摘要:** Cancer survival prediction requires integrating pathological Whole Slide Images (WSIs) and genomic profiles, a challenging task due to the inherent heterogeneity and the complexity of modeling both inter- and intra-modality interactions. Current methods often employ straightforward fusion strategies for multimodal feature integration, failing to comprehensively capture modality-specific and modality-common interactions, resulting in a limited understanding of multimodal correlations and suboptimal predictive performance. To mitigate these limitations, this paper presents a Multimodal Representation Decoupling Network (MurreNet) to advance cancer survival analysis. Specifically, we first propose a Multimodal Representation Decomposition (MRD) module to explicitly decompose paired input data into modality-specific and modality-shared representations, thereby reducing redundancy between modalities. Furthermore, the disentangled representations are further refined then updated through a novel training regularization strategy that imposes constraints on distributional similarity, difference, and representativeness of modality features. Finally, the augmented multimodal features are integrated into a joint representation via proposed Deep Holistic Orthogonal Fusion (DHOF) strategy. Extensive experiments conducted on six TCGA cancer cohorts demonstrate that our MurreNet achieves state-of-the-art (SOTA) performance in survival prediction.
>
---
#### [new 293] Event2Audio: Event-Based Optical Vibration Sensing
- **分类: eess.IV; cs.CV; eess.AS**

- **简介: 该论文属于音频恢复任务，解决从视频中提取声音的问题。通过事件相机提升振动感知，实现快速高质的音频重建。**

- **链接: [http://arxiv.org/pdf/2507.03273v1](http://arxiv.org/pdf/2507.03273v1)**

> **作者:** Mingxuan Cai; Dekel Galor; Amit Pal Singh Kohli; Jacob L. Yates; Laura Waller
>
> **备注:** 14 pages, 13 figures
>
> **摘要:** Small vibrations observed in video can unveil information beyond what is visual, such as sound and material properties. It is possible to passively record these vibrations when they are visually perceptible, or actively amplify their visual contribution with a laser beam when they are not perceptible. In this paper, we improve upon the active sensing approach by leveraging event-based cameras, which are designed to efficiently capture fast motion. We demonstrate our method experimentally by recovering audio from vibrations, even for multiple simultaneous sources, and in the presence of environmental distortions. Our approach matches the state-of-the-art reconstruction quality at much faster speeds, approaching real-time processing.
>
---
#### [new 294] Emerging Frameworks for Objective Task-based Evaluation of Quantitative Medical Imaging Methods
- **分类: physics.med-ph; cs.CV; eess.IV**

- **简介: 该论文属于医学影像评估任务，旨在解决定量成像方法的客观评价问题。文章提出四种评估框架，涵盖虚拟试验、无金标准评估、联合检测与量化及多维参数评估。**

- **链接: [http://arxiv.org/pdf/2507.04591v1](http://arxiv.org/pdf/2507.04591v1)**

> **作者:** Yan Liu; Huitian Xia; Nancy A. Obuchowski; Richard Laforest; Arman Rahmim; Barry A. Siegel; Abhinav K. Jha
>
> **备注:** 19 pages, 7 figures
>
> **摘要:** Quantitative imaging (QI) is demonstrating strong promise across multiple clinical applications. For clinical translation of QI methods, objective evaluation on clinically relevant tasks is essential. To address this need, multiple evaluation strategies are being developed. In this paper, based on previous literature, we outline four emerging frameworks to perform evaluation studies of QI methods. We first discuss the use of virtual imaging trials (VITs) to evaluate QI methods. Next, we outline a no-gold-standard evaluation framework to clinically evaluate QI methods without ground truth. Third, a framework to evaluate QI methods for joint detection and quantification tasks is outlined. Finally, we outline a framework to evaluate QI methods that output multi-dimensional parameters, such as radiomic features. We review these frameworks, discussing their utilities and limitations. Further, we examine future research areas in evaluation of QI methods. Given the recent advancements in PET, including long axial field-of-view scanners and the development of artificial-intelligence algorithms, we present these frameworks in the context of PET.
>
---
#### [new 295] Critiques of World Models
- **分类: cs.LG; cs.AI; cs.CL; cs.CV; cs.RO**

- **简介: 该论文属于人工智能领域，探讨世界模型的构建与评估，旨在解决如何有效模拟现实世界以支持智能体决策。论文提出一种新的多层级世界模型架构。**

- **链接: [http://arxiv.org/pdf/2507.05169v1](http://arxiv.org/pdf/2507.05169v1)**

> **作者:** Eric Xing; Mingkai Deng; Jinyu Hou; Zhiting Hu
>
> **摘要:** World Model, the supposed algorithmic surrogate of the real-world environment which biological agents experience with and act upon, has been an emerging topic in recent years because of the rising needs to develop virtual agents with artificial (general) intelligence. There has been much debate on what a world model really is, how to build it, how to use it, and how to evaluate it. In this essay, starting from the imagination in the famed Sci-Fi classic Dune, and drawing inspiration from the concept of "hypothetical thinking" in psychology literature, we offer critiques of several schools of thoughts on world modeling, and argue the primary goal of a world model to be simulating all actionable possibilities of the real world for purposeful reasoning and acting. Building on the critiques, we propose a new architecture for a general-purpose world model, based on hierarchical, multi-level, and mixed continuous/discrete representations, and a generative and self-supervision learning framework, with an outlook of a Physical, Agentic, and Nested (PAN) AGI system enabled by such a model.
>
---
#### [new 296] UltraDfeGAN: Detail-Enhancing Generative Adversarial Networks for High-Fidelity Functional Ultrasound Synthesis
- **分类: eess.IV; cs.CV; physics.med-ph**

- **简介: 该论文属于图像生成任务，旨在解决fUS数据稀缺和生成真实图像的问题。通过改进GAN架构，提升生成图像的质量与生理合理性。**

- **链接: [http://arxiv.org/pdf/2507.03341v1](http://arxiv.org/pdf/2507.03341v1)**

> **作者:** Zhuo Li; Xuhang Chen; Shuqiang Wang
>
> **摘要:** Functional ultrasound (fUS) is a neuroimaging technique known for its high spatiotemporal resolution, enabling non-invasive observation of brain activity through neurovascular coupling. Despite its potential in clinical applications such as neonatal monitoring and intraoperative guidance, the development of fUS faces challenges related to data scarcity and limitations in generating realistic fUS images. This paper explores the use of a generative adversarial network (GAN) framework tailored for fUS image synthesis. The proposed method incorporates architectural enhancements, including feature enhancement modules and normalization techniques, aiming to improve the fidelity and physiological plausibility of generated images. The study evaluates the performance of the framework against existing generative models, demonstrating its capability to produce high-quality fUS images under various experimental conditions. Additionally, the synthesized images are assessed for their utility in downstream tasks, showing improvements in classification accuracy when used for data augmentation. Experimental results are based on publicly available fUS datasets, highlighting the framework's effectiveness in addressing data limitations.
>
---
#### [new 297] Differentiable High-Performance Ray Tracing-Based Simulation of Radio Propagation with Point Clouds
- **分类: eess.SP; cs.CV**

- **简介: 该论文属于无线传播仿真任务，解决环境模型与电磁属性建模问题，提出基于点云的可微光线追踪模拟方法，实现高效多路径仿真。**

- **链接: [http://arxiv.org/pdf/2507.04021v1](http://arxiv.org/pdf/2507.04021v1)**

> **作者:** Niklas Vaara; Pekka Sangi; Miguel Bordallo López; Janne Heikkilä
>
> **摘要:** Ray tracing is a widely used deterministic method for radio propagation simulations, capable of producing physically accurate multipath components. The accuracy depends on the quality of the environment model and its electromagnetic properties. Recent advances in computer vision and machine learning have made it possible to reconstruct detailed environment models augmented with semantic segmentation labels. In this letter, we propose a differentiable ray tracing-based radio propagation simulator that operates directly on point clouds. We showcase the efficiency of our method by simulating multi-bounce propagation paths with up to five interactions with specular reflections and diffuse scattering in two indoor scenarios, each completing in less than 90 ms. Lastly, we demonstrate how the differentiability of electromagnetic computations can be combined with segmentation labels to learn the electromagnetic properties of the environment.
>
---
#### [new 298] Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking
- **分类: cs.AI; cs.CV; cs.HC**

- **简介: 该论文属于烹饪辅助任务，旨在解决视障人士在非视觉环境下跟踪食谱进度的问题。通过对象状态识别，构建了OSCAR系统以提升步骤预测准确性。**

- **链接: [http://arxiv.org/pdf/2507.03330v1](http://arxiv.org/pdf/2507.03330v1)**

> **作者:** Franklin Mingzhe Li; Kaitlyn Ng; Bin Zhu; Patrick Carrington
>
> **备注:** ASSETS 2025
>
> **摘要:** Cooking plays a vital role in everyday independence and well-being, yet remains challenging for people with vision impairments due to limited support for tracking progress and receiving contextual feedback. Object status - the condition or transformation of ingredients and tools - offers a promising but underexplored foundation for context-aware cooking support. In this paper, we present OSCAR (Object Status Context Awareness for Recipes), a technical pipeline that explores the use of object status recognition to enable recipe progress tracking in non-visual cooking. OSCAR integrates recipe parsing, object status extraction, visual alignment with cooking steps, and time-causal modeling to support real-time step tracking. We evaluate OSCAR on 173 instructional videos and a real-world dataset of 12 non-visual cooking sessions recorded by BLV individuals in their homes. Our results show that object status consistently improves step prediction accuracy across vision-language models, and reveal key factors that impact performance in real-world conditions, such as implicit tasks, camera placement, and lighting. We contribute the pipeline of context-aware recipe progress tracking, an annotated real-world non-visual cooking dataset, and design insights to guide future context-aware assistive cooking systems.
>
---
#### [new 299] Neural Dynamic Modes: Computational Imaging of Dynamical Systems from Sparse Observations
- **分类: cs.LG; astro-ph.IM; cs.CV; physics.ao-ph; 68T45, 68T07; I.4.8; I.2.6**

- **简介: 该论文属于动态系统成像任务，解决从稀疏观测中重建时空动态的问题。通过结合神经隐式表示与DMD，实现高效、稳定的动态重构与预测。**

- **链接: [http://arxiv.org/pdf/2507.03094v1](http://arxiv.org/pdf/2507.03094v1)**

> **作者:** Ali SaraerToosi; Renbo Tu; Kamyar Azizzadenesheli; Aviad Levis
>
> **备注:** 24 pages, 18 figures
>
> **摘要:** Dynamical systems are ubiquitous within science and engineering, from turbulent flow across aircraft wings to structural variability of proteins. Although some systems are well understood and simulated, scientific imaging often confronts never-before-seen dynamics observed through indirect, noisy, and highly sparse measurements. We present NeuralDMD, a model-free framework that combines neural implicit representations with Dynamic Mode Decomposition (DMD) to reconstruct continuous spatio-temporal dynamics from such measurements. The expressiveness of neural representations enables capturing complex spatial structures, while the linear dynamical modes of DMD introduce an inductive bias that guides training and supports stable, low-dimensional representations and forecasting. We validate NeuralDMD on two real-world problems: reconstructing near-surface wind-speed fields over North America from sparse station observations, and recovering the evolution of plasma near the Galactic-center black hole, Sgr A*. In both cases, NeuralDMD outperforms established baselines, demonstrating its potential as a general tool for imaging dynamical systems across geoscience, astronomy, and beyond.
>
---
#### [new 300] FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System
- **分类: cs.AI; cs.CV**

- **简介: 该论文属于家具装饰任务，解决人工装饰耗时且需专业技能的问题。提出FurniMAS系统，利用多智能体协作实现自动装饰。**

- **链接: [http://arxiv.org/pdf/2507.04770v1](http://arxiv.org/pdf/2507.04770v1)**

> **作者:** Toan Nguyen; Tri Le; Quang Nguyen; Anh Nguyen
>
> **摘要:** Furniture decoration is an important task in various industrial applications. However, achieving a high-quality decorative result is often time-consuming and requires specialized artistic expertise. To tackle these challenges, we explore how multi-agent systems can assist in automating the decoration process. We propose FurniMAS, a multi-agent system for automatic furniture decoration. Specifically, given a human prompt and a household furniture item such as a working desk or a TV stand, our system suggests relevant assets with appropriate styles and materials, and arranges them on the item, ensuring the decorative result meets functionality, aesthetic, and ambiance preferences. FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each fulfilling distinct roles in a typical decoration project. These agents collaborate through communication, logical reasoning, and validation to transform the requirements into the final outcome. Extensive experiments demonstrate that our FurniMAS significantly outperforms other baselines in generating high-quality 3D decor.
>
---
#### [new 301] PLUS: Plug-and-Play Enhanced Liver Lesion Diagnosis Model on Non-Contrast CT Scans
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于医学图像分析任务，旨在解决非增强CT上肝部病灶良恶性区分难题。提出PLUS框架提升现有分割模型的诊断能力。**

- **链接: [http://arxiv.org/pdf/2507.03872v1](http://arxiv.org/pdf/2507.03872v1)**

> **作者:** Jiacheng Hao; Xiaoming Zhang; Wei Liu; Xiaoli Yin; Yuan Gao; Chunli Li; Ling Zhang; Le Lu; Yu Shi; Xu Han; Ke Yan
>
> **备注:** MICCAI 2025 (Early Accepted)
>
> **摘要:** Focal liver lesions (FLL) are common clinical findings during physical examination. Early diagnosis and intervention of liver malignancies are crucial to improving patient survival. Although the current 3D segmentation paradigm can accurately detect lesions, it faces limitations in distinguishing between malignant and benign liver lesions, primarily due to its inability to differentiate subtle variations between different lesions. Furthermore, existing methods predominantly rely on specialized imaging modalities such as multi-phase contrast-enhanced CT and magnetic resonance imaging, whereas non-contrast CT (NCCT) is more prevalent in routine abdominal imaging. To address these limitations, we propose PLUS, a plug-and-play framework that enhances FLL analysis on NCCT images for arbitrary 3D segmentation models. In extensive experiments involving 8,651 patients, PLUS demonstrated a significant improvement with existing methods, improving the lesion-level F1 score by 5.66%, the malignant patient-level F1 score by 6.26%, and the benign patient-level F1 score by 4.03%. Our results demonstrate the potential of PLUS to improve malignant FLL screening using widely available NCCT imaging substantially.
>
---
#### [new 302] LVM4CSI: Enabling Direct Application of Pre-Trained Large Vision Models for Wireless Channel Tasks
- **分类: cs.IT; cs.AI; cs.CV; cs.LG; math.IT**

- **简介: 该论文属于无线通信任务，旨在解决CSI获取与利用的问题。通过将CSI转换为视觉数据，直接应用预训练视觉模型，无需微调，提升性能并减少参数量。**

- **链接: [http://arxiv.org/pdf/2507.05121v1](http://arxiv.org/pdf/2507.05121v1)**

> **作者:** Jiajia Guo; Peiwen Jiang; Chao-Kai Wen; Shi Jin; Jun Zhang
>
> **备注:** This work has been submitted for possible publication
>
> **摘要:** Accurate channel state information (CSI) is critical to the performance of wireless communication systems, especially with the increasing scale and complexity introduced by 5G and future 6G technologies. While artificial intelligence (AI) offers a promising approach to CSI acquisition and utilization, existing methods largely depend on task-specific neural networks (NNs) that require expert-driven design and large training datasets, limiting their generalizability and practicality. To address these challenges, we propose LVM4CSI, a general and efficient framework that leverages the structural similarity between CSI and computer vision (CV) data to directly apply large vision models (LVMs) pre-trained on extensive CV datasets to wireless tasks without any fine-tuning, in contrast to large language model-based methods that generally necessitate fine-tuning. LVM4CSI maps CSI tasks to analogous CV tasks, transforms complex-valued CSI into visual formats compatible with LVMs, and integrates lightweight trainable layers to adapt extracted features to specific communication objectives. We validate LVM4CSI through three representative case studies, including channel estimation, human activity recognition, and user localization. Results demonstrate that LVM4CSI achieves comparable or superior performance to task-specific NNs, including an improvement exceeding 9.61 dB in channel estimation and approximately 40% reduction in localization error. Furthermore, it significantly reduces the number of trainable parameters and eliminates the need for task-specific NN design.
>
---
#### [new 303] Neuralocks: Real-Time Dynamic Neural Hair Simulation
- **分类: cs.GR; cs.CV**

- **简介: 该论文属于虚拟角色动画任务，旨在解决实时动态头发模拟问题。通过提出一种自监督神经方法，实现高效稳定的头发动态仿真。**

- **链接: [http://arxiv.org/pdf/2507.05191v1](http://arxiv.org/pdf/2507.05191v1)**

> **作者:** Gene Wei-Chin Lin; Egor Larionov; Hsiao-yu Chen; Doug Roble; Tuur Stuyck
>
> **摘要:** Real-time hair simulation is a vital component in creating believable virtual avatars, as it provides a sense of immersion and authenticity. The dynamic behavior of hair, such as bouncing or swaying in response to character movements like jumping or walking, plays a significant role in enhancing the overall realism and engagement of virtual experiences. Current methods for simulating hair have been constrained by two primary approaches: highly optimized physics-based systems and neural methods. However, state-of-the-art neural techniques have been limited to quasi-static solutions, failing to capture the dynamic behavior of hair. This paper introduces a novel neural method that breaks through these limitations, achieving efficient and stable dynamic hair simulation while outperforming existing approaches. We propose a fully self-supervised method which can be trained without any manual intervention or artist generated training data allowing the method to be integrated with hair reconstruction methods to enable automatic end-to-end methods for avatar reconstruction. Our approach harnesses the power of compact, memory-efficient neural networks to simulate hair at the strand level, allowing for the simulation of diverse hairstyles without excessive computational resources or memory requirements. We validate the effectiveness of our method through a variety of hairstyle examples, showcasing its potential for real-world applications.
>
---
#### [new 304] Transformer Model for Alzheimer's Disease Progression Prediction Using Longitudinal Visit Sequences
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文属于阿尔茨海默病进展预测任务，旨在通过纵向就诊序列预测患者病情阶段，解决早期诊断与病情转换识别问题。**

- **链接: [http://arxiv.org/pdf/2507.03899v1](http://arxiv.org/pdf/2507.03899v1)**

> **作者:** Mahdi Moghaddami; Clayton Schubring; Mohammad-Reza Siadat
>
> **备注:** Conference on Health, Inference, and Learning (CHIL, 2025)
>
> **摘要:** Alzheimer's disease (AD) is a neurodegenerative disorder with no known cure that affects tens of millions of people worldwide. Early detection of AD is critical for timely intervention to halt or slow the progression of the disease. In this study, we propose a Transformer model for predicting the stage of AD progression at a subject's next clinical visit using features from a sequence of visits extracted from the subject's visit history. We also rigorously compare our model to recurrent neural networks (RNNs) such as long short-term memory (LSTM), gated recurrent unit (GRU), and minimalRNN and assess their performances based on factors such as the length of prior visits and data imbalance. We test the importance of different feature categories and visit history, as well as compare the model to a newer Transformer-based model optimized for time series. Our model demonstrates strong predictive performance despite missing visits and missing features in available visits, particularly in identifying converter subjects -- individuals transitioning to more severe disease stages -- an area that has posed significant challenges in longitudinal prediction. The results highlight the model's potential in enhancing early diagnosis and patient outcomes.
>
---
#### [new 305] Comprehensive Modeling of Camera Spectral and Color Behavior
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于相机光谱建模任务，解决缺乏全面模型的问题。通过提出新方法，提升颜色和光谱精度，适用于机器视觉等领域。**

- **链接: [http://arxiv.org/pdf/2507.04617v1](http://arxiv.org/pdf/2507.04617v1)**

> **作者:** Sanush K Abeysekera; Ye Chow Kuang; Melanie Po-Leen Ooi
>
> **备注:** 6 pages, 11 figures, 2025 I2MTC IEEE Instrumentation and Measurement Society Conference
>
> **摘要:** The spectral response of a digital camera defines the mapping between scene radiance and pixel intensity. Despite its critical importance, there is currently no comprehensive model that considers the end-to-end interaction between light input and pixel intensity output. This paper introduces a novel technique to model the spectral response of an RGB digital camera, addressing this gap. Such models are indispensable for applications requiring accurate color and spectral data interpretation. The proposed model is tested across diverse imaging scenarios by varying illumination conditions and is validated against experimental data. Results demonstrate its effectiveness in improving color fidelity and spectral accuracy, with significant implications for applications in machine vision, remote sensing, and spectral imaging. This approach offers a powerful tool for optimizing camera systems in scientific, industrial, and creative domains where spectral precision is paramount.
>
---
#### [new 306] RAM-W600: A Multi-Task Wrist Dataset and Benchmark for Rheumatoid Arthritis
- **分类: eess.IV; cs.CV**

- **简介: 该论文提出RAM-W600数据集，解决RA腕部图像分析难题，包含骨分割与BE评分任务，促进CAD研究。**

- **链接: [http://arxiv.org/pdf/2507.05193v1](http://arxiv.org/pdf/2507.05193v1)**

> **作者:** Songxiao Yang; Haolin Wang; Yao Fu; Ye Tian; Tamotsu Kamishima; Masayuki Ikebe; Yafei Ou; Masatoshi Okutomi
>
> **摘要:** Rheumatoid arthritis (RA) is a common autoimmune disease that has been the focus of research in computer-aided diagnosis (CAD) and disease monitoring. In clinical settings, conventional radiography (CR) is widely used for the screening and evaluation of RA due to its low cost and accessibility. The wrist is a critical region for the diagnosis of RA. However, CAD research in this area remains limited, primarily due to the challenges in acquiring high-quality instance-level annotations. (i) The wrist comprises numerous small bones with narrow joint spaces, complex structures, and frequent overlaps, requiring detailed anatomical knowledge for accurate annotation. (ii) Disease progression in RA often leads to osteophyte, bone erosion (BE), and even bony ankylosis, which alter bone morphology and increase annotation difficulty, necessitating expertise in rheumatology. This work presents a multi-task dataset for wrist bone in CR, including two tasks: (i) wrist bone instance segmentation and (ii) Sharp/van der Heijde (SvdH) BE scoring, which is the first public resource for wrist bone instance segmentation. This dataset comprises 621 wrist conventional radiographs of 227 patients from four medical centers, with pixel-level instance segmentation annotations for 443 images and SvdH BE scores for 548 images. This dataset can potentially support a wide range of research tasks related to RA, including joint space narrowing (JSN) progression quantification, BE detection, bone deformity evaluation, and osteophyte detection. It may also be applied to other wrist-related tasks, such as carpal bone fracture localization. We hope this dataset will significantly lower the barrier to research on wrist RA and accelerate progress in CAD research within the RA-related domain.
>
---
#### [new 307] Street design and driving behavior: evidence from a large-scale study in Milan, Amsterdam, and Dubai
- **分类: physics.soc-ph; cs.CV**

- **简介: 该论文属于城市规划与交通管理任务，旨在解决如何通过街景设计提高限速合规性。研究分析了米兰、阿姆斯特丹和迪拜的街道特征对驾驶行为的影响，并构建了预测模型。**

- **链接: [http://arxiv.org/pdf/2507.04434v1](http://arxiv.org/pdf/2507.04434v1)**

> **作者:** Giacomo Orsi; Titus Venverloo; Andrea La Grotteria; Umberto Fugiglando; Fábio Duarte; Paolo Santi; Carlo Ratti
>
> **摘要:** In recent years, cities have increasingly reduced speed limits from 50 km/h to 30 km/h to enhance road safety, reduce noise pollution, and promote sustainable modes of transportation. However, achieving compliance with these new limits remains a key challenge for urban planners. This study investigates drivers' compliance with the 30 km/h speed limit in Milan and examines how street characteristics influence driving behavior. Our findings suggest that the mere introduction of lower speed limits is not sufficient to reduce driving speeds effectively, highlighting the need to understand how street design can improve speed limit adherence. To comprehend this relationship, we apply computer vision-based semantic segmentation models to Google Street View images. A large-scale analysis reveals that narrower streets and densely built environments are associated with lower speeds, whereas roads with greater visibility and larger sky views encourage faster driving. To evaluate the influence of the local context on speeding behaviour, we apply the developed methodological framework to two additional cities: Amsterdam, which, similar to Milan, is a historic European city not originally developed for cars, and Dubai, which instead has developed in recent decades with a more car-centric design. The results of the analyses largely confirm the findings obtained in Milan, which demonstrates the broad applicability of the road design guidelines for driver speed compliance identified in this paper. Finally, we develop a machine learning model to predict driving speeds based on street characteristics. We showcase the model's predictive power by estimating the compliance with speed limits in Milan if the city were to adopt a 30 km/h speed limit city-wide. The tool provides actionable insights for urban planners, supporting the design of interventions to improve speed limit compliance.
>
---
#### [new 308] PhotIQA: A photoacoustic image data set with image quality ratings
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于图像质量评估任务，旨在解决医学图像缺乏质量评级数据的问题。作者构建了PhotIQA数据集，并进行了基准实验验证方法有效性。**

- **链接: [http://arxiv.org/pdf/2507.03478v1](http://arxiv.org/pdf/2507.03478v1)**

> **作者:** Anna Breger; Janek Gröhl; Clemens Karner; Thomas R Else; Ian Selby; Jonathan Weir-McCall; Carola-Bibiane Schönlieb
>
> **备注:** 12 pages
>
> **摘要:** Image quality assessment (IQA) is crucial in the evaluation stage of novel algorithms operating on images, including traditional and machine learning based methods. Due to the lack of available quality-rated medical images, most commonly used IQA methods employing reference images (i.e. full-reference IQA) have been developed and tested for natural images. Reported application inconsistencies arising when employing such measures for medical images are not surprising, as they rely on different properties than natural images. In photoacoustic imaging (PAI), especially, standard benchmarking approaches for assessing the quality of image reconstructions are lacking. PAI is a multi-physics imaging modality, in which two inverse problems have to be solved, which makes the application of IQA measures uniquely challenging due to both, acoustic and optical, artifacts. To support the development and testing of full- and no-reference IQA measures we assembled PhotIQA, a data set consisting of 1134 reconstructed photoacoustic (PA) images that were rated by 2 experts across five quality properties (overall quality, edge visibility, homogeneity, inclusion and background intensity), where the detailed rating enables usage beyond PAI. To allow full-reference assessment, highly characterised imaging test objects were used, providing a ground truth. Our baseline experiments show that HaarPSI$_{med}$ significantly outperforms SSIM in correlating with the quality ratings (SRCC: 0.83 vs. 0.62). The dataset is publicly available at https://doi.org/10.5281/zenodo.13325196.
>
---
#### [new 309] An Explainable Transformer Model for Alzheimer's Disease Detection Using Retinal Imaging
- **分类: cs.LG; cs.CV**

- **简介: 该论文属于阿尔茨海默病检测任务，旨在通过视网膜影像实现早期诊断。提出Retformer模型，结合Transformer与可解释AI，提升检测准确性并揭示关键特征。**

- **链接: [http://arxiv.org/pdf/2507.04259v1](http://arxiv.org/pdf/2507.04259v1)**

> **作者:** Saeed Jamshidiha; Alireza Rezaee; Farshid Hajati; Mojtaba Golzan; Raymond Chiong
>
> **备注:** 20 pages, 8 figures
>
> **摘要:** Alzheimer's disease (AD) is a neurodegenerative disorder that affects millions worldwide. In the absence of effective treatment options, early diagnosis is crucial for initiating management strategies to delay disease onset and slow down its progression. In this study, we propose Retformer, a novel transformer-based architecture for detecting AD using retinal imaging modalities, leveraging the power of transformers and explainable artificial intelligence. The Retformer model is trained on datasets of different modalities of retinal images from patients with AD and age-matched healthy controls, enabling it to learn complex patterns and relationships between image features and disease diagnosis. To provide insights into the decision-making process of our model, we employ the Gradient-weighted Class Activation Mapping algorithm to visualize the feature importance maps, highlighting the regions of the retinal images that contribute most significantly to the classification outcome. These findings are compared to existing clinical studies on detecting AD using retinal biomarkers, allowing us to identify the most important features for AD detection in each imaging modality. The Retformer model outperforms a variety of benchmark algorithms across different performance metrics by margins of up to 11\.
>
---
#### [new 310] MedGemma Technical Report
- **分类: cs.AI; cs.CL; cs.CV**

- **简介: 该论文属于医疗AI领域，旨在解决医疗数据多样性和隐私保护问题。工作包括开发MedGemma医学视觉语言模型和MedSigLIP医学视觉编码器，提升医疗任务性能。**

- **链接: [http://arxiv.org/pdf/2507.05201v1](http://arxiv.org/pdf/2507.05201v1)**

> **作者:** Andrew Sellergren; Sahar Kazemzadeh; Tiam Jaroensri; Atilla Kiraly; Madeleine Traverse; Timo Kohlberger; Shawn Xu; Fayaz Jamil; Cían Hughes; Charles Lau; Justin Chen; Fereshteh Mahvar; Liron Yatziv; Tiffany Chen; Bram Sterling; Stefanie Anna Baby; Susanna Maria Baby; Jeremy Lai; Samuel Schmidgall; Lu Yang; Kejia Chen; Per Bjornsson; Shashir Reddy; Ryan Brush; Kenneth Philbrick; Howard Hu; Howard Yang; Richa Tiwari; Sunny Jansen; Preeti Singh; Yun Liu; Shekoofeh Azizi; Aishwarya Kamath; Johan Ferret; Shreya Pathak; Nino Vieillard; Ramona Merhej; Sarah Perrin; Tatiana Matejovicova; Alexandre Ramé; Morgane Riviere; Louis Rouillard; Thomas Mesnard; Geoffrey Cideron; Jean-bastien Grill; Sabela Ramos; Edouard Yvinec; Michelle Casbon; Elena Buchatskaya; Jean-Baptiste Alayrac; Dmitry; Lepikhin; Vlad Feinberg; Sebastian Borgeaud; Alek Andreev; Cassidy Hardin; Robert Dadashi; Léonard Hussenot; Armand Joulin; Olivier Bachem; Yossi Matias; Katherine Chou; Avinatan Hassidim; Kavi Goel; Clement Farabet; Joelle Barral; Tris Warkentin; Jonathon Shlens; David Fleet; Victor Cotruta; Omar Sanseviero; Gus Martins; Phoebe Kirk; Anand Rao; Shravya Shetty; David F. Steiner; Can Kirmizibayrak; Rory Pilgrim; Daniel Golden; Lin Yang
>
> **摘要:** Artificial intelligence (AI) has significant potential in healthcare applications, but its training and deployment faces challenges due to healthcare's diverse data, complex tasks, and the need to preserve privacy. Foundation models that perform well on medical tasks and require less task-specific tuning data are critical to accelerate the development of healthcare AI applications. We introduce MedGemma, a collection of medical vision-language foundation models based on Gemma 3 4B and 27B. MedGemma demonstrates advanced medical understanding and reasoning on images and text, significantly exceeding the performance of similar-sized generative models and approaching the performance of task-specific models, while maintaining the general capabilities of the Gemma 3 base models. For out-of-distribution tasks, MedGemma achieves 2.6-10% improvement on medical multimodal question answering, 15.5-18.1% improvement on chest X-ray finding classification, and 10.8% improvement on agentic evaluations compared to the base models. Fine-tuning MedGemma further improves performance in subdomains, reducing errors in electronic health record information retrieval by 50% and reaching comparable performance to existing specialized state-of-the-art methods for pneumothorax classification and histopathology patch classification. We additionally introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP. MedSigLIP powers the visual understanding capabilities of MedGemma and as an encoder achieves comparable or better performance than specialized medical image encoders. Taken together, the MedGemma collection provides a strong foundation of medical image and text capabilities, with potential to significantly accelerate medical research and development of downstream applications. The MedGemma collection, including tutorials and model weights, can be found at https://goo.gle/medgemma.
>
---
#### [new 311] Rethinking Data Protection in the (Generative) Artificial Intelligence Era
- **分类: cs.LG; cs.AI; cs.CR; cs.CV; cs.CY**

- **简介: 该论文属于数据保护任务，旨在解决AI时代数据安全问题。提出四层分类框架，分析技术方法与监管盲点，以加强数据保护。**

- **链接: [http://arxiv.org/pdf/2507.03034v1](http://arxiv.org/pdf/2507.03034v1)**

> **作者:** Yiming Li; Shuo Shao; Yu He; Junfeng Guo; Tianwei Zhang; Zhan Qin; Pin-Yu Chen; Michael Backes; Philip Torr; Dacheng Tao; Kui Ren
>
> **备注:** Perspective paper for a broader scientific audience. The first two authors contributed equally to this paper. 13 pages
>
> **摘要:** The (generative) artificial intelligence (AI) era has profoundly reshaped the meaning and value of data. No longer confined to static content, data now permeates every stage of the AI lifecycle from the training samples that shape model parameters to the prompts and outputs that drive real-world model deployment. This shift renders traditional notions of data protection insufficient, while the boundaries of what needs safeguarding remain poorly defined. Failing to safeguard data in AI systems can inflict societal and individual, underscoring the urgent need to clearly delineate the scope of and rigorously enforce data protection. In this perspective, we propose a four-level taxonomy, including non-usability, privacy preservation, traceability, and deletability, that captures the diverse protection needs arising in modern (generative) AI models and systems. Our framework offers a structured understanding of the trade-offs between data utility and control, spanning the entire AI pipeline, including training datasets, model weights, system prompts, and AI-generated content. We analyze representative technical approaches at each level and reveal regulatory blind spots that leave critical assets exposed. By offering a structured lens to align future AI technologies and governance with trustworthy data practices, we underscore the urgency of rethinking data protection for modern AI techniques and provide timely guidance for developers, researchers, and regulators alike.
>
---
#### [new 312] SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于图像生成任务，旨在解决多视角X-ray图像合成问题。通过扩散模型实现单视角到多视角的高质量X-ray图像生成，提升临床与教育应用。**

- **链接: [http://arxiv.org/pdf/2507.05148v1](http://arxiv.org/pdf/2507.05148v1)**

> **作者:** Chun Xie; Yuichi Yoshii; Itaru Kitahara
>
> **备注:** Accepted by MICCAI2025
>
> **摘要:** X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at GitHub.
>
---
#### [new 313] Latent Motion Profiling for Annotation-free Cardiac Phase Detection in Adult and Fetal Echocardiography Videos
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于心脏相位检测任务，解决无标注数据下自动识别心室收缩和舒张阶段的问题。通过自监督学习提取潜在运动轨迹，实现无需人工标注的精准检测。**

- **链接: [http://arxiv.org/pdf/2507.05154v1](http://arxiv.org/pdf/2507.05154v1)**

> **作者:** Yingyu Yang; Qianye Yang; Kangning Cui; Can Peng; Elena D'Alberti; Netzahualcoyotl Hernandez-Cruz; Olga Patey; Aris T. Papageorghiou; J. Alison Noble
>
> **摘要:** The identification of cardiac phase is an essential step for analysis and diagnosis of cardiac function. Automatic methods, especially data-driven methods for cardiac phase detection, typically require extensive annotations, which is time-consuming and labor-intensive. In this paper, we present an unsupervised framework for end-diastole (ED) and end-systole (ES) detection through self-supervised learning of latent cardiac motion trajectories from 4-chamber-view echocardiography videos. Our method eliminates the need for manual annotations, including ED and ES indices, segmentation, or volumetric measurements, by training a reconstruction model to encode interpretable spatiotemporal motion patterns. Evaluated on the EchoNet-Dynamic benchmark, the approach achieves mean absolute error (MAE) of 3 frames (58.3 ms) for ED and 2 frames (38.8 ms) for ES detection, matching state-of-the-art supervised methods. Extended to fetal echocardiography, the model demonstrates robust performance with MAE 1.46 frames (20.7 ms) for ED and 1.74 frames (25.3 ms) for ES, despite the fact that the fetal heart model is built using non-standardized heart views due to fetal heart positioning variability. Our results demonstrate the potential of the proposed latent motion trajectory strategy for cardiac phase detection in adult and fetal echocardiography. This work advances unsupervised cardiac motion analysis, offering a scalable solution for clinical populations lacking annotated data. Code will be released at https://github.com/YingyuYyy/CardiacPhase.
>
---
#### [new 314] AutoLayout: Closed-Loop Layout Synthesis via Slow-Fast Collaborative Reasoning
- **分类: cs.RO; cs.CV**

- **简介: 该论文属于布局生成任务，解决空间幻觉和语义与物理一致性问题。提出AutoLayout，结合慢速推理与快速验证，提升布局的物理合理性与语义准确度。**

- **链接: [http://arxiv.org/pdf/2507.04293v1](http://arxiv.org/pdf/2507.04293v1)**

> **作者:** Weixing Chen; Dafeng Chi; Yang Liu; Yuxi Yang; Yexin Zhang; Yuzheng Zhuang; Xingyue Quan; Jianye Hao; Guanbin Li; Liang Lin
>
> **摘要:** The automated generation of layouts is vital for embodied intelligence and autonomous systems, supporting applications from virtual environment construction to home robot deployment. Current approaches, however, suffer from spatial hallucination and struggle with balancing semantic fidelity and physical plausibility, often producing layouts with deficits such as floating or overlapping objects and misaligned stacking relation. In this paper, we propose AutoLayout, a fully automated method that integrates a closed-loop self-validation process within a dual-system framework. Specifically, a slow system harnesses detailed reasoning with a Reasoning-Reflection-Generation (RRG) pipeline to extract object attributes and spatial constraints. Then, a fast system generates discrete coordinate sets and a topological relation set that are jointly validated. To mitigate the limitations of handcrafted rules, we further introduce an LLM-based Adaptive Relation Library (ARL) for generating and evaluating layouts. Through the implementation of Slow-Fast Collaborative Reasoning, the AutoLayout efficiently generates layouts after thorough deliberation, effectively mitigating spatial hallucination. Its self-validation mechanism establishes a closed-loop process that iteratively corrects potential errors, achieving a balance between physical stability and semantic consistency. The effectiveness of AutoLayout was validated across 8 distinct scenarios, where it demonstrated a significant 10.1% improvement over SOTA methods in terms of physical plausibility, semantic consistency, and functional completeness.
>
---
#### [new 315] ConBatch-BAL: Batch Bayesian Active Learning under Budget Constraints
- **分类: cs.LG; cs.CV**

- **简介: 该论文属于主动学习任务，解决标注成本高和预算限制问题。提出两种基于贝叶斯神经网络的批量主动学习方法，提升效率并降低费用。**

- **链接: [http://arxiv.org/pdf/2507.04929v1](http://arxiv.org/pdf/2507.04929v1)**

> **作者:** Pablo G. Morato; Charalampos P. Andriotis; Seyran Khademi
>
> **摘要:** Varying annotation costs among data points and budget constraints can hinder the adoption of active learning strategies in real-world applications. This work introduces two Bayesian active learning strategies for batch acquisition under constraints (ConBatch-BAL), one based on dynamic thresholding and one following greedy acquisition. Both select samples using uncertainty metrics computed via Bayesian neural networks. The dynamic thresholding strategy redistributes the budget across the batch, while the greedy one selects the top-ranked sample at each step, limited by the remaining budget. Focusing on scenarios with costly data annotation and geospatial constraints, we also release two new real-world datasets containing geolocated aerial images of buildings, annotated with energy efficiency or typology classes. The ConBatch-BAL strategies are benchmarked against a random acquisition baseline on these datasets under various budget and cost scenarios. The results show that the developed ConBatch-BAL strategies can reduce active learning iterations and data acquisition costs in real-world settings, and even outperform the unconstrained baseline solutions.
>
---
#### [new 316] Accurate and Efficient World Modeling with Masked Latent Transformers
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文属于强化学习中的世界建模任务，旨在解决现有方法在信息压缩和训练效率上的不足。提出EMERALD模型，通过空间潜在状态和MaskGIT预测提升准确性和效率。**

- **链接: [http://arxiv.org/pdf/2507.04075v1](http://arxiv.org/pdf/2507.04075v1)**

> **作者:** Maxime Burchi; Radu Timofte
>
> **摘要:** The Dreamer algorithm has recently obtained remarkable performance across diverse environment domains by training powerful agents with simulated trajectories. However, the compressed nature of its world model's latent space can result in the loss of crucial information, negatively affecting the agent's performance. Recent approaches, such as $\Delta$-IRIS and DIAMOND, address this limitation by training more accurate world models. However, these methods require training agents directly from pixels, which reduces training efficiency and prevents the agent from benefiting from the inner representations learned by the world model. In this work, we propose an alternative approach to world modeling that is both accurate and efficient. We introduce EMERALD (Efficient MaskEd latent tRAnsformer worLD model), a world model using a spatial latent state with MaskGIT predictions to generate accurate trajectories in latent space and improve the agent performance. On the Crafter benchmark, EMERALD achieves new state-of-the-art performance, becoming the first method to surpass human experts performance within 10M environment steps. Our method also succeeds to unlock all 22 Crafter achievements at least once during evaluation.
>
---
#### [new 317] Regulation Compliant AI for Fusion: Real-Time Image Analysis-Based Control of Divertor Detachment in Tokamaks
- **分类: cs.LG; cs.CV; cs.SY; eess.SY; physics.plasm-ph**

- **简介: 该论文属于融合控制任务，解决AI在监管环境中的合规性问题。通过实时图像分析实现托卡马克偏滤器分离控制，构建可解释的线性控制系统。**

- **链接: [http://arxiv.org/pdf/2507.02897v1](http://arxiv.org/pdf/2507.02897v1)**

> **作者:** Nathaniel Chen; Cheolsik Byun; Azarakash Jalalvand; Sangkyeun Kim; Andrew Rothstein; Filippo Scotti; Steve Allen; David Eldon; Keith Erickson; Egemen Kolemen
>
> **摘要:** While artificial intelligence (AI) has been promising for fusion control, its inherent black-box nature will make compliant implementation in regulatory environments a challenge. This study implements and validates a real-time AI enabled linear and interpretable control system for successful divertor detachment control with the DIII-D lower divertor camera. Using D2 gas, we demonstrate feedback divertor detachment control with a mean absolute difference of 2% from the target for both detachment and reattachment. This automatic training and linear processing framework can be extended to any image based diagnostic for regulatory compliant controller necessary for future fusion reactors.
>
---
#### [new 318] CP-Dilatation: A Copy-and-Paste Augmentation Method for Preserving the Boundary Context Information of Histopathology Images
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于医学图像分割任务，旨在解决标注数据不足的问题。提出CP-Dilatation方法，在复制粘贴增强中加入膨胀操作，以保留病变边界上下文信息。**

- **链接: [http://arxiv.org/pdf/2507.04660v1](http://arxiv.org/pdf/2507.04660v1)**

> **作者:** Sungrae Hong; Sol Lee; Mun Yong Yi
>
> **备注:** 5 pages, 5 figures
>
> **摘要:** Medical AI diagnosis including histopathology segmentation has derived benefits from the recent development of deep learning technology. However, deep learning itself requires a large amount of training data and the medical image segmentation masking, in particular, requires an extremely high cost due to the shortage of medical specialists. To mitigate this issue, we propose a new data augmentation method built upon the conventional Copy and Paste (CP) augmentation technique, called CP-Dilatation, and apply it to histopathology image segmentation. To the well-known traditional CP technique, the proposed method adds a dilation operation that can preserve the boundary context information of the malignancy, which is important in histopathological image diagnosis, as the boundary between the malignancy and its margin is mostly unclear and a significant context exists in the margin. In our experiments using histopathology benchmark datasets, the proposed method was found superior to the other state-of-the-art baselines chosen for comparison.
>
---
#### [new 319] QMoE: A Quantum Mixture of Experts Framework for Scalable Quantum Neural Networks
- **分类: quant-ph; cs.CV**

- **简介: 该论文属于量子机器学习任务，旨在解决QML模型的可扩展性和表达力问题。提出QMoE架构，结合专家模型和量子路由机制，提升分类性能。**

- **链接: [http://arxiv.org/pdf/2507.05190v1](http://arxiv.org/pdf/2507.05190v1)**

> **作者:** Hoang-Quan Nguyen; Xuan-Bac Nguyen; Sankalp Pandey; Samee U. Khan; Ilya Safro; Khoa Luu
>
> **摘要:** Quantum machine learning (QML) has emerged as a promising direction in the noisy intermediate-scale quantum (NISQ) era, offering computational and memory advantages by harnessing superposition and entanglement. However, QML models often face challenges in scalability and expressiveness due to hardware constraints. In this paper, we propose quantum mixture of experts (QMoE), a novel quantum architecture that integrates the mixture of experts (MoE) paradigm into the QML setting. QMoE comprises multiple parameterized quantum circuits serving as expert models, along with a learnable quantum routing mechanism that selects and aggregates specialized quantum experts per input. The empirical results from the proposed QMoE on quantum classification tasks demonstrate that it consistently outperforms standard quantum neural networks, highlighting its effectiveness in learning complex data patterns. Our work paves the way for scalable and interpretable quantum learning frameworks.
>
---
#### [new 320] README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model
- **分类: cs.CR; cs.CV**

- **简介: 该论文属于数字签名任务，解决图像水印在错误环境下的可靠性问题。提出README框架，提升嵌入2048位签名的鲁棒性与准确性。**

- **链接: [http://arxiv.org/pdf/2507.04495v1](http://arxiv.org/pdf/2507.04495v1)**

> **作者:** Hyunwook Choi; Sangyun Won; Daeyeon Hwang; Junhyeok Choi
>
> **摘要:** Deep learning-based watermarking has emerged as a promising solution for robust image authentication and protection. However, existing models are limited by low embedding capacity and vulnerability to bit-level errors, making them unsuitable for cryptographic applications such as digital signatures, which require over 2048 bits of error-free data. In this paper, we propose README (Robust Error-Aware Digital Signature via Deep WaterMarking ModEl), a novel framework that enables robust, verifiable, and error-tolerant digital signatures within images. Our method combines a simple yet effective cropping-based capacity scaling mechanism with ERPA (ERror PAinting Module), a lightweight error correction module designed to localize and correct bit errors using Distinct Circular Subsum Sequences (DCSS). Without requiring any fine-tuning of existing pretrained watermarking models, README significantly boosts the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when embedding 2048-bit digital signatures into a single image, even under real-world distortions. Moreover, our use of perceptual hash-based signature verification ensures public verifiability and robustness against tampering. The proposed framework unlocks a new class of high-assurance applications for deep watermarking, bridging the gap between signal-level watermarking and cryptographic security.
>
---
#### [new 321] Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference
- **分类: cs.AI; cs.CV; cs.LG; cs.RO**

- **简介: 该论文属于人工智能领域，旨在解决传统AI缺乏生物智能的问题。通过构建千脑系统Monty，实现快速、鲁棒的学习与推理，提升3D物体识别与姿态估计能力。**

- **链接: [http://arxiv.org/pdf/2507.04494v1](http://arxiv.org/pdf/2507.04494v1)**

> **作者:** Niels Leadholm; Viviane Clay; Scott Knudstrup; Hojae Lee; Jeff Hawkins
>
> **备注:** 32 pages, 8 figures
>
> **摘要:** Current AI systems achieve impressive performance on many tasks, yet they lack core attributes of biological intelligence, including rapid, continual learning, representations grounded in sensorimotor interactions, and structured knowledge that enables efficient generalization. Neuroscience theory suggests that mammals evolved flexible intelligence through the replication of a semi-independent, sensorimotor module, a functional unit known as a cortical column. To address the disparity between biological and artificial intelligence, thousand-brains systems were proposed as a means of mirroring the architecture of cortical columns and their interactions. In the current work, we evaluate the unique properties of Monty, the first implementation of a thousand-brains system. We focus on 3D object perception, and in particular, the combined task of object recognition and pose estimation. Utilizing the YCB dataset of household objects, we first assess Monty's use of sensorimotor learning to build structured representations, finding that these enable robust generalization. These representations include an emphasis on classifying objects by their global shape, as well as a natural ability to detect object symmetries. We then explore Monty's use of model-free and model-based policies to enable rapid inference by supporting principled movements. We find that such policies complement Monty's modular architecture, a design that can accommodate communication between modules to further accelerate inference speed via a novel `voting' algorithm. Finally, we examine Monty's use of associative, Hebbian-like binding to enable rapid, continual, and computationally efficient learning, properties that compare favorably to current deep learning architectures. While Monty is still in a nascent stage of development, these findings support thousand-brains systems as a powerful and promising new approach to AI.
>
---
#### [new 322] Efficacy of Image Similarity as a Metric for Augmenting Small Dataset Retinal Image Segmentation
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于医学图像分割任务，研究如何通过合成图像增强小数据集。工作是评估FID指标与分割性能提升的关系，发现相似度高（低FID）的合成数据更有效。**

- **链接: [http://arxiv.org/pdf/2507.04862v1](http://arxiv.org/pdf/2507.04862v1)**

> **作者:** Thomas Wallace; Ik Siong Heng; Senad Subasic; Chris Messenger
>
> **备注:** 30 pages, 10 figures
>
> **摘要:** Synthetic images are an option for augmenting limited medical imaging datasets to improve the performance of various machine learning models. A common metric for evaluating synthetic image quality is the Fr\'echet Inception Distance (FID) which measures the similarity of two image datasets. In this study we evaluate the relationship between this metric and the improvement which synthetic images, generated by a Progressively Growing Generative Adversarial Network (PGGAN), grant when augmenting Diabetes-related Macular Edema (DME) intraretinal fluid segmentation performed by a U-Net model with limited amounts of training data. We find that the behaviour of augmenting with standard and synthetic images agrees with previously conducted experiments. Additionally, we show that dissimilar (high FID) datasets do not improve segmentation significantly. As FID between the training and augmenting datasets decreases, the augmentation datasets are shown to contribute to significant and robust improvements in image segmentation. Finally, we find that there is significant evidence to suggest that synthetic and standard augmentations follow separate log-normal trends between FID and improvements in model performance, with synthetic data proving more effective than standard augmentation techniques. Our findings show that more similar datasets (lower FID) will be more effective at improving U-Net performance, however, the results also suggest that this improvement may only occur when images are sufficiently dissimilar.
>
---
#### [new 323] Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting
- **分类: cs.LG; cs.AI; cs.CV**

- **简介: 该论文属于时空预测任务，旨在解决复杂模型训练效率低和内存消耗大的问题。提出SDKD框架，通过频域对齐的知识蒸馏方法，提升轻量模型的性能。**

- **链接: [http://arxiv.org/pdf/2507.02939v1](http://arxiv.org/pdf/2507.02939v1)**

> **作者:** Yuqi Li; Chuanguang Yang; Hansheng Zeng; Zeyu Dong; Zhulin An; Yongjun Xu; Yingli Tian; Hao Wu
>
> **备注:** Accepted by ICCV-2025, 11 pages
>
> **摘要:** Spatiotemporal forecasting tasks, such as traffic flow, combustion dynamics, and weather forecasting, often require complex models that suffer from low training efficiency and high memory consumption. This paper proposes a lightweight framework, Spectral Decoupled Knowledge Distillation (termed SDKD), which transfers the multi-scale spatiotemporal representations from a complex teacher model to a more efficient lightweight student network. The teacher model follows an encoder-latent evolution-decoder architecture, where its latent evolution module decouples high-frequency details and low-frequency trends using convolution and Transformer (global low-frequency modeler). However, the multi-layer convolution and deconvolution structures result in slow training and high memory usage. To address these issues, we propose a frequency-aligned knowledge distillation strategy, which extracts multi-scale spectral features from the teacher's latent space, including both high and low frequency components, to guide the lightweight student model in capturing both local fine-grained variations and global evolution patterns. Experimental results show that SDKD significantly improves performance, achieving reductions of up to 81.3% in MSE and in MAE 52.3% on the Navier-Stokes equation dataset. The framework effectively captures both high-frequency variations and long-term trends while reducing computational complexity. Our codes are available at https://github.com/itsnotacie/SDKD
>
---
#### [new 324] MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic Rewarded Group Relative Policy Optimization
- **分类: cs.LG; cs.CV**

- **简介: 该论文属于医学图像定位任务，解决传统方法依赖昂贵标注的问题。通过引入空间语义奖励和链式框模板，提升模型在无CoT标注下的定位能力。**

- **链接: [http://arxiv.org/pdf/2507.02994v1](http://arxiv.org/pdf/2507.02994v1)**

> **作者:** Huihui Xu; Yuanpeng Nie; Hualiang Wang; Ying Chen; Wei Li; Junzhi Ning; Lihao Liu; Hongqiu Wang; Lei Zhu; Jiyao Liu; Xiaomeng Li; Junjun He
>
> **备注:** MICCAI2025 Early Accept
>
> **摘要:** Medical Image Grounding (MIG), which involves localizing specific regions in medical images based on textual descriptions, requires models to not only perceive regions but also deduce spatial relationships of these regions. Existing Vision-Language Models (VLMs) for MIG often rely on Supervised Fine-Tuning (SFT) with large amounts of Chain-of-Thought (CoT) reasoning annotations, which are expensive and time-consuming to acquire. Recently, DeepSeek-R1 demonstrated that Large Language Models (LLMs) can acquire reasoning abilities through Group Relative Policy Optimization (GRPO) without requiring CoT annotations. In this paper, we adapt the GRPO reinforcement learning framework to VLMs for Medical Image Grounding. We propose the Spatial-Semantic Rewarded Group Relative Policy Optimization to train the model without CoT reasoning annotations. Specifically, we introduce Spatial-Semantic Rewards, which combine spatial accuracy reward and semantic consistency reward to provide nuanced feedback for both spatially positive and negative completions. Additionally, we propose to use the Chain-of-Box template, which integrates visual information of referring bounding boxes into the <think> reasoning process, enabling the model to explicitly reason about spatial regions during intermediate steps. Experiments on three datasets MS-CXR, ChestX-ray8, and M3D-RefSeg demonstrate that our method achieves state-of-the-art performance in Medical Image Grounding. Ablation studies further validate the effectiveness of each component in our approach. Code, checkpoints, and datasets are available at https://github.com/bio-mlhui/MedGround-R1
>
---
#### [new 325] EdgeSRIE: A hybrid deep learning framework for real-time speckle reduction and image enhancement on portable ultrasound systems
- **分类: eess.IV; cs.AI; cs.CV**

- **简介: 该论文属于医学图像处理任务，旨在解决便携式超声系统中的斑点噪声问题。提出EdgeSRIE框架，实现实时去噪与图像增强。**

- **链接: [http://arxiv.org/pdf/2507.03937v1](http://arxiv.org/pdf/2507.03937v1)**

> **作者:** Hyunwoo Cho; Jongsoo Lee; Jinbum Kang; Yangmo Yoo
>
> **摘要:** Speckle patterns in ultrasound images often obscure anatomical details, leading to diagnostic uncertainty. Recently, various deep learning (DL)-based techniques have been introduced to effectively suppress speckle; however, their high computational costs pose challenges for low-resource devices, such as portable ultrasound systems. To address this issue, EdgeSRIE, which is a lightweight hybrid DL framework for real-time speckle reduction and image enhancement in portable ultrasound imaging, is introduced. The proposed framework consists of two main branches: an unsupervised despeckling branch, which is trained by minimizing a loss function between speckled images, and a deblurring branch, which restores blurred images to sharp images. For hardware implementation, the trained network is quantized to 8-bit integer precision and deployed on a low-resource system-on-chip (SoC) with limited power consumption. In the performance evaluation with phantom and in vivo analyses, EdgeSRIE achieved the highest contrast-to-noise ratio (CNR) and average gradient magnitude (AGM) compared with the other baselines (different 2-rule-based methods and other 4-DL-based methods). Furthermore, EdgeSRIE enabled real-time inference at over 60 frames per second while satisfying computational requirements (< 20K parameters) on actual portable ultrasound hardware. These results demonstrated the feasibility of EdgeSRIE for real-time, high-quality ultrasound imaging in resource-limited environments.
>
---
#### [new 326] Surg-SegFormer: A Dual Transformer-Based Model for Holistic Surgical Scene Segmentation
- **分类: eess.IV; cs.AI; cs.CV**

- **简介: 该论文属于手术场景分割任务，解决机器人手术中实时解释困难和专家不足的问题。提出Surg-SegFormer模型，实现自动、准确的手术区域分割。**

- **链接: [http://arxiv.org/pdf/2507.04304v1](http://arxiv.org/pdf/2507.04304v1)**

> **作者:** Fatimaelzahraa Ahmed; Muraam Abdel-Ghani; Muhammad Arsalan; Mahmoud Ali; Abdulaziz Al-Ali; Shidin Balakrishnan
>
> **备注:** Accepted in IEEE Case 2025
>
> **摘要:** Holistic surgical scene segmentation in robot-assisted surgery (RAS) enables surgical residents to identify various anatomical tissues, articulated tools, and critical structures, such as veins and vessels. Given the firm intraoperative time constraints, it is challenging for surgeons to provide detailed real-time explanations of the operative field for trainees. This challenge is compounded by the scarcity of expert surgeons relative to trainees, making the unambiguous delineation of go- and no-go zones inconvenient. Therefore, high-performance semantic segmentation models offer a solution by providing clear postoperative analyses of surgical procedures. However, recent advanced segmentation models rely on user-generated prompts, rendering them impractical for lengthy surgical videos that commonly exceed an hour. To address this challenge, we introduce Surg-SegFormer, a novel prompt-free model that outperforms current state-of-the-art techniques. Surg-SegFormer attained a mean Intersection over Union (mIoU) of 0.80 on the EndoVis2018 dataset and 0.54 on the EndoVis2017 dataset. By providing robust and automated surgical scene comprehension, this model significantly reduces the tutoring burden on expert surgeons, empowering residents to independently and effectively understand complex surgical environments.
>
---
#### [new 327] Grid-Reg: Grid-Based SAR and Optical Image Registration Across Platforms
- **分类: eess.IV; cs.CV**

- **简介: 该论文属于跨平台图像配准任务，解决SAR与光学图像间的几何和辐射差异问题。提出Grid-Reg框架，结合HSCMLNet和Grid-solver提升配准精度。**

- **链接: [http://arxiv.org/pdf/2507.04233v1](http://arxiv.org/pdf/2507.04233v1)**

> **作者:** Xiaochen Wei; Weiwei Guo; Zenghui Zhang; Wenxian Yu
>
> **摘要:** Registering airborne SAR with spaceborne optical images is crucial for SAR image interpretation and geo-localization. It is challenging for this cross-platform heterogeneous image registration due to significant geometric and radiation differences, which current methods fail to handle. To tackle these challenges, we propose a novel grid-based multimodal registration framework (Grid-Reg) across airborne and space-born platforms, including a new domain-robust descriptor extraction network, Hybrid Siamese Correlation Metric Learning Network (HSCMLNet) and a grid-based solver (Grid-solver) for transformation parameters estimation. Our Grid-Reg is based on detector-free and global matching loss rather than accurate keypoint correspondences. These accurate correspondences are inherently difficult in heterogeneous images with large geometric deformation. By Grid-Solver, our Grid-Reg estimates transformation parameters by optimizing robust global matching loss-based patch correspondences of whole images in a coarse-to-fine strategy. To robustly calculate the similarity between patches, specifically that have noise and change objects, we propose HSCMLNet, including a hybrid Siamese module to extract high-level features of multimodal images and a correlation learning module (CMLModule) based equiangular unit basis vectors (EUBVs). Moreover, we propose a manifold loss EUBVsLoss to constrain the normalized correlation between local embeddings of patches and EUBVs. Furthermore, we curate a new challenging benchmark dataset of SAR-to-optical registration using real-world UAV MiniSAR data and optical images from Google Earth. We extensively analyze factors affecting registration accuracy and compare our method with state-of-the-art techniques on this dataset, showing superior performance.
>
---
#### [new 328] EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation
- **分类: cs.SD; cs.AI; cs.CV; cs.MM; eess.AS**

- **简介: 该论文属于多模态音乐生成任务，旨在通过面部表情和肢体动作控制生成同步且富有表现力的音乐。工作包括引入PEFT方法和时间对齐策略，提升音乐质量与视频同步性。**

- **链接: [http://arxiv.org/pdf/2507.04955v1](http://arxiv.org/pdf/2507.04955v1)**

> **作者:** Fathinah Izzati; Xinyue Li; Gus Xia
>
> **摘要:** We propose Expotion (Facial Expression and Motion Control for Multimodal Music Generation), a generative model leveraging multimodal visual controls - specifically, human facial expressions and upper-body motion - as well as text prompts to produce expressive and temporally accurate music. We adopt parameter-efficient fine-tuning (PEFT) on the pretrained text-to-music generation model, enabling fine-grained adaptation to the multimodal controls using a small dataset. To ensure precise synchronization between video and music, we introduce a temporal smoothing strategy to align multiple modalities. Experiments demonstrate that integrating visual features alongside textual descriptions enhances the overall quality of generated music in terms of musicality, creativity, beat-tempo consistency, temporal alignment with the video, and text adherence, surpassing both proposed baselines and existing state-of-the-art video-to-music generation models. Additionally, we introduce a novel dataset consisting of 7 hours of synchronized video recordings capturing expressive facial and upper-body gestures aligned with corresponding music, providing significant potential for future research in multimodal and interactive music generation.
>
---
## 更新

#### [replaced 001] Mitigating Bias Using Model-Agnostic Data Attribution
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2405.05031v2](http://arxiv.org/pdf/2405.05031v2)**

> **作者:** Sander De Coninck; Sam Leroux; Pieter Simoens
>
> **备注:** Accepted to the 2024 IEEE CVPR Workshop on Fair, Data-efficient, and Trusted Computer Vision. Code available at https://github.com/sdeconinck/ModelAgnosticDataAttribution
>
> **摘要:** Mitigating bias in machine learning models is a critical endeavor for ensuring fairness and equity. In this paper, we propose a novel approach to address bias by leveraging pixel image attributions to identify and regularize regions of images containing significant information about bias attributes. Our method utilizes a model-agnostic approach to extract pixel attributions by employing a convolutional neural network (CNN) classifier trained on small image patches. By training the classifier to predict a property of the entire image using only a single patch, we achieve region-based attributions that provide insights into the distribution of important information across the image. We propose utilizing these attributions to introduce targeted noise into datasets with confounding attributes that bias the data, thereby constraining neural networks from learning these biases and emphasizing the primary attributes. Our approach demonstrates its efficacy in enabling the training of unbiased classifiers on heavily biased datasets.
>
---
#### [replaced 002] DiT4SR: Taming Diffusion Transformer for Real-World Image Super-Resolution
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.23580v2](http://arxiv.org/pdf/2503.23580v2)**

> **作者:** Zheng-Peng Duan; Jiawei Zhang; Xin Jin; Ziheng Zhang; Zheng Xiong; Dongqing Zou; Jimmy S. Ren; Chun-Le Guo; Chongyi Li
>
> **摘要:** Large-scale pre-trained diffusion models are becoming increasingly popular in solving the Real-World Image Super-Resolution (Real-ISR) problem because of their rich generative priors. The recent development of diffusion transformer (DiT) has witnessed overwhelming performance over the traditional UNet-based architecture in image generation, which also raises the question: Can we adopt the advanced DiT-based diffusion model for Real-ISR? To this end, we propose our DiT4SR, one of the pioneering works to tame the large-scale DiT model for Real-ISR. Instead of directly injecting embeddings extracted from low-resolution (LR) images like ControlNet, we integrate the LR embeddings into the original attention mechanism of DiT, allowing for the bidirectional flow of information between the LR latent and the generated latent. The sufficient interaction of these two streams allows the LR stream to evolve with the diffusion process, producing progressively refined guidance that better aligns with the generated latent at each diffusion step. Additionally, the LR guidance is injected into the generated latent via a cross-stream convolution layer, compensating for DiT's limited ability to capture local information. These simple but effective designs endow the DiT model with superior performance in Real-ISR, which is demonstrated by extensive experiments. Project Page: https://adam-duan.github.io/projects/dit4sr/.
>
---
#### [replaced 003] Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2506.09695v2](http://arxiv.org/pdf/2506.09695v2)**

> **作者:** Changwei Wu; Yifei Chen; Yuxin Du; Jinying Zong; Jie Dong; Mingxuan Liu; Yong Peng; Jin Fan; Feiwei Qin; Changmiao Wang
>
> **备注:** 11 pages, 5 figures
>
> **摘要:** Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive impairment (MCI) stage, is vital yet hindered by subjective assessments and the high cost of multimodal imaging modalities. Although deep learning methods offer automated alternatives, their energy inefficiency and computational demands limit real-world deployment, particularly in resource-constrained settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are inherently well-suited for modeling the sparse, event-driven patterns of neural degeneration in AD, offering a promising foundation for interpretable and low-power medical diagnostics. However, existing SNNs often suffer from weak expressiveness and unstable training, which restrict their effectiveness in complex medical tasks. To address these limitations, we propose FasterSNN, a hybrid neural architecture that integrates biologically inspired LIF neurons with region-adaptive convolution and multi-scale spiking attention. This design enables sparse, efficient processing of 3D MRI while preserving diagnostic accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves competitive performance with substantially improved efficiency and stability, supporting its potential for practical AD screening. Our source code is available at https://github.com/wuchangw/FasterSNN.
>
---
#### [replaced 004] Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.02826v2](http://arxiv.org/pdf/2507.02826v2)**

> **作者:** Panpan Ji; Junni Song; Hang Xiao; Hanyu Liu; Chao Li
>
> **摘要:** Sensor-based Human Activity Recognition (HAR) is a core technology that enables intelligent systems to perceive and interact with their environment. However, multimodal HAR systems still encounter key challenges, such as difficulties in cross-modal feature alignment and imbalanced modality contributions. To address these issues, we propose a novel framework called the Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three key components. First, a dual-path feature extraction architecture is employed, where ResNet and DenseNet branches collaboratively process multimodal sensor data. Second, a multi-stage contrastive learning mechanism is introduced to achieve progressive alignment from local perception to semantic abstraction. Third, we present a confidence-driven gradient modulation strategy that dynamically monitors and adjusts the learning intensity of each modality branch during backpropagation, effectively alleviating modality competition. In addition, a momentum-based gradient accumulation strategy is adopted to enhance training stability. We conduct ablation studies to validate the effectiveness of each component and perform extensive comparative experiments on four public benchmark datasets.
>
---
#### [replaced 005] Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.05163v2](http://arxiv.org/pdf/2505.05163v2)**

> **作者:** Aishwarya Venkataramanan; Paul Bodesheim; Joachim Denzler
>
> **备注:** UAI 2025, 22 pages
>
> **摘要:** Vision-Language Models (VLMs) learn joint representations by mapping images and text into a shared latent space. However, recent research highlights that deterministic embeddings from standard VLMs often struggle to capture the uncertainties arising from the ambiguities in visual and textual descriptions and the multiple possible correspondences between images and texts. Existing approaches tackle this by learning probabilistic embeddings during VLM training, which demands large datasets and does not leverage the powerful representations already learned by large-scale VLMs like CLIP. In this paper, we propose GroVE, a post-hoc approach to obtaining probabilistic embeddings from frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model (GPLVM) to learn a shared low-dimensional latent space where image and text inputs are mapped to a unified representation, optimized through single-modal embedding reconstruction and cross-modal alignment objectives. Once trained, the Gaussian Process model generates uncertainty-aware probabilistic embeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty calibration across multiple downstream tasks, including cross-modal retrieval, visual question answering, and active learning.
>
---
#### [replaced 006] Distilling High Diagnostic Value Patches for Whole Slide Image Classification Using Attention Mechanism
- **分类: eess.IV; cs.CV; q-bio.TO**

- **链接: [http://arxiv.org/pdf/2407.19821v3](http://arxiv.org/pdf/2407.19821v3)**

> **作者:** Tianhang Nan; Hao Quan; Yong Ding; Xingyu Li; Kai Yang; Xiaoyu Cui
>
> **摘要:** Multiple Instance Learning (MIL) has garnered widespread attention in the field of Whole Slide Image (WSI) classification as it replaces pixel-level manual annotation with diagnostic reports as labels, significantly reducing labor costs. Recent research has shown that bag-level MIL methods often yield better results because they can consider all patches of the WSI as a whole. However, a drawback of such methods is the incorporation of more redundant patches, leading to interference. To extract patches with high diagnostic value while excluding interfering patches to address this issue, we developed an attention-based feature distillation multi-instance learning (AFD-MIL) approach. This approach proposed the exclusion of redundant patches as a preprocessing operation in weakly supervised learning, directly mitigating interference from extensive noise. It also pioneers the use of attention mechanisms to distill features with high diagnostic value, as opposed to the traditional practice of indiscriminately and forcibly integrating all patches. Additionally, we introduced global loss optimization to finely control the feature distillation module. AFD-MIL is orthogonal to many existing MIL methods, leading to consistent performance improvements. This approach has surpassed the current state-of-the-art method, achieving 91.47% ACC (accuracy) and 94.29% AUC (area under the curve) on the Camelyon16 (Camelyon Challenge 2016, breast cancer), while 93.33% ACC and 98.17% AUC on the TCGA-NSCLC (The Cancer Genome Atlas Program: non-small cell lung cancer). Different feature distillation methods were used for the two datasets, tailored to the specific diseases, thereby improving performance and interpretability.
>
---
#### [replaced 007] CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation
- **分类: eess.IV; cs.AI; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2506.23121v2](http://arxiv.org/pdf/2506.23121v2)**

> **作者:** Xinlei Yu; Changmiao Wang; Hui Jin; Ahmed Elazab; Gangyong Jia; Xiang Wan; Changqing Zou; Ruiquan Ge
>
> **备注:** Accepted By ACMMM25
>
> **摘要:** Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: https://github.com/YU-deep/CRISP_SAM2.git.
>
---
#### [replaced 008] CCi-YOLOv8n: Enhanced Fire Detection with CARAFE and Context-Guided Modules
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2411.11011v3](http://arxiv.org/pdf/2411.11011v3)**

> **作者:** Kunwei Lv; Ruobing Wu; Suyang Chen; Ping Lan
>
> **备注:** 13 pages,7 figures
>
> **摘要:** Fire incidents in urban and forested areas pose serious threats,underscoring the need for more effective detection technologies. To address these challenges, we present CCi-YOLOv8n, an enhanced YOLOv8 model with targeted improvements for detecting small fires and smoke. The model integrates the CARAFE up-sampling operator and a context-guided module to reduce information loss during up-sampling and down-sampling, thereby retaining richer feature representations. Additionally, an inverted residual mobile block enhanced C2f module captures small targets and fine smoke patterns, a critical improvement over the original model's detection capacity.For validation, we introduce Web-Fire, a dataset curated for fire and smoke detection across diverse real-world scenarios. Experimental results indicate that CCi-YOLOv8n outperforms YOLOv8n in detection precision, confirming its effectiveness for robust fire detection tasks.
>
---
#### [replaced 009] EventVAD: Training-Free Event-Aware Video Anomaly Detection
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.13092v2](http://arxiv.org/pdf/2504.13092v2)**

> **作者:** Yihua Shao; Haojin He; Sijie Li; Siyu Chen; Xinwei Long; Fanhu Zeng; Yuxuan Fan; Muyang Zhang; Ziyang Yan; Ao Ma; Xiaochen Wang; Hao Tang; Yan Wang; Shuyan Li
>
> **备注:** Accepted by ACM MM 2025
>
> **摘要:** Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos. Supervised methods require an amount of in-domain training data and often struggle to generalize to unseen anomalies. In contrast, training-free methods leverage the intrinsic world knowledge of large language models (LLMs) to detect anomalies but face challenges in localizing fine-grained visual transitions and diverse events. Therefore, we propose EventVAD, an event-aware video anomaly detection framework that combines tailored dynamic graph architectures and multimodal LLMs through temporal-event reasoning. Specifically, EventVAD first employs dynamic spatiotemporal graph modeling with time-decay constraints to capture event-aware video features. Then, it performs adaptive noise filtering and uses signal ratio thresholding to detect event boundaries via unsupervised statistical features. The statistical boundary detection module reduces the complexity of processing long videos for MLLMs and improves their temporal reasoning through event consistency. Finally, it utilizes a hierarchical prompting strategy to guide MLLMs in performing reasoning before determining final decisions. We conducted extensive experiments on the UCF-Crime and XD-Violence datasets. The results demonstrate that EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free settings, outperforming strong baselines that use 7B or larger MLLMs.
>
---
#### [replaced 010] Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.10943v3](http://arxiv.org/pdf/2412.10943v3)**

> **作者:** Zhangjun Zhou; Yiping Li; Chunlin Zhong; Jianuo Huang; Jialun Pei; Hua Li; He Tang
>
> **备注:** 17 pages, 11 figures
>
> **摘要:** While the human visual system employs distinct mechanisms to perceive salient and camouflaged objects, existing models struggle to disentangle these tasks. Specifically, salient object detection (SOD) models frequently misclassify camouflaged objects as salient, while camouflaged object detection (COD) models conversely misinterpret salient objects as camouflaged. We hypothesize that this can be attributed to two factors: (i) the specific annotation paradigm of current SOD and COD datasets, and (ii) the lack of explicit attribute relationship modeling in current models. Prevalent SOD/COD datasets enforce a mutual exclusivity constraint, assuming scenes contain either salient or camouflaged objects, which poorly aligns with the real world. Furthermore, current SOD/COD methods are primarily designed for these highly constrained datasets and lack explicit modeling of the relationship between salient and camouflaged objects. In this paper, to promote the development of unconstrained salient and camouflaged object detection, we construct a large-scale dataset, USC12K, which features comprehensive labels and four different scenes that cover all possible logical existence scenarios of both salient and camouflaged objects. To explicitly model the relationship between salient and camouflaged objects, we propose a model called USCNet, which introduces two distinct prompt query mechanisms for modeling inter-sample and intra-sample attribute relationships. Additionally, to assess the model's ability to distinguish between salient and camouflaged objects, we design an evaluation metric called CSCS. The proposed method achieves state-of-the-art performance across all scenes in various metrics. The code and dataset will be available at https://github.com/ssecv/USCNet.
>
---
#### [replaced 011] FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.20147v2](http://arxiv.org/pdf/2505.20147v2)**

> **作者:** Jin Wang; Yao Lai; Aoxue Li; Shifeng Zhang; Jiacheng Sun; Ning Kang; Chengyue Wu; Zhenguo Li; Ping Luo
>
> **备注:** 37 pages, 12 figures
>
> **摘要:** The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning.
>
---
#### [replaced 012] GlaGAN: A Generative Unsupervised Model for High-Precision Segmentation of Retinal Main Vessels toward Early Detection of Glaucoma
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2503.06743v4](http://arxiv.org/pdf/2503.06743v4)**

> **作者:** Cheng Huang; Weizheng Xie; Tsengdar J. Lee; Jui-Kai Wang; Karanjit Kooner; Ning Zhang; Jia Zhang
>
> **摘要:** Structural changes in the main retinal blood vessels are critical biomarkers for glaucoma onset and progression. Identifying these vessels is essential for vascular modeling yet highly challenging. This paper introduces GlaGAN, an unsupervised generative AI model for segmenting main blood vessels in Optical Coherence Tomography Angiography (OCTA) images. The process begins with the Space Colonization Algorithm (SCA) to rapidly generate vessel skeletons, including radius estimations. By synergistically integrating generative adversarial networks (GANs) with biostatistical modeling of vessel radii, GlaGAN efficiently reconstructs 2D and 3D representations, achieving nearly 100\% segmentation accuracy without requiring labeled data or high-performance computing resources. To address data scarcity, we also present GSS-RetVein, a high-definition mixed 2D/3D glaucoma retinal dataset featuring clear capillary structures. Designed for robustness testing, GSS-RetVein incorporates controlled noise while maintaining sharp capillary boundaries in 2D and enhancing 3D vascular reconstruction for blood flow prediction and glaucoma progression simulations. Experimental results demonstrate GSS-RetVein outperforms existing datasets in evaluating main vessel segmentation. Code and dataset are available: https://github.com/VikiXie/SatMar8.
>
---
#### [replaced 013] MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.11868v2](http://arxiv.org/pdf/2505.11868v2)**

> **作者:** Hongyi Zhou; Xiaogang Wang; Yulan Guo; Kai Xu
>
> **摘要:** Accurately analyzing the motion parts and their motion attributes in dynamic environments is crucial for advancing key areas such as embodied intelligence. Addressing the limitations of existing methods that rely on dense multi-view images or detailed part-level annotations, we propose an innovative framework that can analyze 3D mobility from monocular videos in a zero-shot manner. This framework can precisely parse motion parts and motion attributes only using a monocular video, completely eliminating the need for annotated training data. Specifically, our method first constructs the scene geometry and roughly analyzes the motion parts and their initial motion attributes combining depth estimation, optical flow analysis and point cloud registration method, then employs 2D Gaussian splatting for scene representation. Building on this, we introduce an end-to-end dynamic scene optimization algorithm specifically designed for articulated objects, refining the initial analysis results to ensure the system can handle 'rotation', 'translation', and even complex movements ('rotation+translation'), demonstrating high flexibility and versatility. To validate the robustness and wide applicability of our method, we created a comprehensive dataset comprising both simulated and real-world scenarios. Experimental results show that our framework can effectively analyze articulated object motions in an annotation-free manner, showcasing its significant potential in future embodied intelligence applications.
>
---
#### [replaced 014] DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.20948v3](http://arxiv.org/pdf/2504.20948v3)**

> **作者:** Yanghui Song; Chengfu Yang
>
> **备注:** 9 pages, 14 figures, 10th International Conference on Computer-Aided Design, Manufacturing, Modeling and Simulation (CDMMS 2025)
>
> **摘要:** Given the severe challenges confronting the global growth security of economic crops, precise identification and prevention of plant diseases has emerged as a critical issue in artificial intelligence-enabled agricultural technology. To address the technical challenges in plant disease recognition, including small-sample learning, leaf occlusion, illumination variations, and high inter-class similarity, this study innovatively proposes a Dynamic Dual-Stream Fusion Network (DS_FusionNet). The network integrates a dual-backbone architecture, deformable dynamic fusion modules, and bidirectional knowledge distillation strategy, significantly enhancing recognition accuracy. Experimental results demonstrate that DS_FusionNet achieves classification accuracies exceeding 90% using only 10% of the PlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the complex PlantWild dataset, exhibiting exceptional generalization capabilities. This research not only provides novel technical insights for fine-grained image classification but also establishes a robust foundation for precise identification and management of agricultural diseases.
>
---
#### [replaced 015] DynOPETs: A Versatile Benchmark for Dynamic Object Pose Estimation and Tracking in Moving Camera Scenarios
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.19625v2](http://arxiv.org/pdf/2503.19625v2)**

> **作者:** Xiangting Meng; Jiaqi Yang; Mingshu Chen; Chenxin Yan; Yujiao Shi; Wenchao Ding; Laurent Kneip
>
> **摘要:** In the realm of object pose estimation, scenarios involving both dynamic objects and moving cameras are prevalent. However, the scarcity of corresponding real-world datasets significantly hinders the development and evaluation of robust pose estimation models. This is largely attributed to the inherent challenges in accurately annotating object poses in dynamic scenes captured by moving cameras. To bridge this gap, this paper presents a novel dataset DynOPETs and a dedicated data acquisition and annotation pipeline tailored for object pose estimation and tracking in such unconstrained environments. Our efficient annotation method innovatively integrates pose estimation and pose tracking techniques to generate pseudo-labels, which are subsequently refined through pose graph optimization. The resulting dataset offers accurate pose annotations for dynamic objects observed from moving cameras. To validate the effectiveness and value of our dataset, we perform comprehensive evaluations using 18 state-of-the-art methods, demonstrating its potential to accelerate research in this challenging domain. The dataset will be made publicly available to facilitate further exploration and advancement in the field.
>
---
#### [replaced 016] AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.02664v2](http://arxiv.org/pdf/2507.02664v2)**

> **作者:** Ziyin Zhou; Yunpeng Luo; Yuanchen Wu; Ke Sun; Jiayi Ji; Ke Yan; Shouhong Ding; Xiaoshuai Sun; Yunsheng Wu; Rongrong Ji
>
> **备注:** Accepted to ICCV 2025
>
> **摘要:** The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes.
>
---
#### [replaced 017] VideoLifter: Lifting Videos to 3D with Fast Hierarchical Stereo Alignment
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2501.01949v3](http://arxiv.org/pdf/2501.01949v3)**

> **作者:** Wenyan Cong; Hanqing Zhu; Kevin Wang; Jiahui Lei; Colton Stearns; Yuanhao Cai; Leonidas Guibas; Zhangyang Wang; Zhiwen Fan
>
> **备注:** project page: https://videolifter.github.io
>
> **摘要:** Efficiently reconstructing 3D scenes from monocular video remains a core challenge in computer vision, vital for applications in virtual reality, robotics, and scene understanding. Recently, frame-by-frame progressive reconstruction without camera poses is commonly adopted, incurring high computational overhead and compounding errors when scaling to longer videos. To overcome these issues, we introduce VideoLifter, a novel video-to-3D pipeline that leverages a local-to-global strategy on a fragment basis, achieving both extreme efficiency and SOTA quality. Locally, VideoLifter leverages learnable 3D priors to register fragments, extracting essential information for subsequent 3D Gaussian initialization with enforced inter-fragment consistency and optimized efficiency. Globally, it employs a tree-based hierarchical merging method with key frame guidance for inter-fragment alignment, pairwise merging with Gaussian point pruning, and subsequent joint optimization to ensure global consistency while efficiently mitigating cumulative errors. This approach significantly accelerates the reconstruction process, reducing training time by over 82% while holding better visual quality than current SOTA methods.
>
---
#### [replaced 018] DriveX: Driving View Synthesis on Free-form Trajectories with Generative Prior
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.01717v2](http://arxiv.org/pdf/2412.01717v2)**

> **作者:** Zeyu Yang; Zijie Pan; Yuankun Yang; Xiatian Zhu; Li Zhang
>
> **备注:** ICCV 2025
>
> **摘要:** Driving view synthesis along free-form trajectories is essential for realistic driving simulations, enabling closed-loop evaluation of end-to-end driving policies. Existing methods excel at view interpolation along recorded paths but struggle to generalize to novel trajectories due to limited viewpoints in driving videos. To tackle this challenge, we propose DriveX, a novel free-form driving view synthesis framework, that progressively distills generative prior into the 3D Gaussian model during its optimization. Within this framework, we utilize a video diffusion model to refine the degraded novel trajectory renderings from the in-training Gaussian model, while the restored videos in turn serve as additional supervision for optimizing the 3D Gaussian. Concretely, we craft an inpainting-based video restoration task, which can disentangle the identification of degraded regions from the generative capability of the diffusion model and remove the need of simulating specific degraded pattern in the training of the diffusion model. To further enhance the consistency and fidelity of generated contents, the pseudo ground truth is progressively updated with gradually improved novel trajectory rendering, allowing both components to co-adapt and reinforce each other while minimizing the disruption on the optimization. By tightly integrating 3D scene representation with generative prior, DriveX achieves high-quality view synthesis beyond recorded trajectories in real time--unlocking new possibilities for flexible and realistic driving simulations on free-form trajectories.
>
---
#### [replaced 019] Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.21646v2](http://arxiv.org/pdf/2504.21646v2)**

> **作者:** Liqin Wang; Qianyue Hu; Wei Lu; Xiangyang Luo
>
> **备注:** Accepted by ACM MM 2025
>
> **摘要:** The success of face recognition (FR) systems has led to serious privacy concerns due to potential unauthorized surveillance and user tracking on social networks. Existing methods for enhancing privacy fail to generate natural face images that can protect facial privacy. In this paper, we propose diffusion-based adversarial identity manipulation (DiffAIM) to generate natural and highly transferable adversarial faces against malicious FR systems. To be specific, we manipulate facial identity within the low-dimensional latent space of a diffusion model. This involves iteratively injecting gradient-based adversarial identity guidance during the reverse diffusion process, progressively steering the generation toward the desired adversarial faces. The guidance is optimized for identity convergence towards a target while promoting semantic divergence from the source, facilitating effective impersonation while maintaining visual naturalness. We further incorporate structure-preserving regularization to preserve facial structure consistency during manipulation. Extensive experiments on both face verification and identification tasks demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger black-box attack transferability while maintaining superior visual quality. We also demonstrate the effectiveness of the proposed approach for commercial FR APIs, including Face++ and Aliyun.
>
---
#### [replaced 020] Seed Selection for Human-Oriented Image Reconstruction via Guided Diffusion
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.05363v2](http://arxiv.org/pdf/2506.05363v2)**

> **作者:** Yui Tatsumi; Ziyue Zeng; Hiroshi Watanabe
>
> **备注:** Accepted by 2025 IEEE 14th Global Conference on Consumer Electronics (GCCE 2025)
>
> **摘要:** Conventional methods for scalable image coding for humans and machines require the transmission of additional information to achieve scalability. A recent diffusion-based method avoids this by generating human-oriented images from machine-oriented images without extra bitrate. This method, however, uses a single random seed, which may lead to suboptimal image quality. In this paper, we propose a seed selection method that identifies the optimal seed from multiple candidates to improve image quality without increasing the bitrate. To reduce computational cost, the selection is performed based on intermediate outputs obtained from early steps of the reverse diffusion process. Experimental results demonstrate that our method outperforms the baseline across multiple metrics.
>
---
#### [replaced 021] Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2403.17377v2](http://arxiv.org/pdf/2403.17377v2)**

> **作者:** Donghoon Ahn; Hyoungwon Cho; Jaewon Min; Wooseok Jang; Jungwoo Kim; SeonHwa Kim; Hyun Hee Park; Kyong Hwan Jin; Seungryong Kim
>
> **备注:** Project page is available at https://ku-cvlab.github.io/Perturbed-Attention-Guidance. This version reflects the ECCV 2024 camera-ready submission
>
> **摘要:** Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms' ability to capture structural information, and guiding the denoising process away from these degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves sample quality in conditional and even unconditional scenarios. Moreover, PAG significantly improves the baseline performance in various downstream tasks where existing guidances such as CG or CFG cannot be fully utilized, including ControlNet with empty prompts and image restoration such as inpainting and deblurring.
>
---
#### [replaced 022] ISLES'24 -- A Real-World Longitudinal Multimodal Stroke Dataset
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2408.11142v2](http://arxiv.org/pdf/2408.11142v2)**

> **作者:** Evamaria Olga Riedel; Ezequiel de la Rosa; The Anh Baran; Moritz Hernandez Petzsche; Hakim Baazaoui; Kaiyuan Yang; Fabio Antonio Musio; Houjing Huang; David Robben; Joaquin Oscar Seia; Roland Wiest; Mauricio Reyes; Ruisheng Su; Claus Zimmer; Tobias Boeckh-Behrens; Maria Berndt; Bjoern Menze; Daniel Rueckert; Benedikt Wiestler; Susanne Wegener; Jan Stefan Kirschke
>
> **摘要:** Stroke remains a leading cause of global morbidity and mortality, imposing a heavy socioeconomic burden. Advances in endovascular reperfusion therapy and CT and MR imaging for treatment guidance have significantly improved patient outcomes. Developing machine learning algorithms that can create accurate models of brain function from stroke images for tasks like lesion identification and tissue survival prediction requires large, diverse, and well annotated public datasets. While several high-quality image datasets in stroke exist, they include only single time point data. Data over different time points are essential to accurately identify lesions and predict prognosis. Here, we provide comprehensive longitudinal stroke data, including (sub-)acute CT imaging with angiography and perfusion, follow-up MRI after 2-9 days, and acute and longitudinal clinical data up to a three-month outcome. The dataset also includes vessel occlusion masks from acute CT angiography and delineated infarction masks in follow-up MRI. This multicenter dataset consists of 245 cases and is a solid basis for developing powerful machine-learning algorithms to facilitate clinical decision-making.
>
---
#### [replaced 023] Riemannian Complex Hermit Positive Definite Convolution Network for Polarimetric SAR Image Classification
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2502.08137v2](http://arxiv.org/pdf/2502.08137v2)**

> **作者:** Junfei Shi; Yuke Li; Mengmeng Nie; Fang Liu; Haiyan Jin; Junhuai Li; Weisi Lin
>
> **备注:** 13 pages, 7 figures
>
> **摘要:** Deep learning has been extensively utilized for PolSAR image classification. However, most existing methods transform the polarimetric covariance matrix into a real- or complex-valued vector to comply with standard deep learning frameworks in Euclidean space. This approach overlooks the inherent structure of the covariance matrix, which is a complex Hermitian positive definite (HPD) matrix residing in the Riemannian manifold. Vectorization disrupts the matrix structure and misrepresents its geometric properties. To mitigate this drawback, we propose HPDNet, a novel framework that directly processes HPD matrices on the Riemannian manifold. The HPDnet fully considers the complex phase information by decomposing a complex HPD matrix into the real- and imaginarymatrices. The proposed HPDnet consists of several HPD mapping layers and rectifying layers, which can preserve the geometric structure of the data and transform them into a more separable manifold representation. Subsequently, a complex LogEig layer is developed to project the manifold data into a tangent space, ensuring that conventional Euclidean-based deep learning networks can be applied to further extract contextual features for classification. Furthermore, to optimize computational efficiency, we design a fast eigenvalue decomposition method for parallelized matrix processing. Experiments conducted on three real-world PolSAR datasets demonstrate that the proposed method outperforms state-of-the-art approaches, especially in heterogeneous regions.
>
---
#### [replaced 024] Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.01943v2](http://arxiv.org/pdf/2506.01943v2)**

> **作者:** Xiao Fu; Xintao Wang; Xian Liu; Jianhong Bai; Runsen Xu; Pengfei Wan; Di Zhang; Dahua Lin
>
> **备注:** Project Page: https://fuxiao0719.github.io/projects/robomaster/ Code: https://github.com/KwaiVGI/RoboMaster
>
> **摘要:** Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation.
>
---
#### [replaced 025] Enhancing Long Video Generation Consistency without Tuning
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2412.17254v2](http://arxiv.org/pdf/2412.17254v2)**

> **作者:** Xingyao Li; Fengzhuo Zhang; Jiachun Pan; Yunlong Hou; Vincent Y. F. Tan; Zhuoran Yang
>
> **备注:** ICML 2025 Workshop on Building Physically Plausible World Models (Best Paper), 32 pages, 17 figures
>
> **摘要:** Despite the considerable progress achieved in the long video generation problem, there is still significant room to improve the consistency of the generated videos, particularly in terms of their smoothness and transitions between scenes. We address these issues to enhance the consistency and coherence of videos generated with either single or multiple prompts. We propose the Time-frequency based temporal Attention Reweighting Algorithm (TiARA), which judiciously edits the attention score matrix based on the Discrete Short-Time Fourier Transform. This method is supported by a frequency-based analysis, ensuring that the edited attention score matrix achieves improved consistency across frames. It represents the first-of-its-kind for frequency-based methods in video diffusion models. For videos generated by multiple prompts, we further uncover key factors such as the alignment of the prompts affecting prompt interpolation quality. Inspired by our analyses, we propose PromptBlend, an advanced prompt interpolation pipeline that systematically aligns the prompts. Extensive experimental results validate the efficacy of our proposed method, demonstrating consistent and substantial improvements over multiple baselines.
>
---
#### [replaced 026] BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance
- **分类: cs.CV; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2506.03589v3](http://arxiv.org/pdf/2506.03589v3)**

> **作者:** Huy Le; Nhat Chung; Tung Kieu; Anh Nguyen; Ngan Le
>
> **备注:** Accepted at ACM MM 2025
>
> **摘要:** Text-video retrieval (TVR) systems often suffer from visual-linguistic biases present in datasets, which cause pre-trained vision-language models to overlook key details. To address this, we propose BiMa, a novel framework designed to mitigate biases in both visual and textual representations. Our approach begins by generating scene elements that characterize each video by identifying relevant entities/objects and activities. For visual debiasing, we integrate these scene elements into the video embeddings, enhancing them to emphasize fine-grained and salient details. For textual debiasing, we introduce a mechanism to disentangle text features into content and bias components, enabling the model to focus on meaningful content while separately handling biased information. Extensive experiments and ablation studies across five major TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo) demonstrate the competitive performance of BiMa. Additionally, the model's bias mitigation capability is consistently validated by its strong results on out-of-distribution retrieval tasks.
>
---
#### [replaced 027] QCResUNet: Joint Subject-level and Voxel-level Segmentation Quality Prediction
- **分类: eess.IV; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2412.07156v2](http://arxiv.org/pdf/2412.07156v2)**

> **作者:** Peijie Qiu; Satrajit Chakrabarty; Phuc Nguyen; Soumyendu Sekhar Ghosh; Aristeidis Sotiras
>
> **备注:** Medical Image Analysis
>
> **摘要:** Deep learning has made significant strides in automated brain tumor segmentation from magnetic resonance imaging (MRI) scans in recent years. However, the reliability of these tools is hampered by the presence of poor-quality segmentation outliers, particularly in out-of-distribution samples, making their implementation in clinical practice difficult. Therefore, there is a need for quality control (QC) to screen the quality of the segmentation results. Although numerous automatic QC methods have been developed for segmentation quality screening, most were designed for cardiac MRI segmentation, which involves a single modality and a single tissue type. Furthermore, most prior works only provided subject-level predictions of segmentation quality and did not identify erroneous parts segmentation that may require refinement. To address these limitations, we proposed a novel multi-task deep learning architecture, termed QCResUNet, which produces subject-level segmentation-quality measures as well as voxel-level segmentation error maps for each available tissue class. To validate the effectiveness of the proposed method, we conducted experiments on assessing its performance on evaluating the quality of two distinct segmentation tasks. First, we aimed to assess the quality of brain tumor segmentation results. For this task, we performed experiments on one internal and two external datasets. Second, we aimed to evaluate the segmentation quality of cardiac Magnetic Resonance Imaging (MRI) data from the Automated Cardiac Diagnosis Challenge. The proposed method achieved high performance in predicting subject-level segmentation-quality metrics and accurately identifying segmentation errors on a voxel basis. This has the potential to be used to guide human-in-the-loop feedback to improve segmentations in clinical settings.
>
---
#### [replaced 028] Active Stereo in the Wild through Virtual Pattern Projection
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2406.04345v2](http://arxiv.org/pdf/2406.04345v2)**

> **作者:** Luca Bartolomei; Matteo Poggi; Fabio Tosi; Andrea Conti; Stefano Mattoccia
>
> **备注:** IJCV extended version of ICCV 2023 paper: "Active Stereo Without Pattern Projector"
>
> **摘要:** This paper presents a novel general-purpose guided stereo paradigm that mimics the active stereo principle by replacing the unreliable physical pattern projector with a depth sensor. It works by projecting virtual patterns consistent with the scene geometry onto the left and right images acquired by a conventional stereo camera, using the sparse hints obtained from a depth sensor, to facilitate the visual correspondence. Purposely, any depth sensing device can be seamlessly plugged into our framework, enabling the deployment of a virtual active stereo setup in any possible environment and overcoming the severe limitations of physical pattern projection, such as the limited working range and environmental conditions. Exhaustive experiments on indoor and outdoor datasets featuring both long and close range, including those providing raw, unfiltered depth hints from off-the-shelf depth sensors, highlight the effectiveness of our approach in notably boosting the robustness and accuracy of algorithms and deep stereo without any code modification and even without re-training. Additionally, we assess the performance of our strategy on active stereo evaluation datasets with conventional pattern projection. Indeed, in all these scenarios, our virtual pattern projection paradigm achieves state-of-the-art performance. The source code is available at: https://github.com/bartn8/vppstereo.
>
---
#### [replaced 029] Transfer Attack for Bad and Good: Explain and Boost Adversarial Transferability across Multimodal Large Language Models
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2405.20090v4](http://arxiv.org/pdf/2405.20090v4)**

> **作者:** Hao Cheng; Erjia Xiao; Jiayan Yang; Jinhao Duan; Yichi Wang; Jiahang Cao; Qiang Zhang; Le Yang; Kaidi Xu; Jindong Gu; Renjing Xu
>
> **备注:** Accepted by ACM MM 2025
>
> **摘要:** Multimodal Large Language Models (MLLMs) demonstrate exceptional performance in cross-modality interaction, yet they also suffer adversarial vulnerabilities. In particular, the transferability of adversarial examples remains an ongoing challenge. In this paper, we specifically analyze the manifestation of adversarial transferability among MLLMs and identify the key factors that influence this characteristic. We discover that the transferability of MLLMs exists in cross-LLM scenarios with the same vision encoder and indicate \underline{\textit{two key Factors}} that may influence transferability. We provide two semantic-level data augmentation methods, Adding Image Patch (AIP) and Typography Augment Transferability Method (TATM), which boost the transferability of adversarial examples across MLLMs. To explore the potential impact in the real world, we utilize two tasks that can have both negative and positive societal impacts: \ding{182} Harmful Content Insertion and \ding{183} Information Protection.
>
---
#### [replaced 030] GaussRender: Learning 3D Occupancy with Gaussian Rendering
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2502.05040v3](http://arxiv.org/pdf/2502.05040v3)**

> **作者:** Loïck Chambon; Eloi Zablocki; Alexandre Boulch; Mickaël Chen; Matthieu Cord
>
> **备注:** ICCV 2025
>
> **摘要:** Understanding the 3D geometry and semantics of driving scenes is critical for safe autonomous driving. Recent advances in 3D occupancy prediction have improved scene representation but often suffer from visual inconsistencies, leading to floating artifacts and poor surface localization. Existing voxel-wise losses (e.g., cross-entropy) fail to enforce visible geometric coherence. In this paper, we propose GaussRender, a module that improves 3D occupancy learning by enforcing projective consistency. Our key idea is to project both predicted and ground-truth 3D occupancy into 2D camera views, where we apply supervision. Our method penalizes 3D configurations that produce inconsistent 2D projections, thereby enforcing a more coherent 3D structure. To achieve this efficiently, we leverage differentiable rendering with Gaussian splatting. GaussRender seamlessly integrates with existing architectures while maintaining efficiency and requiring no inference-time modifications. Extensive evaluations on multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate that GaussRender significantly improves geometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc, Symphonies), achieving state-of-the-art results, particularly on surface-sensitive metrics such as RayIoU. The code is open-sourced at https://github.com/valeoai/GaussRender.
>
---
#### [replaced 031] AniMer: Animal Pose and Shape Estimation Using Family Aware Transformer
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.00837v2](http://arxiv.org/pdf/2412.00837v2)**

> **作者:** Jin Lyu; Tianyi Zhu; Yi Gu; Li Lin; Pujin Cheng; Yebin Liu; Xiaoying Tang; Liang An
>
> **备注:** Accepted by CVPR25
>
> **摘要:** Quantitative analysis of animal behavior and biomechanics requires accurate animal pose and shape estimation across species, and is important for animal welfare and biological research. However, the small network capacity of previous methods and limited multi-species dataset leave this problem underexplored. To this end, this paper presents AniMer to estimate animal pose and shape using family aware Transformer, enhancing the reconstruction accuracy of diverse quadrupedal families. A key insight of AniMer is its integration of a high-capacity Transformer-based backbone and an animal family supervised contrastive learning scheme, unifying the discriminative understanding of various quadrupedal shapes within a single framework. For effective training, we aggregate most available open-sourced quadrupedal datasets, either with 3D or 2D labels. To improve the diversity of 3D labeled data, we introduce CtrlAni3D, a novel large-scale synthetic dataset created through a new diffusion-based conditional image generation pipeline. CtrlAni3D consists of about 10k images with pixel-aligned SMAL labels. In total, we obtain 41.3k annotated images for training and validation. Consequently, the combination of a family aware Transformer network and an expansive dataset enables AniMer to outperform existing methods not only on 3D datasets like Animal3D and CtrlAni3D, but also on out-of-distribution Animal Kingdom dataset. Ablation studies further demonstrate the effectiveness of our network design and CtrlAni3D in enhancing the performance of AniMer for in-the-wild applications. The project page of AniMer is https://luoxue-star.github.io/AniMer_project_page/.
>
---
#### [replaced 032] Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models
- **分类: cs.CV; cs.AI; cs.CL; cs.LG**

- **链接: [http://arxiv.org/pdf/2503.16980v4](http://arxiv.org/pdf/2503.16980v4)**

> **作者:** Haichao Zhang; Yun Fu
>
> **摘要:** Token-based video representation has emerged as a promising approach for enabling large language models (LLMs) to interpret video content. However, existing token reduction techniques, such as pruning and merging, often disrupt essential positional embeddings and rely on continuous visual tokens sampled from nearby pixels with similar spatial-temporal locations. By removing only a small fraction of tokens, these methods still produce relatively lengthy continuous sequences, which falls short of the extreme compression required to balance computational efficiency and token count in video LLMs. In this paper, we introduce the novel task of Extreme Short Token Reduction, which aims to represent entire videos using a minimal set of discrete tokens. We propose VQToken, a neural discrete token representation framework that (i) applies adaptive vector quantization to continuous ViT embeddings to learn a compact codebook and (ii) preserves spatial-temporal positions via a token hash function by assigning each grid-level token to its nearest codebook entry. On the Extreme Short Token Reduction task, our VQToken compresses sequences to just 0.07 percent of their original length while incurring only a 0.66 percent drop in accuracy on the NextQA-MC benchmark. It also achieves comparable performance on ActNet-QA, Long Video Bench, and VideoMME. We further introduce the Token Information Density (TokDense) metric and formalize fixed-length and adaptive-length subtasks, achieving state-of-the-art results in both settings. Our approach dramatically lowers theoretical complexity, increases information density, drastically reduces token counts, and enables efficient video LLMs in resource-constrained environments.
>
---
#### [replaced 033] Holistic Tokenizer for Autoregressive Image Generation
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2507.02358v2](http://arxiv.org/pdf/2507.02358v2)**

> **作者:** Anlin Zheng; Haochen Wang; Yucheng Zhao; Weipeng Deng; Tiancai Wang; Xiangyu Zhang; Xiaojuan Qi
>
> **备注:** 17 pages, 10 figures
>
> **摘要:** The vanilla autoregressive image generation model generates visual tokens in a step-by-step fashion, which limits the ability to capture holistic relationships among token sequences. Moreover, most visual tokenizers map local image patches into latent tokens, leading to limited global information. To address this, we introduce \textit{Hita}, a novel image tokenizer for autoregressive (AR) image generation. It introduces a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens. Besides, Hita incorporates two key strategies for improved alignment with the AR generation process: 1) it arranges a sequential structure with holistic tokens at the beginning followed by patch-level tokens while using causal attention to maintain awareness of previous tokens; and 2) before feeding the de-quantized tokens into the decoder, Hita adopts a lightweight fusion module to control information flow to prioritize holistic tokens. Extensive experiments show that Hita accelerates the training speed of AR generators and outperforms those trained with vanilla tokenizers, achieving \textbf{2.59 FID} and \textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the holistic representation highlights its ability to capture global image properties such as textures, materials, and shapes. Additionally, Hita also demonstrates effectiveness in zero-shot style transfer and image in-painting. The code is available at \href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}
>
---
#### [replaced 034] HPPP: Halpern-type Preconditioned Proximal Point Algorithms and Applications to Image Restoration
- **分类: cs.CV; math.OC**

- **链接: [http://arxiv.org/pdf/2407.13120v5](http://arxiv.org/pdf/2407.13120v5)**

> **作者:** Shuchang Zhang; Hui Zhang; Hongxia Wang
>
> **摘要:** Recently, the degenerate preconditioned proximal point (PPP) method provides a unified and flexible framework for designing and analyzing operator-splitting algorithms such as Douglas-Rachford (DR). However, the degenerate PPP method exhibits weak convergence in the infinite-dimensional Hilbert space and lacks accelerated variants. To address these issues, we propose a Halpern-type PPP (HPPP) algorithm, which leverages the strong convergence and acceleration properties of Halpern's iteration method. Moreover, we propose a novel algorithm for image restoration by combining HPPP with denoiser priors such as Plug-and-Play (PnP) prior, which can be viewed as an accelerated PnP method. Finally, numerical experiments including several toy examples and image restoration validate the effectiveness of our proposed algorithms.
>
---
#### [replaced 035] Brain3D: Generating 3D Objects from fMRI
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2405.15239v4](http://arxiv.org/pdf/2405.15239v4)**

> **作者:** Yuankun Yang; Li Zhang; Ziyang Xie; Zhiyuan Yuan; Jianfeng Feng; Xiatian Zhu; Yu-Gang Jiang
>
> **备注:** 24 pages, 13 figures, project page: https://brain-3d.github.io/
>
> **摘要:** Understanding the hidden mechanisms behind human's visual perception is a fundamental question in neuroscience. To that end, investigating into the neural responses of human mind activities, such as functional Magnetic Resonance Imaging (fMRI), has been a significant research vehicle. However, analyzing fMRI signals is challenging, costly, daunting, and demanding for professional training. Despite remarkable progress in fMRI analysis, existing approaches are limited to generating 2D images and far away from being biologically meaningful and practically useful. Under this insight, we propose to generate visually plausible and functionally more comprehensive 3D outputs decoded from brain signals, enabling more sophisticated modeling of fMRI data. Conceptually, we reformulate this task as a {\em fMRI conditioned 3D object generation} problem. We design a novel 3D object representation learning method, Brain3D, that takes as input the fMRI data of a subject who was presented with a 2D image, and yields as output the corresponding 3D object images. The key capabilities of this model include tackling the noises with high-level semantic signals and a two-stage architecture design for progressive high-level information integration. Extensive experiments validate the superior capability of our model over previous state-of-the-art 3D object generation methods. Importantly, we show that our model captures the distinct functionalities of each region of human vision system as well as their intricate interplay relationships, aligning remarkably with the established discoveries in neuroscience. Further, preliminary evaluations indicate that Brain3D can successfully identify the disordered brain regions in simulated scenarios, such as V1, V2, V3, V4, and the medial temporal lobe (MTL) within the human visual system. Our data and code will be available at https://brain-3d.github.io/.
>
---
#### [replaced 036] ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in Robotic Surgical Skill Assessment
- **分类: cs.CV; cs.AI; cs.LG; eess.IV**

- **链接: [http://arxiv.org/pdf/2407.05180v4](http://arxiv.org/pdf/2407.05180v4)**

> **作者:** Julien Quarez; Marc Modat; Sebastien Ourselin; Jonathan Shapey; Alejandro Granados
>
> **摘要:** In surgical skill assessment, the Objective Structured Assessments of Technical Skills (OSATS) and Global Rating Scale (GRS) are well-established tools for evaluating surgeons during training. These metrics, along with performance feedback, help surgeons improve and reach practice standards. Recent research on the open-source JIGSAWS dataset, which includes both GRS and OSATS labels, has focused on regressing GRS scores from kinematic data, video, or their combination. However, we argue that regressing GRS alone is limiting, as it aggregates OSATS scores and overlooks clinically meaningful variations during a surgical trial. To address this, we developed a weakly-supervised recurrent transformer model that tracks a surgeon's performance throughout a session by mapping hidden states to six OSATS, derived from kinematic data. These OSATS scores are averaged to predict GRS, allowing us to compare our model's performance against state-of-the-art (SOTA) methods. We report Spearman's Correlation Coefficients (SCC) demonstrating that our model outperforms SOTA using kinematic data (SCC 0.83-0.88), and matches performance with video-based models. Our model also surpasses SOTA in most tasks for average OSATS predictions (SCC 0.46-0.70) and specific OSATS (SCC 0.56-0.95). The generation of pseudo-labels at the segment level translates quantitative predictions into qualitative feedback, vital for automated surgical skill assessment pipelines. A senior surgeon validated our model's outputs, agreeing with 77\% of the weakly-supervised predictions \(p=0.006\).
>
---
#### [replaced 037] IAP: Improving Continual Learning of Vision-Language Models via Instance-Aware Prompting
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.20612v2](http://arxiv.org/pdf/2503.20612v2)**

> **作者:** Hao Fu; Hanbin Zhao; Jiahua Dong; Henghui Ding; Chao Zhang; Hui Qian
>
> **备注:** Code can be found at https://github.com/FerdinandZJU/IAP
>
> **摘要:** Recent pre-trained vision-language models (PT-VLMs) often face a Multi-Domain Task Incremental Learning (MTIL) scenario in practice, where several classes and domains of multi-modal tasks are incrementally arrived. Without access to previously seen tasks and unseen tasks, memory-constrained MTIL suffers from forward and backward forgetting. To alleviate the above challenges, parameter-efficient fine-tuning techniques (PEFT), such as prompt tuning, are employed to adapt the PT-VLM to the diverse incrementally learned tasks. To achieve effective new task adaptation, existing methods only consider the effect of PEFT strategy selection, but neglect the influence of PEFT parameter setting (e.g., prompting). In this paper, we tackle the challenge of optimizing prompt designs for diverse tasks in MTIL and propose an Instance-Aware Prompting (IAP) framework. Specifically, our Instance-Aware Gated Prompting (IA-GP) strategy enhances adaptation to new tasks while mitigating forgetting by adaptively assigning prompts across transformer layers at the instance level. Our Instance-Aware Class-Distribution-Driven Prompting (IA-CDDP) improves the task adaptation process by determining an accurate task-label-related confidence score for each instance. Experimental evaluations across 11 datasets, using three performance metrics, demonstrate the effectiveness of our proposed method. The source codes are available at https://github.com/FerdinandZJU/IAP.
>
---
#### [replaced 038] Stepwise Decomposition and Dual-stream Focus: A Novel Approach for Training-free Camouflaged Object Segmentation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.06818v2](http://arxiv.org/pdf/2506.06818v2)**

> **作者:** Chao Yin; Hao Li; Kequan Yang; Jide Li; Pinpin Zhu; Xiaoqiang Li
>
> **备注:** accepted by ACM MM2025
>
> **摘要:** While promptable segmentation (\textit{e.g.}, SAM) has shown promise for various segmentation tasks, it still requires manual visual prompts for each object to be segmented. In contrast, task-generic promptable segmentation aims to reduce the need for such detailed prompts by employing only a task-generic prompt to guide segmentation across all test samples. However, when applied to Camouflaged Object Segmentation (COS), current methods still face two critical issues: 1) \textit{\textbf{semantic ambiguity in getting instance-specific text prompts}}, which arises from insufficient discriminative cues in holistic captions, leading to foreground-background confusion; 2) \textit{\textbf{semantic discrepancy combined with spatial separation in getting instance-specific visual prompts}}, which results from global background sampling far from object boundaries with low feature correlation, causing SAM to segment irrelevant regions. To address the issues above, we propose \textbf{RDVP-MSD}, a novel training-free test-time adaptation framework that synergizes \textbf{R}egion-constrained \textbf{D}ual-stream \textbf{V}isual \textbf{P}rompting (RDVP) via \textbf{M}ultimodal \textbf{S}tepwise \textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT progressively disentangles image captions to eliminate semantic ambiguity, while RDVP injects spatial constraints into visual prompting and independently samples visual prompts for foreground and background points, effectively mitigating semantic discrepancy and spatial separation. Without requiring any training or supervision, RDVP-MSD achieves a state-of-the-art segmentation result on multiple COS benchmarks and delivers a faster inference speed than previous methods, demonstrating significantly improved accuracy and efficiency. The codes will be available at \href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}
>
---
#### [replaced 039] StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.23577v2](http://arxiv.org/pdf/2506.23577v2)**

> **作者:** Yanning Hou; Yanran Ruan; Junfa Li; Shanshan Wang; Jianfeng Qiu; Ke Xu
>
> **摘要:** Enhancing the alignment between text and image features in the CLIP model is a critical challenge in zero-shot industrial anomaly detection tasks. Recent studies predominantly utilize specific category prompts during pretraining, which can cause overfitting to the training categories and limit model generalization. To address this, we propose a method that transforms category names through multicategory name stacking to create stacked prompts, forming the basis of our StackCLIP model. Our approach introduces two key components. The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts by stacking semantically analogous categories, while utilizing multi-object textual feature fusion to amplify discriminative anomalies among similar objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific linear layers tailored for each stack cluster and adaptively integrates them based on the attributes of test categories. These modules work together to deliver superior training speed, stability, and convergence, significantly boosting anomaly segmentation performance. Additionally, our stacked prompt framework offers robust generalization across classification tasks. To further improve performance, we introduce the Regulating Prompt Learning (RPL) module, which leverages the generalization power of stacked prompts to refine prompt learning, elevating results in anomaly detection classification tasks. Extensive testing on seven industrial anomaly detection datasets demonstrates that our method achieves state-of-the-art performance in both zero-shot anomaly detection and segmentation tasks.
>
---
#### [replaced 040] Continual Visual Reinforcement Learning with A Life-Long World Model
- **分类: cs.LG; cs.CV**

- **链接: [http://arxiv.org/pdf/2303.06572v2](http://arxiv.org/pdf/2303.06572v2)**

> **作者:** Minting Pan; Wendong Zhang; Geng Chen; Xiangming Zhu; Siyu Gao; Yunbo Wang; Xiaokang Yang
>
> **备注:** Accepted by ECML 2025
>
> **摘要:** Learning physical dynamics in a series of non-stationary environments is a challenging but essential task for model-based reinforcement learning (MBRL) with visual inputs. It requires the agent to consistently adapt to novel tasks without forgetting previous knowledge. In this paper, we present a new continual learning approach for visual dynamics modeling and explore its efficacy in visual control. The key assumption is that an ideal world model can provide a non-forgetting environment simulator, which enables the agent to optimize the policy in a multi-task learning manner based on the imagined trajectories from the world model. To this end, we first introduce the life-long world model, which learns task-specific latent dynamics using a mixture of Gaussians and incorporates generative experience replay to mitigate catastrophic forgetting. Then, we further address the value estimation challenge for previous tasks with the exploratory-conservative behavior learning approach. Our model remarkably outperforms the straightforward combinations of existing continual learning and visual RL algorithms on DeepMind Control Suite and Meta-World benchmarks with continual visual control tasks.
>
---
#### [replaced 041] UnitModule: A Lightweight Joint Image Enhancement Module for Underwater Object Detection
- **分类: cs.CV; I.4**

- **链接: [http://arxiv.org/pdf/2309.04708v2](http://arxiv.org/pdf/2309.04708v2)**

> **作者:** Zhuoyan Liu; Bo Wang; Ye Li; Jiaxian He; Yunfeng Li
>
> **备注:** 15 pages, 10 figures, 13 tables, accepted by PR
>
> **摘要:** Underwater object detection faces the problem of underwater image degradation, which affects the performance of the detector. Underwater object detection methods based on noise reduction and image enhancement usually do not provide images preferred by the detector or require additional datasets. In this paper, we propose a plug-and-play \textbf{U}nderwater joi\textbf{n}t \textbf{i}mage enhancemen\textbf{t} \textbf{Module} (UnitModule) that provides the input image preferred by the detector. We design an unsupervised learning loss for the joint training of UnitModule with the detector without additional datasets to improve the interaction between UnitModule and the detector. Furthermore, a color cast predictor with the assisting color cast loss and a data augmentation called Underwater Color Random Transfer (UCRT) are designed to improve the performance of UnitModule on underwater images with different color casts. Extensive experiments are conducted on DUO for different object detection models, where UnitModule achieves the highest performance improvement of 2.6 AP for YOLOv5-S and gains the improvement of 3.3 AP on the brand-new test set (\(\text{URPC}_{test}\)). And UnitModule significantly improves the performance of all object detection models we test, especially for models with a small number of parameters. In addition, UnitModule with a small number of parameters of 31K has little effect on the inference speed of the original object detection model. Our quantitative and visual analysis also demonstrates the effectiveness of UnitModule in enhancing the input image and improving the perception ability of the detector for object features. The code is available at https://github.com/LEFTeyex/UnitModule.
>
---
#### [replaced 042] Mask Approximation Net: A Novel Diffusion Model Approach for Remote Sensing Change Captioning
- **分类: cs.CV; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2412.19179v3](http://arxiv.org/pdf/2412.19179v3)**

> **作者:** Dongwei Sun; Jing Yao; Wu Xue; Changsheng Zhou; Pedram Ghamisi; Xiangyong Cao
>
> **摘要:** Remote sensing image change description represents an innovative multimodal task within the realm of remote sensing processing.This task not only facilitates the detection of alterations in surface conditions, but also provides comprehensive descriptions of these changes, thereby improving human interpretability and interactivity.Current deep learning methods typically adopt a three stage framework consisting of feature extraction, feature fusion, and change localization, followed by text generation. Most approaches focus heavily on designing complex network modules but lack solid theoretical guidance, relying instead on extensive empirical experimentation and iterative tuning of network components. This experience-driven design paradigm may lead to overfitting and design bottlenecks, thereby limiting the model's generalizability and adaptability.To address these limitations, this paper proposes a paradigm that shift towards data distribution learning using diffusion models, reinforced by frequency-domain noise filtering, to provide a theoretically motivated and practically effective solution to multimodal remote sensing change description.The proposed method primarily includes a simple multi-scale change detection module, whose output features are subsequently refined by a well-designed diffusion model.Furthermore, we introduce a frequency-guided complex filter module to boost the model performance by managing high-frequency noise throughout the diffusion process. We validate the effectiveness of our proposed method across several datasets for remote sensing change detection and description, showcasing its superior performance compared to existing techniques. The code will be available at \href{https://github.com/sundongwei}{MaskApproxNet}.
>
---
#### [replaced 043] Weakly Supervised Segmentation Framework for Thyroid Nodule Based on High-confidence Labels and High-rationality Losses
- **分类: cs.CV; J.3.3**

- **链接: [http://arxiv.org/pdf/2502.19707v3](http://arxiv.org/pdf/2502.19707v3)**

> **作者:** Jianning Chi; Zelan Li; Geng Lin; MingYang Sun; Xiaosheng Yu
>
> **备注:** 24 pages, 14 figures, 7 tables
>
> **摘要:** Weakly supervised segmentation methods can delineate thyroid nodules in ultrasound images efficiently using training data with coarse labels, but suffer from: 1) low-confidence pseudo-labels that follow topological priors, introducing significant label noise, and 2) low-rationality loss functions that rigidly compare segmentation with labels, ignoring discriminative information for nodules with diverse and complex shapes. To solve these issues, we clarify the objective and references for weakly supervised ultrasound image segmentation, presenting a framework with high-confidence pseudo-labels to represent topological and anatomical information and high-rationality losses to capture multi-level discriminative features. Specifically, we fuse geometric transformations of four-point annotations and MedSAM model results prompted by specific annotations to generate high-confidence box, foreground, and background labels. Our high-rationality learning strategy includes: 1) Alignment loss measuring spatial consistency between segmentation and box label, and topological continuity within the foreground label, guiding the network to perceive nodule location; 2) Contrastive loss pulling features from labeled foreground regions while pushing features from labeled foreground and background regions, guiding the network to learn nodule and background feature distribution; 3) Prototype correlation loss measuring consistency between correlation maps derived by comparing features with foreground and background prototypes, refining uncertain regions to accurate nodule edges. Experimental results show that our method achieves state-of-the-art performance on the TN3K and DDTI datasets. The code is available at https://github.com/bluehenglee/MLI-MSC.
>
---
#### [replaced 044] CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.21843v3](http://arxiv.org/pdf/2503.21843v3)**

> **作者:** Hanyu Liu; Siyao Li; Ying Yu; Yixuan Jiang; Hang Xiao; Jingxi Long; Haotian Tang; Chao Li
>
> **摘要:** Human Activity Recognition (HAR) is a fundamental technology for numerous human - centered intelligent applications. Although deep learning methods have been utilized to accelerate feature extraction, issues such as multimodal data mixing, activity heterogeneity, and complex model deployment remain largely unresolved. The aim of this paper is to address issues such as multimodal data mixing, activity heterogeneity, and complex model deployment in sensor-based human activity recognition. We propose a spatiotemporal attention modal decomposition alignment fusion strategy to tackle the problem of the mixed distribution of sensor data. Key discriminative features of activities are captured through cross-modal spatio-temporal disentangled representation, and gradient modulation is combined to alleviate data heterogeneity. In addition, a wearable deployment simulation system is constructed. We conducted experiments on a large number of public datasets, demonstrating the effectiveness of the model.
>
---
#### [replaced 045] Memory Storyboard: Leveraging Temporal Segmentation for Streaming Self-Supervised Learning from Egocentric Videos
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2501.12254v2](http://arxiv.org/pdf/2501.12254v2)**

> **作者:** Yanlai Yang; Mengye Ren
>
> **备注:** Fourth Conference on Lifelong Learning Agents - CoLLAs 2025 (Oral)
>
> **摘要:** Self-supervised learning holds the promise of learning good representations from real-world continuous uncurated data streams. However, most existing works in visual self-supervised learning focus on static images or artificial data streams. Towards exploring a more realistic learning substrate, we investigate streaming self-supervised learning from long-form real-world egocentric video streams. Inspired by the event segmentation mechanism in human perception and memory, we propose "Memory Storyboard" that groups recent past frames into temporal segments for more effective summarization of the past visual streams for memory replay. To accommodate efficient temporal segmentation, we propose a two-tier memory hierarchy: the recent past is stored in a short-term memory, and the storyboard temporal segments are then transferred to a long-term memory. Experiments on real-world egocentric video datasets including SAYCam and KrishnaCam show that contrastive learning objectives on top of storyboard frames result in semantically meaningful representations that outperform those produced by state-of-the-art unsupervised continual learning methods.
>
---
#### [replaced 046] ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated Objects using 3D Gaussian Splatting
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.08135v2](http://arxiv.org/pdf/2503.08135v2)**

> **作者:** Junfu Guo; Yu Xin; Gaoyi Liu; Kai Xu; Ligang Liu; Ruizhen Hu
>
> **摘要:** We tackle the challenge of concurrent reconstruction at the part level with the RGB appearance and estimation of motion parameters for building digital twins of articulated objects using the 3D Gaussian Splatting (3D-GS) method. With two distinct sets of multi-view imagery, each depicting an object in separate static articulation configurations, we reconstruct the articulated object in 3D Gaussian representations with both appearance and geometry information at the same time. Our approach decoupled multiple highly interdependent parameters through a multi-step optimization process, thereby achieving a stable optimization procedure and high-quality outcomes. We introduce ArticulatedGS, a self-supervised, comprehensive framework that autonomously learns to model shapes and appearances at the part level and synchronizes the optimization of motion parameters, all without reliance on 3D supervision, motion cues, or semantic labels. Our experimental results demonstrate that, among comparable methodologies, our approach has achieved optimal outcomes in terms of part segmentation accuracy, motion estimation accuracy, and visual quality.
>
---
#### [replaced 047] CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.07003v2](http://arxiv.org/pdf/2505.07003v2)**

> **作者:** Peng Li; Suizhi Ma; Jialiang Chen; Yuan Liu; Congyi Zhang; Wei Xue; Wenhan Luo; Alla Sheffer; Wenping Wang; Yike Guo
>
> **备注:** SIGGRAPH 2025, Page: https://penghtyx.github.io/CMD/
>
> **摘要:** Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image.
>
---
#### [replaced 048] Event-based Stereo Depth Estimation: A Survey
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2409.17680v3](http://arxiv.org/pdf/2409.17680v3)**

> **作者:** Suman Ghosh; Guillermo Gallego
>
> **备注:** 28 pages, 24 figures, 7 tables. Project page: https://github.com/tub-rip/EventStereoSurvey
>
> **摘要:** Stereopsis has widespread appeal in robotics as it is the predominant way by which living beings perceive depth to navigate our 3D world. Event cameras are novel bio-inspired sensors that detect per-pixel brightness changes asynchronously, with very high temporal resolution and high dynamic range, enabling machine perception in high-speed motion and broad illumination conditions. The high temporal precision also benefits stereo matching, making disparity (depth) estimation a popular research area for event cameras ever since its inception. Over the last 30 years, the field has evolved rapidly, from low-latency, low-power circuit design to current deep learning (DL) approaches driven by the computer vision community. The bibliography is vast and difficult to navigate for non-experts due its highly interdisciplinary nature. Past surveys have addressed distinct aspects of this topic, in the context of applications, or focusing only on a specific class of techniques, but have overlooked stereo datasets. This survey provides a comprehensive overview, covering both instantaneous stereo and long-term methods suitable for simultaneous localization and mapping (SLAM), along with theoretical and empirical comparisons. It is the first to extensively review DL methods as well as stereo datasets, even providing practical suggestions for creating new benchmarks to advance the field. The main advantages and challenges faced by event-based stereo depth estimation are also discussed. Despite significant progress, challenges remain in achieving optimal performance in not only accuracy but also efficiency, a cornerstone of event-based computing. We identify several gaps and propose future research directions. We hope this survey inspires future research in this area, by serving as an accessible entry point for newcomers, as well as a practical guide for seasoned researchers in the community.
>
---
#### [replaced 049] Multimodal Latent Diffusion Model for Complex Sewing Pattern Generation
- **分类: cs.CV; cs.GR; cs.LG**

- **链接: [http://arxiv.org/pdf/2412.14453v2](http://arxiv.org/pdf/2412.14453v2)**

> **作者:** Shengqi Liu; Yuhao Cheng; Zhuo Chen; Xingyu Ren; Wenhan Zhu; Lincheng Li; Mengxiao Bi; Xiaokang Yang; Yichao Yan
>
> **备注:** Our project page: https://shengqiliu1.github.io/SewingLDM
>
> **摘要:** Generating sewing patterns in garment design is receiving increasing attention due to its CG-friendly and flexible-editing nature. Previous sewing pattern generation methods have been able to produce exquisite clothing, but struggle to design complex garments with detailed control. To address these issues, we propose SewingLDM, a multi-modal generative model that generates sewing patterns controlled by text prompts, body shapes, and garment sketches. Initially, we extend the original vector of sewing patterns into a more comprehensive representation to cover more intricate details and then compress them into a compact latent space. To learn the sewing pattern distribution in the latent space, we design a two-step training strategy to inject the multi-modal conditions, \ie, body shapes, text prompts, and garment sketches, into a diffusion model, ensuring the generated garments are body-suited and detail-controlled. Comprehensive qualitative and quantitative experiments show the effectiveness of our proposed method, significantly surpassing previous approaches in terms of complex garment design and various body adaptability. Our project page: https://shengqiliu1.github.io/SewingLDM.
>
---
#### [replaced 050] Event-based Photometric Bundle Adjustment
- **分类: cs.CV; cs.RO; eess.SP; math.OC**

- **链接: [http://arxiv.org/pdf/2412.14111v2](http://arxiv.org/pdf/2412.14111v2)**

> **作者:** Shuang Guo; Guillermo Gallego
>
> **备注:** 21 pages, 19 figures, 10 tables. Project page: https://github.com/tub-rip/epba
>
> **摘要:** We tackle the problem of bundle adjustment (i.e., simultaneous refinement of camera poses and scene map) for a purely rotating event camera. Starting from first principles, we formulate the problem as a classical non-linear least squares optimization. The photometric error is defined using the event generation model directly in the camera rotations and the semi-dense scene brightness that triggers the events. We leverage the sparsity of event data to design a tractable Levenberg-Marquardt solver that handles the very large number of variables involved. To the best of our knowledge, our method, which we call Event-based Photometric Bundle Adjustment (EPBA), is the first event-only photometric bundle adjustment method that works on the brightness map directly and exploits the space-time characteristics of event data, without having to convert events into image-like representations. Comprehensive experiments on both synthetic and real-world datasets demonstrate EPBA's effectiveness in decreasing the photometric error (by up to 90%), yielding results of unparalleled quality. The refined maps reveal details that were hidden using prior state-of-the-art rotation-only estimation methods. The experiments on modern high-resolution event cameras show the applicability of EPBA to panoramic imaging in various scenarios (without map initialization, at multiple resolutions, and in combination with other methods, such as IMU dead reckoning or previous event-based rotation estimation methods). We make the source code publicly available. https://github.com/tub-rip/epba
>
---
#### [replaced 051] EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models
- **分类: cs.CL; cs.AI; cs.CV; cs.HC; cs.LG**

- **链接: [http://arxiv.org/pdf/2504.15133v2](http://arxiv.org/pdf/2504.15133v2)**

> **作者:** Ziwen Xu; Shuxun Wang; Kewei Xu; Haoming Xu; Mengru Wang; Xinle Deng; Yunzhi Yao; Guozhou Zheng; Huajun Chen; Ningyu Zhang
>
> **备注:** Work in progress. Demo: https://www.youtube.com/watch?v=AkfoiPfp5rQ; code: https://github.com/zjunlp/EasyEdit
>
> **摘要:** In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model's behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model's responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit along with a demonstration notebook. In addition, we provide a demo video at https://www.youtube.com/watch?v=AkfoiPfp5rQ for a quick introduction.
>
---
#### [replaced 052] Many-Task Federated Fine-Tuning via Unified Task Vectors
- **分类: cs.LG; cs.CV**

- **链接: [http://arxiv.org/pdf/2502.06376v2](http://arxiv.org/pdf/2502.06376v2)**

> **作者:** Vasileios Tsouvalas; Tanir Ozcelebi; Nirvana Meratnia
>
> **备注:** 10 pages, 6 figures, accepted in FedGenAI-IJCAI 2025
>
> **摘要:** Federated Learning (FL) traditionally assumes homogeneous client tasks; however, in real-world scenarios, clients often specialize in diverse tasks, introducing task heterogeneity. To address this challenge, Many-Task FL (MaT-FL) has emerged, enabling clients to collaborate effectively despite task diversity. Existing MaT-FL approaches rely on client grouping or personalized layers, requiring the server to manage individual models and failing to account for clients handling multiple tasks. We propose MaTU, a MaT-FL approach that enables joint learning of task vectors across clients, eliminating the need for clustering or client-specific weight storage at the server. Our method introduces a novel aggregation mechanism that determines task similarity based on the direction of clients task vectors and constructs a unified task vector encapsulating all tasks. To address task-specific requirements, we augment the unified task vector with lightweight modulators that facilitate knowledge transfer among related tasks while disentangling dissimilar ones. Evaluated across 30 datasets, MaTU achieves superior performance over state-of-the-art MaT-FL approaches, with results comparable to per-task fine-tuning, while delivering significant communication savings.
>
---
#### [replaced 053] Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.22139v2](http://arxiv.org/pdf/2506.22139v2)**

> **作者:** Shaojie Zhang; Jiahui Yang; Jianqin Yin; Zhenbo Luo; Jian Luan
>
> **备注:** Accepted at ICCV 2025
>
> **摘要:** Multimodal Large Language Models (MLLMs) have demonstrated significant success in visual understanding tasks. However, challenges persist in adapting these models for video comprehension due to the large volume of data and temporal complexity. Existing Video-LLMs using uniform frame sampling often struggle to capture the query-related crucial spatiotemporal clues of videos effectively. In this paper, we introduce Q-Frame, a novel approach for adaptive frame selection and multi-resolution scaling tailored to the video's content and the specific query. Q-Frame employs a training-free, plug-and-play strategy generated by a text-image matching network like CLIP, utilizing the Gumbel-Max trick for efficient frame selection. Q-Frame allows Video-LLMs to process more frames without exceeding computational limits, thereby preserving critical temporal and spatial information. We demonstrate Q-Frame's effectiveness through extensive experiments on benchmark datasets, including MLVU, LongVideoBench, and Video-MME, illustrating its superiority over existing methods and its applicability across various video understanding tasks.
>
---
#### [replaced 054] EAP4EMSIG -- Enhancing Event-Driven Microscopy for Microfluidic Single-Cell Analysis
- **分类: q-bio.QM; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2504.00047v2](http://arxiv.org/pdf/2504.00047v2)**

> **作者:** Nils Friederich; Angelo Jovin Yamachui Sitcheu; Annika Nassal; Erenus Yildiz; Matthias Pesch; Maximilian Beichter; Lukas Scholtes; Bahar Akbaba; Thomas Lautenschlager; Oliver Neumann; Dietrich Kohlheyer; Hanno Scharr; Johannes Seiffarth; Katharina Nöh; Ralf Mikut
>
> **备注:** Submitted to: at - Automatisierungstechnik
>
> **摘要:** Microfluidic Live-Cell Imaging (MLCI) yields data on microbial cell factories. However, continuous acquisition is challenging as high-throughput experiments often lack real-time insights, delaying responses to stochastic events. We introduce three components in the Experiment Automation Pipeline for Event-Driven Microscopy to Smart Microfluidic Single-Cell Analysis (EAP4EMSIG): a fast, accurate Multi-Layer Perceptron (MLP)-based autofocusing method predicting the focus offset, an evaluation of real-time segmentation methods and a real-time data analysis dashboard. Our MLP-based autofocusing achieves a Mean Absolute Error (MAE) of 0.105 $\mu$m with inference times from 87 ms. Among eleven evaluated Deep Learning (DL) segmentation methods, Cellpose reached a Panoptic Quality (PQ) of 93.36 %, while a distance-based method was fastest (121 ms, Panoptic Quality 93.02 %).
>
---
#### [replaced 055] MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2507.02314v2](http://arxiv.org/pdf/2507.02314v2)**

> **作者:** JaeHyuck Choi; MinJun Kim; JeHyeong Hong
>
> **备注:** 10 pages, 6 figures. Code: https://github.com/Jaeihk/MAGIC-Anomaly-generation
>
> **摘要:** Few-shot anomaly generation is emerging as a practical solution for augmenting the scarce anomaly data in industrial quality control settings. An ideal generator would meet three demands at once, namely (i) keep the normal background intact, (ii) inpaint anomalous regions to tightly overlap with the corresponding anomaly masks, and (iii) generate anomalous regions in a semantically valid location, while still producing realistic, diverse appearances from only a handful of real examples. Existing diffusion-based methods usually satisfy at most two of these requirements: global anomaly generators corrupt the background, whereas mask-guided ones often falter when the mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting with multi-level perturbations and Context-aware alignment--to resolve all three issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting backbone that preserves normal regions and ensures strict adherence of the synthesized anomaly to the supplied mask, directly addressing background corruption and misalignment. To offset the diversity loss that fine-tuning can cause, MAGIC adds two complementary perturbation strategies: (i) Gaussian prompt-level perturbation applied during fine-tuning and inference that broadens the global appearance of anomalies while avoiding low-fidelity textual appearances, and (ii) mask-guided spatial noise injection that enriches local texture variations. Additionally, the context-aware mask alignment module forms semantic correspondences and relocates masks so that every anomaly remains plausibly contained within the host object, eliminating out-of-boundary artifacts. Under a consistent identical evaluation protocol on the MVTec-AD dataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly tasks.
>
---
#### [replaced 056] Agentic 3D Scene Generation with Spatially Contextualized VLMs
- **分类: cs.CV; cs.GR**

- **链接: [http://arxiv.org/pdf/2505.20129v3](http://arxiv.org/pdf/2505.20129v3)**

> **作者:** Xinhang Liu; Yu-Wing Tai; Chi-Keung Tang
>
> **备注:** Project page: https://spatctxvlm.github.io/project_page/
>
> **摘要:** Despite recent advances in multimodal content generation enabled by vision-language models (VLMs), their ability to reason about and generate structured 3D scenes remains largely underexplored. This limitation constrains their utility in spatially grounded tasks such as embodied AI, immersive simulations, and interactive 3D applications. We introduce a new paradigm that enables VLMs to generate, understand, and edit complex 3D environments by injecting a continually evolving spatial context. Constructed from multimodal input, this context consists of three components: a scene portrait that provides a high-level semantic blueprint, a semantically labeled point cloud capturing object-level geometry, and a scene hypergraph that encodes rich spatial relationships, including unary, binary, and higher-order constraints. Together, these components provide the VLM with a structured, geometry-aware working memory that integrates its inherent multimodal reasoning capabilities with structured 3D understanding for effective spatial reasoning. Building on this foundation, we develop an agentic 3D scene generation pipeline in which the VLM iteratively reads from and updates the spatial context. The pipeline features high-quality asset generation with geometric restoration, environment setup with automatic verification, and ergonomic adjustment guided by the scene hypergraph. Experiments show that our framework can handle diverse and challenging inputs, achieving a level of generalization not observed in prior work. Further results demonstrate that injecting spatial context enables VLMs to perform downstream tasks such as interactive scene editing and path planning, suggesting strong potential for spatially intelligent systems in computer graphics, 3D vision, and embodied applications. Project page: https://spatctxvlm.github.io/project_page/.
>
---
#### [replaced 057] Polarization Multi-Image Synthesis with Birefringent Metasurfaces
- **分类: cs.CV; physics.optics**

- **链接: [http://arxiv.org/pdf/2307.08106v4](http://arxiv.org/pdf/2307.08106v4)**

> **作者:** Dean Hazineh; Soon Wei Daniel Lim; Qi Guo; Federico Capasso; Todd Zickler
>
> **备注:** Published in the Proceedings of the 2023 IEEE International Conference of Computational Photography
>
> **摘要:** Optical metasurfaces composed of precisely engineered nanostructures have gained significant attention for their ability to manipulate light and implement distinct functionalities based on the properties of the incident field. Computational imaging systems have started harnessing this capability to produce sets of coded measurements that benefit certain tasks when paired with digital post-processing. Inspired by these works, we introduce a new system that uses a birefringent metasurface with a polarizer-mosaicked photosensor to capture four optically-coded measurements in a single exposure. We apply this system to the task of incoherent opto-electronic filtering, where digital spatial-filtering operations are replaced by simpler, per-pixel sums across the four polarization channels, independent of the spatial filter size. In contrast to previous work on incoherent opto-electronic filtering that can realize only one spatial filter, our approach can realize a continuous family of filters from a single capture, with filters being selected from the family by adjusting the post-capture digital summation weights. To find a metasurface that can realize a set of user-specified spatial filters, we introduce a form of gradient descent with a novel regularizer that encourages light efficiency and a high signal-to-noise ratio. We demonstrate several examples in simulation and with fabricated prototypes, including some with spatial filters that have prescribed variations with respect to depth and wavelength. Visit the Project Page at https://deanhazineh.github.io/publications/Multi_Image_Synthesis/MIS_Home.html
>
---
#### [replaced 058] MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2502.12632v2](http://arxiv.org/pdf/2502.12632v2)**

> **作者:** Sihyun Yu; Meera Hahn; Dan Kondratyuk; Jinwoo Shin; Agrim Gupta; José Lezama; Irfan Essa; David Ross; Jonathan Huang
>
> **备注:** CVPR 2025 Workshop on AI for Content Creation Workshop (Oral)
>
> **摘要:** Diffusion models are successful for synthesizing high-quality videos but are limited to generating short clips (e.g., 2-10 seconds). Synthesizing sustained footage (e.g. over minutes) still remains an open research question. In this paper, we propose MALT Diffusion (using Memory-Augmented Latent Transformers), a new diffusion model specialized for long video generation. MALT Diffusion (or just MALT) handles long videos by subdividing them into short segments and doing segment-level autoregressive generation. To achieve this, we first propose recurrent attention layers that encode multiple segments into a compact memory latent vector; by maintaining this memory vector over time, MALT is able to condition on it and continuously generate new footage based on a long temporal context. We also present several training techniques that enable the model to generate frames over a long horizon with consistent quality and minimal degradation. We validate the effectiveness of MALT through experiments on long video benchmarks. We first perform extensive analysis of MALT in long-contextual understanding capability and stability using popular long video benchmarks. For example, MALT achieves an FVD score of 220.4 on 128-frame video generation on UCF-101, outperforming the previous state-of-the-art of 648.4. Finally, we explore MALT's capabilities in a text-to-video generation setting and show that it can produce long videos compared with recent techniques for long text-to-video generation.
>
---
#### [replaced 059] DMesh: A Differentiable Mesh Representation
- **分类: cs.CV; cs.GR**

- **链接: [http://arxiv.org/pdf/2404.13445v3](http://arxiv.org/pdf/2404.13445v3)**

> **作者:** Sanghyun Son; Matheus Gadelha; Yang Zhou; Zexiang Xu; Ming C. Lin; Yi Zhou
>
> **备注:** 36 pages, 24 figures. Updated with camera-ready version
>
> **摘要:** We present a differentiable representation, DMesh, for general 3D triangular meshes. DMesh considers both the geometry and connectivity information of a mesh. In our design, we first get a set of convex tetrahedra that compactly tessellates the domain based on Weighted Delaunay Triangulation (WDT), and select triangular faces on the tetrahedra to define the final mesh. We formulate probability of faces to exist on the actual surface in a differentiable manner based on the WDT. This enables DMesh to represent meshes of various topology in a differentiable way, and allows us to reconstruct the mesh under various observations, such as point cloud and multi-view images using gradient-based optimization. The source code and full paper is available at: https://sonsang.github.io/dmesh-project.
>
---
#### [replaced 060] Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view RGB and Event Streams
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.06770v3](http://arxiv.org/pdf/2412.06770v3)**

> **作者:** Viktor Rudnev; Gereon Fox; Mohamed Elgharib; Christian Theobalt; Vladislav Golyanik
>
> **备注:** 17 pages, 13 figures, 7 tables; CVPRW 2025
>
> **摘要:** Volumetric reconstruction of dynamic scenes is an important problem in computer vision. It is especially challenging in poor lighting and with fast motion. This is partly due to limitations of RGB cameras: To capture frames under low lighting, the exposure time needs to be increased, which leads to more motion blur. In contrast, event cameras, which record changes in pixel brightness asynchronously, are much less dependent on lighting, making them more suitable for recording fast motion. We hence propose the first method to spatiotemporally reconstruct a scene from sparse multi-view event streams and sparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF models, one per short recording segment. The individual segments are supervised with a set of event- and RGB-based losses and sparse-view regularisation. We assemble a real-world multi-view camera rig with six static event cameras around the object and record a benchmark multi-view event stream dataset of challenging motions. Our work outperforms RGB-based baselines, producing state-of-the-art results, and opens up the topic of multi-view event-based reconstruction as a new path for fast scene capture beyond RGB cameras. The code and the data are released at https://4dqv.mpi-inf.mpg.de/DynEventNeRF/
>
---
#### [replaced 061] ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models
- **分类: cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2409.09318v4](http://arxiv.org/pdf/2409.09318v4)**

> **作者:** Yahan Tu; Rui Hu; Jitao Sang
>
> **摘要:** Hallucination poses a persistent challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are generally static, which may overlook the potential risk of data contamination. To address this issue, we propose ODE, an open-set, dynamic protocol designed to evaluate object hallucinations in MLLMs at both the existence and attribute levels. ODE employs a graph-based structure to represent real-world object concepts, their attributes, and the distributional associations between them. This structure facilitates the extraction of concept combinations based on diverse distributional criteria, generating varied samples for structured queries that evaluate hallucinations in both generative and discriminative tasks. Through the generation of new samples, dynamic concept combinations, and varied distribution frequencies, ODE mitigates the risk of data contamination and broadens the scope of evaluation. This protocol is applicable to both general and specialized scenarios, including those with limited data. Experimental results demonstrate the effectiveness of our protocol, revealing that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated samples, which indicates potential data contamination. Furthermore, these generated samples aid in analyzing hallucination patterns and fine-tuning models, offering an effective approach to mitigating hallucinations in MLLMs.
>
---
#### [replaced 062] LEDA: Log-Euclidean Diffeomorphism Autoencoder for Efficient Statistical Analysis of Diffeomorphisms
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2412.16129v2](http://arxiv.org/pdf/2412.16129v2)**

> **作者:** Krithika Iyer; Shireen Elhabian; Sarang Joshi
>
> **摘要:** Image registration is a core task in computational anatomy that establishes correspondences between images. Invertible deformable registration, which computes a deformation field and handles complex, non-linear transformations, is essential for tracking anatomical variations, especially in neuroimaging applications where inter-subject differences and longitudinal changes are key. Analyzing the deformation fields is challenging due to their non-linearity, which limits statistical analysis. However, traditional approaches for analyzing deformation fields are computationally expensive, sensitive to initialization, and prone to numerical errors, especially when the deformation is far from the identity. To address these limitations, we propose the Log-Euclidean Diffeomorphism Autoencoder (LEDA), an innovative framework designed to compute the principal logarithm of deformation fields by efficiently predicting consecutive square roots. LEDA operates within a linearized latent space that adheres to the diffeomorphisms group action laws, enhancing our model's robustness and applicability. We also introduce a loss function to enforce inverse consistency, ensuring accurate latent representations of deformation fields. Extensive experiments with the OASIS-1 dataset demonstrate the effectiveness of LEDA in accurately modeling and analyzing complex non-linear deformations while maintaining inverse consistency. Additionally, we evaluate its ability to capture and incorporate clinical variables, enhancing its relevance for clinical applications.
>
---
#### [replaced 063] Fine-Grained Captioning of Long Videos through Scene Graph Consolidation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2502.16427v2](http://arxiv.org/pdf/2502.16427v2)**

> **作者:** Sanghyeok Chu; Seonguk Seo; Bohyung Han
>
> **备注:** Accepted to the 42nd International Conference on Machine Learning (ICML 2025)
>
> **摘要:** Recent advances in vision-language models have led to impressive progress in caption generation for images and short video clips. However, these models remain constrained by their limited temporal receptive fields, making it difficult to produce coherent and comprehensive captions for long videos. While several methods have been proposed to aggregate information across video segments, they often rely on supervised fine-tuning or incur significant computational overhead. To address these challenges, we introduce a novel framework for long video captioning based on graph consolidation. Our approach first generates segment-level captions, corresponding to individual frames or short video intervals, using off-the-shelf visual captioning models. These captions are then parsed into individual scene graphs, which are subsequently consolidated into a unified graph representation that preserves both holistic context and fine-grained details throughout the video. A lightweight graph-to-text decoder then produces the final video-level caption. This framework effectively extends the temporal understanding capabilities of existing models without requiring any additional fine-tuning on long video datasets. Experimental results show that our method significantly outperforms existing LLM-based consolidation approaches, achieving strong zero-shot performance while substantially reducing computational costs.
>
---
#### [replaced 064] Deep Transformer Network for Monocular Pose Estimation of Shipborne Unmanned Aerial Vehicle
- **分类: cs.CV; cs.AI; cs.RO; eess.IV**

- **链接: [http://arxiv.org/pdf/2406.09260v2](http://arxiv.org/pdf/2406.09260v2)**

> **作者:** Maneesha Wickramasuriya; Taeyoung Lee; Murray Snyder
>
> **备注:** 23 pages, 25 figures, 3 tables
>
> **摘要:** This paper introduces a deep transformer network for estimating the relative 6D pose of a Unmanned Aerial Vehicle (UAV) with respect to a ship using monocular images. A synthetic dataset of ship images is created and annotated with 2D keypoints of multiple ship parts. A Transformer Neural Network model is trained to detect these keypoints and estimate the 6D pose of each part. The estimates are integrated using Bayesian fusion. The model is tested on synthetic data and in-situ flight experiments, demonstrating robustness and accuracy in various lighting conditions. The position estimation error is approximately 0.8\% and 1.0\% of the distance to the ship for the synthetic data and the flight experiments, respectively. The method has potential applications for ship-based autonomous UAV landing and navigation.
>
---
#### [replaced 065] AVTENet: A Human-Cognition-Inspired Audio-Visual Transformer-Based Ensemble Network for Video Deepfake Detection
- **分类: cs.CV; cs.AI; cs.LG; cs.MM; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2310.13103v2](http://arxiv.org/pdf/2310.13103v2)**

> **作者:** Ammarah Hashmi; Sahibzada Adil Shahzad; Chia-Wen Lin; Yu Tsao; Hsin-Min Wang
>
> **摘要:** The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous studies on detecting artificial intelligence-generated fake videos only utilize visual modality or audio modality. While some methods exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multimodal datasets of deepfake videos involving acoustic and visual manipulations, and are mostly based on convolutional neural networks with low detection accuracy. Considering that human cognition instinctively integrates multisensory information including audio and visual cues to perceive and interpret content and the success of transformer in various fields, this study introduces the audio-visual transformer-based ensemble network (AVTENet). This innovative framework tackles the complexities of deepfake technology by integrating both acoustic and visual manipulations to enhance the accuracy of video forgery detection. Specifically, the proposed model integrates several purely transformer-based variants that capture video, audio, and audio-visual salient cues to reach a consensus in prediction. For evaluation, we use the recently released benchmark multimodal audio-video FakeAVCeleb dataset. For a detailed analysis, we evaluate AVTENet, its variants, and several existing methods on multiple test sets of the FakeAVCeleb dataset. Experimental results show that the proposed model outperforms all existing methods and achieves state-of-the-art performance on Testset-I and Testset-II of the FakeAVCeleb dataset. We also compare AVTENet against humans in detecting video forgery. The results show that AVTENet significantly outperforms humans.
>
---
#### [replaced 066] Investigating the diversity and stylization of contemporary user generated visual arts in the complexity entropy plane
- **分类: cs.CV; physics.data-an; physics.soc-ph**

- **链接: [http://arxiv.org/pdf/2408.10356v3](http://arxiv.org/pdf/2408.10356v3)**

> **作者:** Seunghwan Kim; Byunghwee Lee; Wonjae Lee
>
> **备注:** 19 pages, 3 figures, 1 table, SI(4 figures, 3 tables)
>
> **摘要:** The advent of computational and numerical methods in recent times has provided new avenues for analyzing art historiographical narratives and tracing the evolution of art styles therein. Here, we investigate an evolutionary process underpinning the emergence and stylization of contemporary user-generated visual art styles using the complexity-entropy (C-H) plane, which quantifies local structures in paintings. Informatizing 149,780 images curated in DeviantArt and Behance platforms from 2010 to 2020, we analyze the relationship between local information of the C-H space and multi-level image features generated by a deep neural network and a feature extraction algorithm. The results reveal significant statistical relationships between the C-H information of visual artistic styles and the dissimilarities of the multi-level image features over time within groups of artworks. By disclosing a particular C-H region where the diversity of image representations is noticeably manifested, our analyses reveal an empirical condition of emerging styles that are both novel in the C-H plane and characterized by greater stylistic diversity. Our research shows that visual art analyses combined with physics-inspired methodologies and machine learning, can provide macroscopic insights into quantitatively mapping relevant characteristics of an evolutionary process underpinning the creative stylization of uncharted visual arts of given groups and time.
>
---
#### [replaced 067] Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models
- **分类: cs.CV; cs.CL**

- **链接: [http://arxiv.org/pdf/2505.04921v2](http://arxiv.org/pdf/2505.04921v2)**

> **作者:** Yunxin Li; Zhenyu Liu; Zitao Li; Xuanyu Zhang; Zhenran Xu; Xinyu Chen; Haoyuan Shi; Shenyuan Jiang; Xintong Wang; Jifang Wang; Shouzheng Huang; Xinping Zhao; Borui Jiang; Lanqing Hong; Longyue Wang; Zhuotao Tian; Baoxing Huai; Wenhan Luo; Weihua Luo; Zheng Zhang; Baotian Hu; Min Zhang
>
> **备注:** v2, 91 Pages, 10 figures; Project: https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models
>
> **摘要:** Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.
>
---
#### [replaced 068] DeepCS-TRD, a Deep Learning-based Cross-Section Tree Ring Detector
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.16242v2](http://arxiv.org/pdf/2504.16242v2)**

> **作者:** Henry Marichal; Verónica Casaravilla; Candice Power; Karolain Mello; Joaquín Mazarino; Christine Lucas; Ludmila Profumo; Diego Passarella; Gregory Randall
>
> **备注:** 12 pages, 6 figures. Accepted in 23rd International Conference on Image Analysis and Processing (ICIAP 2025), 15-19 September 2025. Rome, Italy
>
> **摘要:** Here, we propose Deep CS-TRD, a new automatic algorithm for detecting tree rings in whole cross-sections. It substitutes the edge detection step of CS-TRD by a deep-learning-based approach (U-Net), which allows the application of the method to different image domains: microscopy, scanner or smartphone acquired, and species (Pinus taeda, Gleditsia triachantos and Salix glauca). Additionally, we introduce two publicly available datasets of annotated images to the community. The proposed method outperforms state-of-the-art approaches in macro images (Pinus taeda and Gleditsia triacanthos) while showing slightly lower performance in microscopy images of Salix glauca. To our knowledge, this is the first paper that studies automatic tree ring detection for such different species and acquisition conditions. The dataset and source code are available in https://github.com/hmarichal93/deepcstrd
>
---
#### [replaced 069] AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.20255v2](http://arxiv.org/pdf/2505.20255v2)**

> **作者:** Muyao Niu; Mingdeng Cao; Yifan Zhan; Qingtian Zhu; Mingze Ma; Jiancheng Zhao; Yanhong Zeng; Zhihang Zhong; Xiao Sun; Yinqiang Zheng
>
> **备注:** Homepage: https://myniuuu.github.io/AniCrafter ; Codes: https://github.com/MyNiuuu/AniCrafter
>
> **摘要:** Recent advances in video diffusion models have significantly improved character animation techniques. However, current approaches rely on basic structural conditions such as DWPose or SMPL-X to animate character images, limiting their effectiveness in open-domain scenarios with dynamic backgrounds or challenging human poses. In this paper, we introduce \textbf{AniCrafter}, a diffusion-based human-centric animation model that can seamlessly integrate and animate a given character into open-domain dynamic backgrounds while following given human motion sequences. Built on cutting-edge Image-to-Video (I2V) diffusion architectures, our model incorporates an innovative ''avatar-background'' conditioning mechanism that reframes open-domain human-centric animation as a restoration task, enabling more stable and versatile animation outputs. Experimental results demonstrate the superior performance of our method. Codes are available at https://github.com/MyNiuuu/AniCrafter.
>
---
#### [replaced 070] Play to Generalize: Learning to Reason Through Game Play
- **分类: cs.CV; cs.CL**

- **链接: [http://arxiv.org/pdf/2506.08011v3](http://arxiv.org/pdf/2506.08011v3)**

> **作者:** Yunfei Xie; Yinsong Ma; Shiyi Lan; Alan Yuille; Junfei Xiao; Chen Wei
>
> **备注:** Project Page: https://yunfeixie233.github.io/ViGaL/
>
> **摘要:** Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base model's performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs.
>
---
#### [replaced 071] TerraMind: Large-Scale Generative Multimodality for Earth Observation
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2504.11171v3](http://arxiv.org/pdf/2504.11171v3)**

> **作者:** Johannes Jakubik; Felix Yang; Benedikt Blumenstiel; Erik Scheurer; Rocco Sedona; Stefano Maurogiovanni; Jente Bosmans; Nikolaos Dionelis; Valerio Marsocci; Niklas Kopp; Rahul Ramachandran; Paolo Fraccaro; Thomas Brunschwiler; Gabriele Cavallaro; Juan Bernabe-Moreno; Nicolas Longépé
>
> **备注:** Accepted at ICCV'25
>
> **摘要:** We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces "Thinking-in-Modalities" (TiM) -- the capability of generating additional artificial data during finetuning and inference to improve the model output -- and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code are open-sourced under a permissive license.
>
---
#### [replaced 072] Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?
- **分类: cs.CL; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.08468v2](http://arxiv.org/pdf/2505.08468v2)**

> **作者:** Md Tahmid Rahman Laskar; Mohammed Saidul Islam; Ridwan Mahbub; Ahmed Masry; Mizanur Rahman; Amran Bhuiyan; Mir Tafseer Nayeem; Shafiq Joty; Enamul Hoque; Jimmy Huang
>
> **备注:** Accepted at ACL 2025 Industry Track
>
> **摘要:** Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (<10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judge's accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist.
>
---
#### [replaced 073] CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement for Time Series Diffusion Model
- **分类: cs.CV; cs.SY; eess.SY**

- **链接: [http://arxiv.org/pdf/2506.03502v2](http://arxiv.org/pdf/2506.03502v2)**

> **作者:** Yuxuan Chen; Haipeng Xie
>
> **摘要:** The denoising diffusion probabilistic model has become a mainstream generative model, achieving significant success in various computer vision tasks. Recently, there has been initial exploration of applying diffusion models to time series tasks. However, existing studies still face challenges in multi-scale feature alignment and generative capabilities across different entities and long-time scales. In this paper, we propose CHIME, a conditional hallucination and integrated multi-scale enhancement framework for time series diffusion models. By employing multi-scale decomposition and integration, CHIME captures the decomposed features of time series, achieving in-domain distribution alignment between generated and original samples. In addition, we introduce a feature hallucination module in the conditional denoising process, enabling the temporal features transfer across long-time scales. Experimental results on publicly available real-world datasets demonstrate that CHIME achieves state-of-the-art performance and exhibits excellent generative generalization capabilities in few-shot scenarios.
>
---
#### [replaced 074] TrajFlow: Multi-modal Motion Prediction via Flow Matching
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2506.08541v2](http://arxiv.org/pdf/2506.08541v2)**

> **作者:** Qi Yan; Brian Zhang; Yutong Zhang; Daniel Yang; Joshua White; Di Chen; Jiachao Liu; Langechuan Liu; Binnan Zhuang; Shaoshuai Shi; Renjie Liao
>
> **备注:** IROS 2025
>
> **摘要:** Efficient and accurate motion prediction is crucial for ensuring safety and informed decision-making in autonomous driving, particularly under dynamic real-world conditions that necessitate multi-modal forecasts. We introduce TrajFlow, a novel flow matching-based motion prediction framework that addresses the scalability and efficiency challenges of existing generative trajectory prediction methods. Unlike conventional generative approaches that employ i.i.d. sampling and require multiple inference passes to capture diverse outcomes, TrajFlow predicts multiple plausible future trajectories in a single pass, significantly reducing computational overhead while maintaining coherence across predictions. Moreover, we propose a ranking loss based on the Plackett-Luce distribution to improve uncertainty estimation of predicted trajectories. Additionally, we design a self-conditioning training technique that reuses the model's own predictions to construct noisy inputs during a second forward pass, thereby improving generalization and accelerating inference. Extensive experiments on the large-scale Waymo Open Motion Dataset (WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across various key metrics, underscoring its effectiveness for safety-critical autonomous driving applications. The code and other details are available on the project website https://traj-flow.github.io/.
>
---
#### [replaced 075] SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound
- **分类: cs.CV; cs.LG; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2406.06612v2](http://arxiv.org/pdf/2406.06612v2)**

> **作者:** Rishit Dagli; Shivesh Prakash; Robert Wu; Houman Khosravani
>
> **备注:** Project Page: https://see2sound.github.io/
>
> **摘要:** Generating combined visual and auditory sensory experiences is critical for the consumption of immersive content. Recent advances in neural generative models have enabled the creation of high-resolution content across multiple modalities such as images, text, speech, and videos. Despite these successes, there remains a significant gap in the generation of high-quality spatial audio that complements generated visual content. Furthermore, current audio generation models excel in either generating natural audio or speech or music but fall short in integrating spatial audio cues necessary for immersive experiences. In this work, we introduce SEE-2-SOUND, a zero-shot approach that decomposes the task into (1) identifying visual regions of interest; (2) locating these elements in 3D space; (3) generating mono-audio for each; and (4) integrating them into spatial audio. Using our framework, we demonstrate compelling results for generating spatial audio for high-quality videos, images, and dynamic images from the internet, as well as media generated by learned approaches.
>
---
#### [replaced 076] UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video Generation
- **分类: cs.MM; cs.AI; cs.CV; cs.SD; eess.AS**

- **链接: [http://arxiv.org/pdf/2502.03897v5](http://arxiv.org/pdf/2502.03897v5)**

> **作者:** Lei Zhao; Linfeng Feng; Dongxu Ge; Rujin Chen; Fangqiu Yi; Chi Zhang; Xiao-Lei Zhang; Xuelong Li
>
> **备注:** Our demos are available at https://uniform-t2av.github.io/
>
> **摘要:** With the rise of diffusion models, audio-video generation has been revolutionized. However, most existing methods rely on separate modules for each modality, with limited exploration of unified generative architectures. In addition, many are confined to a single task and small-scale datasets. To overcome these limitations, we introduce UniForm, a unified multi-task diffusion transformer that generates both audio and visual modalities in a shared latent space. By using a unified denoising network, UniForm captures the inherent correlations between sound and vision. Additionally, we propose task-specific noise schemes and task tokens, enabling the model to support multiple tasks with a single set of parameters, including video-to-audio, audio-to-video and text-to-audio-video generation. Furthermore, by leveraging large language models and a large-scale text-audio-video combined dataset, UniForm achieves greater generative diversity than prior approaches. Experiments show that UniForm achieves performance close to the state-of-the-art single-task models across three generation tasks, with generated content that is not only highly aligned with real-world data distributions but also enables more diverse and fine-grained generation.
>
---
#### [replaced 077] 3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.07759v3](http://arxiv.org/pdf/2412.07759v3)**

> **作者:** Xiao Fu; Xian Liu; Xintao Wang; Sida Peng; Menghan Xia; Xiaoyu Shi; Ziyang Yuan; Pengfei Wan; Di Zhang; Dahua Lin
>
> **备注:** ICLR 2025. Project Page & Code & Data: http://fuxiao0719.github.io/projects/3dtrajmaster
>
> **摘要:** This paper aims to manipulate multi-entity 3D motions in video generation. Previous methods on controllable video generation primarily leverage 2D control signals to manipulate object motions and have achieved remarkable synthesis results. However, 2D control signals are inherently limited in expressing the 3D nature of object motions. To overcome this problem, we introduce 3DTrajMaster, a robust controller that regulates multi-entity dynamics in 3D space, given user-desired 6DoF pose (location and rotation) sequences of entities. At the core of our approach is a plug-and-play 3D-motion grounded object injector that fuses multiple input entities with their respective 3D trajectories through a gated self-attention mechanism. In addition, we exploit an injector architecture to preserve the video diffusion prior, which is crucial for generalization ability. To mitigate video quality degradation, we introduce a domain adaptor during training and employ an annealed sampling strategy during inference. To address the lack of suitable training data, we construct a 360-Motion Dataset, which first correlates collected 3D human and animal assets with GPT-generated trajectory and then captures their motion with 12 evenly-surround cameras on diverse 3D UE platforms. Extensive experiments show that 3DTrajMaster sets a new state-of-the-art in both accuracy and generalization for controlling multi-entity 3D motions. Project page: http://fuxiao0719.github.io/projects/3dtrajmaster
>
---
#### [replaced 078] AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.01801v2](http://arxiv.org/pdf/2507.01801v2)**

> **作者:** Bin Rao; Haicheng Liao; Yanchen Guan; Chengyue Wang; Bonan Wang; Jiaxun Zhang; Zhenning Li
>
> **摘要:** Accurately predicting the future trajectories of traffic agents is essential in autonomous driving. However, due to the inherent imbalance in trajectory distributions, tail data in natural datasets often represents more complex and hazardous scenarios. Existing studies typically rely solely on a base model's prediction error, without considering the diversity and uncertainty of long-tail trajectory patterns. We propose an adaptive momentum and decoupled contrastive learning framework (AMD), which integrates unsupervised and supervised contrastive learning strategies. By leveraging an improved momentum contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module, our framework enhances the model's ability to recognize rare and complex trajectories. Additionally, we design four types of trajectory random augmentation methods and introduce an online iterative clustering strategy, allowing the model to dynamically update pseudo-labels and better adapt to the distributional shifts in long-tail data. We propose three different criteria to define long-tail trajectories and conduct extensive comparative experiments on the nuScenes and ETH$/$UCY datasets. The results show that AMD not only achieves optimal performance in long-tail trajectory prediction but also demonstrates outstanding overall prediction accuracy.
>
---
#### [replaced 079] Specialized Foundation Models for Intelligent Operating Rooms
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.12890v2](http://arxiv.org/pdf/2505.12890v2)**

> **作者:** Ege Özsoy; Chantal Pellegrini; David Bani-Harouni; Kun Yuan; Matthias Keicher; Nassir Navab
>
> **摘要:** Surgical procedures unfold in complex environments demanding coordination between surgical teams, tools, imaging and increasingly, intelligent robotic systems. Ensuring safety and efficiency in ORs of the future requires intelligent systems, like surgical robots, smart instruments and digital copilots, capable of understanding complex activities and hazards of surgeries. Yet, existing computational approaches, lack the breadth, and generalization needed for comprehensive OR understanding. We introduce ORQA, a multimodal foundation model unifying visual, auditory, and structured data for holistic surgical understanding. ORQA's question-answering framework empowers diverse tasks, serving as an intelligence core for a broad spectrum of surgical technologies. We benchmark ORQA against generalist vision-language models, including ChatGPT and Gemini, and show that while they struggle to perceive surgical scenes, ORQA delivers substantially stronger, consistent performance. Recognizing the extensive range of deployment settings across clinical practice, we design, and release a family of smaller ORQA models tailored to different computational requirements. This work establishes a foundation for the next wave of intelligent surgical solutions, enabling surgical teams and medical technology providers to create smarter and safer operating rooms.
>
---
#### [replaced 080] ObjectAdd: Adding Objects into Image via a Training-Free Diffusion Modification Fashion
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2404.17230v5](http://arxiv.org/pdf/2404.17230v5)**

> **作者:** Ziyue Zhang; Yuxin Zhang; Quanjian Song; Rongrong Ji
>
> **备注:** 12 pages in total
>
> **摘要:** We introduce ObjectAdd, a training-free diffusion modification method to add user-expected objects into user-specified area. The motive of ObjectAdd stems from: first, describing everything in one prompt can be difficult, and second, users often need to add objects into the generated image. To accommodate with real world, our ObjectAdd maintains accurate image consistency after adding objects with technical innovations in: (1) embedding-level concatenation to ensure correct text embedding coalesce; (2) object-driven layout control with latent and attention injection to ensure objects accessing user-specified area; (3) prompted image inpainting in an attention refocusing & object expansion fashion to ensure rest of the image stays the same. With a text-prompted image, our ObjectAdd allows users to specify a box and an object, and achieves: (1) adding object inside the box area; (2) exact content outside the box area; (3) flawless fusion between the two areas
>
---
#### [replaced 081] LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2507.00790v2](http://arxiv.org/pdf/2507.00790v2)**

> **作者:** Huaqiu Li; Yong Wang; Tongwen Huang; Hailang Huang; Haoqian Wang; Xiangxiang Chu
>
> **摘要:** Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data will be available at https://github.com/AMAP-ML/LD-RPS.
>
---
#### [replaced 082] Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2507.00493v2](http://arxiv.org/pdf/2507.00493v2)**

> **作者:** Fenil R. Doshi; Thomas Fel; Talia Konkle; George Alvarez
>
> **备注:** Project page: https://www.fenildoshi.com/configural-shape/ updated email address
>
> **摘要:** Humans are able to recognize objects based on both local texture cues and the configuration of object parts, yet contemporary vision models primarily harvest local texture cues, yielding brittle, non-compositional features. Work on shape-vs-texture bias has pitted shape and texture representations in opposition, measuring shape relative to texture, ignoring the possibility that models (and humans) can simultaneously rely on both types of cues, and obscuring the absolute quality of both types of representation. We therefore recast shape evaluation as a matter of absolute configural competence, operationalized by the Configural Shape Score (CSS), which (i) measures the ability to recognize both images in Object-Anagram pairs that preserve local texture while permuting global part arrangement to depict different object categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii) uncovers a broad spectrum of configural sensitivity with fully self-supervised and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes reveal that (iii) high-CSS networks depend on long-range interactions: radius-controlled attention masks abolish performance showing a distinctive U-shaped integration profile, and representational-similarity analyses expose a mid-depth transition from local to global coding. A BagNet control remains at chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that configural shape score also predicts other shape-dependent evals. Overall, we propose that the path toward truly robust, generalizable, and human-like vision systems may not lie in forcing an artificial choice between shape and texture, but rather in architectural and learning frameworks that seamlessly integrate both local-texture and global configural shape.
>
---
#### [replaced 083] Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds
- **分类: cs.CV; eess.SP**

- **链接: [http://arxiv.org/pdf/2503.07435v4](http://arxiv.org/pdf/2503.07435v4)**

> **作者:** Riccardo Mazzieri; Jacopo Pegoraro; Michele Rossi
>
> **摘要:** The adoption of Millimeter-Wave (mmWave) radar devices for human sensing, particularly gait recognition, has recently gathered significant attention due to their efficiency, resilience to environmental conditions, and privacy-preserving nature. In this work, we tackle the challenging problem of Open-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike most existing research, which assumes a closed-set scenario, our work considers the more realistic open-set case, where unknown subjects might be present at inference time, and should be correctly recognized by the system. Point clouds are well-suited for edge computing applications with resource constraints, but are more significantly affected by noise and random fluctuations than other representations, like the more common micro-Doppler signature. This is the first work addressing open-set gait recognition with sparse point cloud data. To do so, we propose a novel neural network architecture that combines supervised classification with unsupervised reconstruction of the point clouds, creating a robust, rich, and highly regularized latent space of gait features. To detect unknown subjects at inference time, we introduce a probabilistic novelty detection algorithm that leverages the structured latent space and offers a tunable trade-off between inference speed and prediction accuracy. Along with this paper, we release mmGait10, an original human gait dataset featuring over five hours of measurements from ten subjects, under varied walking modalities. Extensive experimental results show that our solution attains F1-Score improvements by 24% over state-of-the-art methods adapted for point clouds, on average, and across multiple openness levels.
>
---
#### [replaced 084] Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2411.10936v2](http://arxiv.org/pdf/2411.10936v2)**

> **作者:** Ni Ou; Zhuo Chen; Xinru Zhang; Junzheng Wang
>
> **备注:** This article is an earlier version of the article arXiv:2506.14706, and most of its content is duplicated.
>
> **摘要:** Cameras and LiDAR are essential sensors for autonomous vehicles. Camera-LiDAR data fusion compensate for deficiencies of stand-alone sensors but relies on precise extrinsic calibration. Many learning-based calibration methods predict extrinsic parameters in a single step. Driven by the growing demand for higher accuracy, a few approaches utilize multi-range models or integrate multiple methods to improve extrinsic parameter predictions, but these strategies incur extended training times and require additional storage for separate models. To address these issues, we propose a single-model iterative approach based on surrogate diffusion to significantly enhance the capacity of individual calibration methods. By applying a buffering technique proposed by us, the inference time of our surrogate diffusion is 43.7% less than that of multi-range models. Additionally, we create a calibration network as our denoiser, featuring both projection-first and encoding-first branches for effective point feature extraction. Extensive experiments demonstrate that our diffusion model outperforms other single-model iterative methods and delivers competitive results compared to multi-range models. Our denoiser exceeds state-of-the-art calibration methods, reducing the rotation error by 24.5% compared to the second-best method. Furthermore, with the proposed diffusion applied, it achieves 20.4% less rotation error and 9.6% less translation error.
>
---
#### [replaced 085] Spatial-Temporal Conditional Random Field for Human Trajectory Prediction
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2311.18198v2](http://arxiv.org/pdf/2311.18198v2)**

> **作者:** Pengqian Han; Jiamou Liu; Jialing He; Zeyu Zhang; Song Yang; Yanni Tang
>
> **摘要:** Trajectory prediction is of significant importance in computer vision. Accurate pedestrian trajectory prediction benefits autonomous vehicles and robots in planning their motion. Pedestrians' trajectories are greatly influenced by their intentions. Prior studies having introduced various deep learning methods only pay attention to the spatial and temporal information of trajectory, overlooking the explicit intention information. In this study, we introduce a novel model, termed the \textbf{S-T CRF}: \textbf{S}patial-\textbf{T}emporal \textbf{C}onditional \textbf{R}andom \textbf{F}ield, which judiciously incorporates intention information besides spatial and temporal information of trajectory. This model uses a Conditional Random Field (CRF) to generate a representation of future intentions, greatly improving the prediction of subsequent trajectories when combined with spatial-temporal representation. Furthermore, the study innovatively devises a space CRF loss and a time CRF loss, meticulously designed to enhance interaction constraints and temporal dynamics, respectively. Extensive experimental evaluations on dataset ETH/UCY and SDD demonstrate that the proposed method surpasses existing baseline approaches.
>
---
#### [replaced 086] Dark Noise Diffusion: Noise Synthesis for Low-Light Image Denoising
- **分类: cs.CV; eess.IV**

- **链接: [http://arxiv.org/pdf/2503.11262v2](http://arxiv.org/pdf/2503.11262v2)**

> **作者:** Liying Lu; Raphaël Achddou; Sabine Süsstrunk
>
> **摘要:** Low-light photography produces images with low signal-to-noise ratios due to limited photons. In such conditions, common approximations like the Gaussian noise model fall short, and many denoising techniques fail to remove noise effectively. Although deep-learning methods perform well, they require large datasets of paired images that are impractical to acquire. As a remedy, synthesizing realistic low-light noise has gained significant attention. In this paper, we investigate the ability of diffusion models to capture the complex distribution of low-light noise. We show that a naive application of conventional diffusion models is inadequate for this task and propose three key adaptations that enable high-precision noise generation: a two-branch architecture to better model signal-dependent and signal-independent noise, the incorporation of positional information to capture fixed-pattern noise, and a tailored diffusion noise schedule. Consequently, our model enables the generation of large datasets for training low-light denoising networks, leading to state-of-the-art performance. Through comprehensive analysis, including statistical evaluation and noise decomposition, we provide deeper insights into the characteristics of the generated data.
>
---
#### [replaced 087] AR Glulam: Accurate Augmented Reality Using Multiple Fiducial Markers for Glulam Fabrication
- **分类: cs.ET; cs.CV; cs.HC**

- **链接: [http://arxiv.org/pdf/2502.08566v2](http://arxiv.org/pdf/2502.08566v2)**

> **作者:** Alexander Htet Kyaw; Arvin Xu; Sasa Zivkovic; Gwyllim Jahn; Cameron Newnham; Nick Van Den Berg
>
> **备注:** 10 Figures, Project Paper for The Proceedings of 2024 Association for Computer Aided Design in Architecture (ACADIA) Conference
>
> **摘要:** Recent advancements in Augmented Reality (AR) have demonstrated applications in architecture, design, and fabrication. Compared to conventional 2D construction drawings, AR can be used to superimpose contextual instructions, display 3D spatial information and enable on-site engagement. Despite the potential of AR, the widespread adoption of the technology in the industry is limited by its precision. Precision is important for projects requiring strict construction tolerances, design fidelity, and fabrication feedback. For example, the manufacturing of glulam beams requires tolerances of less than 2mm. The goal of this project is to explore the industrial application of using multiple fiducial markers for high-precision AR fabrication. While the method has been validated in lab settings with a precision of 0.97, this paper focuses on fabricating glulam beams in a factory setting with an industry manufacturer, Unalam Factory.
>
---
#### [replaced 088] Structured light with a million light planes per second
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2411.18597v2](http://arxiv.org/pdf/2411.18597v2)**

> **作者:** Dhawal Sirikonda; Praneeth Chakravarthula; Ioannis Gkioulekas; Adithya Pediredla
>
> **备注:** Accepted to ICCP 2025 (PAMI Special Issue)
>
> **摘要:** We introduce a structured light system that enables full-frame 3D scanning at speeds of $1000\text{ fps}$, four times faster than the previous fastest systems. Our key innovation is the use of a custom acousto-optic light scanning device capable of projecting two million light planes per second. Coupling this device with an event camera allows our system to overcome the key bottleneck preventing previous structured light systems based on event cameras from achieving higher scanning speeds -- the limited rate of illumination steering. Unlike these previous systems, ours uses the event camera's full-frame bandwidth, shifting the speed bottleneck from the illumination side to the imaging side. To mitigate this new bottleneck and further increase scanning speed, we introduce adaptive scanning strategies that leverage the event camera's asynchronous operation by selectively illuminating regions of interest, thereby achieving effective scanning speeds an order of magnitude beyond the camera's theoretical limit.
>
---
#### [replaced 089] Domain Adaptation of VLM for Soccer Video Understanding
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.13860v2](http://arxiv.org/pdf/2505.13860v2)**

> **作者:** Tiancheng Jiang; Henry Wang; Md Sirajus Salekin; Parmida Atighehchian; Shinan Zhang
>
> **备注:** 8 pages, 5 figures, accepted to the 11th IEEE International Workshop on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix included
>
> **摘要:** Vision Language Models (VLMs) have demonstrated strong performance in multi-modal tasks by effectively aligning visual and textual representations. However, most video understanding VLM research has been domain-agnostic, leaving the understanding of their transfer learning capability to specialized domains under-explored. In this work, we address this by exploring the adaptability of open-source VLMs to specific domains, and focusing on soccer as an initial case study. Our approach uses large-scale soccer datasets and LLM to create instruction-following data, and use them to iteratively fine-tune the general-domain VLM in a curriculum learning fashion (first teaching the model key soccer concepts to then question answering tasks). The final adapted model, trained using a curated dataset of 20k video clips, exhibits significant improvement in soccer-specific tasks compared to the base model, with a 37.5% relative improvement for the visual question-answering task and an accuracy improvement from 11.8% to 63.5% for the downstream soccer action classification task.
>
---
#### [replaced 090] Explainable Coarse-to-Fine Ancient Manuscript Duplicates Discovery
- **分类: cs.IR; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.03836v2](http://arxiv.org/pdf/2505.03836v2)**

> **作者:** Chongsheng Zhang; Shuwen Wu; Yingqi Chen; Yi Men; Gaojuan Fan; Matthias Aßenmacher; Christian Heumann; João Gama
>
> **备注:** Explainable Coarse-to-Fine Ancient Manuscript Duplicates Discovery, with Oracle Bones as a Case Study
>
> **摘要:** Ancient manuscripts are the primary source of ancient linguistic corpora. However, many ancient manuscripts exhibit duplications due to unintentional repeated publication or deliberate forgery. The Dead Sea Scrolls, for example, include counterfeit fragments, whereas Oracle Bones (OB) contain both republished materials and fabricated specimens. Identifying ancient manuscript duplicates is of great significance for both archaeological curation and ancient history study. In this work, we design a progressive OB duplicate discovery framework that combines unsupervised low-level keypoints matching with high-level text-centric content-based matching to refine and rank the candidate OB duplicates with semantic awareness and interpretability. We compare our model with state-of-the-art content-based image retrieval and image matching methods, showing that our model yields comparable recall performance and the highest simplified mean reciprocal rank scores for both Top-5 and Top-15 retrieval results, and with significantly accelerated computation efficiency. We have discovered over 60 pairs of new OB duplicates in real-world deployment, which were missed by domain experts for decades. Code, model and real-world results are available at: https://github.com/cszhangLMU/OBD-Finder/.
>
---
#### [replaced 091] Learning Traffic Anomalies from Generative Models on Real-Time Observations
- **分类: cs.LG; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2502.01391v4](http://arxiv.org/pdf/2502.01391v4)**

> **作者:** Fotis I. Giasemis; Alexandros Sopasakis
>
> **摘要:** Accurate detection of traffic anomalies is crucial for effective urban traffic management and congestion mitigation. We use the Spatiotemporal Generative Adversarial Network (STGAN) framework combining Graph Neural Networks and Long Short-Term Memory networks to capture complex spatial and temporal dependencies in traffic data. We apply STGAN to real-time, minute-by-minute observations from 42 traffic cameras across Gothenburg, Sweden, collected over several months in 2020. The images are processed to compute a flow metric representing vehicle density, which serves as input for the model. Training is conducted on data from April to November 2020, and validation is performed on a separate dataset from November 14 to 23, 2020. Our results demonstrate that the model effectively detects traffic anomalies with high precision and low false positive rates. The detected anomalies include camera signal interruptions, visual artifacts, and extreme weather conditions affecting traffic flow.
>
---
#### [replaced 092] Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition
- **分类: cs.CV; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2505.15367v2](http://arxiv.org/pdf/2505.15367v2)**

> **作者:** Dasol Choi; Seunghyun Lee; Youngsook Song
>
> **摘要:** Vision-Language Models (VLMs) have shown capabilities in interpreting visual content, but their reliability in safety-critical everyday life scenarios remains insufficiently explored. We introduce VERI (Visual Emergency Recognition Dataset), a diagnostic benchmark comprising 200 images organized into 100 contrastive pairs. Each emergency scene is paired with a visually similar but safe counterpart through human verification and refinement. Using a two-stage evaluation protocol - risk identification and emergency response - we assess 14 VLMs (2B to 124B parameters) across medical emergencies, accidents, and natural disasters. Our analysis reveals an "overreaction problem", where models accurately identify genuine emergencies (70-100 percent success rate) but produce high false-positive rates, misclassifying 31-96 percent of safe situations as dangerous. Ten safe scenarios were universally misclassified by all models regardless of scale. This "better-safe-than-sorry" bias primarily results from contextual overinterpretation (88-93 percent of errors), challenging VLM reliability in safety-critical applications. These findings highlight fundamental limitations in current VLM architectures, which persist despite increased model scale. Our results demonstrate an urgent need for strategies specifically improving contextual reasoning in ambiguous visual situations. The consistently low performance of the model indicates that these data serve effectively as a diagnostic dataset.
>
---
#### [replaced 093] Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk
- **分类: cs.CV; cs.GR**

- **链接: [http://arxiv.org/pdf/2507.02477v2](http://arxiv.org/pdf/2507.02477v2)**

> **作者:** Gaochao Song; Zibo Zhao; Haohan Weng; Jingbo Zeng; Rongfei Jia; Shenghua Gao
>
> **备注:** 9 pages main text, 14 pages appendix, 23 figures
>
> **摘要:** We introduce Mesh Silksong, a compact and efficient mesh representation tailored to generate the polygon mesh in an auto-regressive manner akin to silk weaving. Existing mesh tokenization methods always produce token sequences with repeated vertex tokens, wasting the network capability. Therefore, our approach tokenizes mesh vertices by accessing each mesh vertice only once, reduces the token sequence's redundancy by 50\%, and achieves a state-of-the-art compression rate of approximately 22\%. Furthermore, Mesh Silksong produces polygon meshes with superior geometric properties, including manifold topology, watertight detection, and consistent face normals, which are critical for practical applications. Experimental results demonstrate the effectiveness of our approach, showcasing not only intricate mesh generation but also significantly improved geometric integrity.
>
---
#### [replaced 094] Learning Differentiable Logic Programs for Abstract Visual Reasoning
- **分类: cs.LG; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2307.00928v2](http://arxiv.org/pdf/2307.00928v2)**

> **作者:** Hikaru Shindo; Viktor Pfanschilling; Devendra Singh Dhami; Kristian Kersting
>
> **备注:** Published at Machine Learning
>
> **摘要:** Visual reasoning is essential for building intelligent agents that understand the world and perform problem-solving beyond perception. Differentiable forward reasoning has been developed to integrate reasoning with gradient-based machine learning paradigms. However, due to the memory intensity, most existing approaches do not bring the best of the expressivity of first-order logic, excluding a crucial ability to solve abstract visual reasoning, where agents need to perform reasoning by using analogies on abstract concepts in different scenarios. To overcome this problem, we propose NEUro-symbolic Message-pAssiNg reasoNer (NEUMANN), which is a graph-based differentiable forward reasoner, passing messages in a memory-efficient manner and handling structured programs with functors. Moreover, we propose a computationally-efficient structure learning algorithm to perform explanatory program induction on complex visual scenes. To evaluate, in addition to conventional visual reasoning tasks, we propose a new task, visual reasoning behind-the-scenes, where agents need to learn abstract programs and then answer queries by imagining scenes that are not observed. We empirically demonstrate that NEUMANN solves visual reasoning tasks efficiently, outperforming neural, symbolic, and neuro-symbolic baselines.
>
---
#### [replaced 095] MPG-SAM 2: Adapting SAM 2 with Mask Priors and Global Context for Referring Video Object Segmentation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2501.13667v3](http://arxiv.org/pdf/2501.13667v3)**

> **作者:** Fu Rong; Meng Lan; Qian Zhang; Lefei Zhang
>
> **备注:** ICCV 2025
>
> **摘要:** Referring video object segmentation (RVOS) aims to segment objects in a video according to textual descriptions, which requires the integration of multimodal information and temporal dynamics perception. The Segment Anything Model 2 (SAM 2) has shown great effectiveness across various video segmentation tasks. However, its application to offline RVOS is challenged by the translation of the text into effective prompts and a lack of global context awareness. In this paper, we propose a novel RVOS framework, termed MPG-SAM 2, to address these challenges. Specifically, MPG-SAM 2 employs a unified multimodal encoder to jointly encode video and textual features, generating semantically aligned video and text embeddings, along with multimodal class tokens. A mask prior generator utilizes the video embeddings and class tokens to create pseudo masks of target objects and global context. These masks are fed into the prompt encoder as dense prompts along with multimodal class tokens as sparse prompts to generate accurate prompts for SAM 2. To provide the online SAM 2 with a global view, we introduce a hierarchical global-historical aggregator, which allows SAM 2 to aggregate global and historical information of target objects at both pixel and object levels, enhancing the target representation and temporal consistency. Extensive experiments on several RVOS benchmarks demonstrate the superiority of MPG-SAM 2 and the effectiveness of our proposed modules. The code is available at https://github.com/rongfu-dsb/MPG-SAM2.
>
---
#### [replaced 096] SwiftSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.23323v2](http://arxiv.org/pdf/2506.23323v2)**

> **作者:** Quang-Huy Che; Vinh-Tiep Nguyen
>
> **摘要:** Open-vocabulary semantic segmentation (OVSS) aims to segment objects from arbitrary text categories without requiring densely annotated datasets. Although contrastive learning based models enable zero-shot segmentation, they often lose fine spatial precision at pixel level, due to global representation bias. In contrast, diffusion-based models naturally encode fine-grained spatial features via attention mechanisms that capture both global context and local details. However, they often face challenges in balancing the number of iterations with the quality of the segmentation. In this work, we propose FastSeg, a novel and efficient training-free framework with only (1+1)-step of reverse process of a pretrained diffusion model (e.g., Stable Diffusion). Moreover, instead of running multiple times for different classes, FastSeg performs segmentation for all classes at once. To further enhance the segmentation quality, FastSeg introduces three key components: (i) a dual-prompt mechanism for discriminative, class-aware attention extraction, (ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time Flipping (TTF) scheme designed to improve spatial consistency. Extensive experiments show that FastSeg achieves state-of-the-art training-free performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context, and COCO Object benchmarks while maintaining superior inference efficiency. Our results demonstrate that FastSeg provides a strong foundation for extendability, bridging the gap between segmentation quality and inference efficiency.
>
---
#### [replaced 097] Spatio-Temporal Control for Masked Motion Synthesis
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2410.10780v2](http://arxiv.org/pdf/2410.10780v2)**

> **作者:** Ekkasit Pinyoanuntapong; Muhammad Usama Saleem; Korrawe Karunratanakul; Pu Wang; Hongfei Xue; Chen Chen; Chuan Guo; Junli Cao; Jian Ren; Sergey Tulyakov
>
> **备注:** Accepted to ICCV. Change name to MaskControl. project page https://exitudio.github.io/ControlMM-page
>
> **摘要:** Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, these models struggle to achieve high-precision control while maintaining high-quality motion generation. To address these challenges, we propose MaskControl, the first approach to introduce controllability to the generative masked motion model. Our approach introduces two key innovations. First, \textit{Logits Regularizer} implicitly perturbs logits at training time to align the distribution of motion tokens with the controlled joint positions, while regularizing the categorical token prediction to ensure high-fidelity generation. Second, \textit{Logit Optimization} explicitly optimizes the predicted logits during inference time, directly reshaping the token distribution that forces the generated motion to accurately align with the controlled joint positions. Moreover, we introduce \textit{Differentiable Expectation Sampling (DES)} to combat the non-differential distribution sampling process encountered by logits regularizer and optimization. Extensive experiments demonstrate that MaskControl outperforms state-of-the-art methods, achieving superior motion quality (FID decreases by ~77\%) and higher control precision (average error 0.91 vs. 1.08). Additionally, MaskControl enables diverse applications, including any-joint-any-frame control, body-part timeline control, and zero-shot objective control. Video visualization can be found at https://www.ekkasit.com/ControlMM-page/
>
---
#### [replaced 098] PVChat: Personalized Video Chat with One-Shot Learning
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.17069v2](http://arxiv.org/pdf/2503.17069v2)**

> **作者:** Yufei Shi; Weilong Yan; Gang Xu; Yumeng Li; Yuchen Chen; Zhenxi Li; Fei Richard Yu; Ming Li; Si Yong Yeo
>
> **摘要:** Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as "Wilson is receiving chemotherapy" or "Tom is discussing with Sarah", limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose a one-shot learning framework PVChat, the first personalized ViLLM that enables subject-aware question answering (QA) from a single video for each subject. Our approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically augmented video-QA dataset, leveraging a progressive image-to-video learning strategy. Specifically, we introduce an automated augmentation pipeline that synthesizes identity-preserving positive samples and retrieves hard negatives from existing video corpora, generating a diverse training dataset with four QA types: existence, appearance, action, and location inquiries. To enhance subject-specific learning, we propose a ReLU Routing MoH attention mechanism, alongside two novel objectives: (1) Smooth Proximity Regularization for progressive learning through exponential distance scaling and (2) Head Activation Enhancement for balanced attention routing. Finally, we adopt a two-stage training strategy, transitioning from image pre-training to video fine-tuning, enabling a gradual learning process from static attributes to dynamic representations. We evaluate PVChat on diverse datasets covering medical scenarios, TV series, anime, and real-world footage, demonstrating its superiority in personalized feature understanding after learning from a single video, compared to state-of-the-art ViLLMs.
>
---
#### [replaced 099] DMesh++: An Efficient Differentiable Mesh for Complex Shapes
- **分类: cs.CV; cs.GR; cs.LG**

- **链接: [http://arxiv.org/pdf/2412.16776v2](http://arxiv.org/pdf/2412.16776v2)**

> **作者:** Sanghyun Son; Matheus Gadelha; Yang Zhou; Matthew Fisher; Zexiang Xu; Yi-Ling Qiao; Ming C. Lin; Yi Zhou
>
> **备注:** 20 pages, 24 figures, 6 tables
>
> **摘要:** Recent probabilistic methods for 3D triangular meshes capture diverse shapes by differentiable mesh connectivity, but face high computational costs with increased shape details. We introduce a new differentiable mesh processing method that addresses this challenge and efficiently handles meshes with intricate structures. Our method reduces time complexity from O(N) to O(log N) and requires significantly less memory than previous approaches. Building on this innovation, we present a reconstruction algorithm capable of generating complex 2D and 3D shapes from point clouds or multi-view images. Visit our project page (https://sonsang.github.io/dmesh2-project) for source code and supplementary material.
>
---
#### [replaced 100] Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2507.01417v2](http://arxiv.org/pdf/2507.01417v2)**

> **作者:** Jiawei Gu; Ziyue Qiao; Zechao Li
>
> **备注:** Accepted to ICCV 2025
>
> **摘要:** Out-of-Distribution (OOD) detection is critical for safely deploying deep models in open-world environments, where inputs may lie outside the training distribution. During inference on a model trained exclusively with In-Distribution (ID) data, we observe a salient gradient phenomenon: around an ID sample, the local gradient directions for "enhancing" that sample's predicted class remain relatively consistent, whereas OOD samples--unseen in training--exhibit disorganized or conflicting gradient directions in the same neighborhood. Motivated by this observation, we propose an inference-stage technique to short-circuit those feature coordinates that spurious gradients exploit to inflate OOD confidence, while leaving ID classification largely intact. To circumvent the expense of recomputing the logits after this gradient short-circuit, we further introduce a local first-order approximation that accurately captures the post-modification outputs without a second forward pass. Experiments on standard OOD benchmarks show our approach yields substantial improvements. Moreover, the method is lightweight and requires minimal changes to the standard inference pipeline, offering a practical path toward robust OOD detection in real-world applications.
>
---
#### [replaced 101] No time to train! Training-Free Reference-Based Instance Segmentation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.02798v2](http://arxiv.org/pdf/2507.02798v2)**

> **作者:** Miguel Espinosa; Chenhongyi Yang; Linus Ericsson; Steven McDonagh; Elliot J. Crowley
>
> **备注:** Preprint
>
> **摘要:** The performance of image segmentation models has historically been constrained by the high cost of collecting large-scale annotated data. The Segment Anything Model (SAM) alleviates this original problem through a promptable, semantics-agnostic, segmentation paradigm and yet still requires manual visual-prompts or complex domain-dependent prompt-generation rules to process a new image. Towards reducing this new burden, our work investigates the task of object segmentation when provided with, alternatively, only a small set of reference images. Our key insight is to leverage strong semantic priors, as learned by foundation models, to identify corresponding regions between a reference and a target image. We find that correspondences enable automatic generation of instance-level segmentation masks for downstream tasks and instantiate our ideas via a multi-stage, training-free method incorporating (1) memory bank construction; (2) representation aggregation and (3) semantic-aware feature matching. Our experiments show significant improvements on segmentation metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).
>
---
#### [replaced 102] K Nearest Neighbor-Guided Trajectory Similarity Learning
- **分类: cs.LG; cs.CV; cs.DB**

- **链接: [http://arxiv.org/pdf/2502.00285v2](http://arxiv.org/pdf/2502.00285v2)**

> **作者:** Yanchuan Chang; Xu Cai; Christian S. Jensen; Jianzhong Qi
>
> **摘要:** Trajectory similarity is fundamental to many spatio-temporal data mining applications. Recent studies propose deep learning models to approximate conventional trajectory similarity measures, exploiting their fast inference time once trained. Although efficient inference has been reported, challenges remain in similarity approximation accuracy due to difficulties in trajectory granularity modeling and in exploiting similarity signals in the training data. To fill this gap, we propose TSMini, a highly effective trajectory similarity model with a sub-view modeling mechanism capable of learning multi-granularity trajectory patterns and a k nearest neighbor-based loss that guides TSMini to learn not only absolute similarity values between trajectories but also their relative similarity ranks. Together, these two innovations enable highly accurate trajectory similarity approximation. Experiments show that TSMini can outperform the state-of-the-art models by 22% in accuracy on average when learning trajectory similarity measures.
>
---
#### [replaced 103] Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.16805v2](http://arxiv.org/pdf/2506.16805v2)**

> **作者:** Chao Chen; Nobel Dang; Juexiao Zhang; Wenkai Sun; Pengfei Zheng; Xuhang He; Yimeng Ye; Jiasheng Zhang; Taarun Srinivas; Chen Feng
>
> **摘要:** Humans exhibit a remarkable ability to recognize co-visibility-the 3D regions simultaneously visible in multiple images-even when these images are sparsely distributed across a complex scene. This capability is foundational in 3D vision, robotic perception, and relies not only on low-level feature matching but also on high-level spatial reasoning and cognitive integration. Yet, it remains unclear whether current vision models can replicate this human-level proficiency. In this work, we introduce the Co-VisiON benchmark, designed to evaluate human-inspired co-visibility reasoning across over 1,000 sparse-view indoor scenarios. Our results show that while co-visibility is often approached as a low-level feature-matching task, it remains challenging for existing vision models under sparse conditions. Notably, a proprietary vision-language model surpasses all vision-only baselines, but all models fall significantly short of human performance. This gap underscores the limitations of current architectures and motivates the need for models that integrate spatial and semantic information in a human-like manner. Inspired by human visual cognition, we propose a novel multi-view baseline, Covis, which achieves top performance among pure vision models and narrows the gap to the proprietary VLM. We hope our benchmark and findings will spur further advancements in developing vision models capable of robust, cognitively inspired reasoning in challenging, sparse environments. Our dataset and source code can be found at https://ai4ce.github.io/CoVISION.
>
---
#### [replaced 104] CODE-CL: Conceptor-Based Gradient Projection for Deep Continual Learning
- **分类: cs.LG; cs.AI; cs.CV; cs.NE**

- **链接: [http://arxiv.org/pdf/2411.15235v3](http://arxiv.org/pdf/2411.15235v3)**

> **作者:** Marco Paul E. Apolinario; Sakshi Choudhary; Kaushik Roy
>
> **备注:** Accepted to the IEEE/CVF International Conference on Computer Vision (ICCV) 2025, 12 pages, 5 figures
>
> **摘要:** Continual learning (CL) - the ability to progressively acquire and integrate new concepts - is essential to intelligent systems to adapt to dynamic environments. However, deep neural networks struggle with catastrophic forgetting (CF) when learning tasks sequentially, as training for new tasks often overwrites previously learned knowledge. To address this, recent approaches constrain updates to orthogonal subspaces using gradient projection, effectively preserving important gradient directions for previous tasks. While effective in reducing forgetting, these approaches inadvertently hinder forward knowledge transfer (FWT), particularly when tasks are highly correlated. In this work, we propose Conceptor-based gradient projection for Deep Continual Learning (CODE-CL), a novel method that leverages conceptor matrix representations, a form of regularized reconstruction, to adaptively handle highly correlated tasks. CODE-CL mitigates CF by projecting gradients onto pseudo-orthogonal subspaces of previous task feature spaces while simultaneously promoting FWT. It achieves this by learning a linear combination of shared basis directions, allowing efficient balance between stability and plasticity and transfer of knowledge between overlapping input feature representations. Extensive experiments on continual learning benchmarks validate CODE-CL's efficacy, demonstrating superior performance, reduced forgetting, and improved FWT as compared to state-of-the-art methods.
>
---
#### [replaced 105] RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth Estimation
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2506.15560v2](http://arxiv.org/pdf/2506.15560v2)**

> **作者:** Xingrui Qin; Wentao Zhao; Chuan Cao; Yihe Niu; Tianchen Deng; Houcheng Jiang; Rui Guo; Jingchuan Wang
>
> **备注:** 10 pages, 7 figures
>
> **摘要:** Dense depth estimation using millimeter-wave radar typically requires dense LiDAR supervision, generated via multi-frame projection and interpolation, for guiding the learning of accurate depth from sparse radar measurements and RGB images. However, this paradigm is both costly and data-intensive. To address this, we propose RaCalNet, a novel framework that eliminates the need for dense supervision by using sparse LiDAR to supervise the learning of refined radar measurements, resulting in a supervision density of merely around 1\% compared to dense-supervised methods. RaCalNet is composed of two key modules. The Radar Recalibration module performs radar point screening and pixel-wise displacement refinement, producing accurate and reliable depth priors from sparse radar inputs. These priors are then used by the Metric Depth Optimization module, which learns to infer scene-level scale priors and fuses them with monocular depth predictions to achieve metrically accurate outputs. This modular design enhances structural consistency and preserves fine-grained geometric details. Despite relying solely on sparse supervision, RaCalNet produces depth maps with clear object contours and fine-grained textures, demonstrating superior visual quality compared to state-of-the-art dense-supervised methods. Quantitatively, it achieves performance comparable to existing methods on the ZJU-4DRadarCam dataset and yields a 34.89\% RMSE reduction in real-world deployment scenarios. We plan to gradually release the code and models in the future at https://github.com/818slam/RaCalNet.git.
>
---
#### [replaced 106] Enhancing Neural Autoregressive Distribution Estimators for Image Reconstruction
- **分类: eess.IV; cs.CV; cs.LG; stat.AP**

- **链接: [http://arxiv.org/pdf/2506.05391v2](http://arxiv.org/pdf/2506.05391v2)**

> **作者:** Ambrose Emmett-Iwaniw; Nathan Kirk
>
> **备注:** Publication MCQMC 2024 Proceedings
>
> **摘要:** Autoregressive models are often employed to learn distributions of image data by decomposing the $D$-dimensional density function into a product of one-dimensional conditional distributions. Each conditional depends on preceding variables (pixels, in the case of image data), making the order in which variables are processed fundamental to the model performance. In this paper, we study the problem of observing a small subset of image pixels (referred to as a pixel patch) to predict the unobserved parts of the image. As our prediction mechanism, we propose a generalized version of the convolutional neural autoregressive distribution estimation (ConvNADE) model adapted for real-valued and color images. Moreover, we investigate the quality of image reconstruction when observing both random pixel patches and low-discrepancy pixel patches inspired by quasi-Monte Carlo theory. Experiments on benchmark datasets demonstrate that, where design permits, pixels sampled or stored to preserve uniform coverage improves reconstruction fidelity and test performance.
>
---
#### [replaced 107] Time Distributed Deep Learning Models for Purely Exogenous Forecasting: Application to Water Table Depth Prediction using Weather Image Time Series
- **分类: cs.LG; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2409.13284v2](http://arxiv.org/pdf/2409.13284v2)**

> **作者:** Matteo Salis; Abdourrahmane M. Atto; Stefano Ferraris; Rosa Meo
>
> **摘要:** Groundwater resources are one of the most relevant elements in the water cycle, therefore developing models to accurately predict them is a pivotal task in the sustainable resource management framework. Deep Learning (DL) models have been revealed to be very effective in hydrology, especially by feeding spatially distributed data (e.g. raster data). In many regions, hydrological measurements are difficult to obtain regularly or periodically in time, and in some cases, the last available data are not up to date. Reversely, weather data, which significantly impacts water resources, are usually more available and with higher quality. More specifically, we have proposed two different DL models to predict the water table depth in the Grana-Maira catchment (Piemonte, IT) using only exogenous weather image time series. To deal with the image time series, both models are made of a first Time Distributed Convolutional Neural Network (TDC) which encodes the image available at each time step into a vectorial representation. The first model, TDC-LSTM uses then a Sequential Module based on an LSTM layer to learn temporal relations and output the predictions. The second model, TDC-UnPWaveNet uses instead a new version of the WaveNet architecture, adapted here to output a sequence shorter and completely shifted in the future with respect to the input one. To this aim, and to deal with the different sequence lengths in the UnPWaveNet, we have designed a new Channel Distributed layer, that acts like a Time Distributed one but on the channel dimension, i.e. applying the same set of operations to each channel of the input. TDC-LSTM and TDC-UnPWaveNet have shown both remarkable results. However, the two models have focused on different learnable information: TDC-LSTM has focused more on lowering the bias, while TDC-UnPWaveNet has focused more on the temporal dynamics, maximizing correlation, and KGE.
>
---
#### [replaced 108] MORPH-LER: Log-Euclidean Regularization for Population-Aware Image Registration
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2502.02029v2](http://arxiv.org/pdf/2502.02029v2)**

> **作者:** Mokshagna Sai Teja Karanam; Krithika Iyer; Sarang Joshi; Shireen Elhabian
>
> **摘要:** Spatial transformations that capture population-level morphological statistics are critical for medical image analysis. Commonly used smoothness regularizers for image registration fail to integrate population statistics, leading to anatomically inconsistent transformations. Inverse consistency regularizers promote geometric consistency but lack population morphometrics integration. Regularizers that constrain deformation to low-dimensional manifold methods address this. However, they prioritize reconstruction over interpretability and neglect diffeomorphic properties, such as group composition and inverse consistency. We introduce MORPH-LER, a Log-Euclidean regularization framework for population-aware unsupervised image registration. MORPH-LER learns population morphometrics from spatial transformations to guide and regularize registration networks, ensuring anatomically plausible deformations. It features a bottleneck autoencoder that computes the principal logarithm of deformation fields via iterative square-root predictions. It creates a linearized latent space that respects diffeomorphic properties and enforces inverse consistency. By integrating a registration network with a diffeomorphic autoencoder, MORPH-LER produces smooth, meaningful deformation fields. The framework offers two main contributions: (1) a data-driven regularization strategy that incorporates population-level anatomical statistics to enhance transformation validity and (2) a linearized latent space that enables compact and interpretable deformation fields for efficient population morphometrics analysis. We validate MORPH-LER across two families of deep learning-based registration networks, demonstrating its ability to produce anatomically accurate, computationally efficient, and statistically meaningful transformations on the OASIS-1 brain imaging dataset. https://github.com/iyerkrithika21/MORPH_LER
>
---
#### [replaced 109] Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2503.07772v2](http://arxiv.org/pdf/2503.07772v2)**

> **作者:** Liwei Che; Tony Qingze Liu; Jing Jia; Weiyi Qin; Ruixiang Tang; Vladimir Pavlovic
>
> **备注:** Accepted to ICCV2025
>
> **摘要:** Despite their remarkable potential, Large Vision-Language Models (LVLMs) still face challenges with object hallucination, a problem where their generated outputs mistakenly incorporate objects that do not actually exist. Although most works focus on addressing this issue within the language-model backbone, our work shifts the focus to the image input source, investigating how specific image tokens contribute to hallucinations. Our analysis reveals a striking finding: a small subset of image tokens with high attention scores are the primary drivers of object hallucination. By removing these hallucinatory image tokens (only 1.5% of all image tokens), the issue can be effectively mitigated. This finding holds consistently across different models and datasets. Building on this insight, we introduce EAZY, a novel, training-free method that automatically identifies and Eliminates hAllucinations by Zeroing out hallucinatorY image tokens. We utilize EAZY for unsupervised object hallucination detection, achieving 15% improvement compared to previous methods. Additionally, EAZY demonstrates remarkable effectiveness in mitigating hallucinations while preserving model utility and seamlessly adapting to various LVLM architectures.
>
---
#### [replaced 110] When Does Pruning Benefit Vision Representations?
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.01722v2](http://arxiv.org/pdf/2507.01722v2)**

> **作者:** Enrico Cassano; Riccardo Renzulli; Andrea Bragagnolo; Marco Grangetto
>
> **摘要:** Pruning is widely used to reduce the complexity of deep learning models, but its effects on interpretability and representation learning remain poorly understood. This paper investigates how pruning influences vision models across three key dimensions: (i) interpretability, (ii) unsupervised object discovery, and (iii) alignment with human perception. We first analyze different vision network architectures to examine how varying sparsity levels affect feature attribution interpretability methods. Additionally, we explore whether pruning promotes more succinct and structured representations, potentially improving unsupervised object discovery by discarding redundant information while preserving essential features. Finally, we assess whether pruning enhances the alignment between model representations and human perception, investigating whether sparser models focus on more discriminative features similarly to humans. Our findings also reveal the presence of sweet spots, where sparse models exhibit higher interpretability, downstream generalization and human alignment. However, these spots highly depend on the network architectures and their size in terms of trainable parameters. Our results suggest a complex interplay between these three dimensions, highlighting the importance of investigating when and how pruning benefits vision representations.
>
---
#### [replaced 111] In-Context Meta LoRA Generation
- **分类: cs.CL; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2501.17635v3](http://arxiv.org/pdf/2501.17635v3)**

> **作者:** Yihua Shao; Minxi Yan; Yang Liu; Siyu Chen; Wenjie Chen; Xinwei Long; Ziyang Yan; Lei Li; Chenyu Zhang; Nicu Sebe; Hao Tang; Yan Wang; Hao Zhao; Mengzhu Wang; Jingcai Guo
>
> **备注:** Accepted by IJCAI 2025
>
> **摘要:** Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task specific fine-tuning. However, in scenarios that involve multiple tasks, training a separate LoRA model for each one results in considerable inefficiency in terms of storage and inference. Moreover, existing parameter generation methods fail to capture the correlations among these tasks, making multi-task LoRA parameter generation challenging. To address these limitations, we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently achieves task-specific customization of large language models (LLMs). Specifically, we use training data from all tasks to train a tailored generator, Conditional Variational Autoencoder (CVAE). CVAE takes task descriptions as inputs and produces task-aware LoRA weights as outputs. These LoRA weights are then merged with LLMs to create task-specialized models without the need for additional fine-tuning. Furthermore, we utilize in-context meta-learning for knowledge enhancement and task mapping, to capture the relationship between tasks and parameter distributions. As a result, our method achieves more accurate LoRA parameter generation for diverse tasks using CVAE. ICM-LoRA enables more accurate LoRA parameter reconstruction than current parameter reconstruction methods and is useful for implementing task-specific enhancements of LoRA parameters. At the same time, our method occupies 283MB, only 1\% storage compared with the original LoRA.
>
---
#### [replaced 112] TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2411.02570v2](http://arxiv.org/pdf/2411.02570v2)**

> **作者:** Leonardo Plini; Luca Scofano; Edoardo De Matteis; Guido Maria D'Amely di Melendugno; Alessandro Flaborea; Andrea Sanchietti; Giovanni Maria Farinella; Fabio Galasso; Antonino Furnari
>
> **摘要:** Identifying procedural errors online from egocentric videos is a critical yet challenging task across various domains, including manufacturing, healthcare, and skill-based training. The nature of such mistakes is inherently open-set, as unforeseen or novel errors may occur, necessitating robust detection systems that do not rely on prior examples of failure. Currently, however, no technique effectively detects open-set procedural mistakes online. We propose a dual branch architecture to address this problem in an online fashion: one branch continuously performs step recognition from the input egocentric video, while the other anticipates future steps based on the recognition module's output. Mistakes are detected as mismatches between the currently recognized action and the action predicted by the anticipation module. The recognition branch takes input frames, predicts the current action, and aggregates frame-level results into action tokens. The anticipation branch, specifically, leverages the solid pattern-matching capabilities of Large Language Models (LLMs) to predict action tokens based on previously predicted ones. Given the online nature of the task, we also thoroughly benchmark the difficulties associated with per-frame evaluations, particularly the need for accurate and timely predictions in dynamic online scenarios. Extensive experiments on two procedural datasets demonstrate the challenges and opportunities of leveraging a dual-branch architecture for mistake detection, showcasing the effectiveness of our proposed approach. In a thorough evaluation including recognition and anticipation variants and state-of-the-art models, our method reveals its robustness and effectiveness in online applications.
>
---
#### [replaced 113] 4D mmWave Radar for Sensing Enhancement in Adverse Environments: Advances and Challenges
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.24091v3](http://arxiv.org/pdf/2503.24091v3)**

> **作者:** Xiangyuan Peng; Miao Tang; Huawei Sun; Kay Bierzynski; Lorenzo Servadei; Robert Wille
>
> **备注:** 8 pages, accepted by ITSC2025
>
> **摘要:** Intelligent transportation systems require accurate and reliable sensing. However, adverse environments, such as rain, snow, and fog, can significantly degrade the performance of LiDAR and cameras. In contrast, 4D mmWave radar not only provides 3D point clouds and velocity measurements but also maintains robustness in challenging conditions. Recently, research on 4D mmWave radar under adverse environments has been growing, but a comprehensive review is still lacking. To bridge this gap, this work reviews the current research on 4D mmWave radar under adverse environments. First, we present an overview of existing 4D mmWave radar datasets encompassing diverse weather and lighting scenarios. Subsequently, we analyze existing learning-based methods leveraging 4D mmWave radar to enhance performance according to different adverse conditions. Finally, the challenges and potential future directions are discussed for advancing 4D mmWave radar applications in harsh environments. To the best of our knowledge, this is the first review specifically concentrating on 4D mmWave radar in adverse environments. The related studies are listed at: https://github.com/XiangyPeng/4D-mmWave-Radar-in-Adverse-Environments.
>
---
#### [replaced 114] Finetuning CLIP to Reason about Pairwise Differences
- **分类: cs.LG; cs.CV**

- **链接: [http://arxiv.org/pdf/2409.09721v2](http://arxiv.org/pdf/2409.09721v2)**

> **作者:** Dylan Sam; Devin Willmott; Joao D. Semedo; J. Zico Kolter
>
> **备注:** 30 pages
>
> **摘要:** Vision-language models (VLMs) such as CLIP are trained via contrastive learning between text and image pairs, resulting in aligned image and text embeddings that are useful for many downstream tasks. A notable drawback of CLIP, however, is that the resulting embedding space seems to lack some of the structure of its purely text-based alternatives. For instance, while text embeddings have long been noted to satisfy analogies in embedding space using vector arithmetic, CLIP has no such property. In this paper, we propose an approach to natively train CLIP in a contrastive manner to reason about differences in embedding space. We finetune CLIP so that text descriptions of differences between images correspond to their difference in image embedding space, using synthetically generated data with large language models on image-caption paired datasets. We first demonstrate that our approach yields significantly improved capabilities in ranking images by a certain attribute (e.g., elephants are larger than cats), which is useful in retrieval or constructing attribute-based classifiers, and improved zeroshot classification performance on many downstream image classification tasks. In addition, our approach enables a new mechanism for inference that we refer to as comparative prompting, where we leverage prior knowledge of text descriptions of differences between classes of interest, achieving even larger performance gains in classification. Finally, we illustrate that the resulting embeddings obey a larger degree of geometric properties in embedding space, such as in text-to-image generation.
>
---
#### [replaced 115] 3D SA-UNet: 3D Spatial Attention UNet with 3D Atrous Spatial Pyramid Pooling for White Matter Hyperintensities Segmentation
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2309.08402v4](http://arxiv.org/pdf/2309.08402v4)**

> **作者:** Changlu Guo
>
> **摘要:** White Matter Hyperintensity (WMH) is an imaging feature related to various diseases such as dementia and stroke. Accurately segmenting WMH using computer technology is crucial for early disease diagnosis. However, this task remains challenging due to the small lesions with low contrast and high discontinuity in the images, which contain limited contextual and spatial information. To address this challenge, we propose a deep learning model called 3D Spatial Attention U-Net (3D SA-UNet) for automatic WMH segmentation using only Fluid Attenuation Inversion Recovery (FLAIR) scans. The 3D SA-UNet introduces a 3D Spatial Attention Module that highlights important lesion features, such as WMH, while suppressing unimportant regions. Additionally, to capture features at different scales, we extend the Atrous Spatial Pyramid Pooling (ASPP) module to a 3D version, enhancing the segmentation performance of the network. We evaluate our method on publicly available dataset and demonstrate the effectiveness of 3D spatial attention module and 3D ASPP in WMH segmentation. Through experimental results, it has been demonstrated that our proposed 3D SA-UNet model achieves higher accuracy compared to other state-of-the-art 3D convolutional neural networks.
>
---
#### [replaced 116] Boundary Exploration of Next Best View Policy in 3D Robotic Scanning
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2412.10444v2](http://arxiv.org/pdf/2412.10444v2)**

> **作者:** Leihui Li; Lixuepiao Wan; Xuping Zhang
>
> **摘要:** The Next Best View (NBV) problem is a pivotal challenge in 3D robotic scanning, with the potential to significantly improve the efficiency of object capture and reconstruction. Existing methods for determining the NBV often overlook view overlap, assume a fixed virtual origin for the camera, and rely on voxel-based representations of 3D data. To address these limitations and enhance the practicality of scanning unknown objects, we propose an NBV policy in which the next view explores the boundary of the scanned point cloud, with overlap intrinsically considered. The scanning or working distance of the camera is user-defined and remains flexible throughout the process. To this end, we first introduce a model-based approach in which candidate views are iteratively proposed based on a reference model. Scores are computed using a carefully designed strategy that accounts for both view overlap and convergence. In addition, we propose a learning-based method, the Boundary Exploration NBV Network (BENBV-Net), which predicts the NBV directly from the scanned data without requiring a reference model. BENBV-Net estimates scores for candidate boundaries, selecting the one with the highest score as the target for the next best view. It offers a significant improvement in NBV generation speed while maintaining the performance level of the model-based approach. We evaluate both methods on the ShapeNet, ModelNet, and 3D Repository datasets. Experimental results demonstrate that our approach outperforms existing methods in terms of scanning efficiency, final coverage, and overlap stability, all of which are critical for practical 3D scanning applications. The related code is available at github.com/leihui6/BENBV.
>
---
#### [replaced 117] Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.05599v2](http://arxiv.org/pdf/2505.05599v2)**

> **作者:** Seraj Al Mahmud Mostafa; Chenxi Wang; Jia Yue; Yuta Hozumi; Jianwu Wang
>
> **备注:** This paper has been accepted to International conference on Advanced Machine Learning and Data Science (AMLDS) 2025
>
> **摘要:** Object localization in satellite imagery is particularly challenging due to the high variability of objects, low spatial resolution, and interference from noise and dominant features such as clouds and city lights. In this research, we focus on three satellite datasets: upper atmospheric Gravity Waves (GW), mesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique challenges. These challenges include the variability in the scale and appearance of the main object patterns, where the size, shape, and feature extent of objects of interest can differ significantly. To address these challenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed to improve object localization in these complex scenarios. YOLO-DCAP incorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture multi-scale features at scale with varying dilation rates, and an Attention-aided Spatial Pooling (AaSP) module to focus on the global relevant spatial regions, enhancing feature selection. These structural improvements help to better localize objects in satellite imagery. Experimental results demonstrate that YOLO-DCAP significantly outperforms both the YOLO base model and state-of-the-art approaches, achieving an average improvement of 20.95% in mAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively over state-of-the-art alternatives, consistently across all three satellite datasets. These consistent gains across all three satellite datasets highlight the robustness and generalizability of the proposed approach. Our code is open sourced at https://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.
>
---
#### [replaced 118] A Novel Automatic Real-time Motion Tracking Method in MRI-guided Radiotherapy Using Enhanced Tracking-Learning-Detection Framework with Automatic Segmentation
- **分类: eess.IV; cs.CV; cs.LG; physics.med-ph; q-bio.TO**

- **链接: [http://arxiv.org/pdf/2411.07503v3](http://arxiv.org/pdf/2411.07503v3)**

> **作者:** Shengqi Chen; Zilin Wang; Jianrong Dai; Shirui Qin; Ying Cao; Ruiao Zhao; Jiayun Chen; Guohua Wu; Yuan Tang
>
> **摘要:** Background and Purpose: Accurate motion tracking in MRI-guided Radiotherapy (MRIgRT) is essential for effective treatment delivery. This study aimed to enhance motion tracking precision in MRIgRT through an automatic real-time markerless tracking method using an enhanced Tracking-Learning-Detection (ETLD) framework with automatic segmentation. Materials and Methods: We developed a novel MRIgRT motion tracking and segmentation method by integrating the ETLD framework with an improved Chan-Vese model (ICV), named ETLD+ICV. The ETLD framework was upgraded for real-time cine MRI, including advanced image preprocessing, no-reference image quality assessment, an enhanced median-flow tracker, and a refined detector with dynamic search region adjustments. ICV was used for precise target volume coverage, refining the segmented region frame by frame using tracking results, with key parameters optimized. The method was tested on 3.5D MRI scans from 10 patients with liver metastases. Results: Evaluation of 106,000 frames across 77 treatment fractions showed sub-millimeter tracking errors of less than 0.8mm, with over 99% precision and 98% recall for all subjects in the Beam Eye View(BEV)/Beam Path View(BPV) orientation. The ETLD+ICV method achieved a dice global score of more than 82% for all subjects, demonstrating the method's extensibility and precise target volume coverage. Conclusion: This study successfully developed an automatic real-time markerless motion tracking method for MRIgRT that significantly outperforms current methods. The novel method not only delivers exceptional precision in tracking and segmentation but also shows enhanced adaptability to clinical demands, making it an indispensable asset in improving the efficacy of radiotherapy treatments.
>
---
#### [replaced 119] UltraBoneUDF: Self-supervised Bone Surface Reconstruction from Ultrasound Based on Neural Unsigned Distance Functions
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.17912v2](http://arxiv.org/pdf/2505.17912v2)**

> **作者:** Luohong Wu; Matthias Seibold; Nicola A. Cavalcanti; Giuseppe Loggia; Lisa Reissner; Bastian Sigrist; Jonas Hein; Lilian Calvet; Arnd Viehöfer; Philipp Fürnstahl
>
> **摘要:** Background: Bone surface reconstruction plays a critical role in computer-assisted orthopedic surgery. Compared to traditional imaging modalities such as CT and MRI, ultrasound offers a radiation-free, cost-effective, and portable alternative. Continuous bone surface reconstruction can be employed for many clinical applications. However, due to the inherent limitations of ultrasound imaging, B-mode ultrasound typically capture only partial bone surfaces. Existing reconstruction methods struggle with such incomplete data, leading to artifacts and increased reconstruction errors. Effective techniques for accurately reconstructing thin and open bone surfaces from real-world 3D ultrasound volumes remain lacking. Methods: We propose UltraBoneUDF, a self-supervised framework designed for reconstructing open bone surfaces from ultrasound using neural Unsigned Distance Functions. To enhance reconstruction quality, we introduce a novel global feature extractor that effectively fuses ultrasound-specific image characteristics. Additionally, we present a novel loss function based on local tangent plane optimization that substantially improves surface reconstruction quality. UltraBoneUDF and baseline models are extensively evaluated on four open-source datasets. Results: Qualitative results highlight the limitations of the state-of-the-art methods for open bone surface reconstruction and demonstrate the effectiveness of UltraBoneUDF. Quantitatively, UltraBoneUDF significantly outperforms competing methods across all evaluated datasets for both open and closed bone surface reconstruction in terms of mean Chamfer distance error: 1.10 mm on the UltraBones100k dataset (39.6\% improvement compared to the SOTA), 0.23 mm on the OpenBoneCT dataset (69.3\% improvement), 0.18 mm on the ClosedBoneCT dataset (70.2\% improvement), and 0.05 mm on the Prostate dataset (55.3\% improvement).
>
---
#### [replaced 120] mmEgoHand: Egocentric Hand Pose Estimation and Gesture Recognition with Head-mounted Millimeter-wave Radar and IMU
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2501.13805v2](http://arxiv.org/pdf/2501.13805v2)**

> **作者:** Yizhe Lv; Tingting Zhang; Zhijian Wang; Yunpeng Song; Han Ding; Jinsong Han; Fei Wang
>
> **备注:** 11 pages, Under Review
>
> **摘要:** Recent advancements in millimeter-wave (mmWave) radar have demonstrated its potential for human action recognition and pose estimation, offering privacy-preserving advantages over conventional cameras while maintaining occlusion robustness, with promising applications in human-computer interaction and wellness care. However, existing mmWave systems typically employ fixed-position configurations, restricting user mobility to predefined zones and limiting practical deployment scenarios. We introduce mmEgoHand, a head-mounted egocentric system for hand pose estimation to support applications such as gesture recognition, VR interaction, skill digitization and assessment, and robotic teleoperation. mmEgoHand synergistically integrates mmWave radar with inertial measurement units (IMUs) to enable dynamic perception. The IMUs actively compensate for radar interference induced by head movements, while our novel end-to-end Transformer architecture simultaneously estimates 3D hand keypoint coordinates through multi-modal sensor fusion. This dual-modality framework achieves spatial-temporal alignment of mmWave heatmaps with IMUs, overcoming viewpoint instability inherent in egocentric sensing scenarios. We further demonstrate that intermediate hand pose representations substantially improve performance in downstream task, e.g., VR gesture recognition. Extensive evaluations with 10 subjects performing 8 gestures across 3 distinct postures -- standing, sitting, lying -- achieve 90.8% recognition accuracy, outperforming state-of-the-art solutions by a large margin. Dataset and code are available at https://github.com/WhisperYi/mmVR.
>
---
#### [replaced 121] FedAli: Personalized Federated Learning Alignment with Prototype Layers for Generalized Mobile Services
- **分类: cs.LG; cs.CR; cs.CV**

- **链接: [http://arxiv.org/pdf/2411.10595v2](http://arxiv.org/pdf/2411.10595v2)**

> **作者:** Sannara Ek; Kaile Wang; François Portet; Philippe Lalanda; Jiannong Cao
>
> **摘要:** Personalized Federated Learning (PFL) enables distributed training on edge devices, allowing models to collaboratively learn global patterns while tailoring their parameters to better fit each client's local data, all while preserving data privacy. However, PFL faces two key challenges in mobile systems: client drift, where heterogeneous data cause model divergence, and the overlooked need for client generalization, as the dynamic of mobile sensing demands adaptation beyond local environments. To overcome these limitations, we introduce Federated Alignment (FedAli), a prototype-based regularization technique that enhances inter-client alignment while strengthening the robustness of personalized adaptations. At its core, FedAli introduces the ALignment with Prototypes (ALP) layer, inspired by human memory, to enhance generalization by guiding inference embeddings toward personalized prototypes while reducing client drift through alignment with shared prototypes during training. By leveraging an optimal transport plan to compute prototype-embedding assignments, our approach allows pre-training the prototypes without any class labels to further accelerate convergence and improve performance. Our extensive experiments show that FedAli significantly enhances client generalization while preserving strong personalization in heterogeneous settings.
>
---
#### [replaced 122] EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event Slicing
- **分类: cs.CV; cs.HC**

- **链接: [http://arxiv.org/pdf/2409.18813v2](http://arxiv.org/pdf/2409.18813v2)**

> **作者:** Argha Sen; Nuwan Bandara; Ila Gokarn; Thivya Kandappu; Archan Misra
>
> **备注:** 32 pages,15 figures,
>
> **摘要:** Eye-tracking technology has gained significant attention in recent years due to its wide range of applications in human-computer interaction, virtual and augmented reality, and wearable health. Traditional RGB camera-based eye-tracking systems often struggle with poor temporal resolution and computational constraints, limiting their effectiveness in capturing rapid eye movements. To address these limitations, we propose EyeTrAES, a novel approach using neuromorphic event cameras for high-fidelity tracking of natural pupillary movement that shows significant kinematic variance. One of EyeTrAES's highlights is the use of a novel adaptive windowing/slicing algorithm that ensures just the right amount of descriptive asynchronous event data accumulation within an event frame, across a wide range of eye movement patterns. EyeTrAES then applies lightweight image processing functions over accumulated event frames from just a single eye to perform pupil segmentation and tracking. We show that these methods boost pupil tracking fidelity by 6+%, achieving IoU~=92%, while incurring at least 3x lower latency than competing pure event-based eye tracking alternatives [38]. We additionally demonstrate that the microscopic pupillary motion captured by EyeTrAES exhibits distinctive variations across individuals and can thus serve as a biometric fingerprint. For robust user authentication, we train a lightweight per-user Random Forest classifier using a novel feature vector of short-term pupillary kinematics, comprising a sliding window of pupil (location, velocity, acceleration) triples. Experimental studies with two different datasets demonstrate that the EyeTrAES-based authentication technique can simultaneously achieve high authentication accuracy (~=0.82) and low processing latency (~=12ms), and significantly outperform multiple state-of-the-art competitive baselines.
>
---
#### [replaced 123] Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.09506v2](http://arxiv.org/pdf/2504.09506v2)**

> **作者:** Yanze Jiang; Yanfeng Gu; Xian Li
>
> **摘要:** Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial and spectral information of ground objects, offering excellent 3D perception and target recognition capabilities. Current approaches for generating HPCs often involve fusion techniques with hyperspectral images and LiDAR point clouds, which inevitably lead to geometric-spectral distortions due to fusion errors and obstacle occlusions. These adverse effects limit their performance in downstream fine-grained tasks across multiple scenarios, particularly in airborne applications. To address these issues, we propose PiV-AHPC, a 3D object detection network for airborne HPCs. To the best of our knowledge, this is the first attempt at this HPCs task. Specifically, we first develop a pillar-voxel dual-branch encoder, where the former captures spectral and vertical structural features from HPCs to overcome spectral distortion, while the latter emphasizes extracting accurate 3D spatial features from point clouds. A multi-level feature fusion mechanism is devised to enhance information interaction between the two branches, achieving neighborhood feature alignment and channel-adaptive selection, thereby organically integrating heterogeneous features and mitigating geometric distortion. Extensive experiments on two airborne HPCs datasets demonstrate that PiV-AHPC possesses state-of-the-art detection performance and high generalization capability.
>
---
#### [replaced 124] Free-Mask: A Novel Paradigm of Integration Between the Segmentation Diffusion Model and Image Editing
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2411.01819v4](http://arxiv.org/pdf/2411.01819v4)**

> **作者:** Bo Gao; Jianhui Wang; Xinyuan Song; Yangfan He; Fangxu Xing; Tianyu Shi
>
> **备注:** Accepted by ACM MM(2025)
>
> **摘要:** Current semantic segmentation models typically require a substantial amount of manually annotated data, a process that is both time-consuming and resource-intensive. Alternatively, leveraging advanced text-to-image models such as Midjourney and Stable Diffusion has emerged as an efficient strategy, enabling the automatic generation of synthetic data in place of manual annotations. However, previous methods have been limited to generating single-instance images, as the generation of multiple instances with Stable Diffusion has proven unstable. To address this limitation and expand the scope and diversity of synthetic datasets, we propose a framework \textbf{Free-Mask} that combines a Diffusion Model for segmentation with advanced image editing capabilities, allowing for the integration of multiple objects into images via text-to-image models. Our method facilitates the creation of highly realistic datasets that closely emulate open-world environments while generating accurate segmentation masks. It reduces the labor associated with manual annotation and also ensures precise mask generation. Experimental results demonstrate that synthetic data generated by \textbf{Free-Mask} enables segmentation models to outperform those trained on real data, especially in zero-shot settings. Notably, \textbf{Free-Mask} achieves new state-of-the-art results on previously unseen classes in the VOC 2012 benchmark.
>
---
#### [replaced 125] HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN
- **分类: eess.IV; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2008.09646v3](http://arxiv.org/pdf/2008.09646v3)**

> **作者:** Abhinav Sagar
>
> **摘要:** High-resolution video generation has emerged as a crucial task in computer vision, with wide-ranging applications in entertainment, simulation, and data augmentation. However, generating temporally coherent and visually realistic videos remains a significant challenge due to the high dimensionality and complex dynamics of video data. In this paper, we propose a novel deep generative network architecture designed specifically for high-resolution video synthesis. Our approach integrates key concepts from Wasserstein Generative Adversarial Networks (WGANs), enforcing a k-Lipschitz continuity constraint on the discriminator to stabilize training and enhance convergence. We further leverage Conditional GAN (cGAN) techniques by incorporating class labels during both training and inference, enabling class-specific video generation with improved semantic consistency. We provide a detailed layer-wise description of the Generator and Discriminator networks, highlighting architectural design choices promoting temporal coherence and spatial detail. The overall combined architecture, training algorithm, and optimization strategy are thoroughly presented. Our training objective combines a pixel-wise mean squared error loss with an adversarial loss to balance frame-level accuracy and video realism. We validate our approach on benchmark datasets including UCF101, Golf, and Aeroplane, encompassing diverse motion patterns and scene contexts. Quantitative evaluations using Inception Score (IS) and Fr\'echet Inception Distance (FID) demonstrate that our model significantly outperforms previous state-of-the-art unsupervised video generation methods in terms of both quality and diversity.
>
---
#### [replaced 126] MORDA: A Synthetic Dataset to Facilitate Adaptation of Object Detectors to Unseen Real-target Domain While Preserving Performance on Real-source Domain
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2501.04950v2](http://arxiv.org/pdf/2501.04950v2)**

> **作者:** Hojun Lim; Heecheol Yoo; Jinwoo Lee; Seungmin Jeon; Hyeongseok Jeon
>
> **备注:** 7 pages, 6 figures, 4 tables, This work has been accepted for publication in IEEE ICRA 2025. The final published version will be available via IEEE Xplore
>
> **摘要:** Deep neural network (DNN) based perception models are indispensable in the development of autonomous vehicles (AVs). However, their reliance on large-scale, high-quality data is broadly recognized as a burdensome necessity due to the substantial cost of data acquisition and labeling. Further, the issue is not a one-time concern, as AVs might need a new dataset if they are to be deployed to another region (real-target domain) that the in-hand dataset within the real-source domain cannot incorporate. To mitigate this burden, we propose leveraging synthetic environments as an auxiliary domain where the characteristics of real domains are reproduced. This approach could enable indirect experience about the real-target domain in a time- and cost-effective manner. As a practical demonstration of our methodology, nuScenes and South Korea are employed to represent real-source and real-target domains, respectively. That means we construct digital twins for several regions of South Korea, and the data-acquisition framework of nuScenes is reproduced. Blending the aforementioned components within a simulator allows us to obtain a synthetic-fusion domain in which we forge our novel driving dataset, MORDA: Mixture Of Real-domain characteristics for synthetic-data-assisted Domain Adaptation. To verify the value of synthetic features that MORDA provides in learning about driving environments of South Korea, 2D/3D detectors are trained solely on a combination of nuScenes and MORDA. Afterward, their performance is evaluated on the unforeseen real-world dataset (AI-Hub) collected in South Korea. Our experiments present that MORDA can significantly improve mean Average Precision (mAP) on AI-Hub dataset while that on nuScenes is retained or slightly enhanced.
>
---
#### [replaced 127] KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality
- **分类: cs.AI; cs.CL; cs.CV; cs.LG; cs.MA**

- **链接: [http://arxiv.org/pdf/2506.19807v2](http://arxiv.org/pdf/2506.19807v2)**

> **作者:** Baochang Ren; Shuofei Qiao; Wenhao Yu; Huajun Chen; Ningyu Zhang
>
> **备注:** Work in progress
>
> **摘要:** Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.
>
---
#### [replaced 128] Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.00392v2](http://arxiv.org/pdf/2507.00392v2)**

> **作者:** Yingping Liang; Yutao Hu; Wenqi Shao; Ying Fu
>
> **备注:** Official Code: https://github.com/Sharpiless/L2M
>
> **摘要:** Feature matching plays a fundamental role in many computer vision tasks, yet existing methods heavily rely on scarce and clean multi-view image collections, which constrains their generalization to diverse and challenging scenarios. Moreover, conventional feature encoders are typically trained on single-view 2D images, limiting their capacity to capture 3D-aware correspondences. In this paper, we propose a novel two-stage framework that lifts 2D images to 3D space, named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and diverse single-view images. To be specific, in the first stage, we learn a 3D-aware feature encoder using a combination of multi-view image synthesis and 3D feature Gaussian representation, which injects 3D geometry knowledge into the encoder. In the second stage, a novel-view rendering strategy, combined with large-scale synthetic data generation from single-view images, is employed to learn a feature decoder for robust feature matching, thus achieving generalization across diverse domains. Extensive experiments demonstrate that our method achieves superior generalization across zero-shot evaluation benchmarks, highlighting the effectiveness of the proposed framework for robust feature matching.
>
---
#### [replaced 129] BS-LDM: Effective Bone Suppression in High-Resolution Chest X-Ray Images with Conditional Latent Diffusion Models
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2412.15670v5](http://arxiv.org/pdf/2412.15670v5)**

> **作者:** Yifei Sun; Zhanghao Chen; Hao Zheng; Wenming Deng; Jin Liu; Wenwen Min; Ahmed Elazab; Xiang Wan; Changmiao Wang; Ruiquan Ge
>
> **备注:** 12 pages, 8 figures, accepted by IEEE Journal of Biomedical and Health Informatics (JBHI) on July 4, 2025
>
> **摘要:** Lung diseases represent a significant global health challenge, with Chest X-Ray (CXR) being a key diagnostic tool due to its accessibility and affordability. Nonetheless, the detection of pulmonary lesions is often hindered by overlapping bone structures in CXR images, leading to potential misdiagnoses. To address this issue, we develop an end-to-end framework called BS-LDM, designed to effectively suppress bone in high-resolution CXR images. This framework is based on conditional latent diffusion models and incorporates a multi-level hybrid loss-constrained vector-quantized generative adversarial network which is crafted for perceptual compression, ensuring the preservation of details. To further enhance the framework's performance, we utilize offset noise in the forward process, and a temporal adaptive thresholding strategy in the reverse process. These additions help minimize discrepancies in generating low-frequency information of soft tissue images. Additionally, we have compiled a high-quality bone suppression dataset named SZCH-X-Rays. This dataset includes 818 pairs of high-resolution CXR and soft tissue images collected from our partner hospital. Moreover, we processed 241 data pairs from the JSRT dataset into negative images, which are more commonly used in clinical practice. Our comprehensive experiments and downstream evaluations reveal that BS-LDM excels in bone suppression, underscoring its clinical value. Our code is available at https://github.com/diaoquesang/BS-LDM.
>
---
#### [replaced 130] Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering
- **分类: cs.LG; cs.CV**

- **链接: [http://arxiv.org/pdf/2409.02426v4](http://arxiv.org/pdf/2409.02426v4)**

> **作者:** Peng Wang; Huijie Zhang; Zekai Zhang; Siyi Chen; Yi Ma; Qing Qu
>
> **备注:** 43 pages, 8 figures, 2 tables
>
> **摘要:** Recent empirical studies have demonstrated that diffusion models can effectively learn the image distribution and generate new samples. Remarkably, these models can achieve this even with a small number of training samples despite a large image dimension, circumventing the curse of dimensionality. In this work, we provide theoretical insights into this phenomenon by leveraging key empirical observations: (i) the low intrinsic dimensionality of image data, (ii) a union of manifold structure of image data, and (iii) the low-rank property of the denoising autoencoder in trained diffusion models. These observations motivate us to assume the underlying data distribution of image data as a mixture of low-rank Gaussians and to parameterize the denoising autoencoder as a low-rank model according to the score function of the assumed distribution. With these setups, we rigorously show that optimizing the training loss of diffusion models is equivalent to solving the canonical subspace clustering problem over the training samples. Based on this equivalence, we further show that the minimal number of samples required to learn the underlying distribution scales linearly with the intrinsic dimensions under the above data and model assumptions. This insight sheds light on why diffusion models can break the curse of dimensionality and exhibit the phase transition in learning distributions. Moreover, we empirically establish a correspondence between the subspaces and the semantic representations of image data, facilitating image editing. We validate these results with corroborated experimental results on both simulated distributions and image datasets.
>
---
#### [replaced 131] PEVLM: Parallel Encoding for Vision-Language Models
- **分类: cs.CV; cs.LG; cs.PF**

- **链接: [http://arxiv.org/pdf/2506.19651v2](http://arxiv.org/pdf/2506.19651v2)**

> **作者:** Letian Kang; Shixian Luo; Yiqiang Li; Xiaoyang Yu; Shenxuan Zhou; Yong Wu
>
> **摘要:** Vision-Language Models (VLMs) have demonstrated strong capabilities in multimodal understanding and generation tasks. However, their application to long video understanding remains hindered by the quadratic complexity of standard attention mechanisms. In this work, we introduce \textbf{PEVLM}, a fine-tuning-free parallel encoding method designed to enhance the prefilling efficiency of VLMs in long video scenarios. PEVLM partitions the input video into context blocks with a shared sink block, while preserving sequential position embeddings to align the attention weight distribution with that of Full-Attention. This design reduces attention complexity from $O((T \times N)^2)$ to $O(T \times N)$ where $T$ is the number of frames and $N$ the number of tokens per frame, without sacrificing accuracy. Extensive experiments across multiple state-of-the-art models and benchmarks demonstrate that PEVLM consistently outperforms existing parallel encoding approaches, achieving up to \textbf{7.47x} speedup in attention computation and reducing end-to-end latency by \textbf{40\%}. Remarkably, PEVLM not only maintains high accuracy, but in some settings even surpasses Full-Attention performance. Under strict latency constraints, it achieves substantial gains, improving accuracy from \textbf{23.26\%} to \textbf{61.03\%}. These results underscore the effectiveness of PEVLM for low-latency, long-context video understanding, making it a promising solution for real-world applications.
>
---
#### [replaced 132] Elevator, Escalator, or Neither? Classifying Conveyor State Using Smartphone under Arbitrary Pedestrian Behavior
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2405.03218v3](http://arxiv.org/pdf/2405.03218v3)**

> **作者:** Tianlang He; Zhiqiu Xia; S. -H. Gary Chan
>
> **备注:** Accepted by IEEE Transactions on Mobile Computing
>
> **摘要:** Knowing a pedestrian's conveyor state of ''elevator,'' ''escalator,'' or ''neither'' is fundamental to many applications such as indoor navigation and people flow management. Previous studies on classifying the conveyor state often rely on specially designed body-worn sensors or make strong assumptions on pedestrian behaviors, which greatly strangles their deployability. To overcome this, we study the classification problem under arbitrary pedestrian behaviors using the inertial navigation system (INS) of the commonly available smartphones (including accelerometer, gyroscope, and magnetometer). This problem is challenging, because the INS signals of the conveyor states are entangled by the arbitrary and diverse pedestrian behaviors. We propose ELESON, a novel and lightweight deep-learning approach that uses phone INS to classify a pedestrian to elevator, escalator, or neither. Using causal decomposition and adversarial learning, ELESON extracts the motion and magnetic features of conveyor state independent of pedestrian behavior, based on which it estimates the state confidence by means of an evidential classifier. We curate a large and diverse dataset with 36,420 instances of pedestrians randomly taking elevators and escalators under arbitrary unknown behaviors. Our extensive experiments show that ELESON is robust against pedestrian behavior, achieving a high accuracy of over 0.9 in F1 score, strong confidence discriminability of 0.81 in AUROC (Area Under the Receiver Operating Characteristics), and low computational and memory requirements fit for common smartphone deployment.
>
---
#### [replaced 133] YOLOv11 Optimization for Efficient Resource Utilization
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.14790v3](http://arxiv.org/pdf/2412.14790v3)**

> **作者:** Areeg Fahad Rasheed; M. Zarkoosh
>
> **备注:** 12 pages, 13 figures, 4 tables
>
> **摘要:** The objective of this research is to optimize the eleventh iteration of You Only Look Once (YOLOv11) by developing size-specific modified versions of the architecture. These modifications involve pruning unnecessary layers and reconfiguring the main architecture of YOLOv11. Each proposed version is tailored to detect objects of specific size ranges, from small to large. To ensure proper model selection based on dataset characteristics, we introduced an object classifier program. This program identifies the most suitable modified version for a given dataset. The proposed models were evaluated on various datasets and compared with the original YOLOv11 and YOLOv8 models. The experimental results highlight significant improvements in computational resource efficiency, with the proposed models maintaining the accuracy of the original YOLOv11. In some cases, the modified versions outperformed the original model regarding detection performance. Furthermore, the proposed models demonstrated reduced model sizes and faster inference times. Models weights and the object size classifier can be found in this repository
>
---
#### [replaced 134] PIP-Net: Pedestrian Intention Prediction in the Wild
- **分类: cs.CV; cs.AI; cs.NE; eess.IV; stat.ML**

- **链接: [http://arxiv.org/pdf/2402.12810v3](http://arxiv.org/pdf/2402.12810v3)**

> **作者:** Mohsen Azarmi; Mahdi Rezaei; He Wang
>
> **备注:** Author Accepted Version in IEEE Transactions on Intelligent Transportation Systems
>
> **摘要:** Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to an enhancement in the model's contextual perception. Depending on the traffic scenario and road environment, the model excels in predicting pedestrian crossing intentions up to 4 seconds in advance, which is a breakthrough in current research studies in pedestrian intention prediction. Finally, for the first time, we present the Urban-PIP dataset, a customised pedestrian intention prediction dataset, with multi-camera annotations in real-world automated driving scenarios.
>
---
#### [replaced 135] Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2404.01988v4](http://arxiv.org/pdf/2404.01988v4)**

> **作者:** Jicheng Yuan; Anh Le-Tuan; Manfred Hauswirth; Danh Le-Phuoc
>
> **备注:** Code is available at https://github.com/jichengyuan/Cooperitive_Students
>
> **摘要:** Unsupervised Domain Adaptation (UDA) has shown significant advancements in object detection under well-lit conditions; however, its performance degrades notably in low-visibility scenarios, especially at night, posing challenges not only for its adaptability in low signal-to-noise ratio (SNR) conditions but also for the reliability and efficiency of automated vehicles. To address this problem, we propose a \textbf{Co}operative \textbf{S}tudents (\textbf{CoS}) framework that innovatively employs global-local transformations (GLT) and a proxy-based target consistency (PTC) mechanism to capture the spatial consistency in day- and night-time scenarios effectively, and thus bridge the significant domain shift across contexts. Building upon this, we further devise an adaptive IoU-informed thresholding (AIT) module to gradually avoid overlooking potential true positives and enrich the latent information in the target domain. Comprehensive experiments show that CoS essentially enhanced UDA performance in low-visibility conditions and surpasses current state-of-the-art techniques, achieving an increase in mAP of 3.0\%, 1.9\%, and 2.5\% on BDD100K, SHIFT, and ACDC datasets, respectively. Code is available at https://github.com/jichengyuan/Cooperitive_Students.
>
---
#### [replaced 136] ShareCMP: Polarization-Aware RGB-P Semantic Segmentation
- **分类: cs.CV; I.4**

- **链接: [http://arxiv.org/pdf/2312.03430v3](http://arxiv.org/pdf/2312.03430v3)**

> **作者:** Zhuoyan Liu; Bo Wang; Lizhi Wang; Chenyu Mao; Ye Li
>
> **备注:** 17 pages, 8 figures, 12 tables, accepted by IEEE TCSVT
>
> **摘要:** Multimodal semantic segmentation is developing rapidly, but the modality of RGB-\textbf{P}olarization remains underexplored. To delve into this problem, we construct a UPLight RGB-P segmentation benchmark with 12 typical underwater semantic classes. In this work, we design the ShareCMP, an RGB-P semantic segmentation framework with a shared dual-branch architecture (ShareCMP Encoder), which reduces the parameters and memory space by about 33.8\% compared to previous dual-branch models. It encompasses a Polarization Generate Attention (PGA) module designed to generate polarization modal images with richer polarization properties for the encoder. In addition, we introduce the Class Polarization-Aware Loss (CPALoss) with Class Polarization-Aware Auxiliary Head (CPAAHead) to improve the learning and understanding of the encoder for polarization modal information and to optimize the PGA module. With extensive experiments on a total of three RGB-P benchmarks, our ShareCMP achieves the best performance in mIoU with fewer parameters on the UPLight (92.45{\small (+0.32)}\%), ZJU (92.7{\small (+0.1)}\%), and MCubeS (50.99{\small (+1.51)}\%) datasets. And our ShareCMP (w/o PGA) achieves competitive or even higher performance on other RGB-X datasets compared to the corresponding state-of-the-art RGB-X methods. The code and datasets are available at https://github.com/LEFTeyex/ShareCMP.
>
---
#### [replaced 137] Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.14706v2](http://arxiv.org/pdf/2506.14706v2)**

> **作者:** Ni Ou; Zhuo Chen; Xinru Zhang; Junzheng Wang
>
> **备注:** 7 pages, 4 figures, accepted by IROS 2025. arXiv admin note: substantial text overlap with arXiv:2411.10936
>
> **摘要:** Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.
>
---
#### [replaced 138] StereoINR: Cross-View Geometry Consistent Stereo Super Resolution with Implicit Neural Representation
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.05509v2](http://arxiv.org/pdf/2505.05509v2)**

> **作者:** Yi Liu; Xinyi Liu; Yi Wan; Panwang Xia; Qiong Wu; Yongjun Zhang
>
> **摘要:** Stereo image super-resolution (SSR) aims to enhance high-resolution details by leveraging information from stereo image pairs. However, existing stereo super-resolution (SSR) upsampling methods (e.g., pixel shuffle) often overlook cross-view geometric consistency and are limited to fixed-scale upsampling. The key issue is that previous upsampling methods use convolution to independently process deep features of different views, lacking cross-view and non-local information perception, making it difficult to select beneficial information from multi-view scenes adaptively. In this work, we propose Stereo Implicit Neural Representation (StereoINR), which innovatively models stereo image pairs as continuous implicit representations. This continuous representation breaks through the scale limitations, providing a unified solution for arbitrary-scale stereo super-resolution reconstruction of left-right views. Furthermore, by incorporating spatial warping and cross-attention mechanisms, StereoINR enables effective cross-view information fusion and achieves significant improvements in pixel-level geometric consistency. Extensive experiments across multiple datasets show that StereoINR outperforms out-of-training-distribution scale upsampling and matches state-of-the-art SSR methods within training-distribution scales.
>
---
#### [replaced 139] Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.02205v2](http://arxiv.org/pdf/2507.02205v2)**

> **作者:** Elena Ryumina; Maxim Markitantov; Alexandr Axyonov; Dmitry Ryumin; Mikhail Dolgushin; Alexey Karpov
>
> **备注:** 7
>
> **摘要:** Compound Expression Recognition (CER), a subfield of affective computing, aims to detect complex emotional states formed by combinations of basic emotions. In this work, we present a novel zero-shot multimodal approach for CER that combines six heterogeneous modalities into a single pipeline: static and dynamic facial expressions, scene and label matching, scene context, audio, and text. Unlike previous approaches relying on task-specific training data, our approach uses zero-shot components, including Contrastive Language-Image Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene understanding. We further introduce a Multi-Head Probability Fusion (MHPF) module that dynamically weights modality-specific predictions, followed by a Compound Expressions (CE) transformation module that uses Pair-Wise Probability Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods to produce interpretable compound emotion outputs. Evaluated under multi-corpus training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02% on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via zero-shot testing, which is comparable to the results of supervised approaches trained on target data. This demonstrates the effectiveness of the proposed approach for capturing CE without domain adaptation. The source code is publicly available.
>
---
#### [replaced 140] SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.05344v2](http://arxiv.org/pdf/2506.05344v2)**

> **作者:** Jiahui Wang; Zuyan Liu; Yongming Rao; Jiwen Lu
>
> **备注:** Accepted to ICCV 2025
>
> **摘要:** Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.
>
---
#### [replaced 141] Symmetry-Robust 3D Orientation Estimation
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2410.02101v4](http://arxiv.org/pdf/2410.02101v4)**

> **作者:** Christopher Scarvelis; David Benhaim; Paul Zhang
>
> **备注:** ICML 2025
>
> **摘要:** Orientation estimation is a fundamental task in 3D shape analysis which consists of estimating a shape's orientation axes: its side-, up-, and front-axes. Using this data, one can rotate a shape into canonical orientation, where its orientation axes are aligned with the coordinate axes. Developing an orientation algorithm that reliably estimates complete orientations of general shapes remains an open problem. We introduce a two-stage orientation pipeline that achieves state of the art performance on up-axis estimation and further demonstrate its efficacy on full-orientation estimation, where one seeks all three orientation axes. Unlike previous work, we train and evaluate our method on all of Shapenet rather than a subset of classes. We motivate our engineering contributions by theory describing fundamental obstacles to orientation estimation for rotationally-symmetric shapes, and show how our method avoids these obstacles.
>
---
#### [replaced 142] HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion
- **分类: cs.CV; cs.AI; I.2**

- **链接: [http://arxiv.org/pdf/2506.06035v2](http://arxiv.org/pdf/2506.06035v2)**

> **作者:** Shiyi Zhang; Dong Liang; Hairong Zheng; Yihang Zhou
>
> **备注:** We have decided to withdraw this paper because the baseline methods used for comparison are outdated and do not reflect the current state-of-the-art. This significantly affects the validity of our performance claims and conclusions. We plan to conduct a more comprehensive evaluation and submit a revised version in the future
>
> **摘要:** Reconstructing visual information from brain activity bridges the gap between neuroscience and computer vision. Even though progress has been made in decoding images from fMRI using generative models, a challenge remains in accurately recovering highly complex visual stimuli. This difficulty stems from their elemental density and diversity, sophisticated spatial structures, and multifaceted semantic information. To address these challenges, we propose HAVIR that contains two adapters: (1) The AutoKL Adapter transforms fMRI voxels into a latent diffusion prior, capturing topological structures; (2) The CLIP Adapter converts the voxels to CLIP text and image embeddings, containing semantic information. These complementary representations are fused by Versatile Diffusion to generate the final reconstructed image. To extract the most essential semantic information from complex scenarios, the CLIP Adapter is trained with text captions describing the visual stimuli and their corresponding semantic images synthesized from these captions. The experimental results demonstrate that HAVIR effectively reconstructs both structural features and semantic information of visual stimuli even in complex scenarios, outperforming existing models.
>
---
#### [replaced 143] AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.02591v2](http://arxiv.org/pdf/2507.02591v2)**

> **作者:** Weili Xu; Enxin Song; Wenhao Chai; Xuexiang Wen; Tian Ye; Gaoang Wang
>
> **备注:** Accepted to ICCV 2025
>
> **摘要:** The challenge of long video understanding lies in its high computational complexity and prohibitive memory cost, since the memory and computation required by transformer-based LLMs scale quadratically with input sequence length. We propose AuroraLong to address this challenge by replacing the LLM component in MLLMs with a linear RNN language model that handles input sequence of arbitrary length with constant-size hidden states. To further increase throughput and efficiency, we combine visual token merge with linear RNN models by reordering the visual tokens by their sizes in ascending order. Despite having only 2B parameters and being trained exclusively on public data, AuroraLong achieves performance comparable to Transformer-based models of similar size trained on private datasets across multiple video benchmarks. This demonstrates the potential of efficient, linear RNNs to democratize long video understanding by lowering its computational entry barrier. To our best knowledge, we are the first to use a linear RNN based LLM backbone in a LLaVA-like model for open-ended video understanding.
>
---
#### [replaced 144] UNSURF: Uncertainty Quantification for Cortical Surface Reconstruction of Clinical Brain MRIs
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2506.00498v2](http://arxiv.org/pdf/2506.00498v2)**

> **作者:** Raghav Mehta; Karthik Gopinath; Ben Glocker; Juan Eugenio Iglesias
>
> **备注:** Paper accepted at MICCAI 2025. Raghav Mehta and Karthik Gopinath contributed equally. Ben Glocker and Juan Eugenio Iglesias contributed equally
>
> **摘要:** We propose UNSURF, a novel uncertainty measure for cortical surface reconstruction of clinical brain MRI scans of any orientation, resolution, and contrast. It relies on the discrepancy between predicted voxel-wise signed distance functions (SDFs) and the actual SDFs of the fitted surfaces. Our experiments on real clinical scans show that traditional uncertainty measures, such as voxel-wise Monte Carlo variance, are not suitable for modeling the uncertainty of surface placement. Our results demonstrate that UNSURF estimates correlate well with the ground truth errors and: \textit{(i)}~enable effective automated quality control of surface reconstructions at the subject-, parcel-, mesh node-level; and \textit{(ii)}~improve performance on a downstream Alzheimer's disease classification task.
>
---
#### [replaced 145] Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations
- **分类: cs.RO; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2507.00990v2](http://arxiv.org/pdf/2507.00990v2)**

> **作者:** Shivansh Patel; Shraddhaa Mohan; Hanlin Mai; Unnat Jain; Svetlana Lazebnik; Yunzhu Li
>
> **备注:** Project Page: https://rigvid-robot.github.io/
>
> **摘要:** This work introduces Robots Imitating Generated Videos (RIGVid), a system that enables robots to perform complex manipulation tasks--such as pouring, wiping, and mixing--purely by imitating AI-generated videos, without requiring any physical demonstrations or robot-specific training. Given a language command and an initial scene image, a video diffusion model generates potential demonstration videos, and a vision-language model (VLM) automatically filters out results that do not follow the command. A 6D pose tracker then extracts object trajectories from the video, and the trajectories are retargeted to the robot in an embodiment-agnostic fashion. Through extensive real-world evaluations, we show that filtered generated videos are as effective as real demonstrations, and that performance improves with generation quality. We also show that relying on generated videos outperforms more compact alternatives such as keypoint prediction using VLMs, and that strong 6D pose tracking outperforms other ways to extract trajectories, such as dense feature point tracking. These findings suggest that videos produced by a state-of-the-art off-the-shelf model can offer an effective source of supervision for robotic manipulation.
>
---
#### [replaced 146] ISLES'24: Final Infarct Prediction with Multimodal Imaging and Clinical Data. Where Do We Stand?
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2408.10966v2](http://arxiv.org/pdf/2408.10966v2)**

> **作者:** Ezequiel de la Rosa; Ruisheng Su; Mauricio Reyes; Evamaria O. Riedel; Hakim Baazaoui; Roland Wiest; Florian Kofler; Kaiyuan Yang; David Robben; Mahsa Mojtahedi; Laura van Poppel; Lucas de Vries; Anthony Winder; Kimberly Amador; Nils D. Forkert; Gyeongyeon Hwang; Jiwoo Song; Dohyun Kim; Eneko Uruñuela; Annabella Bregazzi; Matthias Wilms; Hyun Yang; Jin Tae Kwak; Sumin Jung; Luan Matheus Trindade Dalmazo; Kumaradevan Punithakumar; Moona Mazher; Abdul Qayyum; Steven Niederer; Jacob Idoko; Mariana Bento; Gouri Ginde; Tianyi Ren; Juampablo Heras Rivera; Mehmet Kurt; Carole Frindel; Susanne Wegener; Jan S. Kirschke; Benedikt Wiestler; Bjoern Menze
>
> **摘要:** Accurate estimation of brain infarction (i.e., irreversibly damaged tissue) is critical for guiding treatment decisions in acute ischemic stroke. Reliable infarct prediction informs key clinical interventions, including the need for patient transfer to comprehensive stroke centers, the potential benefit of additional reperfusion attempts during mechanical thrombectomy, decisions regarding secondary neuroprotective treatments, and ultimately, prognosis of clinical outcomes. This work introduces the Ischemic Stroke Lesion Segmentation (ISLES) 2024 challenge, which focuses on the prediction of final infarct volumes from pre-interventional acute stroke imaging and clinical data. ISLES24 provides a comprehensive, multimodal setting where participants can leverage all clinically and practically available data, including full acute CT imaging, sub-acute follow-up MRI, and structured clinical information, across a train set of 150 cases. On the hidden test set of 98 cases, the top-performing model, a multimodal nnU-Net-based architecture, achieved a Dice score of 0.285 (+/- 0.213) and an absolute volume difference of 21.2 (+/- 37.2) mL, underlining the significant challenges posed by this task and the need for further advances in multimodal learning. This work makes two primary contributions: first, we establish a standardized, clinically realistic benchmark for post-treatment infarct prediction, enabling systematic evaluation of multimodal algorithmic strategies on a longitudinal stroke dataset; second, we analyze current methodological limitations and outline key research directions to guide the development of next-generation infarct prediction models.
>
---
#### [replaced 147] Anymate: A Dataset and Baselines for Learning 3D Object Rigging
- **分类: cs.GR; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.06227v2](http://arxiv.org/pdf/2505.06227v2)**

> **作者:** Yufan Deng; Yuhao Zhang; Chen Geng; Shangzhe Wu; Jiajun Wu
>
> **备注:** SIGGRAPH 2025. Project page: https://anymate3d.github.io/
>
> **摘要:** Rigging and skinning are essential steps to create realistic 3D animations, often requiring significant expertise and manual effort. Traditional attempts at automating these processes rely heavily on geometric heuristics and often struggle with objects of complex geometry. Recent data-driven approaches show potential for better generality, but are often constrained by limited training data. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets paired with expert-crafted rigging and skinning information -- 70 times larger than existing datasets. Using this dataset, we propose a learning-based auto-rigging framework with three sequential modules for joint, connectivity, and skinning weight prediction. We systematically design and experiment with various architectures as baselines for each module and conduct comprehensive evaluations on our dataset to compare their performance. Our models significantly outperform existing methods, providing a foundation for comparing future methods in automated rigging and skinning. Code and dataset can be found at https://anymate3d.github.io/.
>
---
#### [replaced 148] Pay Attention to the Keys: Visual Piano Transcription Using Transformers
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2411.09037v2](http://arxiv.org/pdf/2411.09037v2)**

> **作者:** Uros Zivanovic; Ivan Pilkov; Carlos Eduardo Cancino-Chacón
>
> **备注:** IJCAI 2025 Camera Ready
>
> **摘要:** Visual piano transcription (VPT) is the task of obtaining a symbolic representation of a piano performance from visual information alone (e.g., from a top-down video of the piano keyboard). In this work we propose a VPT system based on the vision transformer (ViT), which surpasses previous methods based on convolutional neural networks (CNNs). Our system is trained on the newly introduced R3 dataset, consisting of ca.~31 hours of synchronized video and MIDI recordings of piano performances. We additionally introduce an approach to predict note offsets, which has not been previously explored in this context. We show that our system outperforms the state-of-the-art on the PianoYT dataset for onset prediction and on the R3 dataset for both onsets and offsets.
>
---
#### [replaced 149] Fault Sneaking Attack: a Stealthy Framework for Misleading Deep Neural Networks
- **分类: cs.LG; cs.CR; cs.CV; stat.ML**

- **链接: [http://arxiv.org/pdf/1905.12032v2](http://arxiv.org/pdf/1905.12032v2)**

> **作者:** Pu Zhao; Siyue Wang; Cheng Gongye; Yanzhi Wang; Yunsi Fei; Xue Lin
>
> **备注:** Accepted by the 56th Design Automation Conference (DAC 2019)
>
> **摘要:** Despite the great achievements of deep neural networks (DNNs), the vulnerability of state-of-the-art DNNs raises security concerns of DNNs in many application domains requiring high reliability.We propose the fault sneaking attack on DNNs, where the adversary aims to misclassify certain input images into any target labels by modifying the DNN parameters. We apply ADMM (alternating direction method of multipliers) for solving the optimization problem of the fault sneaking attack with two constraints: 1) the classification of the other images should be unchanged and 2) the parameter modifications should be minimized. Specifically, the first constraint requires us not only to inject designated faults (misclassifications), but also to hide the faults for stealthy or sneaking considerations by maintaining model accuracy. The second constraint requires us to minimize the parameter modifications (using L0 norm to measure the number of modifications and L2 norm to measure the magnitude of modifications). Comprehensive experimental evaluation demonstrates that the proposed framework can inject multiple sneaking faults without losing the overall test accuracy performance.
>
---
#### [replaced 150] Playing with Transformer at 30+ FPS via Next-Frame Diffusion
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2506.01380v2](http://arxiv.org/pdf/2506.01380v2)**

> **作者:** Xinle Cheng; Tianyu He; Jiayi Xu; Junliang Guo; Di He; Jiang Bian
>
> **备注:** Project page: https://nextframed.github.io/
>
> **摘要:** Autoregressive video models offer distinct advantages over bidirectional diffusion models in creating interactive video content and supporting streaming applications with arbitrary duration. In this work, we present Next-Frame Diffusion (NFD), an autoregressive diffusion transformer that incorporates block-wise causal attention, enabling iterative sampling and efficient inference via parallel token generation within each frame. Nonetheless, achieving real-time video generation remains a significant challenge for such models, primarily due to the high computational cost associated with diffusion sampling and the hardware inefficiencies inherent to autoregressive generation. To address this, we introduce two innovations: (1) We extend consistency distillation to the video domain and adapt it specifically for video models, enabling efficient inference with few sampling steps; (2) To fully leverage parallel computation, motivated by the observation that adjacent frames often share the identical action input, we propose speculative sampling. In this approach, the model generates next few frames using current action input, and discard speculatively generated frames if the input action differs. Experiments on a large-scale action-conditioned video generation benchmark demonstrate that NFD beats autoregressive baselines in terms of both visual quality and sampling efficiency. We, for the first time, achieves autoregressive video generation at over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.
>
---
#### [replaced 151] Fairness Evolution in Continual Learning for Medical Imaging
- **分类: eess.IV; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2406.02480v2](http://arxiv.org/pdf/2406.02480v2)**

> **作者:** Marina Ceccon; Davide Dalle Pezze; Alessandro Fabris; Gian Antonio Susto
>
> **摘要:** Deep Learning has advanced significantly in medical applications, aiding disease diagnosis in Chest X-ray images. However, expanding model capabilities with new data remains a challenge, which Continual Learning (CL) aims to address. Previous studies have evaluated CL strategies based on classification performance; however, in sensitive domains such as healthcare, it is crucial to assess performance across socially salient groups to detect potential biases. This study examines how bias evolves across tasks using domain-specific fairness metrics and how different CL strategies impact this evolution. Our results show that Learning without Forgetting and Pseudo-Label achieve optimal classification performance, but Pseudo-Label is less biased.
>
---
#### [replaced 152] Mind the Context: Attention-Guided Weak-to-Strong Consistency for Enhanced Semi-Supervised Medical Image Segmentation
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2410.12419v3](http://arxiv.org/pdf/2410.12419v3)**

> **作者:** Yuxuan Cheng; Chenxi Shao; Jie Ma; Yunfei Xie; Guoliang Li
>
> **摘要:** Medical image segmentation is a pivotal step in diagnostic and therapeutic processes, relying on high-quality annotated data that is often challenging and costly to obtain. Semi-supervised learning offers a promising approach to enhance model performance by leveraging unlabeled data. Although weak-to-strong consistency is a prevalent method in semi-supervised image segmentation, there is a scarcity of research on perturbation strategies specifically tailored for semi-supervised medical image segmentation tasks. To address this challenge, this paper introduces a simple yet efficient semi-supervised learning framework named Attention-Guided weak-to-strong Consistency Match (AIGCMatch). The AIGCMatch framework incorporates attention-guided perturbation strategies at both the image and feature levels to achieve weak-to-strong consistency regularization. This method not only preserves the structural information of medical images but also enhances the model's ability to process complex semantic information. Extensive experiments conducted on the ACDC and ISIC-2017 datasets have validated the effectiveness of AIGCMatch. Our method achieved a 90.4\% Dice score in the 7-case scenario on the ACDC dataset, surpassing the state-of-the-art methods and demonstrating its potential and efficacy in clinical settings.
>
---
#### [replaced 153] Neuroverse3D: Developing In-Context Learning Universal Model for Neuroimaging in 3D
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2503.02410v2](http://arxiv.org/pdf/2503.02410v2)**

> **作者:** Jiesi Hu; Chenfei Ye; Yanwu Yang; Xutao Guo; Yang Shang; Pengcheng Shi; Hanyang Peng; Ting Ma
>
> **摘要:** In-context learning (ICL), a type of universal model, demonstrates exceptional generalization across a wide range of tasks without retraining by leveraging task-specific guidance from context, making it particularly effective for the intricate demands of neuroimaging. However, current ICL models, limited to 2D inputs and thus exhibiting suboptimal performance, struggle to extend to 3D inputs due to the high memory demands of ICL. In this regard, we introduce Neuroverse3D, an ICL model capable of performing multiple neuroimaging tasks in 3D (e.g., segmentation, denoising, inpainting). Neuroverse3D overcomes the large memory consumption associated with 3D inputs through adaptive parallel-sequential context processing and a U-shaped fusion strategy, allowing it to handle an unlimited number of context images. Additionally, we propose an optimized loss function to balance multi-task training and enhance focus on anatomical boundaries. Our study incorporates 43,674 3D multi-modal scans from 19 neuroimaging datasets and evaluates Neuroverse3D on 14 diverse tasks using held-out test sets. The results demonstrate that Neuroverse3D significantly outperforms existing ICL models and closely matches task-specific models, enabling flexible adaptation to medical center variations without retraining. The code and model weights are publicly available at https://github.com/jiesihu/Neuroverse3D.
>
---
#### [replaced 154] AASeg: Attention Aware Network for Real Time Semantic Segmentation
- **分类: cs.CV; cs.LG; eess.IV**

- **链接: [http://arxiv.org/pdf/2108.04349v4](http://arxiv.org/pdf/2108.04349v4)**

> **作者:** Abhinav Sagar
>
> **摘要:** Semantic segmentation is a fundamental task in computer vision that involves dense pixel-wise classification for scene understanding. Despite significant progress, achieving high accuracy while maintaining real-time performance remains a challenging trade-off, particularly for deployment in resource-constrained or latency-sensitive applications. In this paper, we propose AASeg, a novel Attention-Aware Network for real-time semantic segmentation. AASeg effectively captures both spatial and channel-wise dependencies through lightweight Spatial Attention (SA) and Channel Attention (CA) modules, enabling enhanced feature discrimination without incurring significant computational overhead. To enrich contextual representation, we introduce a Multi-Scale Context (MSC) module that aggregates dense local features across multiple receptive fields. The outputs from attention and context modules are adaptively fused to produce high-resolution segmentation maps. Extensive experiments on Cityscapes, ADE20K, and CamVid demonstrate that AASeg achieves a compelling trade-off between accuracy and efficiency, outperforming prior real-time methods.
>
---
#### [replaced 155] AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2506.23537v2](http://arxiv.org/pdf/2506.23537v2)**

> **作者:** Xinyue Li; Zhangkai Ni; Wenhan Yang
>
> **备注:** Accepted to International Conference on Computer Vision (ICCV) 2025
>
> **摘要:** Existing learning-based methods effectively reconstruct HDR images from multi-exposure LDR inputs with extended dynamic range and improved detail, but they rely more on empirical design rather than theoretical foundation, which can impact their reliability. To address these limitations, we propose the cross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR reconstruction is systematically decoupled into two interleaved subtasks -- alignment and fusion -- optimized through alternating refinement, achieving synergy between the two subtasks to enhance the overall performance. Our method formulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP) estimation perspective, explicitly incorporating spatial correspondence priors across LDR images and naturally bridging the alignment and fusion subproblems through joint constraints. Building on the mathematical foundation, we reimagine traditional iterative optimization through unfolding -- transforming the conventional solution process into an end-to-end trainable AFUNet with carefully designed modules that work progressively. Specifically, each iteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that alternates between a Spatial Alignment Module (SAM) for alignment and a Channel Fusion Module (CFM) for adaptive feature fusion, progressively bridging misaligned content and exposure discrepancies. Extensive qualitative and quantitative evaluations demonstrate AFUNet's superior performance, consistently surpassing state-of-the-art methods. Our code is available at: https://github.com/eezkni/AFUNet
>
---
#### [replaced 156] Looking Locally: Object-Centric Vision Transformers as Foundation Models for Efficient Segmentation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2502.02763v2](http://arxiv.org/pdf/2502.02763v2)**

> **作者:** Manuel Traub; Martin V. Butz
>
> **摘要:** Current state-of-the-art segmentation models encode entire images before focusing on specific objects. As a result, they waste computational resources - particularly when small objects are to be segmented in high-resolution scenes. We introduce FLIP (Fovea-Like Input Patching), a parameter-efficient vision model that realizes object segmentation through biologically-inspired top-down attention. FLIP selectively samples multi-resolution patches centered on objects of interest from the input. As a result, it allocates high-resolution processing to object centers while maintaining coarser peripheral context. This off-grid, scale-invariant design enables FLIP to outperform META's Segment Anything models (SAM) by large margins: With more than 1000x fewer parameters, FLIP-Tiny (0.51M parameters) reaches a mean IoU of 78.24% while SAM-H reaches 75.41% IoU (641.1M parameters). FLIP-Large even achieves 80.33% mean IoU (96.6M parameters), still running about 6$\times$ faster than SAM-H. We evaluate on six benchmarks in total. In five established benchmarks (Hypersim, KITTI-360, OpenImages, COCO, LVIS) FLIP consistently outperforms SAM and various variants of it. In our novel ObjaScale dataset, which stress-tests scale invariance with objects ranging from 0.0001% up-to 25% of the image area, we show that FLIP segments even very small objects accurately, where existing models fail severely. FLIP opens new possibilities for real-time, object-centric vision applications and offers much higher energy efficiency. We believe that FLIP can act as a powerful foundation model, as it is very well-suited to track objects over time, for example, when being integrated into slot-based scene segmentation architectures.
>
---
#### [replaced 157] Towards Better Visualizing the Decision Basis of Networks via Unfold and Conquer Attribution Guidance
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2312.14201v2](http://arxiv.org/pdf/2312.14201v2)**

> **作者:** Jung-Ho Hong; Woo-Jeoung Nam; Kyu-Sung Jeon; Seong-Whan Lee
>
> **备注:** 9 pages, 5 figures, Accepted paper in AAAI Conference on Artificial Intelligence (AAAI), 2023
>
> **摘要:** Revealing the transparency of Deep Neural Networks (DNNs) has been widely studied to describe the decision mechanisms of network inner structures. In this paper, we propose a novel post-hoc framework, Unfold and Conquer Attribution Guidance (UCAG), which enhances the explainability of the network decision by spatially scrutinizing the input features with respect to the model confidence. Addressing the phenomenon of missing detailed descriptions, UCAG sequentially complies with the confidence of slices of the image, leading to providing an abundant and clear interpretation. Therefore, it is possible to enhance the representation ability of explanation by preserving the detailed descriptions of assistant input features, which are commonly overwhelmed by the main meaningful regions. We conduct numerous evaluations to validate the performance in several metrics: i) deletion and insertion, ii) (energy-based) pointing games, and iii) positive and negative density maps. Experimental results, including qualitative comparisons, demonstrate that our method outperforms the existing methods with the nature of clear and detailed explanations and applicability.
>
---
#### [replaced 158] Multi-person Physics-based Pose Estimation for Combat Sports
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2504.08175v3](http://arxiv.org/pdf/2504.08175v3)**

> **作者:** Hossein Feiz; David Labbé; Thomas Romeas; Jocelyn Faubert; Sheldon Andrews
>
> **摘要:** We propose a novel framework for accurate 3D human pose estimation in combat sports using sparse multi-camera setups. Our method integrates robust multi-view 2D pose tracking via a transformer-based top-down approach, employing epipolar geometry constraints and long-term video object segmentation for consistent identity tracking across views. Initial 3D poses are obtained through weighted triangulation and spline smoothing, followed by kinematic optimization to refine pose accuracy. We further enhance pose realism and robustness by introducing a multi-person physics-based trajectory optimization step, effectively addressing challenges such as rapid motions, occlusions, and close interactions. Experimental results on diverse datasets, including a new benchmark of elite boxing footage, demonstrate state-of-the-art performance. Additionally, we release comprehensive annotated video datasets to advance future research in multi-person pose estimation for combat sports.
>
---
#### [replaced 159] Particle Trajectory Representation Learning with Masked Point Modeling
- **分类: hep-ex; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2502.02558v3](http://arxiv.org/pdf/2502.02558v3)**

> **作者:** Sam Young; Yeon-jae Jwa; Kazuhiro Terao
>
> **备注:** Preprint. 28 pages, 18 figures. v3 includes new results on data efficiency and attention maps
>
> **摘要:** Effective self-supervised learning (SSL) techniques have been key to unlocking large datasets for representation learning. While many promising methods have been developed using online corpora and captioned photographs, their application to scientific domains, where data encodes highly specialized knowledge, remains a challenge. Liquid Argon Time Projection Chambers (LArTPCs) provide high-resolution 3D imaging for fundamental physics, but analysis of their sparse, complex point cloud data often relies on supervised methods trained on large simulations, introducing potential biases. We introduce the Point-based Liquid Argon Masked Autoencoder (PoLAr-MAE), applying masked point modeling to unlabeled LArTPC images using domain-specific volumetric tokenization and energy prediction. We show this SSL approach learns physically meaningful trajectory representations directly from data. This yields remarkable data efficiency: fine-tuning on just 100 labeled events achieves track/shower semantic segmentation performance comparable to the state-of-the-art supervised baseline trained on $>$100,000 events. Furthermore, internal attention maps exhibit emergent instance segmentation of particle trajectories. While challenges remain, particularly for fine-grained features, we make concrete SSL's potential for building a foundation model for LArTPC image analysis capable of serving as a common base for all data reconstruction tasks. To facilitate further progress, we release PILArNet-M, a large dataset of 1M LArTPC events. Project site: https://youngsm.com/polarmae.
>
---
#### [replaced 160] High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.10605v2](http://arxiv.org/pdf/2506.10605v2)**

> **作者:** Eshan Ramesh; Takayuki Nishio
>
> **备注:** 6 pages, 4 figures
>
> **摘要:** We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM's denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM's pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability.
>
---
#### [replaced 161] LoRA as a Flexible Framework for Securing Large Vision Systems
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.00661v2](http://arxiv.org/pdf/2506.00661v2)**

> **作者:** Zander W. Blasingame; Richard E. Neddo; Chen Liu
>
> **备注:** Updated pre-print. Under review
>
> **摘要:** Adversarial attacks have emerged as a critical threat to autonomous driving systems. These attacks exploit the underlying neural network, allowing small -- nearly invisible -- perturbations to completely alter the behavior of such systems in potentially malicious ways. E.g., causing a traffic sign classification network to misclassify a stop sign as a speed limit sign. Prior working in hardening such systems to adversarial attacks have looked at robust training of the system or adding additional pre-processing steps to the input pipeline. Such solutions either have a hard time generalizing, require knowledge of the adversarial attacks during training, or are computationally undesirable. Instead, we propose to take insights for parameter efficient fine-tuning and use low-rank adaptation (LoRA) to train a lightweight security patch -- enabling us to dynamically patch a large preexisting vision system as new vulnerabilities are discovered. We demonstrate that our framework can patch a pre-trained model to improve classification accuracy by up to 78.01% in the presence of adversarial examples.
>
---
#### [replaced 162] AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.02419v2](http://arxiv.org/pdf/2507.02419v2)**

> **作者:** Yiming Zhong; Xiaolin Zhang; Ligang Liu; Yao Zhao; Yunchao Wei
>
> **摘要:** Similar to facial beautification in real life, 3D virtual avatars require personalized customization to enhance their visual appeal, yet this area remains insufficiently explored. Although current 3D Gaussian editing methods can be adapted for facial makeup purposes, these methods fail to meet the fundamental requirements for achieving realistic makeup effects: 1) ensuring a consistent appearance during drivable expressions, 2) preserving the identity throughout the makeup process, and 3) enabling precise control over fine details. To address these, we propose a specialized 3D makeup method named AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup patterns from a single reference photo of any individual. We adopt a coarse-to-fine idea to first maintain the consistent appearance and identity, and then to refine the details. In particular, the diffusion model is employed to generate makeup images as supervision. Due to the uncertainties in diffusion process, the generated images are inconsistent across different viewpoints and expressions. Therefore, we propose a Coherent Duplication method to coarsely apply makeup to the target while ensuring consistency across dynamic and multiview effects. Coherent Duplication optimizes a global UV map by recoding the averaged facial attributes among the generated makeup images. By querying the global UV map, it easily synthesizes coherent makeup guidance from arbitrary views and expressions to optimize the target avatar. Given the coarse makeup avatar, we further enhance the makeup by incorporating a Refinement Module into the diffusion model to achieve high makeup quality. Experiments demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation.
>
---
#### [replaced 163] VideoMolmo: Spatio-Temporal Grounding Meets Pointing
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.05336v2](http://arxiv.org/pdf/2506.05336v2)**

> **作者:** Ghazi Shazan Ahmad; Ahmed Heakl; Hanan Gani; Abdelrahman Shaker; Zhiqiang Shen; Fahad Shahbaz Khan; Salman Khan
>
> **备注:** 20 pages, 13 figures
>
> **摘要:** Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at https://github.com/mbzuai-oryx/VideoMolmo.
>
---
#### [replaced 164] PAVLM: Advancing Point Cloud based Affordance Understanding Via Vision-Language Model
- **分类: cs.RO; cs.CV**

- **链接: [http://arxiv.org/pdf/2410.11564v2](http://arxiv.org/pdf/2410.11564v2)**

> **作者:** Shang-Ching Liu; Van Nhiem Tran; Wenkai Chen; Wei-Lun Cheng; Yen-Lin Huang; I-Bin Liao; Yung-Hui Li; Jianwei Zhang
>
> **摘要:** Affordance understanding, the task of identifying actionable regions on 3D objects, plays a vital role in allowing robotic systems to engage with and operate within the physical world. Although Visual Language Models (VLMs) have excelled in high-level reasoning and long-horizon planning for robotic manipulation, they still fall short in grasping the nuanced physical properties required for effective human-robot interaction. In this paper, we introduce PAVLM (Point cloud Affordance Vision-Language Model), an innovative framework that utilizes the extensive multimodal knowledge embedded in pre-trained language models to enhance 3D affordance understanding of point cloud. PAVLM integrates a geometric-guided propagation module with hidden embeddings from large language models (LLMs) to enrich visual semantics. On the language side, we prompt Llama-3.1 models to generate refined context-aware text, augmenting the instructional input with deeper semantic cues. Experimental results on the 3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods for both full and partial point clouds, particularly excelling in its generalization to novel open-world affordance tasks of 3D objects. For more information, visit our project site: pavlm-source.github.io.
>
---
#### [replaced 165] Static Segmentation by Tracking: A Label-Efficient Approach for Fine-Grained Specimen Image Segmentation
- **分类: cs.CV; cs.AI**

- **链接: [http://arxiv.org/pdf/2501.06749v2](http://arxiv.org/pdf/2501.06749v2)**

> **作者:** Zhenyang Feng; Zihe Wang; Jianyang Gu; Saul Ibaven Bueno; Tomasz Frelek; Advikaa Ramesh; Jingyan Bai; Lemeng Wang; Zanming Huang; Jinsu Yoo; Tai-Yu Pan; Arpita Chowdhury; Michelle Ramirez; Elizabeth G. Campolongo; Matthew J. Thompson; Christopher G. Lawrence; Sydne Record; Neil Rosser; Anuj Karpatne; Daniel Rubenstein; Hilmar Lapp; Charles V. Stewart; Tanya Berger-Wolf; Yu Su; Wei-Lun Chao
>
> **摘要:** We study image segmentation in the biological domain, particularly trait segmentation from specimen images (e.g., butterfly wing stripes, beetle elytra). This fine-grained task is crucial for understanding the biology of organisms, but it traditionally requires manually annotating segmentation masks for hundreds of images per species, making it highly labor-intensive. To address this challenge, we propose a label-efficient approach, Static Segmentation by Tracking (SST), based on a key insight: while specimens of the same species exhibit natural variation, the traits of interest show up consistently. This motivates us to concatenate specimen images into a ``pseudo-video'' and reframe trait segmentation as a tracking problem. Specifically, SST generates masks for unlabeled images by propagating annotated or predicted masks from the ``pseudo-preceding'' images. Built upon recent video segmentation models, such as Segment Anything Model 2, SST achieves high-quality trait segmentation with only one labeled image per species, marking a breakthrough in specimen image analysis. To further enhance segmentation quality, we introduce a cycle-consistent loss for fine-tuning, again requiring only one labeled image. Additionally, we demonstrate the broader potential of SST, including one-shot instance segmentation in natural images and trait-based image retrieval.
>
---
#### [replaced 166] HOTS3D: Hyper-Spherical Optimal Transport for Semantic Alignment of Text-to-3D Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2407.14419v2](http://arxiv.org/pdf/2407.14419v2)**

> **作者:** Zezeng Li; Weimin Wang; Yuming Zhao; Wenhai Li; Na Lei; Xianfeng Gu
>
> **备注:** accepted by tvcg
>
> **摘要:** Recent CLIP-guided 3D generation methods have achieved promising results but struggle with generating faithful 3D shapes that conform with input text due to the gap between text and image embeddings. To this end, this paper proposes HOTS3D which makes the first attempt to effectively bridge this gap by aligning text features to the image features with spherical optimal transport(SOT). However, in high-dimensional situations, solving the SOT remains a challenge. To obtain the SOT map for high-dimensional features obtained from CLIP encoding of two modalities, we mathematically formulate and derive the solution based on Villani's theorem, which can directly align two hyper-sphere distributions without manifold exponential maps. Furthermore, we implement it by leveraging input convex neural networks (ICNNs) for the optimal Kantorovich potential. With the optimally mapped features, a diffusion-based generator is utilized to decode them into 3D shapes. Extensive quantitative and qualitative comparisons with state-of-the-art methods demonstrate the superiority of HOTS3D for text-to-3D generation, especially in the consistency with text semantics.
>
---
#### [replaced 167] Efficient generative adversarial networks using linear additive-attention Transformers
- **分类: cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2401.09596v5](http://arxiv.org/pdf/2401.09596v5)**

> **作者:** Emilio Morales-Juarez; Gibran Fuentes-Pineda
>
> **备注:** 13 pages, 8 figures
>
> **摘要:** Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present a novel GAN architecture which we call LadaGAN. This architecture is based on a linear attention Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms existing convolutional and Transformer GANs on benchmark datasets at different resolutions while being significantly more efficient. Moreover, LadaGAN shows competitive performance compared to state-of-the-art multi-step generative models (e.g. DMs) using orders of magnitude less computational resources.
>
---
#### [replaced 168] Uncertainty in Real-Time Semantic Segmentation on Embedded Systems
- **分类: cs.CV; cs.LG; eess.IV**

- **链接: [http://arxiv.org/pdf/2301.01201v5](http://arxiv.org/pdf/2301.01201v5)**

> **作者:** Ethan Goan; Clinton Fookes
>
> **摘要:** Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present when applied on embedded real-time systems. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful epistemic uncertainty on embedded hardware in real-time whilst maintaining predictive performance.
>
---
#### [replaced 169] End-to-end 2D-3D Registration between Image and LiDAR Point Cloud for Vehicle Localization
- **分类: cs.RO; cs.CV**

- **链接: [http://arxiv.org/pdf/2306.11346v2](http://arxiv.org/pdf/2306.11346v2)**

> **作者:** Guangming Wang; Yu Zheng; Yuxuan Wu; Yanfeng Guo; Zhe Liu; Yixiang Zhu; Wolfram Burgard; Hesheng Wang
>
> **备注:** Accepted to T-RO. Source codes are released at https://github.com/IRMVLab/I2PNet
>
> **摘要:** Robot localization using a built map is essential for a variety of tasks including accurate navigation and mobile manipulation. A popular approach to robot localization is based on image-to-point cloud registration, which combines illumination-invariant LiDAR-based mapping with economical image-based localization. However, the recent works for image-to-point cloud registration either divide the registration into separate modules or project the point cloud to the depth image to register the RGB and depth images. In this paper, we present I2PNet, a novel end-to-end 2D-3D registration network, which directly registers the raw 3D point cloud with the 2D RGB image using differential modules with a united target. The 2D-3D cost volume module for differential 2D-3D association is proposed to bridge feature extraction and pose regression. The soft point-to-pixel correspondence is implicitly constructed on the intrinsic-independent normalized plane in the 2D-3D cost volume module. Moreover, we introduce an outlier mask prediction module to filter the outliers in the 2D-3D association before pose regression. Furthermore, we propose the coarse-to-fine 2D-3D registration architecture to increase localization accuracy. Extensive localization experiments are conducted on the KITTI, nuScenes, M2DGR, Argoverse, Waymo, and Lyft5 datasets. The results demonstrate that I2PNet outperforms the state-of-the-art by a large margin and has a higher efficiency than the previous works. Moreover, we extend the application of I2PNet to the camera-LiDAR online calibration and demonstrate that I2PNet outperforms recent approaches on the online calibration task. Source codes are released at https://github.com/IRMVLab/I2PNet.
>
---
#### [replaced 170] UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.02713v2](http://arxiv.org/pdf/2507.02713v2)**

> **作者:** Qin Guo; Ailing Zeng; Dongxu Yue; Ceyuan Yang; Yang Cao; Hanzhong Guo; Fei Shen; Wei Liu; Xihui Liu; Dan Xu
>
> **摘要:** Although significant advancements have been achieved in the progress of keypoint-guided Text-to-Image diffusion models, existing mainstream keypoint-guided models encounter challenges in controlling the generation of more general non-rigid objects beyond humans (e.g., animals). Moreover, it is difficult to generate multiple overlapping humans and animals based on keypoint controls solely. These challenges arise from two main aspects: the inherent limitations of existing controllable methods and the lack of suitable datasets. First, we design a DiT-based framework, named UniMC, to explore unifying controllable multi-class image generation. UniMC integrates instance- and keypoint-level conditions into compact tokens, incorporating attributes such as class, bounding box, and keypoint coordinates. This approach overcomes the limitations of previous methods that struggled to distinguish instances and classes due to their reliance on skeleton images as conditions. Second, we propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K images with 2.9M instances. This dataset features extensive annotations such as keypoints, bounding boxes, and fine-grained captions for both humans and animals, along with rigorous manual inspection to ensure annotation accuracy. Extensive experiments demonstrate the high quality of HAIG-2.9M and the effectiveness of UniMC, particularly in heavy occlusions and multi-class scenarios.
>
---
#### [replaced 171] Orientation Scores should be a Piece of Cake
- **分类: math.DG; cs.CV**

- **链接: [http://arxiv.org/pdf/2504.00702v2](http://arxiv.org/pdf/2504.00702v2)**

> **作者:** Finn M. Sherry; Chase van de Geijn; Erik J. Bekkers; Remco Duits
>
> **备注:** Accepted in the 7th International Conference on Geometric Science of Information
>
> **摘要:** We axiomatically derive a family of wavelets for an orientation score, lifting from position space $\mathbb{R}^2$ to position and orientation space $\mathbb{R}^2\times S^1$, with fast reconstruction property, that minimise position-orientation uncertainty. We subsequently show that these minimum uncertainty states are well-approximated by cake wavelets: for standard parameters, the uncertainty gap of cake wavelets is less than 1.1, and in the limit, we prove the uncertainty gap tends to the minimum of 1. Next, we complete a previous theoretical argument that one does not have to train the lifting layer in (PDE-)G-CNNs, but can instead use cake wavelets. Finally, we show experimentally that in this way we can reduce the network complexity and improve the interpretability of (PDE-)G-CNNs, with only a slight impact on the model's performance.
>
---
#### [replaced 172] SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.17721v2](http://arxiv.org/pdf/2505.17721v2)**

> **作者:** Dekai Zhu; Yan Di; Stefan Gavranovic; Slobodan Ilic
>
> **备注:** Accepted by CVPR 2025
>
> **摘要:** Denoising diffusion probabilistic models have achieved significant success in point cloud generation, enabling numerous downstream applications, such as generative data augmentation and 3D model editing. However, little attention has been given to generating point clouds with point-wise segmentation labels, as well as to developing evaluation metrics for this task. Therefore, in this paper, we present SeaLion, a novel diffusion model designed to generate high-quality and diverse point clouds with fine-grained segmentation labels. Specifically, we introduce the semantic part-aware latent point diffusion technique, which leverages the intermediate features of the generative models to jointly predict the noise for perturbed latent points and associated part segmentation labels during the denoising process, and subsequently decodes the latent points to point clouds conditioned on part segmentation labels. To effectively evaluate the quality of generated point clouds, we introduce a novel point cloud pairwise distance calculation method named part-aware Chamfer distance (p-CD). This method enables existing metrics, such as 1-NNA, to measure both the local structural quality and inter-part coherence of generated point clouds. Experiments on the large-scale synthetic dataset ShapeNet and real-world medical dataset IntrA demonstrate that SeaLion achieves remarkable performance in generation quality and diversity, outperforming the existing state-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across the two datasets. Experimental analysis shows that SeaLion can be trained semi-supervised, thereby reducing the demand for labeling efforts. Lastly, we validate the applicability of SeaLion in generative data augmentation for training segmentation models and the capability of SeaLion to serve as a tool for part-aware 3D shape editing.
>
---
#### [replaced 173] RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.15784v2](http://arxiv.org/pdf/2503.15784v2)**

> **作者:** Parham Saremi; Amar Kumar; Mohamed Mohamed; Zahra TehraniNasab; Tal Arbel
>
> **摘要:** Vision-Language Foundation Models (VLFM) have shown a tremendous increase in performance in terms of generating high-resolution, photorealistic natural images. While VLFMs show a rich understanding of semantic content across modalities, they often struggle with fine-grained alignment tasks that require precise correspondence between image regions and textual descriptions, a limitation in medical imaging, where accurate localization and detection of clinical features are essential for diagnosis and analysis. To address this issue, we propose a multi-stage architecture where a pre-trained VLFM (e.g. Stable Diffusion) provides a cursory semantic understanding, while a reinforcement learning (RL) algorithm refines the alignment through an iterative process that optimizes for understanding semantic context. The reward signal is designed to align the semantic information of the text with synthesized images. Experiments on the public ISIC2019 skin lesion dataset demonstrate that the proposed method improves (a) the quality of the generated images, and (b) the alignment with the text prompt over the original fine-tuned Stable Diffusion baseline. We also show that the synthesized samples could be used to improve disease classifier performance for underrepresented subgroups through augmentation. Our code is accessible through the project website: https://parhamsaremi.github.io/rl4med-ddpo
>
---
#### [replaced 174] SCJD: Sparse Correlation and Joint Distillation for Efficient 3D Human Pose Estimation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2503.14097v2](http://arxiv.org/pdf/2503.14097v2)**

> **作者:** Weihong Chen; Xuemiao Xu; Haoxin Yang; Yi Xie; Peng Xiao; Cheng Xu; Huaidong Zhang; Pheng-Ann Heng
>
> **摘要:** Existing 3D Human Pose Estimation (HPE) methods achieve high accuracy but suffer from computational overhead and slow inference, while knowledge distillation methods fail to address spatial relationships between joints and temporal correlations in multi-frame inputs. In this paper, we propose Sparse Correlation and Joint Distillation (SCJD), a novel framework that balances efficiency and accuracy for 3D HPE. SCJD introduces Sparse Correlation Input Sequence Downsampling to reduce redundancy in student network inputs while preserving inter-frame correlations. For effective knowledge transfer, we propose Dynamic Joint Spatial Attention Distillation, which includes Dynamic Joint Embedding Distillation to enhance the student's feature representation using the teacher's multi-frame context feature, and Adjacent Joint Attention Distillation to improve the student network's focus on adjacent joint relationships for better spatial understanding. Additionally, Temporal Consistency Distillation aligns the temporal correlations between teacher and student networks through upsampling and global supervision. Extensive experiments demonstrate that SCJD achieves state-of-the-art performance. Code is available at https://github.com/wileychan/SCJD.
>
---
#### [replaced 175] EndoFlow-SLAM: Real-Time Endoscopic SLAM with Flow-Constrained Gaussian Splatting
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2506.21420v2](http://arxiv.org/pdf/2506.21420v2)**

> **作者:** Taoyu Wu; Yiyi Miao; Zhuoxiao Li; Haocheng Zhao; Kang Dang; Jionglong Su; Limin Yu; Haoang Li
>
> **备注:** This paper has been accepted at MICCAI2025
>
> **摘要:** Efficient three-dimensional reconstruction and real-time visualization are critical in surgical scenarios such as endoscopy. In recent years, 3D Gaussian Splatting (3DGS) has demonstrated remarkable performance in efficient 3D reconstruction and rendering. Most 3DGS-based Simultaneous Localization and Mapping (SLAM) methods only rely on the appearance constraints for optimizing both 3DGS and camera poses. However, in endoscopic scenarios, the challenges include photometric inconsistencies caused by non-Lambertian surfaces and dynamic motion from breathing affects the performance of SLAM systems. To address these issues, we additionally introduce optical flow loss as a geometric constraint, which effectively constrains both the 3D structure of the scene and the camera motion. Furthermore, we propose a depth regularisation strategy to mitigate the problem of photometric inconsistencies and ensure the validity of 3DGS depth rendering in endoscopic scenes. In addition, to improve scene representation in the SLAM system, we improve the 3DGS refinement strategy by focusing on viewpoints corresponding to Keyframes with suboptimal rendering quality frames, achieving better rendering results. Extensive experiments on the C3VD static dataset and the StereoMIS dynamic dataset demonstrate that our method outperforms existing state-of-the-art methods in novel view synthesis and pose estimation, exhibiting high performance in both static and dynamic surgical scenes.
>
---
#### [replaced 176] Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.15484v4](http://arxiv.org/pdf/2412.15484v4)**

> **作者:** Saehyung Lee; Seunghyun Yoon; Trung Bui; Jing Shi; Sungroh Yoon
>
> **备注:** ICML 2025
>
> **摘要:** Multimodal large language models (MLLMs) excel at generating highly detailed captions but often produce hallucinations. Our analysis reveals that existing hallucination detection methods struggle with detailed captions. We attribute this to the increasing reliance of MLLMs on their generated text, rather than the input image, as the sequence length grows. To address this issue, we propose a multiagent approach that leverages LLM-MLLM collaboration to correct given captions. Additionally, we introduce an evaluation framework and a benchmark dataset to facilitate the systematic analysis of detailed captions. Our experiments demonstrate that our proposed evaluation method better aligns with human judgments of factuality than existing metrics and that existing approaches to improve the MLLM factuality may fall short in hyper-detailed image captioning tasks. In contrast, our proposed method significantly enhances the factual accuracy of captions, even improving those generated by GPT-4V. Finally, we highlight a limitation of VQA-centric benchmarking by demonstrating that an MLLM's performance on VQA benchmarks may not correlate with its ability to generate detailed image captions. Our code and data are available at https://github.com/adobe-research/CapMAS.
>
---
#### [replaced 177] HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2312.06553v3](http://arxiv.org/pdf/2312.06553v3)**

> **作者:** Xiaogang Peng; Yiming Xie; Zizhao Wu; Varun Jampani; Deqing Sun; Huaizu Jiang
>
> **备注:** Project Page: https://neu-vi.github.io/HOI-Diff/
>
> **摘要:** We address the problem of generating realistic 3D human-object interactions (HOIs) driven by textual prompts. To this end, we take a modular design and decompose the complex task into simpler sub-tasks. We first develop a dual-branch diffusion model (HOI-DM) to generate both human and object motions conditioned on the input text, and encourage coherent motions by a cross-attention communication module between the human and object motion generation branches. We also develop an affordance prediction diffusion model (APDM) to predict the contacting area between the human and object during the interactions driven by the textual prompt. The APDM is independent of the results by the HOI-DM and thus can correct potential errors by the latter. Moreover, it stochastically generates the contacting points to diversify the generated motions. Finally, we incorporate the estimated contacting points into the classifier-guidance to achieve accurate and close contact between humans and objects. To train and evaluate our approach, we annotate BEHAVE dataset with text descriptions. Experimental results on BEHAVE and OMOMO demonstrate that our approach produces realistic HOIs with various interactions and different types of objects.
>
---
#### [replaced 178] LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.00505v3](http://arxiv.org/pdf/2507.00505v3)**

> **作者:** Haoran Lou; Chunxiao Fan; Ziyan Liu; Yuexin Wu; Xinliang Wang
>
> **备注:** Accepted to ICCV 2025
>
> **摘要:** The architecture of multimodal large language models (MLLMs) commonly connects a vision encoder, often based on CLIP-ViT, to a large language model. While CLIP-ViT works well for capturing global image features, it struggles to model local relationships between adjacent patches, leading to weaker visual representation, which in turn affects the detailed understanding ability of MLLMs. To solve this, we propose LLaVA-SP, which only adds six spatial visual tokens to the original visual tokens to enhance the visual representation. Our approach offers three key advantages: 1) We propose a novel Projector, which uses convolutional kernels to derive visual spatial tokens from ViT patch features, simulating two visual spatial ordering approaches: "from central region to global" and "from abstract to specific". Then, a cross-attention mechanism is applied to fuse fine-grained visual information, enriching the overall visual representation. 2) We present two model variants: LLaVA-SP-Cropping, which focuses on detail features through progressive cropping, and LLaVA-SP-Pooling, which captures global semantics through adaptive pooling, enabling the model to handle diverse visual understanding tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA, achieves significant performance improvements across various multimodal benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple tasks with nearly identical inference latency. The code and models are available at https://github.com/CnFaker/LLaVA-SP.
>
---
#### [replaced 179] SEAL: Vision-Language Model-Based Safe End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling
- **分类: cs.RO; cs.AI; cs.CV**

- **链接: [http://arxiv.org/pdf/2506.21041v2](http://arxiv.org/pdf/2506.21041v2)**

> **作者:** Junwei You; Pei Li; Zhuoyu Jiang; Zilin Huang; Rui Gan; Haotian Shi; Bin Ran
>
> **摘要:** Autonomous driving technologies face significant safety challenges while operating under rare, diverse, and visually degraded weather scenarios. These challenges become more critical in cooperative settings, where vehicles and infrastructure jointly perceive and reason across complex environments. To address these issues, we propose SEAL, a vision-language model-based framework with adaptive multimodal learning for robust cooperative autonomous driving under long-tail scenarios. SEAL introduces three core innovations: (i) a prompt-driven long-tail scenario generation and evaluation pipeline that leverages foundation models to synthesize realistic long-tail conditions such as snow and fog across vehicle- and infrastructure-side views, enriching training diversity efficiently; (ii) a gated multi-scenario adaptive attention module that modulates the visual stream using scenario priors to recalibrate ambiguous or corrupted features; and (iii) a multi-task scenario-aware contrastive learning objective that improves multimodal alignment and promotes cross-scenario feature separability. Extensive experiments demonstrate that SEAL significantly outperforms existing baselines in reasoning, safety, and planning accuracy under complex, challenging driving conditions, advancing the safety, robustness, and scalability of autonomous driving.
>
---
#### [replaced 180] EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2505.05209v3](http://arxiv.org/pdf/2505.05209v3)**

> **作者:** Haizhen Xie; Kunpeng Du; Qiangyu Yan; Sen Lu; Jianhong Han; Hanting Chen; Hailin Hu; Jie Hu
>
> **备注:** Some parts of the thesis require further improvement
>
> **摘要:** Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind Super-Resolution (BSR) has become a predominant approach in the field. While T2I models have traditionally relied on U-Net architectures, recent advancements have demonstrated that Diffusion Transformers (DiT) achieve significantly higher performance in this domain. In this work, we introduce Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and outperforms previous U-Net-based approaches. We introduce a novel block, $\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance capabilities of T2I models and enhance their generalization in BSR, we introduce a progressive Masked Image Modeling strategy, which also reduces training costs. Additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. This strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of T2I diffusion priors. Our experiments demonstrate that EAM achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality.
>
---
#### [replaced 181] CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2506.23347v2](http://arxiv.org/pdf/2506.23347v2)**

> **作者:** Yi Liu; Shengqian Li; Zuzeng Lin; Feng Wang; Si Liu
>
> **备注:** Accepted to ICCV 2025. Code available at: https://github.com/IamCreateAI/CycleVAR
>
> **摘要:** The current conditional autoregressive image generation methods have shown promising results, yet their potential remains largely unexplored in the practical unsupervised image translation domain, which operates without explicit cross-domain correspondences. A critical limitation stems from the discrete quantization inherent in traditional Vector Quantization-based frameworks, which disrupts gradient flow between the Variational Autoencoder decoder and causal Transformer, impeding end-to-end optimization during adversarial training in image space. To tackle this issue, we propose using Softmax Relaxed Quantization, a novel approach that reformulates codebook selection as a continuous probability mixing process via Softmax, thereby preserving gradient propagation. Building upon this differentiable foundation, we introduce CycleVAR, which reformulates image-to-image translation as image-conditional visual autoregressive generation by injecting multi-scale source image tokens as contextual prompts, analogous to prefix-based conditioning in language models. CycleVAR exploits two modes to generate the target image tokens, including (1) serial multi-step generation, enabling iterative refinement across scales, and (2) parallel one-step generation synthesizing all resolution outputs in a single forward pass. Experimental findings indicate that the parallel one-step generation mode attains superior translation quality with quicker inference speed than the serial multi-step mode in unsupervised scenarios. Furthermore, both quantitative and qualitative results indicate that CycleVAR surpasses previous state-of-the-art unsupervised image translation models, \textit{e}.\textit{g}., CycleGAN-Turbo.
>
---
#### [replaced 182] Semantic Alignment and Reinforcement for Data-Free Quantization of Vision Transformers
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2412.16553v4](http://arxiv.org/pdf/2412.16553v4)**

> **作者:** Yunshan Zhong; Yuyao Zhou; Yuxin Zhang; Wanchen Sui; Shen Li; Yong Li; Fei Chao; Rongrong Ji
>
> **备注:** ICCV2025
>
> **摘要:** Data-free quantization (DFQ) enables model quantization without accessing real data, addressing concerns regarding data security and privacy. With the growing adoption of Vision Transformers (ViTs), DFQ for ViTs has garnered significant attention. However, existing DFQ methods exhibit two limitations: (1) semantic distortion, where the semantics of synthetic images deviate substantially from those of real images, and (2) semantic inadequacy, where synthetic images contain extensive regions with limited content and oversimplified textures, leading to suboptimal quantization performance. To address these limitations, we propose SARDFQ, a novel Semantics Alignment and Reinforcement Data-Free Quantization method for ViTs. To address semantic distortion, SARDFQ incorporates Attention Priors Alignment (APA), which optimizes synthetic images to follow randomly generated structure attention priors. To mitigate semantic inadequacy, SARDFQ introduces Multi-Semantic Reinforcement (MSR), leveraging localized patch optimization to enhance semantic richness across synthetic images. Furthermore, SARDFQ employs Soft-Label Learning (SL), wherein multiple semantic targets are adapted to facilitate the learning of multi-semantic images augmented by MSR. Extensive experiments demonstrate the effectiveness of SARDFQ, significantly surpassing existing methods. For example, SARDFQ improves top-1 accuracy on ImageNet by 15.52% for W4A4 ViT-B. The code is at https://github.com/zysxmu/SARDFQ.
>
---
#### [replaced 183] DynamicFace: High-Quality and Consistent Face Swapping for Image and Video using Composable 3D Facial Priors
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2501.08553v2](http://arxiv.org/pdf/2501.08553v2)**

> **作者:** Runqi Wang; Yang Chen; Sijie Xu; Tianyao He; Wei Zhu; Dejia Song; Nemo Chen; Xu Tang; Yao Hu
>
> **备注:** Accepted by ICCV 2025. Project page: https://dynamic-face.github.io/
>
> **摘要:** Face swapping transfers the identity of a source face to a target face while retaining the attributes like expression, pose, hair, and background of the target face. Advanced face swapping methods have achieved attractive results. However, these methods often inadvertently transfer identity information from the target face, compromising expression-related details and accurate identity. We propose a novel method DynamicFace that leverages the power of diffusion models and plug-and-play adaptive attention layers for image and video face swapping. First, we introduce four fine-grained facial conditions using 3D facial priors. All conditions are designed to be disentangled from each other for precise and unique control. Then, we adopt Face Former and ReferenceNet for high-level and detailed identity injection. Through experiments on the FF++ dataset, we demonstrate that our method achieves state-of-the-art results in face swapping, showcasing superior image quality, identity preservation, and expression accuracy. Our framework seamlessly adapts to both image and video domains. Our code and results will be available on the project page: https://dynamic-face.github.io/
>
---
#### [replaced 184] Label-free evaluation of lung and heart transplant biopsies using tissue autofluorescence-based virtual staining
- **分类: physics.med-ph; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2409.05255v2](http://arxiv.org/pdf/2409.05255v2)**

> **作者:** Yuzhu Li; Nir Pillar; Tairan Liu; Guangdong Ma; Yuxuan Qi; Kevin de Haan; Yijie Zhang; Xilin Yang; Adrian J. Correa; Guangqian Xiao; Kuang-Yu Jen; Kenneth A. Iczkowski; Yulun Wu; William Dean Wallace; Aydogan Ozcan
>
> **备注:** 25 Pages, 5 Figures
>
> **摘要:** Organ transplantation serves as the primary therapeutic strategy for end-stage organ failures. However, allograft rejection is a common complication of organ transplantation. Histological assessment is essential for the timely detection and diagnosis of transplant rejection and remains the gold standard. Nevertheless, the traditional histochemical staining process is time-consuming, costly, and labor-intensive. Here, we present a panel of virtual staining neural networks for lung and heart transplant biopsies, which digitally convert autofluorescence microscopic images of label-free tissue sections into their brightfield histologically stained counterparts, bypassing the traditional histochemical staining process. Specifically, we virtually generated Hematoxylin and Eosin (H&E), Masson's Trichrome (MT), and Elastic Verhoeff-Van Gieson (EVG) stains for label-free transplant lung tissue, along with H&E and MT stains for label-free transplant heart tissue. Subsequent blind evaluations conducted by three board-certified pathologists have confirmed that the virtual staining networks consistently produce high-quality histology images with high color uniformity, closely resembling their well-stained histochemical counterparts across various tissue features. The use of virtually stained images for the evaluation of transplant biopsies achieved comparable diagnostic outcomes to those obtained via traditional histochemical staining, with a concordance rate of 82.4% for lung samples and 91.7% for heart samples. Moreover, virtual staining models create multiple stains from the same autofluorescence input, eliminating structural mismatches observed between adjacent sections stained in the traditional workflow, while also saving tissue, expert time, and staining costs.
>
---
#### [replaced 185] BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression
- **分类: eess.IV; cs.CV**

- **链接: [http://arxiv.org/pdf/2505.09193v3](http://arxiv.org/pdf/2505.09193v3)**

> **作者:** Wei Jiang; Junru Li; Kai Zhang; Li Zhang
>
> **备注:** Accepted to ACMMM 2025
>
> **摘要:** Recent forward prediction-based learned video compression (LVC) methods have achieved impressive results, even surpassing VVC reference software VTM under the Low Delay B (LDB) configuration. In contrast, learned bidirectional video compression (BVC) remains underexplored and still lags behind its forward-only counterparts. This performance gap is mainly due to the limited ability to extract diverse and accurate contexts: most existing BVCs primarily exploit temporal motion while neglecting non-local correlations across frames. Moreover, they lack the adaptability to dynamically suppress harmful contexts arising from fast motion or occlusion. To tackle these challenges, we propose BiECVC, a BVC framework that incorporates diversified local and non-local context modeling along with adaptive context gating. For local context enhancement, BiECVC reuses high-quality features from lower layers and aligns them using decoded motion vectors without introducing extra motion overhead. To model non-local dependencies efficiently, we adopt a linear attention mechanism that balances performance and complexity. To further mitigate the impact of inaccurate context prediction, we introduce Bidirectional Context Gating, inspired by data-dependent decay in recent autoregressive language models, to dynamically filter contextual information based on conditional coding results. Extensive experiments demonstrate that BiECVC achieves state-of-the-art performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2 under the Random Access (RA) configuration with intra periods of 32 and 64, respectively. To our knowledge, BiECVC is the first learned video codec to surpass VTM 13.2 RA across all standard test datasets. Code will be available at https://github.com/JiangWeibeta/ECVC.
>
---
#### [replaced 186] FruitNinja: 3D Object Interior Texture Generation with Gaussian Splatting
- **分类: cs.CV; cs.GR; cs.HC**

- **链接: [http://arxiv.org/pdf/2411.12089v3](http://arxiv.org/pdf/2411.12089v3)**

> **作者:** Fangyu Wu; Yuhao Chen
>
> **备注:** accepted in CVPR 2025, project page https://fanguw.github.io/FruitNinja3D
>
> **摘要:** In the real world, objects reveal internal textures when sliced or cut, yet this behavior is not well-studied in 3D generation tasks today. For example, slicing a virtual 3D watermelon should reveal flesh and seeds. Given that no available dataset captures an object's full internal structure and collecting data from all slices is impractical, generative methods become the obvious approach. However, current 3D generation and inpainting methods often focus on visible appearance and overlook internal textures. To bridge this gap, we introduce FruitNinja, the first method to generate internal textures for 3D objects undergoing geometric and topological changes. Our approach produces objects via 3D Gaussian Splatting (3DGS) with both surface and interior textures synthesized, enabling real-time slicing and rendering without additional optimization. FruitNinja leverages a pre-trained diffusion model to progressively inpaint cross-sectional views and applies voxel-grid-based smoothing to achieve cohesive textures throughout the object. Our OpaqueAtom GS strategy overcomes 3DGS limitations by employing densely distributed opaque Gaussians, avoiding biases toward larger particles that destabilize training and sharp color transitions for fine-grained textures. Experimental results show that FruitNinja substantially outperforms existing approaches, showcasing unmatched visual quality in real-time rendered internal views across arbitrary geometry manipulations.
>
---
#### [replaced 187] LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.00554v2](http://arxiv.org/pdf/2507.00554v2)**

> **作者:** Zhenya Yang; Bingchen Gong; Kai Chen
>
> **摘要:** Despite the advancements in quality and efficiency achieved by 3D Gaussian Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent challenge. Existing approaches primarily rely on low-pass filtering to mitigate aliasing. However, these methods are not sensitive to the sampling rate, often resulting in under-filtering and over-smoothing renderings. To address this limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework for Gaussian Splatting, which dynamically predicts the optimal filtering strength for each 3D Gaussian primitive. Specifically, we introduce a set of basis functions to each Gaussian, which take the sampling rate as input to model appearance variations, enabling sampling-rate-sensitive filtering. These basis function parameters are jointly optimized with the 3D Gaussian in an end-to-end manner. The sampling rate is influenced by both focal length and camera distance. However, existing methods and datasets rely solely on down-sampling to simulate focal length changes for anti-aliasing evaluation, overlooking the impact of camera distance. To enable a more comprehensive assessment, we introduce a new synthetic dataset featuring objects rendered at varying camera distances. Extensive experiments on both public datasets and our newly collected dataset demonstrate that our method achieves SOTA rendering quality while effectively eliminating aliasing. The code and dataset have been open-sourced.
>
---
#### [replaced 188] OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes
- **分类: cs.CV**

- **链接: [http://arxiv.org/pdf/2507.01455v2](http://arxiv.org/pdf/2507.01455v2)**

> **作者:** Yuxing Liu; Ji Zhang; Zhou Xuchuan; Jingzhong Xiao; Huimin Yang; Jiaxin Zhong
>
> **备注:** Accepted by ACM MM2025; 12 pages, 5 figures
>
> **摘要:** Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous objects within images. Existing pixel-wise methods typically assign anomaly scores individually and employ a global thresholding strategy to segment anomalies. Despite their effectiveness, these approaches encounter significant challenges in real-world applications: (1) neglecting spatial correlations among pixels within the same object, resulting in fragmented segmentation; (2) variabil ity in anomaly score distributions across image regions, causing global thresholds to either generate false positives in background areas or miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel multi-level anomaly segmentation framework designed to address these limitations through a coarse-to-fine anomaly detection strategy. OoDDINO combines an uncertainty-guided anomaly detection model with a pixel-level segmentation model within a two-stage cascade architecture. Initially, we propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that sequentially integrates multiple uncertainty metrics with visual representations, employing orthogonal constraints to strengthen the detection model's capacity for localizing anomalous regions accurately. Subsequently, we develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically generates region-specific thresholds based on object-level detection outputs and pixel-wise anomaly scores. This approach allows for distinct thresholding strategies within foreground and background areas, achieving fine-grained anomaly segmentation. The proposed framework is compatible with other pixel-wise anomaly detection models, which acts as a plug-in to boost the performance. Extensive experiments on two benchmark datasets validate our framework's superiority and compatibility over state-of-the-art methods.
>
---
#### [replaced 189] Establishing Causal Relationship Between Whole Slide Image Predictions and Diagnostic Evidence Subregions in Deep Learning
- **分类: cs.CV; q-bio.TO**

- **链接: [http://arxiv.org/pdf/2407.17157v3](http://arxiv.org/pdf/2407.17157v3)**

> **作者:** Tianhang Nan; Yong Ding; Hao Quan; Deliang Li; Lisha Li; Guanghong Zhao; Xiaoyu Cui
>
> **摘要:** Due to the lack of fine-grained annotation guidance, current Multiple Instance Learning (MIL) struggles to establish a robust causal relationship between Whole Slide Image (WSI) diagnosis and evidence sub-images, just like fully supervised learning. So many noisy images can undermine the network's prediction. The proposed Causal Inference Multiple Instance Learning (CI-MIL), uses out-of-distribution generalization to reduce the recognition confusion of sub-images by MIL network, without requiring pixelwise annotations. Specifically, feature distillation is introduced to roughly identify the feature representation of lesion patches. Then, in the random Fourier feature space, these features are re-weighted to minimize the cross-correlation, effectively correcting the feature distribution deviation. These processes reduce the uncertainty when tracing the prediction results back to patches. Predicted diagnoses are more direct and reliable because the causal relationship between them and diagnostic evidence images is more clearly recognized by the network. Experimental results demonstrate that CI-MIL outperforms state-of-the-art methods, achieving 92.25% accuracy and 95.28% AUC on the Camelyon16 dataset (breast cancer), while 94.29% accuracy and 98.07% AUC on the TCGA-NSCLC dataset (non-small cell lung cancer). Additionally, CI-MIL exhibits superior interpretability, as its selected regions demonstrate high consistency with ground truth annotations, promising more reliable diagnostic assistance for pathologists.
>
---
#### [replaced 190] VGMShield: Mitigating Misuse of Video Generative Models
- **分类: cs.CR; cs.AI; cs.CV; cs.LG; eess.IV**

- **链接: [http://arxiv.org/pdf/2402.13126v2](http://arxiv.org/pdf/2402.13126v2)**

> **作者:** Yan Pang; Baicheng Chen; Yang Zhang; Tianhao Wang
>
> **备注:** 18 pages
>
> **摘要:** With the rapid advancement in video generation, people can conveniently use video generation models to create videos tailored to their specific desires. As a result, there are also growing concerns about the potential misuse of video generation for spreading illegal content and misinformation. In this work, we introduce VGMShield: a set of straightforward but effective mitigations through the lifecycle of fake video generation. We start from fake video detection, trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the fake video source tracing problem, which maps a fake video back to the model that generated it. Towards these, we propose to leverage pre-trained models that focus on spatial-temporal dynamics as the backbone to identify inconsistencies in videos. In detail, we analyze fake videos from the perspective of the generation process. Based on the observation of attention shifts, motion variations, and frequency fluctuations, we identify common patterns in the generated video. These patterns serve as the foundation for our experiments on fake video detection and source tracing. Through experiments on seven state-of-the-art open-source models, we demonstrate that current models still cannot reliably reproduce spatial-temporal relationships, and thus, we can accomplish detection and source tracing with over 90% accuracy. Furthermore, anticipating future generative model improvements, we propose a prevention method that adds invisible perturbations to the query images to make the generated videos look unreal. Together with detection and tracing, our multi-faceted set of solutions can effectively mitigate misuse of video generative models.
>
---
