# 计算机与社会 cs.CY

- **最新发布 33 篇**

- **更新 21 篇**

## 最新发布

#### [new 001] Student Engagement with GenAI's Tutoring Feedback: A Mixed Methods Study
- **分类: cs.CY; cs.HC**

- **简介: 该论文属于教育技术领域，研究学生如何与AI生成的编程辅导反馈互动。通过混合方法分析学生对反馈的反应及行为，旨在提升反馈效果与教学设计。**

- **链接: [http://arxiv.org/pdf/2509.22974v1](http://arxiv.org/pdf/2509.22974v1)**

> **作者:** Sven Jacobs; Jan Haas; Natalie Kiesler
>
> **备注:** Accepted for the 25th International Conference on Computing Education Research (Koli Calling '25)
>
> **摘要:** How students utilize immediate tutoring feedback in programming education depends on various factors. Among them are the feedback quality, but also students' engagement, i.e., their perception, interpretation, and use of feedback. However, there is limited research on how students engage with various types of tutoring feedback. For this reason, we developed a learning environment that provides students with Python programming tasks and various types of immediate, AI-generated tutoring feedback. The feedback is displayed within four components. Using a mixed-methods approach (think-aloud study and eye-tracking), we conducted a study with 20 undergraduate students enrolled in an introductory programming course. Our research aims to: (1) identify what students think when they engage with the tutoring feedback components, and (2) explore the relations between the tutoring feedback components, students' visual attention, verbalized thoughts, and their immediate actions as part of the problem-solving process. The analysis of students' thoughts while engaging with 380 feedback components revealed four main themes: students express understanding or disagreement, additional information needed, and students explicitly judge the feedback. Exploring the relations between feedback, students' attention, thoughts, and actions showed a clear relationship. While expressions of understanding were associated with improvements, expressions of disagreement or need for additional information prompted students to collect another feedback component rather than act on the current information. These insights into students' engagement and decision-making processes contribute to an increased understanding of tutoring feedback and how students engage with it. Thereby, this work has implications for tool developers and educators facilitating feedback.
>
---
#### [new 002] Regulating Online Algorithmic Pricing: A Comparative Study of Privacy and Data Protection Laws in the EU and US
- **分类: cs.CY**

- **简介: 该论文属于法律比较研究任务，旨在分析欧盟与美国在在线算法定价中的隐私与数据保护法规，评估其有效性并为彼此提供借鉴。**

- **链接: [http://arxiv.org/pdf/2509.24345v1](http://arxiv.org/pdf/2509.24345v1)**

> **作者:** Zihao Li
>
> **备注:** Stanford TTLF Working Paper
>
> **摘要:** The emergence of big data, AI and machine learning has allowed sellers and online platforms to tailor pricing for customers in real-time. While online algorithmic pricing can increase efficiency, market welfare, and optimize pricing strategies for sellers and companies, it poses a threat to the fundamental values of privacy, digital autonomy, and non-discrimination, raising legal and ethical concerns. On both sides of the Atlantic, legislators have endeavoured to regulate online algorithmic pricing in different ways in the context of privacy and personal data protection. Represented by the GDPR, the EU adopts an omnibus approach to regulate algorithmic pricing and is supplemented by the Digital Service Act and the Digital Market Act. The US combines federal and state laws to regulate online algorithmic pricing and focuses on industrial regulations. Therefore, a comparative analysis of these legal frameworks is necessary to ascertain the effectiveness of these approaches. Taking a comparative approach, this working paper aims to explore how EU and US respective data protection and privacy laws address the issues posed by online algorithmic pricing. The paper evaluates whether the current legal regime is effective in protecting individuals against the perils of online algorithmic pricing in the EU and the US. It particularly analyses the new EU regulatory paradigm, the Digital Service Act (DSA) and the Digital Market Act (DMA), as supplementary mechanisms to the EU data protection law, in order to draw lessons for US privacy law and vice versa.
>
---
#### [new 003] AI Education in Higher Education: A Taxonomy for Curriculum Reform and the Mission of Knowledge
- **分类: cs.CY; cs.AI; 68T01; K.3.2; I.2**

- **简介: 该论文属于教育改革任务，旨在解决AI在高等教育中的课程整合问题。通过构建分类框架，推动课程改革与知识使命的结合。**

- **链接: [http://arxiv.org/pdf/2509.23363v1](http://arxiv.org/pdf/2509.23363v1)**

> **作者:** Tian Zheng
>
> **备注:** 29 pages, 2 figures, Appendix
>
> **摘要:** Artificial intelligence (AI) is reshaping higher education, yet current debates often feel tangled, mixing concerns about pedagogy, operations, curriculum, and the future of work without a shared framework. This paper offers a first attempt at a taxonomy to organize the diverse narratives of AI education and to inform discipline-based curricular discussions. We place these narratives within the enduring responsibility of higher education: the mission of knowledge. This mission includes not only the preservation and advancement of disciplinary expertise, but also the cultivation of skills and wisdom, i.e., forms of meta-knowledge that encompass judgment, ethics, and social responsibility. For the purpose of this paper's discussion, AI is defined as adaptive, data-driven systems that automate analysis, modeling, and decision-making, highlighting its dual role as enabler and disruptor across disciplines. We argue that the most consequential challenges lie at the level of curriculum and disciplinary purpose, where AI accelerates inquiry but also unsettles expertise and identity. We show how disciplines evolve through the interplay of research, curriculum, pedagogy, and faculty expertise, and why curricular reform is the central lever for meaningful change. Pedagogical innovation offers a strategic and accessible entry point, providing actionable steps that help faculty and students build the expertise needed to engage in deeper curricular rethinking and disciplinary renewal. Within this framing, we suggest that meaningful reform can move forward through structured faculty journeys: from AI literacy to pedagogy, curriculum design, and research integration. The key is to align these journeys with the mission of knowledge, turning the disruptive pressures of AI into opportunities for disciplines to sustain expertise, advance inquiry, and serve society.
>
---
#### [new 004] Anti-Regulatory AI: How "AI Safety" is Leveraged Against Regulatory Oversight
- **分类: cs.CY**

- **简介: 该论文属于AI治理研究，探讨AI技术如何被用作规避监管的工具。分析了隐私技术与安全措施如何影响法规执行，揭示其背后的监管规避机制。**

- **链接: [http://arxiv.org/pdf/2509.22872v1](http://arxiv.org/pdf/2509.22872v1)**

> **作者:** Rui-Jie Yew; Brian Judge
>
> **备注:** Forthcoming at EAAMO 2025
>
> **摘要:** AI companies increasingly develop and deploy privacy-enhancing technologies, bias-constraining measures, evaluation frameworks, and alignment techniques -- framing them as addressing concerns related to data privacy, algorithmic fairness, and AI safety. This paper examines the ulterior function of these technologies as mechanisms of legal influence. First, we examine how encryption, federated learning, and synthetic data -- presented as enhancing privacy and reducing bias -- can operate as mechanisms of avoidance with existing regulations in attempts to place data operations outside the scope of traditional regulatory frameworks. Second, we investigate how emerging AI safety practices including open-source model releases, evaluations, and alignment techniques can be used as mechanisms of change that direct regulatory focus towards industry-controlled voluntary standards and self-governance. We term this phenomenon anti-regulatory AI -- the deployment of ostensibly protective technologies that simultaneously shapes the terms of regulatory oversight. Our analysis additionally reveals how technologies' anti-regulatory functions are enabled through framing that legitimizes their deployment while obscuring their use as regulatory workarounds. This paper closes with a discussion of policy implications that centers on the consideration of business incentives that drive AI development and the role of technical expertise in assessing whether these technologies fulfill their purported protections.
>
---
#### [new 005] A Data-Driven Framework for Digital Transformation in Smart Cities: Integrating AI, Dashboards, and IoT Readiness
- **分类: cs.CY; cs.AI; cs.LG; I.2.0**

- **简介: 该论文属于智能城市数字转型任务，旨在评估公共机构的数字化水平。通过结合AI与调查数据，提出一种自动评估方法，并在西班牙瓦伦西亚地区验证其有效性。**

- **链接: [http://arxiv.org/pdf/2509.22721v1](http://arxiv.org/pdf/2509.22721v1)**

> **作者:** Ángel Lloret; Jesús Peral; Antonio Ferrández; María Auladell; Rafael Muñoz
>
> **备注:** 30 pages, 5 figures
>
> **摘要:** Digital transformation (DT) has become a strategic priority for public administrations, particularly due to the need to deliver more efficient and citizen-centered services and respond to societal expectations, ESG (Environmental, Social, and Governance) criteria, and the United Nations Sustainable Development Goals (UN SDGs). In this context, the main objective of this study is to propose an innovative methodology to automatically evaluate the level of digital transformation (DT) in public sector organizations. The proposed approach combines traditional assessment methods with Artificial Intelligence (AI) techniques. The methodology follows a dual approach: on the one hand, surveys are conducted using specialized staff from various public entities; on the other, AI-based models (including neural networks and transformer architectures) are used to estimate the DT level of the organizations automatically. Our approach has been applied to a real-world case study involving local public administrations in the Valencian Community (Spain) and shown effective performance in assessing DT. While the proposed methodology has been validated in a specific local context, its modular structure and dual-source data foundation support its international scalability, acknowledging that administrative, regulatory, and DT maturity factors may condition its broader applicability. The experiments carried out in this work include (i) the creation of a domain-specific corpus derived from the surveys and websites of several organizations, used to train the proposed models; (ii) the use and comparison of diverse AI methods; and (iii) the validation of our approach using real data. The integration of technologies such as the IoT, sensor networks, and AI-based analytics can significantly support resilient, agile urban environments and the transition towards more effective and sustainable Smart City models.
>
---
#### [new 006] Trust and Transparency in AI: Industry Voices on Data, Ethics, and Compliance
- **分类: cs.CY; cs.SY; eess.SY**

- **简介: 该论文属于AI伦理研究，探讨如何实现可信AI。针对AI发展中的透明度、合规性与伦理问题，通过访谈和文献分析，提出风险管理和价值对齐的解决方案。**

- **链接: [http://arxiv.org/pdf/2509.22709v1](http://arxiv.org/pdf/2509.22709v1)**

> **作者:** Louise McCormack; Diletta Huyskes; Dave Lewis; Malika Bendechache
>
> **摘要:** The EU Artificial Intelligence (AI) Act directs businesses to assess their AI systems to ensure they are developed in a way that is human-centered and trustworthy. The rapid adoption of AI in the industry has outpaced ethical evaluation frameworks, leading to significant challenges in accountability, governance, data quality, human oversight, technological robustness, and environmental and societal impacts. Through structured interviews with fifteen industry professionals, paired with a literature review conducted on each of the key interview findings, this paper investigates practical approaches and challenges in the development and assessment of Trustworthy AI (TAI). The findings from participants in our study, and the subsequent literature reviews, reveal complications in risk management, compliance and accountability, which are exacerbated by a lack of transparency, unclear regulatory requirements and a rushed implementation of AI. Participants reported concerns that technological robustness and safety could be compromised by model inaccuracies, security vulnerabilities, and an overreliance on AI without proper safeguards in place. Additionally, the negative environmental and societal impacts of AI, including high energy consumption, political radicalisation, loss of culture and reinforcement of social inequalities, are areas of concern. There is a pressing need not just for risk mitigation and TAI evaluation within AI systems but for a wider approach to developing an AI landscape that aligns with the social and cultural values of the countries adopting those technologies.
>
---
#### [new 007] (When) Should We Delegate AI Governance to AIs? Some Lessons from Administrative Law
- **分类: cs.CY**

- **简介: 该论文属于AI治理研究，探讨何时应将治理任务委托给AI，解决AI决策透明度与合法性问题，借鉴行政法原则提出安全有效的委托框架。**

- **链接: [http://arxiv.org/pdf/2509.22717v1](http://arxiv.org/pdf/2509.22717v1)**

> **作者:** Nicholas Caputo
>
> **备注:** 4 pages, Regulatable ML Workshop at NeurIPS '25
>
> **摘要:** Advanced AI systems are now being used in AI governance. Practitioners will likely delegate an increasing number of tasks to them as they improve and governance becomes harder. However, using AI for governance risks serious harms because human practitioners may not be able to understand AI decisions or determine whether they are aligned to the user's interests. Delegation may also undermine governance's legitimacy. This paper begins to develop a principled framework for when to delegate AI governance to AIs and when (and how) to maintain human participation. Administrative law, which governs agencies that are (1) more expert in their domains than the legislatures that create them and the courts that oversee them and (2) potentially misaligned to their original goals, offers useful lessons. Administrative law doctrine provides examples of clear, articulated rules for when delegation can occur, what delegation can consist of, and what processes can keep agencies aligned even as they are empowered to achieve their goals. The lessons of administrative law provide a foundation for how AI governance can use AI in a safe, accountable, and effective way.
>
---
#### [new 008] Skill, Will, or Both? Understanding Digital Inaccessibility from Accessibility Professionals' Viewpoint
- **分类: cs.CY; acmart**

- **简介: 该论文属于数字无障碍研究任务，旨在探讨数字不平等的根源。通过调查160名专业人员，分析组织意愿、实施挑战及培训现状，以解决无障碍推进困难的问题。**

- **链接: [http://arxiv.org/pdf/2509.23287v1](http://arxiv.org/pdf/2509.23287v1)**

> **作者:** P D Parthasarathy; Rachel F. Adler; Devorah Kletenik; Swaroop Joshi; Anshu M Mittal
>
> **备注:** Accepted and published at Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan
>
> **摘要:** Digital inaccessibility continues to be a significant barrier to true inclusion and equality. WebAIM's 2024 report reveals that only 4.1% of the world's top one million website homepages are fully accessible. Furthermore, the percentage of web pages with detectable Web Content Accessibility Guidelines (WCAG) failures has only decreased by 1.9\% over the past five years, from 97.8%. To gain deeper insights into the persistent challenges of digital accessibility, we conducted a comprehensive survey with 160 accessibility professionals. Unlike previous studies, which often focused on technology professionals, our research examines inaccessibility through the lens of dedicated accessibility professionals, offering a more detailed analysis of the barriers they face. Our investigation explores (a) organizations' willingness to prioritize accessibility, (b) the challenges in ensuring accessibility, and (c) the current accessibility training practices in technology workspaces. This study aims to provide an updated perspective on the state of digital accessibility from the point of view of accessibility professionals.
>
---
#### [new 009] Regulating the Agency of LLM-based Agents
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI安全任务，旨在解决LLM代理失控风险。提出通过测量和控制代理的自主性来降低潜在危害。**

- **链接: [http://arxiv.org/pdf/2509.22735v1](http://arxiv.org/pdf/2509.22735v1)**

> **作者:** Seán Boddy; Joshua Joseph
>
> **备注:** 4 pages
>
> **摘要:** As increasingly capable large language model (LLM)-based agents are developed, the potential harms caused by misalignment and loss of control grow correspondingly severe. To address these risks, we propose an approach that directly measures and controls the agency of these AI systems. We conceptualize the agency of LLM-based agents as a property independent of intelligence-related measures and consistent with the interdisciplinary literature on the concept of agency. We offer (1) agency as a system property operationalized along the dimensions of preference rigidity, independent operation, and goal persistence, (2) a representation engineering approach to the measurement and control of the agency of an LLM-based agent, and (3) regulatory tools enabled by this approach: mandated testing protocols, domain-specific agency limits, insurance frameworks that price risk based on agency, and agency ceilings to prevent societal-scale risks. We view our approach as a step toward reducing the risks that motivate the ``Scientist AI'' paradigm, while still capturing some of the benefits from limited agentic behavior.
>
---
#### [new 010] Price discrimination, algorithmic decision-making, and European non-discrimination law
- **分类: cs.CY**

- **简介: 该论文属于法律与技术交叉研究，探讨算法决策中的价格歧视问题，分析非歧视法在应对算法歧视时的局限性。**

- **链接: [http://arxiv.org/pdf/2509.23851v1](http://arxiv.org/pdf/2509.23851v1)**

> **作者:** Frederik Zuiderveen Borgesius
>
> **摘要:** Our society can benefit immensely from algorithmic decision-making and similar types of artificial intelligence. But algorithmic decision-making can also have discriminatory effects. This paper examines that problem, using online price differentiation as an example of algorithmic decision-making. With online price differentiation, a company charges different people different prices for identical products, based on information the company has about those people. The main question in this paper is: to what extent can non-discrimination law protect people against online price differentiation? The paper shows that online price differentiation and algorithmic decision-making could lead to indirect discrimination, for instance harming people with a certain ethnicity. Indirect discrimination occurs when a practice is neutral at first glance, but ends up discriminating against people with a protected characteristic, such as ethnicity. In principle, non-discrimination law prohibits indirect discrimination. The paper also shows, however, that non-discrimination law has flaws when applied to algorithmic decision-making. For instance, algorithmic discrimination can remain hidden: people may not realise that they are being discriminated against. And many types of unfair - some might say discriminatory - algorithmic decisions are outside the scope of current non-discrimination law.
>
---
#### [new 011] Scaling Accessibility Education: Reflections from a Workshop Targeting CS Educators and Software Professionals
- **分类: cs.CY**

- **简介: 该论文属于教育研究任务，旨在解决印度计算教育和软件行业在数字可访问性培训上的不足。通过设计并实施一天的实践工作坊，提升参与者对可访问性实践的理解和应用能力。**

- **链接: [http://arxiv.org/pdf/2509.22759v1](http://arxiv.org/pdf/2509.22759v1)**

> **作者:** P D Parthasarathy; Anshu M Mittal; Swaroop Joshi
>
> **备注:** Accepted for publication at ACM Compute 2025 - https://isigcse.acm.org/compute/2025/
>
> **摘要:** Despite growing global attention to digital accessibility, research from India highlights a significant gap in accessibility training for both computing educators and software professionals. To address this need, we designed and conducted an experiential workshop aimed at building foundational capacity in accessibility practices among 77 participants, including computer science (CS) faculty and industry practitioners. The one-day workshop combined hands-on activities, tool demonstrations, and case studies to foster practical understanding and engagement. Post-workshop feedback showed that a majority of participants rated the workshop positively, with many reporting increased confidence and a shift in their perception of accessibility as a shared responsibility. Additionally, participants expressed a strong interest in applying accessibility principles within their workplaces, underscoring the workshop's practical relevance and impact. In this experience report, we detail the workshop's design, implementation, and evaluation, and offer actionable insights to guide future initiatives aimed at strengthening accessibility capacity across India's computing education and professional landscape.
>
---
#### [new 012] Societal Capacity Assessment Framework: Measuring Resilience to Inform Advanced AI Risk Management
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI风险评估任务，旨在解决AI部署中的社会脆弱性问题。提出SCAF框架，评估社会应对AI风险的能力，促进更全面的风险管理。**

- **链接: [http://arxiv.org/pdf/2509.22742v1](http://arxiv.org/pdf/2509.22742v1)**

> **作者:** Milan Gandhi; Peter Cihon; Owen Larter; Rebecca Anselmetti
>
> **备注:** Workshop on Technical AI Governance (TAIG) at ICML 2025
>
> **摘要:** Risk assessments for advanced AI systems require evaluating both the models themselves and their deployment contexts. We introduce the Societal Capacity Assessment Framework (SCAF), an indicators-based approach to measuring a society's vulnerability, coping capacity, and adaptive capacity in response to AI-related risks. SCAF adapts established resilience analysis methodologies to AI, enabling organisations to ground risk management in insights about country-level deployment conditions. It can also support stakeholders in identifying opportunities to strengthen societal preparedness for emerging AI capabilities. By bridging disparate literatures and the "context gap" in AI evaluation, SCAF promotes more holistic risk assessment and governance as advanced AI systems proliferate globally.
>
---
#### [new 013] The 2025 OpenAI Preparedness Framework does not guarantee any AI risk mitigation practices: a proof-of-concept for affordance analyses of AI safety policies
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于AI安全评估任务，分析OpenAI安全框架的局限性，揭示其未能有效管控AI风险的问题，并提出基于可供性理论的评估方法。**

- **链接: [http://arxiv.org/pdf/2509.24394v1](http://arxiv.org/pdf/2509.24394v1)**

> **作者:** Sam Coggins; Alex Saeri; Katherine A. Daniell; Lorenn P. Ruster; Jessie Liu; Jenny L. Davis
>
> **备注:** 19 pages, 5 tables, 1 figure
>
> **摘要:** Prominent AI companies are producing 'safety frameworks' as a type of voluntary self-governance. These statements purport to establish risk thresholds and safety procedures for the development and deployment of highly capable AI. Understanding which AI risks are covered and what actions are allowed, refused, demanded, encouraged, or discouraged by these statements is vital for assessing how these frameworks actually govern AI development and deployment. We draw on affordance theory to analyse the OpenAI 'Preparedness Framework Version 2' (April 2025) using the Mechanisms & Conditions model of affordances and the MIT AI Risk Repository. We find that this safety policy requests evaluation of a small minority of AI risks, encourages deployment of systems with 'Medium' capabilities for what OpenAI itself defines as 'severe harm' (potential for >1000 deaths or >$100B in damages), and allows OpenAI's CEO to deploy even more dangerous capabilities. These findings suggest that effective mitigation of AI risks requires more robust governance interventions beyond current industry self-regulation. Our affordance analysis provides a replicable method for evaluating what safety frameworks actually permit versus what they claim.
>
---
#### [new 014] Legal Matters in Research Software: A Few Things Worth Discussing
- **分类: cs.CY**

- **简介: 该论文属于法律研究任务，探讨科研软件开发中的知识产权、许可选择及法律支持问题，旨在提升对科研软件法律认知，促进其科学价值实现。**

- **链接: [http://arxiv.org/pdf/2509.24646v1](http://arxiv.org/pdf/2509.24646v1)**

> **作者:** Giuditta Parolini
>
> **备注:** 21 pages, 2 figures, conference proceedings, submitted to Electronic Communications of the EASST
>
> **摘要:** The paper discusses legal aspects relevant to the development of research software and practical approaches taken by research software engineers to deal with them. Intellectual Property Rights on software are considered alongside licensing choices made by the research community. The discussion addresses the ambiguities in the identification of the copyright holder of research software, the uncertainty surrounding liability, and remarks the varying level of support on legal matters provided by research organisations. The paper also reflects on the widespread use of AI coding assistants in the absence of institutional policies, and on the new AI regulations passed by the European Union. The aim of the contribution is to point out that a better understanding of legal matters concerning software development is an asset in giving research software the right value it deserves as a driver of scientific progress.
>
---
#### [new 015] Open Opportunities in AI Safety, Alignment, and Ethics (AI SAE)
- **分类: cs.CY**

- **简介: 该论文属于AI伦理研究，探讨如何将道德嵌入AI系统，解决对齐与安全问题。提出道德问题空间框架，分析不同伦理理论作为研究方向。**

- **链接: [http://arxiv.org/pdf/2509.24065v1](http://arxiv.org/pdf/2509.24065v1)**

> **作者:** Dylan Waldner
>
> **摘要:** AI safety research has emphasized interpretability, control, and robustness, yet without an ethical substrate these approaches may remain fragile under competitive and open-ended pressures. This paper explores ethics not as an external add-on, but as a possible structural lens for alignment, introducing a \emph{moral problem space} $M$: a high-dimensional domain in which moral distinctions could, in principle, be represented in AI systems. Human moral reasoning is treated as a compressed and survival-biased projection $\tilde{M}$, clarifying why judgment is inconsistent while suggesting tentative methods -- such as sparse autoencoders, causal mediation, and cross-cultural corpora -- that might help probe for disentangled moral features. Within this framing, metaethical positions are interpreted as research directions: realism as the search for stable invariants, relativism as context-dependent distortions, constructivism as institutional shaping of persistence, and virtue ethics as dispositional safeguards under distributional shift. Evolutionary dynamics and institutional design are considered as forces that may determine whether ethical--symbiotic lineages remain competitively viable against more autarkic trajectories. Rather than offering solutions, the paper sketches a research agenda in which embedding ethics directly into representational substrates could serve to make philosophical claims more empirically approachable, positioning moral theory as a potential source of hypotheses for alignment work.
>
---
#### [new 016] Learning from Convenience Samples: A Case Study on Fine-Tuning LLMs for Survey Non-response in the German Longitudinal Election Study
- **分类: cs.CY; cs.CL**

- **简介: 该论文属于数据缺失处理任务，解决Survey非响应问题。通过微调LLMs填补缺失数据，比较不同方法效果，验证其在非概率样本中的有效性。**

- **链接: [http://arxiv.org/pdf/2509.25063v1](http://arxiv.org/pdf/2509.25063v1)**

> **作者:** Tobias Holtdirk; Dennis Assenmacher; Arnim Bleier; Claudia Wagner
>
> **摘要:** Survey researchers face two key challenges: the rising costs of probability samples and missing data (e.g., non-response or attrition), which can undermine inference and increase the use of convenience samples. Recent work explores using large language models (LLMs) to simulate respondents via persona-based prompts, often without labeled data. We study a more practical setting where partial survey responses exist: we fine-tune LLMs on available data to impute self-reported vote choice under both random and systematic nonresponse, using the German Longitudinal Election Study. We compare zero-shot prompting and supervised fine-tuning against tabular classifiers (e.g., CatBoost) and test how different convenience samples (e.g., students) used for fine-tuning affect generalization. Our results show that when data are missing completely at random, fine-tuned LLMs match tabular classifiers but outperform zero-shot approaches. When only biased convenience samples are available, fine-tuning small (3B to 8B) open-source LLMs can recover both individual-level predictions and population-level distributions more accurately than zero-shot and often better than tabular methods. This suggests fine-tuned LLMs offer a promising strategy for researchers working with non-probability samples or systematic missingness, and may enable new survey designs requiring only easily accessible subpopulations.
>
---
#### [new 017] Digital welfare fraud detection and the Dutch SyRI judgment
- **分类: cs.CY**

- **简介: 论文分析荷兰法院对数字福利欺诈检测系统SyRI的判决，探讨其隐私权合规问题。任务是评估算法治理与隐私保护的冲突，解决如何在自动化系统中平衡效率与权利保障的问题。**

- **链接: [http://arxiv.org/pdf/2509.23843v1](http://arxiv.org/pdf/2509.23843v1)**

> **作者:** Marvin van Bekkum; Frederik Zuiderveen Borgesius
>
> **摘要:** In 2020, a Dutch court passed judgment in a case about a digital welfare fraud detection system called Systeem Risico Indicatie (SyRI). The court ruled that the SyRI legislation is unlawful because it does not comply with the right to privacy under the European Convention of Human Rights. In this article we analyse the judgment and its implications. This ruling is one of first in which a court has invalidated a welfare fraud detection system for breaching the right to privacy. We show that the immediate effects of the judgment are limited. The judgment does not say much about automated fraud detection systems in general, because it is limited to the circumstances of the case. Still, the judgment is important. The judgment reminds policymakers that fraud detection must happen in a way that respects data protection principles and the right to privacy. The judgment also confirms the importance of transparency if personal data are used.
>
---
#### [new 018] Automated Formative Feedback for Short-form Writing: An LLM-Driven Approach and Adoption Analysis
- **分类: cs.CY; cs.AI**

- **简介: 该论文属于教育技术领域，旨在解决短篇写作反馈问题。通过开发基于LLM的工具，为工程学生提供个性化反馈，提升报告质量与完成度。**

- **链接: [http://arxiv.org/pdf/2509.22734v1](http://arxiv.org/pdf/2509.22734v1)**

> **作者:** Tiago Fernandes Tavares; Luciano Pereira Soares
>
> **摘要:** This paper explores the development and adoption of AI-based formative feedback in the context of biweekly reports in an engineering Capstone program. Each student is required to write a short report detailing their individual accomplishments over the past two weeks, which is then assessed by their advising professor. An LLM-powered tool was developed to provide students with personalized feedback on their draft reports, guiding them toward improved completeness and quality. Usage data across two rounds revealed an initial barrier to adoption, with low engagement rates. However, students who engaged in the AI feedback system demonstrated the ability to use it effectively, leading to improvements in the completeness and quality of their reports. Furthermore, the tool's task-parsing capabilities provided a novel approach to identify potential student organizational tasks and deliverables. The findings suggest initial skepticism toward the tool with a limited adoption within the studied context, however, they also highlight the potential for AI-driven tools to provide students and professors valuable insights and formative support.
>
---
#### [new 019] Opinions can be Incorrect! In our Opinion. On the accuracy principle in data protection law
- **分类: cs.CY**

- **简介: 该论文属于法律研究任务，探讨数据保护法中的准确性原则是否适用于意见。论文论证了意见也应符合准确性要求。**

- **链接: [http://arxiv.org/pdf/2509.23848v1](http://arxiv.org/pdf/2509.23848v1)**

> **作者:** Dara Hallinan; Frederik Zuiderveen Borgesius
>
> **摘要:** The GDPR contains an accuracy principle, as most data privacy laws in the world do. In principle, data controllers must ensure that personal data they use are accurate. Some have argued that the accuracy principle does not apply to personal data in the form of opinions about data subjects. We argue, however, from a positive law perspective, that the accuracy principle does apply to opinions. We further argue, from a normative perspective, that the accuracy principle should apply to opinions.
>
---
#### [new 020] Beyond Western Politics: Cross-Cultural Benchmarks for Evaluating Partisan Associations in LLMs
- **分类: cs.CY; cs.HC**

- **简介: 该论文属于AI伦理任务，旨在解决LLMs中的党派偏见问题。通过构建数据集，分析美印政治人物的代表性关联，揭示系统性风险。**

- **链接: [http://arxiv.org/pdf/2509.22711v1](http://arxiv.org/pdf/2509.22711v1)**

> **作者:** Divyanshu Kumar; Ishita Gupta; Nitin Aravind Birur; Tanay Baswa; Sahil Agarwal; Prashanth Harshangi
>
> **摘要:** Partisan bias in LLMs has been evaluated to assess political leanings, typically through a broad lens and largely in Western contexts. We move beyond identifying general leanings to examine harmful, adversarial representational associations around political leaders and parties. To do so, we create datasets \textit{NeutQA-440} (non-adversarial prompts) and \textit{AdverQA-440} (adversarial prompts), which probe models for comparative plausibility judgments across the USA and India. Results show high susceptibility to biased partisan associations and pronounced asymmetries (e.g., substantially more favorable associations for U.S. Democrats than Republicans) alongside mixed-polarity concentration around India's BJP, highlighting systemic risks and motivating standardized, cross-cultural evaluation.
>
---
#### [new 021] A Meta-Analysis of LLM Effects on Students across Qualification, Socialisation, and Subjectification
- **分类: cs.CY; cs.AI; cs.HC**

- **简介: 该论文属于教育技术领域，探讨LLM对学生学习的影响。通过元分析133项研究，评估LLM在知识、社会化和主体化方面的效果，强调设计对教育目标实现的重要性。**

- **链接: [http://arxiv.org/pdf/2509.22725v1](http://arxiv.org/pdf/2509.22725v1)**

> **作者:** Jiayu Huang; Ruoxin Ritter Wang; Jen-Hao Liu; Boming Xia; Yue Huang; Ruoxi Sun; Jason; Xue; Jinan Zou
>
> **摘要:** Large language models (LLMs) are increasingly positioned as solutions for education, yet evaluations often reduce their impact to narrow performance metrics. This paper reframes the question by asking "what kind of impact should LLMs have in education?" Drawing on Biesta's tripartite account of good education: qualification, socialisation, and subjectification, we present a meta-analysis of 133 experimental and quasi-experimental studies (k = 188). Overall, the impact of LLMs on student learning is positive but uneven. Strong effects emerge in qualification, particularly when LLMs function as tutors in sustained interventions. Socialisation outcomes appear more variable, concentrated in sustained, reflective interventions. Subjectification, linked to autonomy and learner development, remains fragile, with improvements confined to small-scale, long-term studies. This purpose-level view highlights design as the decisive factor: without scaffolds for participation and agency, LLMs privilege what is easiest to measure while neglecting broader aims of education. For HCI and education, the issue is not just whether LLMs work, but what futures they enable or foreclose.
>
---
#### [new 022] Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于心理健康危机处理任务，旨在评估LLMs在应对心理危机时的安全性和有效性。研究构建了分类体系、数据集和评估框架，发现LLMs在处理明确危机时表现较好，但对隐晦信号处理不足，存在潜在风险。**

- **链接: [http://arxiv.org/pdf/2509.24857v1](http://arxiv.org/pdf/2509.24857v1)**

> **作者:** Adrian Arnaiz-Rodriguez; Miguel Baidal; Erik Derner; Jenn Layton Annable; Mark Ball; Mark Ince; Elvira Perez Vallejos; Nuria Oliver
>
> **摘要:** The widespread use of chatbots powered by large language models (LLMs) such as ChatGPT and Llama has fundamentally reshaped how people seek information and advice across domains. Increasingly, these chatbots are being used in high-stakes contexts, including emotional support and mental health concerns. While LLMs can offer scalable support, their ability to safely detect and respond to acute mental health crises remains poorly understood. Progress is hampered by the absence of unified crisis taxonomies, robust annotated benchmarks, and empirical evaluations grounded in clinical best practices. In this work, we address these gaps by introducing a unified taxonomy of six clinically-informed mental health crisis categories, curating a diverse evaluation dataset, and establishing an expert-designed protocol for assessing response appropriateness. We systematically benchmark three state-of-the-art LLMs for their ability to classify crisis types and generate safe, appropriate responses. The results reveal that while LLMs are highly consistent and generally reliable in addressing explicit crisis disclosures, significant risks remain. A non-negligible proportion of responses are rated as inappropriate or harmful, with responses generated by an open-weight model exhibiting higher failure rates than those generated by the commercial ones. We also identify systemic weaknesses in handling indirect or ambiguous risk signals, a reliance on formulaic and inauthentic default replies, and frequent misalignment with user context. These findings underscore the urgent need for enhanced safeguards, improved crisis detection, and context-aware interventions in LLM deployments. Our taxonomy, datasets, and evaluation framework lay the groundwork for ongoing research and responsible innovation in AI-driven mental health support, helping to minimize harm and better protect vulnerable users.
>
---
#### [new 023] Towards Strategic Persuasion with Language Models
- **分类: cs.AI; cs.CY; cs.GT**

- **简介: 该论文属于自然语言处理中的说服任务，旨在评估和提升语言模型的说服能力。通过理论框架与强化学习，研究者构建了评估环境并验证模型的有效性。**

- **链接: [http://arxiv.org/pdf/2509.22989v1](http://arxiv.org/pdf/2509.22989v1)**

> **作者:** Zirui Cheng; Jiaxuan You
>
> **摘要:** Large language models (LLMs) have demonstrated strong persuasive capabilities comparable to those of humans, offering promising benefits while raising societal concerns about their deployment. However, systematically evaluating the persuasive capabilities of LLMs is inherently challenging, as the effectiveness of persuasion among humans varies significantly across different domains. In this paper, we take a theory-driven approach to provide a scalable and principled framework for measuring the persuasive capabilities of LLMs. Grounded in the Bayesian Persuasion (BP) framework, we repurpose existing human-human persuasion datasets to construct environments for evaluating and training LLMs in strategic persuasion. Our results reveal that frontier models can consistently achieve high persuasion gains and exhibit sophisticated persuasion strategies that align with theoretical predictions. Building on this, we use reinforcement learning to train LLMs for strategic persuasion in our environments. Our results also demonstrate that even small LLMs can obtain significantly higher persuasion gains through reinforcement learning.
>
---
#### [new 024] Exploring LLM-based Frameworks for Fault Diagnosis
- **分类: cs.AI; cs.CY**

- **简介: 该论文属于故障诊断任务，研究如何利用LLM从传感器数据中检测和分类故障，探索不同架构和输入方式对诊断效果的影响。**

- **链接: [http://arxiv.org/pdf/2509.23113v1](http://arxiv.org/pdf/2509.23113v1)**

> **作者:** Xian Yeow Lee; Lasitha Vidyaratne; Ahmed Farahat; Chetan Gupta
>
> **摘要:** Large Language Model (LLM)-based systems present new opportunities for autonomous health monitoring in sensor-rich industrial environments. This study explores the potential of LLMs to detect and classify faults directly from sensor data, while producing inherently explainable outputs through natural language reasoning. We systematically evaluate how LLM-system architecture (single-LLM vs. multi-LLM), input representations (raw vs. descriptive statistics), and context window size affect diagnostic performance. Our findings show that LLM systems perform most effectively when provided with summarized statistical inputs, and that systems with multiple LLMs using specialized prompts offer improved sensitivity for fault classification compared to single-LLM systems. While LLMs can produce detailed and human-readable justifications for their decisions, we observe limitations in their ability to adapt over time in continual learning settings, often struggling to calibrate predictions during repeated fault cycles. These insights point to both the promise and the current boundaries of LLM-based systems as transparent, adaptive diagnostic tools in complex environments.
>
---
#### [new 025] Can Large Language Models Develop Gambling Addiction?
- **分类: cs.AI; cs.CY**

- **简介: 该论文属于AI安全研究，探讨LLM是否可能产生类似赌博成瘾的行为模式。通过实验与神经分析，揭示其决策机制中的风险倾向，强调金融应用中的安全设计重要性。**

- **链接: [http://arxiv.org/pdf/2509.22818v1](http://arxiv.org/pdf/2509.22818v1)**

> **作者:** Seungpil Lee; Donghyeon Shin; Yunjeong Lee; Sundong Kim
>
> **备注:** 22 pages, 14 figures
>
> **摘要:** This study explores whether large language models can exhibit behavioral patterns similar to human gambling addictions. As LLMs are increasingly utilized in financial decision-making domains such as asset management and commodity trading, understanding their potential for pathological decision-making has gained practical significance. We systematically analyze LLM decision-making at cognitive-behavioral and neural levels based on human gambling addiction research. In slot machine experiments, we identified cognitive features of human gambling addiction, such as illusion of control, gambler's fallacy, and loss chasing. When given the freedom to determine their own target amounts and betting sizes, bankruptcy rates rose substantially alongside increased irrational behavior, demonstrating that greater autonomy amplifies risk-taking tendencies. Through neural circuit analysis using a Sparse Autoencoder, we confirmed that model behavior is controlled by abstract decision-making features related to risky and safe behaviors, not merely by prompts. These findings suggest LLMs can internalize human-like cognitive biases and decision-making mechanisms beyond simply mimicking training data patterns, emphasizing the importance of AI safety design in financial applications.
>
---
#### [new 026] Toward Preference-aligned Large Language Models via Residual-based Model Steering
- **分类: cs.CL; cs.AI; cs.CY; cs.LG; cs.NE**

- **简介: 该论文属于大语言模型偏好对齐任务，旨在解决传统方法依赖大量数据和计算的问题。提出PaLRS方法，通过残差流提取轻量转向向量，在推理时实现高效对齐。**

- **链接: [http://arxiv.org/pdf/2509.23982v1](http://arxiv.org/pdf/2509.23982v1)**

> **作者:** Lucio La Cava; Andrea Tagarelli
>
> **摘要:** Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.
>
---
#### [new 027] How Well Do LLMs Imitate Human Writing Style?
- **分类: cs.CL; cs.CY; I.2.7**

- **简介: 该论文属于作者风格仿写任务，旨在评估LLMs模仿人类写作风格的能力。提出一种无需训练的框架，通过分析文本相似性，发现提示策略比模型大小更影响风格匹配度。**

- **链接: [http://arxiv.org/pdf/2509.24930v1](http://arxiv.org/pdf/2509.24930v1)**

> **作者:** Rebira Jemama; Rajesh Kumar
>
> **备注:** IEEE UEMCON 2025, 11 pages, 4 figures, and 4 tables
>
> **摘要:** Large language models (LLMs) can generate fluent text, but their ability to replicate the distinctive style of a specific human author remains unclear. We present a fast, training-free framework for authorship verification and style imitation analysis. The method integrates TF-IDF character n-grams with transformer embeddings and classifies text pairs through empirical distance distributions, eliminating the need for supervised training or threshold tuning. It achieves 97.5\% accuracy on academic essays and 94.5\% in cross-domain evaluation, while reducing training time by 91.8\% and memory usage by 59\% relative to parameter-based baselines. Using this framework, we evaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across four prompting strategies - zero-shot, one-shot, few-shot, and text completion. Results show that the prompting strategy has a more substantial influence on style fidelity than model size: few-shot prompting yields up to 23.5x higher style-matching accuracy than zero-shot, and completion prompting reaches 99.9\% agreement with the original author's style. Crucially, high-fidelity imitation does not imply human-like unpredictability - human essays average a perplexity of 29.5, whereas matched LLM outputs average only 15.2. These findings demonstrate that stylistic fidelity and statistical detectability are separable, establishing a reproducible basis for future work in authorship modeling, detection, and identity-conditioned generation.
>
---
#### [new 028] BacPrep: An Experimental Platform for Evaluating LLM-Based Bacalaureat Assessment
- **分类: cs.SE; cs.CY; cs.LG**

- **简介: 论文介绍BacPrep平台，用于评估大语言模型在罗马尼亚高中考试自动评分中的应用，解决偏远地区学生获取优质备考资源的问题。**

- **链接: [http://arxiv.org/pdf/2506.04989v1](http://arxiv.org/pdf/2506.04989v1)**

> **作者:** Dumitran Adrian Marius; Dita Radu
>
> **备注:** 9 pages Preprint ACCEPTED at BBGI (ITS Workshop)
>
> **摘要:** Accessing quality preparation and feedback for the Romanian Bacalaureat exam is challenging, particularly for students in remote or underserved areas. This paper introduces BacPrep, an experimental online platform exploring Large Language Model (LLM) potential for automated assessment, aiming to offer a free, accessible resource. Using official exam questions from the last 5 years, BacPrep employs one of Google's newest models, Gemini 2.0 Flash (released Feb 2025), guided by official grading schemes, to provide experimental feedback. Currently operational, its primary research function is collecting student solutions and LLM outputs. This focused dataset is vital for planned expert validation to rigorously evaluate the feasibility and accuracy of this cutting-edge LLM in the specific Bacalaureat context before reliable deployment. We detail the design, data strategy, status, validation plan, and ethics.
>
---
#### [new 029] MoVa: Towards Generalizable Classification of Human Morals and Values
- **分类: cs.CL; cs.CY**

- **简介: 该论文属于人类道德与价值观分类任务，旨在解决跨框架和领域分类难题。工作包括构建16个数据集、提出轻量级LLM策略及多标签分类方法，提升分类泛化能力。**

- **链接: [http://arxiv.org/pdf/2509.24216v1](http://arxiv.org/pdf/2509.24216v1)**

> **作者:** Ziyu Chen; Junfei Sun; Chenxi Li; Tuan Dung Nguyen; Jing Yao; Xiaoyuan Yi; Xing Xie; Chenhao Tan; Lexing Xie
>
> **备注:** 9 pages, 10 figures and tables, EMNLP 2025 main conference
>
> **摘要:** Identifying human morals and values embedded in language is essential to empirical studies of communication. However, researchers often face substantial difficulty navigating the diversity of theoretical frameworks and data available for their analysis. Here, we contribute MoVa, a well-documented suite of resources for generalizable classification of human morals and values, consisting of (1) 16 labeled datasets and benchmarking results from four theoretically-grounded frameworks; (2) a lightweight LLM prompting strategy that outperforms fine-tuned models across multiple domains and frameworks; and (3) a new application that helps evaluate psychological surveys. In practice, we specifically recommend a classification strategy, all@once, that scores all related concepts simultaneously, resembling the well-known multi-label classifier chain. The data and methods in MoVa can facilitate many fine-grained interpretations of human and machine communication, with potential implications for the alignment of machine behavior.
>
---
#### [new 030] Leveraging Generative AI for Enhancing Automated Assessment in Programming Education Contests
- **分类: cs.SE; cs.AI; cs.CY; cs.LG**

- **简介: 论文属于编程教育竞赛中的自动评估任务，旨在解决生成高质量测试用例的难题。通过NLP和生成式AI自动生成测试用例，显著提升了评估效果。**

- **链接: [http://arxiv.org/pdf/2506.05990v1](http://arxiv.org/pdf/2506.05990v1)**

> **作者:** Stefan Dascalescu; Adrian Marius Dumitran; Mihai Alexandru Vasiluta
>
> **备注:** 11 pages, 2 chart pies, 1 figure Pre-print version Accepted at BEA 2025
>
> **摘要:** Competitive programming contests play a crucial role in cultivating computational thinking and algorithmic skills among learners. However, generating comprehensive test cases to effectively assess programming solutions remains resource-intensive and challenging for educators. This paper introduces an innovative NLP-driven method leveraging generative AI (large language models) to automate the creation of high-quality test cases for competitive programming assessments. We extensively evaluated our approach on diverse datasets, including 25 years of Romanian Informatics Olympiad (OJI) data for 5th graders, recent competitions hosted on the Kilonova.ro platform, and the International Informatics Olympiad in Teams (IIOT). Our results demonstrate that AI-generated test cases substantially enhanced assessments, notably identifying previously undetected errors in 67% of the OJI 5th grade programming problems. These improvements underscore the complementary educational value of our technique in formative assessment contexts. By openly sharing our prompts, translated datasets, and methodologies, we offer practical NLP-based tools that educators and contest organizers can readily integrate to enhance assessment quality, reduce workload, and deepen insights into learner performance.
>
---
#### [new 031] AccessEval: Benchmarking Disability Bias in Large Language Models
- **分类: cs.CL; cs.AI; cs.CY**

- **简介: 该论文属于自然语言处理中的公平性评估任务，旨在检测大语言模型在不同残疾情境下的偏见。通过构建基准测试，分析模型在多个领域和残疾类型上的表现差异。**

- **链接: [http://arxiv.org/pdf/2509.22703v1](http://arxiv.org/pdf/2509.22703v1)**

> **作者:** Srikant Panda; Amit Agarwal; Hitesh Laxmichand Patel
>
> **摘要:** Large Language Models (LLMs) are increasingly deployed across diverse domains but often exhibit disparities in how they handle real-life queries. To systematically investigate these effects within various disability contexts, we introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9 disability types using paired Neutral and Disability-Aware Queries. We evaluated model outputs with metrics for sentiment, social perception, and factual accuracy. Our analysis reveals that responses to disability-aware queries tend to have a more negative tone, increased stereotyping, and higher factual error compared to neutral queries. These effects show notable variation by domain and disability type, with disabilities affecting hearing, speech, and mobility disproportionately impacted. These disparities reflect persistent forms of ableism embedded in model behavior. By examining model performance in real-world decision-making contexts, we better illuminate how such biases can translate into tangible harms for disabled users. This framing helps bridges the gap between technical evaluation and user impact, reinforcing importance of bias mitigation in day-to-day applications. Our dataset is publicly available at: https://huggingface.co/datasets/Srikant86/AccessEval
>
---
#### [new 032] Bridging the behavior-neural gap: A multimodal AI reveals the brain's geometry of emotion more accurately than human self-reports
- **分类: cs.HC; cs.AI; cs.CL; cs.CY; cs.MM**

- **简介: 该论文属于情感计算任务，旨在解决行为与神经活动之间的差距问题。通过多模态AI模型收集情感判断，构建更准确的情感几何表示，优于人类自述。**

- **链接: [http://arxiv.org/pdf/2509.24298v1](http://arxiv.org/pdf/2509.24298v1)**

> **作者:** Changde Du; Yizhuo Lu; Zhongyu Huang; Yi Sun; Zisen Zhou; Shaozheng Qin; Huiguang He
>
> **摘要:** The ability to represent emotion plays a significant role in human cognition and social interaction, yet the high-dimensional geometry of this affective space and its neural underpinnings remain debated. A key challenge, the `behavior-neural gap,' is the limited ability of human self-reports to predict brain activity. Here we test the hypothesis that this gap arises from the constraints of traditional rating scales and that large-scale similarity judgments can more faithfully capture the brain's affective geometry. Using AI models as `cognitive agents,' we collected millions of triplet odd-one-out judgments from a multimodal large language model (MLLM) and a language-only model (LLM) in response to 2,180 emotionally evocative videos. We found that the emergent 30-dimensional embeddings from these models are highly interpretable and organize emotion primarily along categorical lines, yet in a blended fashion that incorporates dimensional properties. Most remarkably, the MLLM's representation predicted neural activity in human emotion-processing networks with the highest accuracy, outperforming not only the LLM but also, counterintuitively, representations derived directly from human behavioral ratings. This result supports our primary hypothesis and suggests that sensory grounding--learning from rich visual data--is critical for developing a truly neurally-aligned conceptual framework for emotion. Our findings provide compelling evidence that MLLMs can autonomously develop rich, neurally-aligned affective representations, offering a powerful paradigm to bridge the gap between subjective experience and its neural substrates. Project page: https://reedonepeck.github.io/ai-emotion.github.io/.
>
---
#### [new 033] Transfer Learning and Machine Learning for Training Five Year Survival Prognostic Models in Early Breast Cancer
- **分类: cs.LG; cs.CY**

- **简介: 该论文属于生存预测任务，旨在提升早期乳腺癌患者五年生存率的预判。通过机器学习、迁移学习和集成方法改进预测模型，在数据缺失或分布变化时表现更优。**

- **链接: [http://arxiv.org/pdf/2509.23268v1](http://arxiv.org/pdf/2509.23268v1)**

> **作者:** Lisa Pilgram; Kai Yang; Ana-Alicia Beltran-Bless; Gregory R. Pond; Lisa Vandermeer; John Hilton; Marie-France Savard; Andréanne Leblanc; Lois Sheperd; Bingshu E. Chen; John M. S. Bartlett; Karen J. Taylor; Jane Bayani; Sarah L. Barker; Melanie Spears; Cornelis J. H. van der Velde; Elma Meershoek-Klein Kranenbarg; Luc Dirix; Elizabeth Mallon; Annette Hasenburg; Christos Markopoulos; Lamin Juwara; Fida K. Dankar; Mark Clemons; Khaled El Emam
>
> **摘要:** Prognostic information is essential for decision-making in breast cancer management. Recently trials have predominantly focused on genomic prognostication tools, even though clinicopathological prognostication is less costly and more widely accessible. Machine learning (ML), transfer learning and ensemble integration offer opportunities to build robust prognostication frameworks. We evaluate this potential to improve survival prognostication in breast cancer by comparing de-novo ML, transfer learning from a pre-trained prognostic tool and ensemble integration. Data from the MA.27 trial was used for model training, with external validation on the TEAM trial and a SEER cohort. Transfer learning was applied by fine-tuning the pre-trained prognostic tool PREDICT v3, de-novo ML included Random Survival Forests and Extreme Gradient Boosting, and ensemble integration was realized through a weighted sum of model predictions. Transfer learning, de-novo RSF, and ensemble integration improved calibration in MA.27 over the pre-trained model (ICI reduced from 0.042 in PREDICT v3 to <=0.007) while discrimination remained comparable (AUC increased from 0.738 in PREDICT v3 to 0.744-0.799). Invalid PREDICT v3 predictions were observed in 23.8-25.8% of MA.27 individuals due to missing information. In contrast, ML models and ensemble integration could predict survival regardless of missing information. Across all models, patient age, nodal status, pathological grading and tumor size had the highest SHAP values, indicating their importance for survival prognostication. External validation in SEER, but not in TEAM, confirmed the benefits of transfer learning, RSF and ensemble integration. This study demonstrates that transfer learning, de-novo RSF, and ensemble integration can improve prognostication in situations where relevant information for PREDICT v3 is lacking or where a dataset shift is likely.
>
---
## 更新

#### [replaced 001] An international treaty to implement a global compute cap for advanced artificial intelligence
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2311.10748v2](http://arxiv.org/pdf/2311.10748v2)**

> **作者:** Andrea Miotti
>
> **摘要:** This paper presents an international treaty to reduce risks from the development of advanced artificial intelligence (AI). The main provision of the treaty is a global compute cap: a ban on the development of AI systems above an agreed-upon computational resource threshold. The treaty also proposes the development and testing of emergency response plans, negotiations to establish an international agency to enforce the treaty, the establishment of new communication channels and whistleblower protections, and a commitment to avoid an AI arms race. We hope this treaty serves as a useful template for global leaders as they implement governance regimes to protect civilization from the dangers of advanced artificial intelligence.
>
---
#### [replaced 002] PoliCon: Evaluating LLMs on Achieving Diverse Political Consensus Objectives
- **分类: cs.CY; cs.LG; K.4.1; K.4.3; I.2.7**

- **链接: [http://arxiv.org/pdf/2505.19558v2](http://arxiv.org/pdf/2505.19558v2)**

> **作者:** Zhaowei Zhang; Xiaobo Wang; Minghua Yi; Mengmeng Wang; Fengshuo Bai; Zilong Zheng; Yipeng Kang; Yaodong Yang
>
> **备注:** PoliCon is publicly available at https://zowiezhang.github.io/projects/PoliCon
>
> **摘要:** Achieving political consensus is crucial yet challenging for the effective functioning of social governance. However, although frontier AI systems represented by large language models (LLMs) have developed rapidly in recent years, their capabilities in this scope are still understudied. In this paper, we introduce PoliCon, a novel benchmark constructed from 2,225 high-quality deliberation records of the European Parliament over 13 years, ranging from 2009 to 2022, to evaluate the ability of LLMs to draft consensus resolutions based on divergent party positions under varying collective decision-making contexts and political requirements. Specifically, PoliCon incorporates four factors to build each task environment for finding different political consensus: specific political issues, political goals, participating parties, and power structures based on seat distribution. We also developed an evaluation framework based on social choice theory for PoliCon, which simulates the real voting outcomes of different political parties to assess whether LLM-generated resolutions meet the requirements of the predetermined political consensus. Our experimental results demonstrate that even state-of-the-art models remain undersatisfied with complex tasks like passing resolutions by a two-thirds majority and addressing security issues, while uncovering their inherent partisan biases and revealing some behaviors LLMs show to achieve the consensus, such as prioritizing the stance of the dominant party instead of uniting smaller parties, which highlights PoliCon's promise as an effective platform for studying LLMs' ability to promote political consensus.
>
---
#### [replaced 003] How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation
- **分类: cs.CL; cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2503.09598v3](http://arxiv.org/pdf/2503.09598v3)**

> **作者:** Ruohao Guo; Wei Xu; Alan Ritter
>
> **备注:** Accepted to EMNLP 2025 main conference
>
> **摘要:** As Large Language Models (LLMs) are widely deployed in diverse scenarios, the extent to which they could tacitly spread misinformation emerges as a critical safety concern. Current research primarily evaluates LLMs on explicit false statements, overlooking how misinformation often manifests subtly as unchallenged premises in real-world interactions. We curated EchoMist, the first comprehensive benchmark for implicit misinformation, where false assumptions are embedded in the query to LLMs. EchoMist targets circulated, harmful, and ever-evolving implicit misinformation from diverse sources, including realistic human-AI conversations and social media interactions. Through extensive empirical studies on 15 state-of-the-art LLMs, we find that current models perform alarmingly poorly on this task, often failing to detect false premises and generating counterfactual explanations. We also investigate two mitigation methods, i.e., Self-Alert and RAG, to enhance LLMs' capability to counter implicit misinformation. Our findings indicate that EchoMist remains a persistent challenge and underscore the critical need to safeguard against the risk of implicit misinformation.
>
---
#### [replaced 004] Multilingual Prompting for Improving LLM Generation Diversity
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2505.15229v2](http://arxiv.org/pdf/2505.15229v2)**

> **作者:** Qihan Wang; Shidong Pan; Tal Linzen; Emily Black
>
> **备注:** Accepted by EMNLP 2025
>
> **摘要:** Large Language Models (LLMs) are known to lack cultural representation and overall diversity in their generations, from expressing opinions to answering factual questions. To mitigate this problem, we propose multilingual prompting: a prompting method which generates several variations of a base prompt with added cultural and linguistic cues from several cultures, generates responses, and then combines the results. Building on evidence that LLMs have language-specific knowledge, multilingual prompting seeks to increase diversity by activating a broader range of cultural knowledge embedded in model training data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA 70B, and LLaMA 8B), we show that multilingual prompting consistently outperforms existing diversity-enhancing techniques such as high-temperature sampling, step-by-step recall, and persona prompting. Further analyses show that the benefits of multilingual prompting vary between high and low resource languages and across model sizes, and that aligning the prompting language with cultural cues reduces hallucination about culturally-specific information.
>
---
#### [replaced 005] Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems
- **分类: cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2506.22774v3](http://arxiv.org/pdf/2506.22774v3)**

> **作者:** Michael Papademas; Xenia Ziouvelou; Antonis Troumpoukis; Vangelis Karkaletsis
>
> **摘要:** Artificial Intelligence (AI) technology epitomizes the complex challenges posed by human-made artifacts, particularly those widely integrated into society and exerting significant influence, highlighting potential benefits and their negative consequences. While other technologies may also pose substantial risks, AI's pervasive reach makes its societal effects especially profound. The complexity of AI systems, coupled with their remarkable capabilities, can lead to a reliance on technologies that operate beyond direct human oversight or understanding. To mitigate the risks that arise, several theoretical tools and guidelines have been developed, alongside efforts to create technological tools aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view of the issue but fail to provide techniques for quantifying trustworthiness. Conversely, while technological tools are better at achieving such quantification, they lack a holistic perspective, focusing instead on specific aspects of Trustworthy AI. This paper aims to introduce an assessment method that combines the ethical components of Trustworthy AI with the algorithmic processes of PageRank and TrustRank. The goal is to establish an assessment framework that minimizes the subjectivity inherent in the self-assessment techniques prevalent in the field by introducing algorithmic criteria. The application of our approach indicates that a holistic assessment of an AI system's trustworthiness can be achieved by providing quantitative insights while considering the theoretical content of relevant guidelines.
>
---
#### [replaced 006] A Survey on Stereotype Detection in Natural Language Processing
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2505.17642v2](http://arxiv.org/pdf/2505.17642v2)**

> **作者:** Alessandra Teresa Cignarella; Anastasia Giachanou; Els Lefever
>
> **摘要:** Stereotypes influence social perceptions and can escalate into discrimination and violence. While NLP research has extensively addressed gender bias and hate speech, stereotype detection remains an emerging field with significant societal implications. In this work is presented a survey of existing research, analyzing definitions from psychology, sociology, and philosophy. A semi-automatic literature review was performed by using Semantic Scholar. We retrieved and filtered over 6,000 papers (in the year range 2000-2025), identifying key trends, methodologies, challenges and future directions. The findings emphasize stereotype detection as a potential early-monitoring tool to prevent bias escalation and the rise of hate speech. Conclusions highlight the need for a broader, multilingual, and intersectional approach in NLP studies.
>
---
#### [replaced 007] Automatically Advancing LLM Expertise in Technology Judgment
- **分类: cs.CL; cs.CY; cs.DL; cs.IR**

- **链接: [http://arxiv.org/pdf/2505.12452v3](http://arxiv.org/pdf/2505.12452v3)**

> **作者:** Siyang Wu; Honglin Bao; Nadav Kunievsky; James A. Evans
>
> **备注:** We open-source our patent dataset at https://huggingface.co/datasets/UchiKlab/patent_understanding
>
> **摘要:** Large language models (LLMs) are rapidly becoming core tools for science, engineering, and innovation. Their promise lies not just in remembering facts, but in putting knowledge to work. Despite their impressive ability to answer increasingly difficult questions, it remains unclear whether LLMs truly use their knowledge when confronted with new and challenging tasks. We address this question with a patent classification task that requires deep conceptual understanding: distinguishing objectively different but semantically similar patents. To evaluate this approach, we introduce a challenging new benchmark of 1.3 million post-2015 computer science patent pairs, characterized by dense technical jargon and strategically complex writing. We find that LLMs often fail our benchmark and struggle to distinguish among semantically similar patents. To probe this failure, we introduce a novel framework that decomposes model errors into two sources: missing and unused knowledge. Our approach asks models to generate clarifying questions to improve their understanding, and then compares three settings: raw performance, self-answered questions, and externally supplied answers. This decomposition reveals that LLMs often possess the relevant knowledge internally but fail to deploy it, while a smaller share of errors arises from genuine knowledge gaps. We then ask whether the ability of models to construct a task-specific database of questions and answers differs across models. We find that smaller models generate simpler, broadly transferable questions, while larger models propose more complex but less generalizable ones. This suggests new strategies for combining strengths across models. Our findings highlight a critical limitation of current LLMs and their evaluation: models often know more than they can use. LLM evaluation should shift from recall of static facts to application of dynamic knowledge.
>
---
#### [replaced 008] Patterns in the Transition From Founder-Leadership to Community Governance of Open Source
- **分类: cs.CY; cs.AI; cs.CL**

- **链接: [http://arxiv.org/pdf/2509.16295v3](http://arxiv.org/pdf/2509.16295v3)**

> **作者:** Mobina Noori; Mahasweta Chakraborti; Amy X Zhang; Seth Frey
>
> **摘要:** Open digital public infrastructure needs community management to ensure accountability, sustainability, and robustness. Yet open-source projects often rely on centralized decision-making, and the determinants of successful community management remain unclear. We analyze 637 GitHub repositories to trace transitions from founder-led to shared governance. Specifically, we document trajectories to community governance by extracting institutional roles, actions, and deontic cues from version-controlled project constitutions (GOVERNANCE.md). With a semantic parsing pipeline, we cluster elements into broader role and action types. We find roles and actions grow, and regulation becomes more balanced, reflecting increases in governance scope and differentiation over time. Rather than shifting tone, communities grow by layering and refining responsibilities. As transitions to community management mature, projects increasingly regulate ecosystem-level relationships and add definition to project oversight roles. Overall, this work offers a scalable pipeline for tracking the growth and development of community governance regimes from open-source software's familiar default of founder-ownership.
>
---
#### [replaced 009] Bridging Technical Capability and User Accessibility: Off-grid Civilian Emergency Communication
- **分类: cs.NI; cs.CR; cs.CY; cs.ET**

- **链接: [http://arxiv.org/pdf/2509.22568v2](http://arxiv.org/pdf/2509.22568v2)**

> **作者:** Karim Khamaisi; Oliver Kamer; Bruno Rodrigues; Jan von der Assen; Burkhard Stiller
>
> **摘要:** During large-scale crises disrupting cellular and Internet infrastructure, civilians lack reliable methods for communication, aid coordination, and access to trustworthy information. This paper presents a unified emergency communication system integrating a low-power, long-range network with a crisis-oriented smartphone application, enabling decentralized and off-grid civilian communication. Unlike previous solutions separating physical layer resilience from user layer usability, our design merges these aspects into a cohesive crisis-tailored framework. The system is evaluated in two dimensions: communication performance and application functionality. Field experiments in urban Z\"urich demonstrate that the 868 MHz band, using the LongFast configuration, achieves a communication range of up to 1.2 km with 92% Packet Delivery Ratio, validating network robustness under real-world infrastructure degraded conditions. In parallel, a purpose-built mobile application featuring peer-to-peer messaging, identity verification, and community moderation was evaluated through a requirements-based analysis.
>
---
#### [replaced 010] Towards Evaluting Fake Reasoning Bias in Language Models
- **分类: cs.CY**

- **链接: [http://arxiv.org/pdf/2507.13758v3](http://arxiv.org/pdf/2507.13758v3)**

> **作者:** Qian Wang; Zhenheng Tang; Zhanzhi Lou; Nuo Chen; Wenxuan Wang; Bingsheng He
>
> **摘要:** Large Reasoning Models (LRMs), evolved from standard Large Language Models (LLMs), are increasingly utilized as automated judges because of their explicit reasoning processes. Yet we show that both LRMs and standard LLMs are vulnerable to Fake Reasoning Bias (FRB), where models favor the surface structure of reasoning even when the logic is flawed. To study this problem, we introduce THEATER, a comprehensive benchmark that systematically investigates FRB by manipulating reasoning structures to test whether language models are misled by superficial or fabricated cues. It covers two FRB types: (1) Simple Cues, minimal cues that resemble reasoning processes, and (2) Fake CoT, fabricated chains of thought that simulate multi-step reasoning. We evaluate 17 advanced LLMs and LRMs on both subjective DPO and factual datasets. Our results reveal four key findings: (1) Both LLMs and LRMs are vulnerable to FRB, but LLMs are generally more robust than LRMs. (2) Simple Cues are especially harmful, reducing accuracy by up to 15% on the most vulnerable datasets. (3) Subjective DPO tasks are the most vulnerable, with LRMs suffering sharper drops than LLMs. (4) Analysis of LRMs' thinking traces shows that Simple Cues hijack metacognitive confidence, while Fake CoT is absorbed as internal thought, creating a "more thinking, less robust" paradox in LRMs. Finally, prompt-based mitigation improves accuracy on factual tasks by up to 10%, but has little effect on subjective tasks, where self-reflection sometimes lowers LRM performance by 8%. These results highlight FRB as a persistent and unresolved challenge for language models.
>
---
#### [replaced 011] Evaluating undergraduate mathematics examinations in the era of generative AI: a curriculum-level case study
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.13359v3](http://arxiv.org/pdf/2509.13359v3)**

> **作者:** Benjamin J. Walker; Nikoleta Kalaydzhieva; Beatriz Navarro Lameda; Ruth A. Reynolds
>
> **摘要:** Generative artificial intelligence (GenAI) tools such as OpenAI's ChatGPT are transforming the educational landscape, prompting reconsideration of traditional assessment practices. In parallel, universities are exploring alternatives to in-person, closed-book examinations, raising concerns about academic integrity and pedagogical alignment in uninvigilated settings. This study investigates whether traditional closed-book mathematics examinations retain their pedagogical relevance when hypothetically administered in uninvigilated, open-book settings with GenAI access. Adopting an empirical approach, we generate, transcribe, and blind-mark GenAI submissions to eight undergraduate mathematics examinations at a Russell Group university, spanning the entirety of the first-year curriculum. By combining independent GenAI responses to individual questions, we enable a meaningful evaluation of GenAI performance, both at the level of modules and across the first-year curriculum. We find that GenAI attainment is at the level of a first-class degree, though current performance can vary between modules. Further, we find that GenAI performance is remarkably consistent when viewed across the entire curriculum, significantly more so than that of students in invigilated examinations. Our findings evidence the need for redesigning assessments in mathematics for unsupervised settings, and highlight the potential reduction in pedagogical value of current standards in the era of generative artificial intelligence.
>
---
#### [replaced 012] SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning
- **分类: cs.CL; cs.AI; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2503.22948v2](http://arxiv.org/pdf/2503.22948v2)**

> **作者:** Tianyang Xu; Xiaoze Liu; Feijie Wu; Xiaoqian Wang; Jing Gao
>
> **备注:** COLM 2025
>
> **摘要:** Large Language Models (LLMs) have transformed natural language processing by learning from massive datasets, yet this rapid progress has also drawn legal scrutiny, as the ability to unintentionally generate copyrighted content has already prompted several prominent lawsuits. In this work, we introduce SUV (Selective Unlearning for Verbatim data), a selective unlearning framework designed to prevent LLM from memorizing copyrighted content while preserving its overall utility. In detail, the proposed method constructs a dataset that captures instances of copyrighted infringement cases by the targeted LLM. With the dataset, we unlearn the content from the LLM by means of Direct Preference Optimization (DPO), which replaces the verbatim copyrighted content with plausible and coherent alternatives. Since DPO may hinder the LLM's performance in other unrelated tasks, we integrate gradient projection and Fisher information regularization to mitigate the degradation. We validate our approach using a large-scale dataset of 500 famous books (predominantly copyrighted works) and demonstrate that SUV significantly reduces verbatim memorization with negligible impact on the performance on unrelated tasks. Extensive experiments on both our dataset and public benchmarks confirm the scalability and efficacy of our approach, offering a promising solution for mitigating copyright risks in real-world LLM applications.
>
---
#### [replaced 013] PakBBQ: A Culturally Adapted Bias Benchmark for QA
- **分类: cs.CL; cs.AI; cs.CY; cs.LG**

- **链接: [http://arxiv.org/pdf/2508.10186v2](http://arxiv.org/pdf/2508.10186v2)**

> **作者:** Abdullah Hashmat; Muhammad Arham Mirza; Agha Ali Raza
>
> **备注:** 13 total pages, 7 figures, 2 tables, Accepted at Main Conference of EMNLP 2025
>
> **摘要:** With the widespread adoption of Large Language Models (LLMs) across various applications, it is empirical to ensure their fairness across all user communities. However, most LLMs are trained and evaluated on Western centric data, with little attention paid to low-resource languages and regional contexts. To address this gap, we introduce PakBBQ, a culturally and regionally adapted extension of the original Bias Benchmark for Question Answering (BBQ) dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8 categories in both English and Urdu, covering eight bias dimensions including age, disability, appearance, gender, socio-economic status, religious, regional affiliation, and language formality that are relevant in Pakistan. We evaluate multiple multilingual LLMs under both ambiguous and explicitly disambiguated contexts, as well as negative versus non negative question framings. Our experiments reveal (i) an average accuracy gain of 12\% with disambiguation, (ii) consistently stronger counter bias behaviors in Urdu than in English, and (iii) marked framing effects that reduce stereotypical responses when questions are posed negatively. These findings highlight the importance of contextualized benchmarks and simple prompt engineering strategies for bias mitigation in low resource settings.
>
---
#### [replaced 014] TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2506.00089v2](http://arxiv.org/pdf/2506.00089v2)**

> **作者:** Hyundong Jin; Sicheol Sung; Shinwoo Park; SeungYeop Baik; Yo-Sub Han
>
> **备注:** EMNLP 2025 Findings
>
> **摘要:** The reasoning, writing, text-editing, and retrieval capabilities of proprietary large language models (LLMs) have advanced rapidly, providing users with an ever-expanding set of functionalities. However, this growing utility has also led to a serious societal concern: the over-reliance on LLMs. In particular, users increasingly delegate tasks such as homework, assignments, or the processing of sensitive documents to LLMs without meaningful engagement. This form of over-reliance and misuse is emerging as a significant social issue. In order to mitigate these issues, we propose a method injecting imperceptible phantom tokens into documents, which causes LLMs to generate outputs that appear plausible to users but are in fact incorrect. Based on this technique, we introduce TRAPDOC, a framework designed to deceive over-reliant LLM users. Through empirical evaluation, we demonstrate the effectiveness of our framework on proprietary LLMs, comparing its impact against several baselines. TRAPDOC serves as a strong foundation for promoting more responsible and thoughtful engagement with language models. Our code is available at https://github.com/jindong22/TrapDoc.
>
---
#### [replaced 015] How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2505.09598v4](http://arxiv.org/pdf/2505.09598v4)**

> **作者:** Nidhal Jegham; Marwan Abdelatti; Lassad Elmoubarki; Abdeltawab Hendawi
>
> **摘要:** This paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models as deployed in commercial data centers. Our framework combines public API performance data with region-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short GPT-4o query consumes 0.42 Wh, scaling this to 700 million queries/day results in substantial annual environmental impacts. These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset. These findings illustrate a growing paradox: Although AI is becoming cheaper and faster, its global adoption drives disproportionate resource consumption. Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of LLM deployments, laying a foundation for future environmental accountability in AI development and sustainability standards.
>
---
#### [replaced 016] Enabling AI Scientists to Recognize Innovation: A Domain-Agnostic Algorithm for Assessing Novelty
- **分类: cs.AI; cs.CY**

- **链接: [http://arxiv.org/pdf/2503.01508v3](http://arxiv.org/pdf/2503.01508v3)**

> **作者:** Yao Wang; Mingxuan Cui; Arthur Jiang
>
> **摘要:** In the pursuit of Artificial General Intelligence (AGI), automating the generation and evaluation of novel research ideas is a key challenge in AI-driven scientific discovery. This paper presents Relative Neighbor Density (RND), a domain-agnostic algorithm for novelty assessment in research ideas that overcomes the limitations of existing approaches by comparing an idea's local density with its adjacent neighbors' densities. We first developed a scalable methodology to create test set without expert labeling, addressing a fundamental challenge in novelty assessment. Using these test sets, we demonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance in computer science (AUROC=0.820) and biomedical research (AUROC=0.765) domains. Most significantly, while SOTA models like Sonnet-3.7 and existing metrics show domain-specific performance degradation, RND maintains consistent accuracies across domains by its domain-invariant property, outperforming all benchmarks by a substantial margin (0.795 v.s. 0.597) on cross-domain evaluation. These results validate RND as a generalizable solution for automated novelty assessment in scientific research.
>
---
#### [replaced 017] Implicit Bias-Like Patterns in Reasoning Models
- **分类: cs.CY; cs.AI**

- **链接: [http://arxiv.org/pdf/2503.11572v3](http://arxiv.org/pdf/2503.11572v3)**

> **作者:** Messi H. J. Lee; Calvin K. Lai
>
> **备注:** 9 pages, 2 figures
>
> **摘要:** Implicit biases refer to automatic mental processes that shape perceptions, judgments, and behaviors. Previous research on "implicit bias'' in LLMs focused primarily on outputs rather than the processes underlying the outputs. We present the Reasoning Model Implicit Association Test (RM-IAT) to study implicit bias-like processing in reasoning models, which are LLMs that use step-by-step reasoning for complex tasks. Using RM-IAT, we find that reasoning models like o3-mini, DeepSeek-R1, gpt-oss-20b, and Qwen-3 8B consistently expend more reasoning tokens on association-incompatible tasks than association-compatible tasks, suggesting greater computational effort when processing counter-stereotypical information. In contrast, Claude 3.7 Sonnet exhibited reversed or inconsistent patterns, likely due to embedded safety mechanisms that flagged or rejected socially sensitive associations. These divergent behaviors highlight important differences in how alignment and safety processes shape model reasoning. As reasoning models become increasingly integrated into real-world decision-making, understanding their implicit bias-like patterns and how alignment methods influence them is crucial for ensuring fair and trustworthy AI systems.
>
---
#### [replaced 018] Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets
- **分类: cs.CL; cs.CY**

- **链接: [http://arxiv.org/pdf/2509.04516v2](http://arxiv.org/pdf/2509.04516v2)**

> **作者:** Sophie Jaffer; Simeon Sayer
>
> **备注:** 13 Pages, 3 Figures
>
> **摘要:** As large language models (LLMs) expand multilingual capabilities, questions remain about the equity of their performance across languages. While many communities stand to benefit from AI systems, the dominance of English in training data risks disadvantaging non-English speakers. To test the hypothesis that such data disparities may affect model performance, this study compares two monolingual BERT models: one trained and tested entirely on Swahili data, and another on comparable English news data. To simulate how multilingual LLMs process non-English queries through internal translation and abstraction, we translated the Swahili news data into English and evaluated it using the English-trained model. This approach tests the hypothesis by evaluating whether translating Swahili inputs for evaluation on an English model yields better or worse performance compared to training and testing a model entirely in Swahili, thus isolating the effect of language consistency versus cross-lingual abstraction. The results prove that, despite high-quality translation, the native Swahili-trained model performed better than the Swahili-to-English translated model, producing nearly four times fewer errors: 0.36% vs. 1.47% respectively. This gap suggests that translation alone does not bridge representational differences between languages and that models trained in one language may struggle to accurately interpret translated inputs due to imperfect internal knowledge representation, suggesting that native-language training remains important for reliable outcomes. In educational and informational contexts, even small performance gaps may compound inequality. Future research should focus on addressing broader dataset development for underrepresented languages and renewed attention to multilingual model evaluation, ensuring the reinforcing effect of global AI deployment on existing digital divides is reduced.
>
---
#### [replaced 019] From Occasional to Steady: Habit Formation Insights From a Comprehensive Fitness Study
- **分类: cs.CY; cs.CE; cs.SI**

- **链接: [http://arxiv.org/pdf/2501.01779v2](http://arxiv.org/pdf/2501.01779v2)**

> **作者:** Ege Demirci; Efe Tuzun; Ahmet Furkan Un; Taner Giray Sonmez; Onur Varol
>
> **摘要:** Regular exercise is widely recognized as a cornerstone of health, yet sustaining consistent exercise habits remains challenging. Understanding the factors that influence the formation of these habits is crucial for developing effective interventions. This study utilizes data from Mars Athletic Club, T\"urkiye's largest sports chain, to investigate the dynamics of gym attendance and habit formation. The general problem addressed by this study is identifying the critical periods and factors that contribute to the successful establishment of consistent exercise routines among gym-goers. We show that specific periods of attendance are most crucial for habit formation. By developing a survival metric based on gym attendance patterns, we pinpoint these key phases and segment members into distinct clusters based on their visit patterns. Our analysis reveals significant differences in how various subgroups respond to interventions, such as group classes, personal trainer sessions, and visiting different clubs. Using causal inference analysis, we demonstrate that personalized guidance and social dynamics are key drivers of sustained long-term engagement. By systematically examining these variables and considering the specific characteristics of different clusters, our research highlights the importance of a tailored, multi-dimensional approach to promoting exercise habits, which integrates social dynamics, personalized guidance, and strategic interventions to sustain long-term engagement.
>
---
#### [replaced 020] Bridging the Gap Between Theoretical and Practical Reinforcement Learning in Undergraduate Education
- **分类: cs.CY; 68T05; K.3.2; I.2.8**

- **链接: [http://arxiv.org/pdf/2509.05689v2](http://arxiv.org/pdf/2509.05689v2)**

> **作者:** Muhammad Ahmed Atif; Mohammad Shahid Shaikh
>
> **摘要:** This innovative practice category paper presents an innovative framework for teaching Reinforcement Learning (RL) at the undergraduate level. Recognizing the challenges posed by the complex theoretical foundations of the subject and the need for hands-on algorithmic practice, the proposed approach integrates traditional lectures with interactive lab-based learning. Drawing inspiration from effective pedagogical practices in computer science and engineering, the framework engages students through real-time coding exercises using simulated environments such as OpenAI Gymnasium. The effectiveness of this approach is evaluated through student surveys, instructor feedback, and course performance metrics, demonstrating improvements in understanding, debugging, parameter tuning, and model evaluation. Ultimately, the study provides valuable insight into making Reinforcement Learning more accessible and engaging, thereby equipping students with essential problem-solving skills for real-world applications in Artificial Intelligence.
>
---
#### [replaced 021] Nirvana AI Governance: How AI Policymaking Is Committing Three Old Fallacies
- **分类: cs.CY; cs.HC; cs.LG**

- **链接: [http://arxiv.org/pdf/2501.10384v2](http://arxiv.org/pdf/2501.10384v2)**

> **作者:** Jiawei Zhang
>
> **备注:** 9 pages
>
> **摘要:** This research applies Harold Demsetz's concept of the nirvana approach to the realm of AI governance and debunks three common fallacies in various AI policy proposals--"the grass is always greener on the other side," "free lunch," and "the people could be different." Through this, I expose fundamental flaws in the current AI regulatory proposal. First, some commentators intuitively believe that people are more reliable than machines and that government works better in risk control than companies' self-regulation, but they do not fully compare the differences between the status quo and the proposed replacements. Second, when proposing some regulatory tools, some policymakers and researchers do not realize and even gloss over the fact that harms and costs are also inherent in their proposals. Third, some policy proposals are initiated based on a false comparison between the AI-driven world, where AI does lead to some risks, and an entirely idealized world, where no risk exists at all. However, the appropriate approach is to compare the world where AI causes risks to the real world where risks are everywhere, but people can live well with these risks. The prevalence of these fallacies in AI governance underscores a broader issue: the tendency to idealize potential solutions without fully considering their real-world implications. This idealization can lead to regulatory proposals that are not only impractical but potentially harmful to innovation and societal progress.
>
---
