# 机器人 cs.RO

- **最新发布 72 篇**

- **更新 34 篇**

## 最新发布

#### [new 001] Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications
- **分类: cs.RO**

- **简介: 该论文研究基于信号时序逻辑（STL）的轨迹优化任务，旨在解决非光滑约束导致的优化困难。通过提出max/min算子的精确光滑重构，实现无近似误差的可微优化，提升规划的准确性与效率。**

- **链接: [http://arxiv.org/pdf/2511.07375v1](http://arxiv.org/pdf/2511.07375v1)**

> **作者:** Shaohang Han; Joris Verhagen; Jana Tumova
>
> **摘要:** We study motion planning under Signal Temporal Logic (STL), a useful formalism for specifying spatial-temporal requirements. We pose STL synthesis as a trajectory optimization problem leveraging the STL robustness semantics. To obtain a differentiable problem without approximation error, we introduce an exact reformulation of the max and min operators. The resulting method is exact, smooth, and sound. We validate it in numerical simulations, demonstrating its practical performance.
>
---
#### [new 002] A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving
- **分类: cs.RO; cs.AI; cs.CV**

- **简介: 该论文针对自动驾驶中视觉语言模型的幻觉问题，提出一种无需外部参考的低秩自洽方法，通过分解句子嵌入矩阵，利用残差幅度排序候选描述，精准筛选低幻觉输出，提升准确率与推理效率。**

- **链接: [http://arxiv.org/pdf/2511.06496v1](http://arxiv.org/pdf/2511.06496v1)**

> **作者:** Keke Long; Jiacheng Guo; Tianyun Zhang; Hongkai Yu; Xiaopeng Li
>
> **摘要:** Vision Language Models (VLMs) are increasingly used in autonomous driving to help understand traffic scenes, but they sometimes produce hallucinations, which are false details not grounded in the visual input. Detecting and mitigating hallucinations is challenging when ground-truth references are unavailable and model internals are inaccessible. This paper proposes a novel self-contained low-rank approach to automatically rank multiple candidate captions generated by multiple VLMs based on their hallucination levels, using only the captions themselves without requiring external references or model access. By constructing a sentence-embedding matrix and decomposing it into a low-rank consensus component and a sparse residual, we use the residual magnitude to rank captions: selecting the one with the smallest residual as the most hallucination-free. Experiments on the NuScenes dataset demonstrate that our approach achieves 87% selection accuracy in identifying hallucination-free captions, representing a 19% improvement over the unfiltered baseline and a 6-10% improvement over multi-agent debate method. The sorting produced by sparse error magnitudes shows strong correlation with human judgments of hallucinations, validating our scoring mechanism. Additionally, our method, which can be easily parallelized, reduces inference time by 51-67% compared to debate approaches, making it practical for real-time autonomous driving applications.
>
---
#### [new 003] OpenVLN: Open-world aerial Vision-Language Navigation
- **分类: cs.RO**

- **简介: 论文提出OpenVLN框架，解决无人机在复杂户外环境中基于语言指令的长程视觉语言导航问题，通过强化学习优化VLM并引入价值驱动的长程规划器，实现数据高效导航，显著提升导航成功率。**

- **链接: [http://arxiv.org/pdf/2511.06182v1](http://arxiv.org/pdf/2511.06182v1)**

> **作者:** Peican Lin; Gan Sun; Chenxi Liu; Fazeng Li; Weihong Ren; Yang Cong
>
> **备注:** Content: 8 pages 4 figures, conference under review
>
> **摘要:** Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.
>
---
#### [new 004] How Do VLAs Effectively Inherit from VLMs?
- **分类: cs.RO; cs.AI**

- **简介: 该论文研究视觉-语言-动作（VLA）模型如何有效继承视觉-语言模型（VLM）的先验知识，提出“GrinningFace”emoji操作任务作为诊断基准，系统评估多种知识迁移方法，揭示保留VLM先验对通用具身智能的关键作用。**

- **链接: [http://arxiv.org/pdf/2511.06619v1](http://arxiv.org/pdf/2511.06619v1)**

> **作者:** Chuheng Zhang; Rushuai Yang; Xiaoyu Chen; Kaixin Wang; Li Zhao; Yi Chen; Jiang Bian
>
> **摘要:** Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.
>
---
#### [new 005] Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills
- **分类: cs.RO**

- **简介: 该论文提出一种基于VLM规划原子技能、强化学习模拟训练与视觉触觉扩散策略蒸馏的框架，解决长周期接触型操作任务中依赖人工演示与高成本的问题，实现无实物演示的可扩展泛化。**

- **链接: [http://arxiv.org/pdf/2511.05855v1](http://arxiv.org/pdf/2511.05855v1)**

> **作者:** Jiayu Zhou; Qiwei Wu; Jian Li; Zhe Chen; Xiaogang Xiong; Renjing Xu
>
> **备注:** Accepted for the 40th Annual AAAI Conference on Artificial Intelligence (2026)
>
> **摘要:** Autonomous execution of long-horizon, contact-rich manipulation tasks traditionally requires extensive real-world data and expert engineering, posing significant cost and scalability challenges. This paper proposes a novel framework integrating hierarchical semantic decomposition, reinforcement learning (RL), visual language models (VLMs), and knowledge distillation to overcome these limitations. Complex tasks are decomposed into atomic skills, with RL-trained policies for each primitive exclusively in simulation. Crucially, our RL formulation incorporates explicit force constraints to prevent object damage during delicate interactions. VLMs perform high-level task decomposition and skill planning, generating diverse expert demonstrations. These are distilled into a unified policy via Visual-Tactile Diffusion Policy for end-to-end execution. We conduct comprehensive ablation studies exploring different VLM-based task planners to identify optimal demonstration generation pipelines, and systematically compare imitation learning algorithms for skill distillation. Extensive simulation experiments and physical deployment validate that our approach achieves policy learning for long-horizon manipulation without costly human demonstrations, while the VLM-guided atomic skill framework enables scalable generalization to diverse tasks.
>
---
#### [new 006] Robust Differentiable Collision Detection for General Objects
- **分类: cs.RO**

- **简介: 该论文提出一种鲁棒可微碰撞检测框架，支持凸凹物体，解决传统方法非可微导致梯度优化受限的问题，通过距离式平滑与自适应采样实现高效梯度计算，并应用于灵巧抓取优化。**

- **链接: [http://arxiv.org/pdf/2511.06267v1](http://arxiv.org/pdf/2511.06267v1)**

> **作者:** Jiayi Chen; Wei Zhao; Liangwang Ruan; Baoquan Chen; He Wang
>
> **摘要:** Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision.
>
---
#### [new 007] Integration of Visual SLAM into Consumer-Grade Automotive Localization
- **分类: cs.RO**

- **简介: 该论文提出融合视觉SLAM与车辆动力学模型的框架，解决消费级汽车定位中陀螺仪标定不准的问题，实现在线校准，显著提升定位精度，优于现有方法。**

- **链接: [http://arxiv.org/pdf/2511.06919v1](http://arxiv.org/pdf/2511.06919v1)**

> **作者:** Luis Diener; Jens Kalkkuhl; Markus Enzweiler
>
> **备注:** This manuscript has been submitted to the IEEE for possible publication
>
> **摘要:** Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is limited by systematic errors and calibration. While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored. This paper investigates how visual SLAM can be integrated into consumer-grade vehicle localization systems to improve performance. We propose a framework that fuses visual SLAM with a lateral vehicle dynamics model to achieve online gyroscope calibration under realistic driving conditions. Experimental results demonstrate that vision-based integration significantly improves gyroscope calibration accuracy and thus enhances overall localization performance, highlighting a promising path toward higher automotive localization accuracy. We provide results on both proprietary and public datasets, showing improved performance and superior localization accuracy on a public benchmark compared to state-of-the-art methods.
>
---
#### [new 008] Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring
- **分类: cs.RO; cs.SY; eess.SY**

- **简介: 该论文提出一种用于生态监测的欠驱动仿生水下机器人，通过强化学习优化最小化驱动的游动行为，解决传统水下机器人能耗高、灵活性差的问题，并在FishGym仿真平台验证尾鳍振荡机制与行为学习效果。**

- **链接: [http://arxiv.org/pdf/2511.06578v1](http://arxiv.org/pdf/2511.06578v1)**

> **作者:** Kaustubh Singh; Shivam Kumar; Shashikant Pawar; Sandeep Manjanna
>
> **备注:** ICRA RUNE 2024 Workshop Paper
>
> **摘要:** In this paper, we present an underactuated biomimetic underwater robot that is suitable for ecosystem monitoring in both marine and freshwater environments. We present an updated mechanical design for a fish-like robot and propose minimal actuation behaviors learned using reinforcement learning techniques. We present our preliminary mechanical design of the tail oscillation mechanism and illustrate the swimming behaviors on FishGym simulator, where the reinforcement learning techniques will be tested on
>
---
#### [new 009] Semi-distributed Cross-modal Air-Ground Relative Localization
- **分类: cs.RO; cs.CV**

- **简介: 该论文提出一种半分布式跨模态空地相对定位框架，解决传统多机器人SLAM耦合严重、带宽高问题。通过UGV融合多传感器进行局部优化，仅传输关键点与描述子，实现高精度、低带宽（<0.3 Mbps）的相对位姿估计。**

- **链接: [http://arxiv.org/pdf/2511.06749v1](http://arxiv.org/pdf/2511.06749v1)**

> **作者:** Weining Lu; Deer Bin; Lian Ma; Ming Ma; Zhihao Ma; Xiangyang Chen; Longfei Wang; Yixiao Feng; Zhouxian Jiang; Yongliang Shi; Bin Liang
>
> **备注:** 7 pages, 3 figures. Accepted by IROS 2025
>
> **摘要:** Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git.
>
---
#### [new 010] An Open-Source, Reproducible Tensegrity Robot that can Navigate Among Obstacles
- **分类: cs.RO**

- **简介: 该论文提出一种开源可复现的三杆张拉整体机器人导航系统，解决其复杂动力学下路径规划与避障难题，集成软硬件实现姿态跟踪与避障控制，并在多环境验证鲁棒性与可复现性。**

- **链接: [http://arxiv.org/pdf/2511.05798v1](http://arxiv.org/pdf/2511.05798v1)**

> **作者:** William R. Johnson III; Patrick Meng; Nelson Chen; Luca Cimatti; Augustin Vercoutere; Mridul Aanjaneya; Rebecca Kramer-Bottiglio; Kostas E. Bekris
>
> **摘要:** Tensegrity robots, composed of rigid struts and elastic tendons, provide impact resistance, low mass, and adaptability to unstructured terrain. Their compliance and complex, coupled dynamics, however, present modeling and control challenges, hindering path planning and obstacle avoidance. This paper presents a complete, open-source, and reproducible system that enables navigation for a 3-bar tensegrity robot. The system comprises: (i) an inexpensive, open-source hardware design, and (ii) an integrated, open-source software stack for physics-based modeling, system identification, state estimation, path planning, and control. All hardware and software are publicly available at https://sites.google.com/view/tensegrity-navigation/. The proposed system tracks the robot's pose and executes collision-free paths to a specified goal among known obstacle locations. System robustness is demonstrated through experiments involving unmodeled environmental challenges, including a vertical drop, an incline, and granular media, culminating in an outdoor field demonstration. To validate reproducibility, experiments were conducted using robot instances at two different laboratories. This work provides the robotics community with a complete navigation system for a compliant, impact-resistant, and shape-morphing robot. This system is intended to serve as a springboard for advancing the navigation capabilities of other unconventional robotic platforms.
>
---
#### [new 011] PlaCo: a QP-based robot planning and control framework
- **分类: cs.RO**

- **简介: 论文提出PlaCo，一个基于二次规划（QP）的机器人规划与控制框架，旨在简化QP问题的建模与求解。通过提供模块化、直观的接口，支持Python快速原型与C++实时部署，降低机器人控制算法开发门槛。**

- **链接: [http://arxiv.org/pdf/2511.06141v1](http://arxiv.org/pdf/2511.06141v1)**

> **作者:** Marc Duclusaud; Grégoire Passault; Vincent Padois; Olivier Ly
>
> **摘要:** This article introduces PlaCo, a software framework designed to simplify the formulation and solution of Quadratic Programming (QP)-based planning and control problems for robotic systems. PlaCo provides a high-level interface that abstracts away the low-level mathematical formulation of QP problems, allowing users to specify tasks and constraints in a modular and intuitive manner. The framework supports both Python bindings for rapid prototyping and a C++ implementation for real-time performance.
>
---
#### [new 012] Rapidly Learning Soft Robot Control via Implicit Time-Stepping
- **分类: cs.RO; cs.AI**

- **简介: 该论文针对软体机器人仿真效率低的问题，提出基于隐式时间步进的DisMech仿真器与delta自然曲率控制方法，实现高效策略学习，在不损失精度下提速6–40倍，突破软体机器人仿真与学习瓶颈。**

- **链接: [http://arxiv.org/pdf/2511.06667v1](http://arxiv.org/pdf/2511.06667v1)**

> **作者:** Andrew Choi; Dezhong Tong
>
> **备注:** Code: https://github.com/QuantuMope/dismech-rl
>
> **摘要:** With the explosive growth of rigid-body simulators, policy learning in simulation has become the de facto standard for most rigid morphologies. In contrast, soft robotic simulation frameworks remain scarce and are seldom adopted by the soft robotics community. This gap stems partly from the lack of easy-to-use, general-purpose frameworks and partly from the high computational cost of accurately simulating continuum mechanics, which often renders policy learning infeasible. In this work, we demonstrate that rapid soft robot policy learning is indeed achievable via implicit time-stepping. Our simulator of choice, DisMech, is a general-purpose, fully implicit soft-body simulator capable of handling both soft dynamics and frictional contact. We further introduce delta natural curvature control, a method analogous to delta joint position control in rigid manipulators, providing an intuitive and effective means of enacting control for soft robot learning. To highlight the benefits of implicit time-stepping and delta curvature control, we conduct extensive comparisons across four diverse soft manipulator tasks against one of the most widely used soft-body frameworks, Elastica. With implicit time-stepping, parallel stepping of 500 environments achieves up to 6x faster speeds for non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a comprehensive sim-to-sim gap evaluation--training policies in one simulator and evaluating them in another--demonstrates that implicit time-stepping provides a rare free lunch: dramatic speedups achieved without sacrificing accuracy.
>
---
#### [new 013] External Photoreflective Tactile Sensing Based on Surface Deformation Measurement
- **分类: cs.RO**

- **简介: 该论文提出一种外部光反射式触觉传感方法，通过测量硅胶皮肤表面形变估算接触力，无需嵌入式传感器。解决了软机器人触觉感知与结构柔顺性矛盾的问题，实现了易部署、高耐久的模块化触觉系统。**

- **链接: [http://arxiv.org/pdf/2511.06311v1](http://arxiv.org/pdf/2511.06311v1)**

> **作者:** Seiichi Yamamoto; Hiroki Ishizuka; Takumi Kawasetsu; Koh Hosoda; Takayuki Kameoka; Kango Yanagida; Takato Horii; Sei Ikeda; Osamu Oshiro
>
> **备注:** This work has been submitted to the IEEE for possible publication
>
> **摘要:** We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration.
>
---
#### [new 014] Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot
- **分类: cs.RO**

- **简介: 该论文针对6自由度轮腿机器人在不平地形下的运动控制问题，构建了完整动力学模型，融合激光惯性里程计与改进PCA进行地形估计，并设计分层优化的全身控制器，实现稳定越障。**

- **链接: [http://arxiv.org/pdf/2511.06397v1](http://arxiv.org/pdf/2511.06397v1)**

> **作者:** Cong Wen; Yunfei Li; Kexin Liu; Yixin Qiu; Xuanhong Liao; Tianyu Wang; Dingchuan Liu; Tao Zhang; Ximin Lyu
>
> **备注:** 8 pages, 8 figures
>
> **摘要:** Wheeled bipedal robots have garnered increasing attention in exploration and inspection. However, most research simplifies calculations by ignoring leg dynamics, thereby restricting the robot's full motion potential. Additionally, robots face challenges when traversing uneven terrain. To address the aforementioned issue, we develop a complete dynamics model and design a whole-body control framework with terrain estimation for a novel 6 degrees of freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics of the robot and a ground contact model based on the estimated ground normal vector. We use a LiDAR inertial odometry framework and improved Principal Component Analysis for terrain estimation. Task controllers, including PD control law and LQR, are employed for pose control and centroidal dynamics-based balance control, respectively. Furthermore, a hierarchical optimization approach is used to solve the whole-body control problem. We validate the performance of the terrain estimation algorithm and demonstrate the algorithm's robustness and ability to traverse uneven terrain through both simulation and real-world experiments.
>
---
#### [new 015] HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects
- **分类: cs.RO**

- **简介: HDCNet提出一种融合Transformer、CNN与Mamba的混合深度补全网络，解决透明与反光物体深度感知难题，提升机器人抓取精度，实验表明其在多个数据集上达SOTA，抓取成功率提升60%。**

- **链接: [http://arxiv.org/pdf/2511.07081v1](http://arxiv.org/pdf/2511.07081v1)**

> **作者:** Guanghu Xie; Mingxu Li; Songwei Wu; Yang Liu; Zongwu Xie; Baoshi Cao; Hong Liu
>
> **摘要:** Depth perception of transparent and reflective objects has long been a critical challenge in robotic manipulation.Conventional depth sensors often fail to provide reliable measurements on such surfaces, limiting the performance of robots in perception and grasping tasks. To address this issue, we propose a novel depth completion network,HDCNet,which integrates the complementary strengths of Transformer,CNN and Mamba architectures.Specifically,the encoder is designed as a dual-branch Transformer-CNN framework to extract modality-specific features. At the shallow layers of the encoder, we introduce a lightweight multimodal fusion module to effectively integrate low-level features. At the network bottleneck,a Transformer-Mamba hybrid fusion module is developed to achieve deep integration of high-level semantic and global contextual information, significantly enhancing depth completion accuracy and robustness. Extensive evaluations on multiple public datasets demonstrate that HDCNet achieves state-of-the-art(SOTA) performance in depth completion tasks.Furthermore,robotic grasping experiments show that HDCNet substantially improves grasp success rates for transparent and reflective objects,achieving up to a 60% increase.
>
---
#### [new 016] 3D Mapping Using a Lightweight and Low-Power Monocular Camera Embedded inside a Gripper of Limbed Climbing Robots
- **分类: cs.RO**

- **简介: 该论文面向月球/火星垂直壁面探测，解决单目相机SLAM尺度未知问题，提出融合关节运动学的因子图优化方法，实现轻量低功耗单目相机实时构建度量尺度3D地图，支持自主抓取。**

- **链接: [http://arxiv.org/pdf/2511.05816v1](http://arxiv.org/pdf/2511.05816v1)**

> **作者:** Taku Okawara; Ryo Nishibe; Mao Kasano; Kentaro Uno; Kazuya Yoshida
>
> **备注:** International Conference on Space Robotics (iSpaRo)
>
> **摘要:** Limbed climbing robots are designed to explore challenging vertical walls, such as the skylights of the Moon and Mars. In such robots, the primary role of a hand-eye camera is to accurately estimate 3D positions of graspable points (i.e., convex terrain surfaces) thanks to its close-up views. While conventional climbing robots often employ RGB-D cameras as hand-eye cameras to facilitate straightforward 3D terrain mapping and graspable point detection, RGB-D cameras are large and consume considerable power. This work presents a 3D terrain mapping system designed for space exploration using limbed climbing robots equipped with a monocular hand-eye camera. Compared to RGB-D cameras, monocular cameras are more lightweight, compact structures, and have lower power consumption. Although monocular SLAM can be used to construct 3D maps, it suffers from scale ambiguity. To address this limitation, we propose a SLAM method that fuses monocular visual constraints with limb forward kinematics. The proposed method jointly estimates time-series gripper poses and the global metric scale of the 3D map based on factor graph optimization. We validate the proposed framework through both physics-based simulations and real-world experiments. The results demonstrate that our framework constructs a metrically scaled 3D terrain map in real-time and enables autonomous grasping of convex terrain surfaces using a monocular hand-eye camera, without relying on RGB-D cameras. Our method contributes to scalable and energy-efficient perception for future space missions involving limbed climbing robots. See the video summary here: https://youtu.be/fMBrrVNKJfc
>
---
#### [new 017] Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation
- **分类: cs.RO**

- **简介: 该论文针对机器人PID控制参数调优耗时、依赖经验的问题，提出一种分层元学习与强化学习框架，结合物理数据增强，实现高效自适应控制，在双机器人平台验证了显著性能提升与优化上限效应。**

- **链接: [http://arxiv.org/pdf/2511.06500v1](http://arxiv.org/pdf/2511.06500v1)**

> **作者:** JiaHao Wu; ShengWen Yu
>
> **备注:** 21 pages,12 tables, 6 figures
>
> **摘要:** Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\% average improvement on Franka Panda (6.26{\deg} MAE), with exceptional gains in high-load joints (J2: 80.4\% improvement from 12.36{\deg} to 2.42{\deg}). Critically, this work discovers the \textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\%, no disturbance: +16.6\%, average: +10.0\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems.
>
---
#### [new 018] From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation
- **分类: cs.RO**

- **简介: 该论文提出一种语言条件安全过滤框架，用于机器人导航中将自然语言指令转化为结构化安全约束并实时执行。解决传统方法约束窄、泛化弱的问题，融合LLM、3D感知与MPC，实现多场景语义与几何安全保障。**

- **链接: [http://arxiv.org/pdf/2511.05889v1](http://arxiv.org/pdf/2511.05889v1)**

> **作者:** Zeyuan Feng; Haimingyue Zhang; Somil Bansal
>
> **摘要:** As robots become increasingly integrated into open-world, human-centered environments, their ability to interpret natural language instructions and adhere to safety constraints is critical for effective and trustworthy interaction. Existing approaches often focus on mapping language to reward functions instead of safety specifications or address only narrow constraint classes (e.g., obstacle avoidance), limiting their robustness and applicability. We propose a modular framework for language-conditioned safety in robot navigation. Our framework is composed of three core components: (1) a large language model (LLM)-based module that translates free-form instructions into structured safety specifications, (2) a perception module that grounds these specifications by maintaining object-level 3D representations of the environment, and (3) a model predictive control (MPC)-based safety filter that enforces both semantic and geometric constraints in real time. We evaluate the effectiveness of the proposed framework through both simulation studies and hardware experiments, demonstrating that it robustly interprets and enforces diverse language-specified constraints across a wide range of environments and scenarios.
>
---
#### [new 019] Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving
- **分类: cs.RO; cs.LG**

- **简介: 该论文针对强化学习从仿真到实车迁移困难的问题，提出解耦运动规划与车辆控制的轨迹对齐框架，通过虚拟车与实车的时空对齐实现零样本迁移，提升实车部署鲁棒性。**

- **链接: [http://arxiv.org/pdf/2511.07155v1](http://arxiv.org/pdf/2511.07155v1)**

> **作者:** Thomas Steinecker; Alexander Bienemann; Denis Trescher; Thorsten Luettel; Mirko Maehlisch
>
> **摘要:** Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.
>
---
#### [new 020] Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning
- **分类: cs.RO; cs.AI**

- **简介: 该论文针对视觉强化学习中目标不可行问题，提出PI-RIG方法，通过物理约束的增强VAE将潜空间分解为动力学与视觉因子，生成符合物理规律的目标，提升机器人自主学习技能的效率与质量。**

- **链接: [http://arxiv.org/pdf/2511.06745v1](http://arxiv.org/pdf/2511.06745v1)**

> **作者:** Lan Thi Ha Nguyen; Kien Ton Manh; Anh Do Duc; Nam Pham Hai
>
> **摘要:** Self-supervised goal-conditioned reinforcement learning enables robots to autonomously acquire diverse skills without human supervision. However, a central challenge is the goal setting problem: robots must propose feasible and diverse goals that are achievable in their current environment. Existing methods like RIG (Visual Reinforcement Learning with Imagined Goals) use variational autoencoder (VAE) to generate goals in a learned latent space but have the limitation of producing physically implausible goals that hinder learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates physical constraints directly into the VAE training process through a novel Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling the generation of physically consistent and achievable goals. Our key innovation is the explicit separation of the latent space into physics variables governing object dynamics and environmental factors capturing visual appearance, while enforcing physical consistency through differential equation constraints and conservation laws. This enables the generation of physically consistent and achievable goals that respect fundamental physical principles such as object permanence, collision constraints, and dynamic feasibility. Through extensive experiments, we demonstrate that this physics-informed goal generation significantly improves the quality of proposed goals, leading to more effective exploration and better skill acquisition in visual robotic manipulation tasks including reaching, pushing, and pick-and-place scenarios.
>
---
#### [new 021] Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets
- **分类: cs.RO**

- **简介: 该论文针对移动机器人车队路由问题，提出一种自动化连续空间路网生成方法，融合运输需求与最小间距约束，通过自由空间离散化与K最短路径优化，提升路网效率与鲁棒性，优于传统网格与随机采样方法。**

- **链接: [http://arxiv.org/pdf/2511.07175v1](http://arxiv.org/pdf/2511.07175v1)**

> **作者:** Marvin Rüdt; Constantin Enke; Kai Furmans
>
> **备注:** submitted to the IEEE for possible publication; 8 pages, 6 figures, 2 tables
>
> **摘要:** Efficient routing of mobile robot fleets is crucial in intralogistics, where delays and deadlocks can substantially reduce system throughput. Roadmap design, specifying feasible transport routes, directly affects fleet coordination and computational performance. Existing approaches are either grid-based, compromising geometric precision, or continuous-space approaches that disregard practical constraints. This paper presents an automated roadmap generation approach that bridges this gap by operating in continuous-space, integrating station-to-station transport demand and enforcing minimum distance constraints for nodes and edges. By combining free space discretization, transport demand-driven $K$-shortest-path optimization, and path smoothing, the approach produces roadmaps tailored to intralogistics applications. Evaluation across multiple intralogistics use cases demonstrates that the proposed approach consistently outperforms established baselines (4-connected grid, 8-connected grid, and random sampling), achieving lower structural complexity, higher redundancy, and near-optimal path lengths, enabling efficient and robust routing of mobile robot fleets.
>
---
#### [new 022] Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control
- **分类: cs.RO; math.DS**

- **简介: 该论文提出基于Koopman算子的全局线性化方法，解决机器人接触动力学切换导致的非凸控制难题，实现接触变化下的凸优化预测控制，支持腿足机器人与机械臂的实时复杂操作。**

- **链接: [http://arxiv.org/pdf/2511.06515v1](http://arxiv.org/pdf/2511.06515v1)**

> **作者:** Cormac O'Neill; Jasmine Terrones; H. Harry Asada
>
> **摘要:** Controlling robots that dynamically engage in contact with their environment is a pressing challenge. Whether a legged robot making-and-breaking contact with a floor, or a manipulator grasping objects, contact is everywhere. Unfortunately, the switching of dynamics at contact boundaries makes control difficult. Predictive controllers face non-convex optimization problems when contact is involved. Here, we overcome this difficulty by applying Koopman operators to subsume the segmented dynamics due to contact changes into a unified, globally-linear model in an embedding space. We show that viscoelastic contact at robot-environment interactions underpins the use of Koopman operators without approximation to control inputs. This methodology enables the convex Model Predictive Control of a legged robot, and the real-time control of a manipulator engaged in dynamic pushing. In this work, we show that our method allows robots to discover elaborate control strategies in real-time over time horizons with multiple contact changes, and the method is applicable to broad fields beyond robotics.
>
---
#### [new 023] VLM-driven Skill Selection for Robotic Assembly Tasks
- **分类: cs.RO**

- **简介: 该论文提出一种基于视觉语言模型（VLM）与模仿学习的机器人装配框架，解决传统装配系统缺乏语义理解与灵活性的问题，通过融合视觉感知、语言理解与结构化技能库，实现可解释、高成功率的自适应装配操作。**

- **链接: [http://arxiv.org/pdf/2511.05680v1](http://arxiv.org/pdf/2511.05680v1)**

> **作者:** Jeong-Jung Kim; Doo-Yeol Koh; Chang-Hyun Kim
>
> **摘要:** This paper presents a robotic assembly framework that combines Vision-Language Models (VLMs) with imitation learning for assembly manipulation tasks. Our system employs a gripper-equipped robot that moves in 3D space to perform assembly operations. The framework integrates visual perception, natural language understanding, and learned primitive skills to enable flexible and adaptive robotic manipulation. Experimental results demonstrate the effectiveness of our approach in assembly scenarios, achieving high success rates while maintaining interpretability through the structured primitive skill decomposition.
>
---
#### [new 024] Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator
- **分类: cs.RO**

- **简介: 该论文提出RGBench基准，解决机器人衣物操作中缺乏高保真模拟器与真实数据的问题，构建了6000+衣物模型、高性能模拟器及评估协议，显著提升模拟精度与速度。**

- **链接: [http://arxiv.org/pdf/2511.06434v1](http://arxiv.org/pdf/2511.06434v1)**

> **作者:** Wenkang Hu; Xincheng Tang; Yanzhi E; Yitong Li; Zhengjie Shu; Wei Li; Huamin Wang; Ruigang Yang
>
> **备注:** 2026 AAAI Accept
>
> **摘要:** While there has been significant progress to use simulated data to learn robotic manipulation of rigid objects, applying its success to deformable objects has been hindered by the lack of both deformable object models and realistic non-rigid body simulators. In this paper, we present Real Garment Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of garments. It features a diverse set of over 6000 garment mesh models, a new high-performance simulator, and a comprehensive protocol to evaluate garment simulation quality with carefully measured real garment dynamics. Our experiments demonstrate that our simulator outperforms currently available cloth simulators by a large margin, reducing simulation error by 20% while maintaining a speed of 3 times faster. We will publicly release RGBench to accelerate future research in robotic garment manipulation. Website: https://rgbench.github.io/
>
---
#### [new 025] Human-Level Actuation for Humanoids
- **分类: cs.RO**

- **简介: 该论文提出一种量化人形机器人“类人驱动”能力的框架（HLAS），解决传统峰值扭矩指标片面的问题。通过标准化关节对比、定义力-功率等效包络和多维度评分，实现对驱动系统性能的科学评估与对比。**

- **链接: [http://arxiv.org/pdf/2511.06796v1](http://arxiv.org/pdf/2511.06796v1)**

> **作者:** MD-Nazmus Sunbeam
>
> **备注:** 61 pages, 8 figures, 7 tables, and 12 numbered equations
>
> **摘要:** Claims that humanoid robots achieve ``human-level'' actuation are common but rarely quantified. Peak torque or speed specifications tell us little about whether a joint can deliver the right combination of torque, power, and endurance at task-relevant postures and rates. We introduce a comprehensive framework that makes ``human-level'' measurable and comparable across systems. Our approach has three components. First, a kinematic \emph{DoF atlas} standardizes joint coordinate systems and ranges of motion using ISB-based conventions, ensuring that human and robot joints are compared in the same reference frames. Second, \emph{Human-Equivalence Envelopes (HEE)} define per-joint requirements by measuring whether a robot meets human torque \emph{and} power simultaneously at the same joint angle and rate $(q,\omega)$, weighted by positive mechanical work in task-specific bands (walking, stairs, lifting, reaching, and hand actions). Third, the \emph{Human-Level Actuation Score (HLAS)} aggregates six physically grounded factors: workspace coverage (ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal sustainability. We provide detailed measurement protocols using dynamometry, electrical power monitoring, and thermal testing that yield every HLAS input from reproducible experiments. A worked example demonstrates HLAS computation for a multi-joint humanoid, showing how the score exposes actuator trade-offs (gearing ratio versus bandwidth and efficiency) that peak-torque specifications obscure. The framework serves as both a design specification for humanoid development and a benchmarking standard for comparing actuation systems, with all components grounded in published human biomechanics data.
>
---
#### [new 026] Unified Humanoid Fall-Safety Policy from a Few Demonstrations
- **分类: cs.RO**

- **简介: 该论文提出一种统一的跌倒安全策略，融合少量人类示范与强化学习，实现跌倒预防、冲击缓解与快速恢复的一体化自主控制，显著提升人形机器人在真实环境中的安全性与韧性。**

- **链接: [http://arxiv.org/pdf/2511.07407v1](http://arxiv.org/pdf/2511.07407v1)**

> **作者:** Zhengjie Xu; Ye Li; Kwan-yee Lin; Stella X. Yu
>
> **摘要:** Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/.
>
---
#### [new 027] Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation
- **分类: cs.RO; cs.AI**

- **简介: 该论文针对开放词汇移动操作中的基座位姿选择问题，提出一种基于语义-几何协同的粗到细探索方法，利用视觉语言模型引导 affordance 驱动的位姿优化，显著提升操作成功率。**

- **链接: [http://arxiv.org/pdf/2511.06240v1](http://arxiv.org/pdf/2511.06240v1)**

> **作者:** Tzu-Jung Lin; Jia-Fong Yeh; Hung-Ting Su; Chung-Yi Lin; Yi-Ting Chen; Winston H. Hsu
>
> **备注:** Accepted to AAAI 2026
>
> **摘要:** In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM.
>
---
#### [new 028] ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval
- **分类: cs.RO; 68T40; I.2.9; I.2.10; I.2.6**

- **简介: 论文提出ExpReS-VLA，专精视觉-语言-动作模型在特定任务上的性能，通过经验回放与检索减少记忆开销97%，结合对比损失提升学习效率，在仿真与实物实验中显著提升成功率，实现快速高效部署。**

- **链接: [http://arxiv.org/pdf/2511.06202v1](http://arxiv.org/pdf/2511.06202v1)**

> **作者:** Shahram Najam Syed; Yatharth Ahuja; Arthur Jakobsson; Jeff Ichnowski
>
> **备注:** 10 pages, 5 figures, submitted to ICRA 2026. Equal contribution by first two authors
>
> **摘要:** Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.
>
---
#### [new 029] Robustness study of the bio-inspired musculoskeletal arm robot based on the data-driven iterative learning algorithm
- **分类: cs.RO**

- **简介: 该论文面向仿生机器人控制任务，旨在提升肌肉骨骼机器人在扰动下的鲁棒性。设计了轻量化肌腱驱动臂与模块化人工肌肉系统，结合数据驱动迭代学习算法，实现高精度轨迹跟踪，验证了其在20%负载扰动下的稳定性。**

- **链接: [http://arxiv.org/pdf/2511.05995v1](http://arxiv.org/pdf/2511.05995v1)**

> **作者:** Jianbo Yuan; Jing Dai; Yerui Fan; Yaxiong Wu; Yunpeng Liang; Weixin Yan
>
> **备注:** 20 pages, 13 figures
>
> **摘要:** The human arm exhibits remarkable capabilities, including both explosive power and precision, which demonstrate dexterity, compliance, and robustness in unstructured environments. Developing robotic systems that emulate human-like operational characteristics through musculoskeletal structures has long been a research focus. In this study, we designed a novel lightweight tendon-driven musculoskeletal arm (LTDM-Arm), featuring a seven degree-of-freedom (DOF) skeletal joint system and a modularized artificial muscular system (MAMS) with 15 actuators. Additionally, we employed a Hilly-type muscle model and data-driven iterative learning control (DDILC) to learn and refine activation signals for repetitive tasks within a finite time frame. We validated the anti-interference capabilities of the musculoskeletal system through both simulations and experiments. The results show that the LTDM-Arm system can effectively achieve desired trajectory tracking tasks, even under load disturbances of 20 % in simulation and 15 % in experiments. This research lays the foundation for developing advanced robotic systems with human-like operational performance.
>
---
#### [new 030] Fair and Safe: A Real-Time Hierarchical Control Framework for Intersections
- **分类: cs.RO**

- **简介: 该论文提出一种实时分层控制框架，解决自动驾驶车辆过交叉口时公平性与安全性难以兼顾的问题。通过顶层公平资源分配与底层安全轨迹执行，实现零碰撞、低延迟与高公平性。**

- **链接: [http://arxiv.org/pdf/2511.05886v1](http://arxiv.org/pdf/2511.05886v1)**

> **作者:** Lei Shi; Yongju Kim; Xinzhi Zhong; Wissam Kontar; Qichao Liu; Soyoung Ahn
>
> **摘要:** Ensuring fairness in the coordination of connected and automated vehicles at intersections is essential for equitable access, social acceptance, and long-term system efficiency, yet it remains underexplored in safety-critical, real-time traffic control. This paper proposes a fairness-aware hierarchical control framework that explicitly integrates inequity aversion into intersection management. At the top layer, a centralized allocation module assigns control authority (i.e., selects a single vehicle to execute its trajectory) by maximizing a utility that accounts for waiting time, urgency, control history, and velocity deviation. At the bottom layer, the authorized vehicle executes a precomputed trajectory using a Linear Quadratic Regulator (LQR) and applies a high-order Control Barrier Function (HOCBF)-based safety filter for real-time collision avoidance. Simulation results across varying traffic demands and demand distributions demonstrate that the proposed framework achieves near-perfect fairness, eliminates collisions, reduces average delay, and maintains real-time feasibility. These results highlight that fairness can be systematically incorporated without sacrificing safety or performance, enabling scalable and equitable coordination for future autonomous traffic systems.
>
---
#### [new 031] Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields
- **分类: cs.RO; cs.AI; cs.CV; cs.DC; cs.GR**

- **简介: 论文提出Lightning Grasp，一种基于接触场的高效程序化抓取生成方法，解决灵巧手对不规则工具类物体的实时多样抓取合成难题，无需复杂能量函数与敏感初始化，速度远超现有方法。**

- **链接: [http://arxiv.org/pdf/2511.07418v1](http://arxiv.org/pdf/2511.07418v1)**

> **作者:** Zhao-Heng Yin; Pieter Abbeel
>
> **备注:** Code: https://github.com/zhaohengyin/lightning-grasp
>
> **摘要:** Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.
>
---
#### [new 032] Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective
- **分类: cs.RO; cs.AI**

- **简介: 该论文研究如何将视觉语言模型（VLMs）作为闭环符号规划器用于机器人控制，从控制理论视角分析控制时域与热启动对规划性能的影响，旨在解决VLMs因黑箱特性导致的不可预测错误问题。**

- **链接: [http://arxiv.org/pdf/2511.07410v1](http://arxiv.org/pdf/2511.07410v1)**

> **作者:** Hao Wang; Sathwik Karnik; Bea Lim; Somil Bansal
>
> **摘要:** Large Language Models (LLMs) and Vision Language Models (VLMs) have been widely used for embodied symbolic planning. Yet, how to effectively use these models for closed-loop symbolic planning remains largely unexplored. Because they operate as black boxes, LLMs and VLMs can produce unpredictable or costly errors, making their use in high-level robotic planning especially challenging. In this work, we investigate how to use VLMs as closed-loop symbolic planners for robotic applications from a control-theoretic perspective. Concretely, we study how the control horizon and warm-starting impact the performance of VLM symbolic planners. We design and conduct controlled experiments to gain insights that are broadly applicable to utilizing VLMs as closed-loop symbolic planners, and we discuss recommendations that can help improve the performance of VLM symbolic planners.
>
---
#### [new 033] A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms
- **分类: cs.RO**

- **简介: 该论文提出统一随机模型，揭示蚂蚁、物理系统与机器人 swarm 共享“不同能量约束下的最大化”机制，解决生物与物理系统行为缺乏统一解释的问题，并验证其在机器人 swarm 中实现高效协同的可行性。**

- **链接: [http://arxiv.org/pdf/2511.05785v1](http://arxiv.org/pdf/2511.05785v1)**

> **作者:** Lianhao Yin; Haiping Yu; Pascal Spino; Daniela Rus
>
> **摘要:** Biological swarms, such as ant colonies, achieve collective goals through decentralized and stochastic individual behaviors. Similarly, physical systems composed of gases, liquids, and solids exhibit random particle motion governed by entropy maximization, yet do not achieve collective objectives. Despite this analogy, no unified framework exists to explain the stochastic behavior in both biological and physical systems. Here, we present empirical evidence from \textit{Formica polyctena} ants that reveals a shared statistical mechanism underlying both systems: maximization under different energy function constraints. We further demonstrate that robotic swarms governed by this principle can exhibit scalable, decentralized cooperation, mimicking physical phase-like behaviors with minimal individual computation. These findings established a unified stochastic model linking biological, physical, and robotic swarms, offering a scalable principle for designing robust and intelligent swarm robotics.
>
---
#### [new 034] Development and testing of novel soft sleeve actuators
- **分类: cs.RO**

- **简介: 该论文面向老龄化与神经肌肉疾病患者，提出一种新型软体袖套执行器，解决传统刚性设备笨重、适配差问题。通过3D打印柔性材料，实现线性、弯曲、扭转多轴运动，提升力传递效率与穿戴舒适性，构建可制造的统一设计框架。**

- **链接: [http://arxiv.org/pdf/2511.06102v1](http://arxiv.org/pdf/2511.06102v1)**

> **作者:** Mohammed Abboodi
>
> **备注:** PhD thesis
>
> **摘要:** Aging populations and the rising prevalence of neurological and musculoskeletal disorders increase the demand for wearable mobility assistive devices that are effective, comfortable, and anatomically compatible. Many existing systems use rigid mechanisms and bulky interfaces that impede force transmission and reduce wearability. This study introduces a soft sleeve actuation architecture that conforms to the limb while transmitting forces and moments efficiently. We develop three soft sleeve actuators that produce linear, bending, and twisting motion, and an omnidirectional design that combines these motions in one device. Actuators are fabricated from thermoplastic elastomers using a customized fused filament fabrication process that produces airtight and compliant structures and resolves leakage observed with conventional methods. A dedicated experimental platform quantifies kinematic outputs such as displacement, angle, and twist, and kinetic outputs such as force and torque under low pneumatic pressures. A parametric study varies geometric features and material properties to determine their influence on performance. Results show reproducible multi axis motion with improved transfer of force to the limb and reduced need for complex attachment hardware. The work establishes a unified and manufacturable framework for soft sleeve actuation that enables compact and user centered assistive technologies with enhanced kinematic and kinetic performance.
>
---
#### [new 035] Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion
- **分类: cs.RO**

- **简介: 该论文研究深度强化学习中双足机器人仿真到现实的迁移问题，分析仿真差距根源，提出模型精度提升与策略鲁棒性训练双路径解决方案，构建系统性框架以提升迁移效果。**

- **链接: [http://arxiv.org/pdf/2511.06465v1](http://arxiv.org/pdf/2511.06465v1)**

> **作者:** Lingfan Bao; Tianhu Peng; Chengxu Zhou
>
> **备注:** Sim-to-real for bipedal locomotion chapter
>
> **摘要:** This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions.
>
---
#### [new 036] Adversarial Game-Theoretic Algorithm for Dexterous Grasp Synthesis
- **分类: cs.RO**

- **简介: 该论文提出一种博弈论驱动的灵巧手抓取合成方法，通过构建机器人与物体的对抗博弈，显式建模物体逃逸行为，提升抓取稳定性。在仿真与实物实验中，成功率显著超越现有方法，且生成速度快，适用于实际部署。**

- **链接: [http://arxiv.org/pdf/2511.05809v1](http://arxiv.org/pdf/2511.05809v1)**

> **作者:** Yu Chen; Botao He; Yuemin Mao; Arthur Jakobsson; Jeffrey Ke; Yiannis Aloimonos; Guanya Shi; Howie Choset; Jiayuan Mao; Jeffrey Ichnowski
>
> **备注:** Submitted to ICRA 2026
>
> **摘要:** For many complex tasks, multi-finger robot hands are poised to revolutionize how we interact with the world, but reliably grasping objects remains a significant challenge. We focus on the problem of synthesizing grasps for multi-finger robot hands that, given a target object's geometry and pose, computes a hand configuration. Existing approaches often struggle to produce reliable grasps that sufficiently constrain object motion, leading to instability under disturbances and failed grasps. A key reason is that during grasp generation, they typically focus on resisting a single wrench, while ignoring the object's potential for adversarial movements, such as escaping. We propose a new grasp-synthesis approach that explicitly captures and leverages the adversarial object motion in grasp generation by formulating the problem as a two-player game. One player controls the robot to generate feasible grasp configurations, while the other adversarially controls the object to seek motions that attempt to escape from the grasp. Simulation experiments on various robot platforms and target objects show that our approach achieves a success rate of 75.78%, up to 19.61% higher than the state-of-the-art baseline. The two-player game mechanism improves the grasping success rate by 27.40% over the method without the game formulation. Our approach requires only 0.28-1.04 seconds on average to generate a grasp configuration, depending on the robot platform, making it suitable for real-world deployment. In real-world experiments, our approach achieves an average success rate of 85.0% on ShadowHand and 87.5% on LeapHand, which confirms its feasibility and effectiveness in real robot setups.
>
---
#### [new 037] Vision-Based System Identification of a Quadrotor
- **分类: cs.RO; cs.CV; cs.SY; eess.SY; math.DS**

- **简介: 该论文属于无人机建模与控制任务，旨在解决传统quadrotor模型中推力与阻力参数不确定性问题。通过机载视觉系统采集数据，采用灰箱建模与LQR控制，验证了视觉系统辨识模型的有效性与一致性。**

- **链接: [http://arxiv.org/pdf/2511.06839v1](http://arxiv.org/pdf/2511.06839v1)**

> **作者:** Selim Ahmet Iz; Mustafa Unel
>
> **摘要:** This paper explores the application of vision-based system identification techniques in quadrotor modeling and control. Through experiments and analysis, we address the complexities and limitations of quadrotor modeling, particularly in relation to thrust and drag coefficients. Grey-box modeling is employed to mitigate uncertainties, and the effectiveness of an onboard vision system is evaluated. An LQR controller is designed based on a system identification model using data from the onboard vision system. The results demonstrate consistent performance between the models, validating the efficacy of vision based system identification. This study highlights the potential of vision-based techniques in enhancing quadrotor modeling and control, contributing to improved performance and operational capabilities. Our findings provide insights into the usability and consistency of these techniques, paving the way for future research in quadrotor performance enhancement, fault detection, and decision-making processes.
>
---
#### [new 038] PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving
- **分类: cs.RO; cs.CV**

- **简介: 论文提出PlanT 2.0，一种面向CARLA的物体中心驾驶规划模型，通过可控输入扰动揭示自动驾驶模型的偏差与结构缺陷，如过拟合、捷径学习等，呼吁转向数据-centric开发，并开源代码。**

- **链接: [http://arxiv.org/pdf/2511.07292v1](http://arxiv.org/pdf/2511.07292v1)**

> **作者:** Simon Gerstenecker; Andreas Geiger; Katrin Renz
>
> **摘要:** Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.
>
---
#### [new 039] Residual Rotation Correction using Tactile Equivariance
- **分类: cs.RO; 14J60 (Primary) 14F05, 14J26 (Secondary), 14J60 (Primary) 14F05,
  14J26 (Secondary)**

- **简介: 该论文提出EquiTac，面向触觉增强的视觉-触觉策略学习，利用SO(2)等变性建模物体旋转对称性，从RGB触觉图像重建法向量场，预测残差旋转动作，实现零样本泛化与高效训练，显著提升接触密集任务的鲁棒性。**

- **链接: [http://arxiv.org/pdf/2511.07381v1](http://arxiv.org/pdf/2511.07381v1)**

> **作者:** Yizhe Zhu; Zhang Ye; Boce Hu; Haibo Zhao; Yu Qi; Dian Wang; Robert Platt
>
> **备注:** 8 pages
>
> **摘要:** Visuotactile policy learning augments vision-only policies with tactile input, facilitating contact-rich manipulation. However, the high cost of tactile data collection makes sample efficiency the key requirement for developing visuotactile policies. We present EquiTac, a framework that exploits the inherent SO(2) symmetry of in-hand object rotation to improve sample efficiency and generalization for visuotactile policy learning. EquiTac first reconstructs surface normals from raw RGB inputs of vision-based tactile sensors, so rotations of the normal vector field correspond to in-hand object rotations. An SO(2)-equivariant network then predicts a residual rotation action that augments a base visuomotor policy at test time, enabling real-time rotation correction without additional reorientation demonstrations. On a real robot, EquiTac accurately achieves robust zero-shot generalization to unseen in-hand orientations with very few training samples, where baselines fail even with more training data. To our knowledge, this is the first tactile learning method to explicitly encode tactile equivariance for policy learning, yielding a lightweight, symmetry-aware module that improves reliability in contact-rich tasks.
>
---
#### [new 040] Robot Learning from a Physical World Model
- **分类: cs.RO; cs.AI; cs.CV**

- **简介: 论文提出PhysWorld框架，通过物理世界建模将生成视频转化为符合物理规律的机器人动作，解决视觉生成动作忽略物理导致的不准确问题，实现无需真实数据的零样本机器人操控。**

- **链接: [http://arxiv.org/pdf/2511.07416v1](http://arxiv.org/pdf/2511.07416v1)**

> **作者:** Jiageng Mao; Sicheng He; Hao-Ning Wu; Yang You; Shuyang Sun; Zhicheng Wang; Yanan Bao; Huizhong Chen; Leonidas Guibas; Vitor Guizilini; Howard Zhou; Yue Wang
>
> **备注:** Project page: https://pointscoder.github.io/PhysWorld_Web/
>
> **摘要:** We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.
>
---
#### [new 041] TumorMap: A Laser-based Surgical Platform for 3D Tumor Mapping and Fully-Automated Tumor Resection
- **分类: cs.RO**

- **简介: TumorMap是一种基于多激光与深度学习的手术机器人平台，旨在实现肿瘤的三维精准建模与全自动无接触切除，解决术中组织识别难、操作受限等问题，已在小鼠模型中验证亚毫米级精度。**

- **链接: [http://arxiv.org/pdf/2511.05723v1](http://arxiv.org/pdf/2511.05723v1)**

> **作者:** Guangshen Ma; Ravi Prakash; Beatrice Schleupner; Jeffrey Everitt; Arpit Mishra; Junqin Chen; Brian Mann; Boyuan Chen; Leila Bridgeman; Pei Zhong; Mark Draelos; William C. Eward; Patrick J. Codd
>
> **备注:** 41 pages, 25 figures
>
> **摘要:** Surgical resection of malignant solid tumors is critically dependent on the surgeon's ability to accurately identify pathological tissue and remove the tumor while preserving surrounding healthy structures. However, building an intraoperative 3D tumor model for subsequent removal faces major challenges due to the lack of high-fidelity tumor reconstruction, difficulties in developing generalized tissue models to handle the inherent complexities of tumor diagnosis, and the natural physical limitations of bimanual operation, physiologic tremor, and fatigue creep during surgery. To overcome these challenges, we introduce "TumorMap", a surgical robotic platform to formulate intraoperative 3D tumor boundaries and achieve autonomous tissue resection using a set of multifunctional lasers. TumorMap integrates a three-laser mechanism (optical coherence tomography, laser-induced endogenous fluorescence, and cutting laser scalpel) combined with deep learning models to achieve fully-automated and noncontact tumor resection. We validated TumorMap in murine osteoscarcoma and soft-tissue sarcoma tumor models, and established a novel histopathological workflow to estimate sensor performance. With submillimeter laser resection accuracy, we demonstrated multimodal sensor-guided autonomous tumor surgery without any human intervention.
>
---
#### [new 042] Robotic versus Human Teleoperation for Remote Ultrasound
- **分类: cs.RO; cs.HC**

- **简介: 该论文对比了机器人与人类远程操控在超声检查中的表现，旨在解决农村地区医疗资源匮乏问题。实验表明，人类远程操控在完成时间、位置精度上与机器人相当，且力控更稳定、更实用经济。**

- **链接: [http://arxiv.org/pdf/2511.07275v1](http://arxiv.org/pdf/2511.07275v1)**

> **作者:** David Black; Septimiu Salcudean
>
> **备注:** Under review at IEEE TMRB. Extended version of a paper presented at the Hamlyn Symposium for Medical Robotics, 2025
>
> **摘要:** Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible.
>
---
#### [new 043] From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies
- **分类: cs.RO; cs.SY; eess.SY; I.2.6; I.2.8; I.2.9; I.2.10**

- **简介: 该论文针对扩散策略在部署时的安全性问题，提出路径一致安全过滤（PACS），通过轨迹级制动与集值可达性分析，在不破坏训练分布的前提下实现实时安全保障，显著提升任务成功率。**

- **链接: [http://arxiv.org/pdf/2511.06385v1](http://arxiv.org/pdf/2511.06385v1)**

> **作者:** Ralf Römer; Julian Balletshofer; Jakob Thumm; Marco Pavone; Angela P. Schoellig; Matthias Althoff
>
> **备注:** Project page: https://tum-lsy.github.io/pacs/. 8 pages, 4 figures
>
> **摘要:** Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/.
>
---
#### [new 044] 10 Open Challenges Steering the Future of Vision-Language-Action Models
- **分类: cs.RO; cs.AI**

- **简介: 该论文系统梳理了视觉-语言-动作（VLA）模型发展的10大开放挑战，如多模态融合、安全、人机协同等，并探讨了空间理解、世界建模等新兴趋势，旨在推动VLA模型向实用化与广泛部署迈进。**

- **链接: [http://arxiv.org/pdf/2511.05936v1](http://arxiv.org/pdf/2511.05936v1)**

> **作者:** Soujanya Poria; Navonil Majumder; Chia-Yu Hung; Amir Ali Bagherzadeh; Chuan Li; Kenneth Kwok; Ziwei Wang; Cheston Tan; Jiajun Wu; David Hsu
>
> **备注:** AAAI 2026 (Senior Track)
>
> **摘要:** Due to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.
>
---
#### [new 045] SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation
- **分类: cs.RO; cs.CV**

- **简介: 该论文提出SlotVLA框架，面向机器人操作任务，解决传统模型视觉表征冗余、不可解释问题。通过LIBERO+数据集与槽注意力机制，构建对象及关系的紧凑表征，实现高效、可解释的多任务操控。**

- **链接: [http://arxiv.org/pdf/2511.06754v1](http://arxiv.org/pdf/2511.06754v1)**

> **作者:** Taisei Hanyu; Nhat Chung; Huy Le; Toan Nguyen; Yuki Ikebe; Anthony Gunderman; Duy Nguyen Ho Minh; Khoa Vo; Tung Kieu; Kashu Yamazaki; Chase Rainwater; Anh Nguyen; Ngan Le
>
> **备注:** under review
>
> **摘要:** Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.
>
---
#### [new 046] ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface
- **分类: cs.RO**

- **简介: 论文提出ViTaMIn-B，面向双手机器人操作任务，解决现有系统触觉感知弱、位姿跟踪漂移问题。设计DuoTact柔性触觉传感器与基于Quest的鲁棒6DoF位姿获取方法，实现高效高精度数据采集与任务执行。**

- **链接: [http://arxiv.org/pdf/2511.05858v1](http://arxiv.org/pdf/2511.05858v1)**

> **作者:** Chuanyu Li; Chaoyi Liu; Daotan Wang; Shuyu Zhang; Lusong Li; Zecui Zeng; Fangchen Liu; Jing Xu; Rui Chen
>
> **摘要:** Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently. However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks. In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks. We first design DuoTact, a novel compliant visuo-tactile sensor built with a flexible frame to withstand large contact forces during manipulation while capturing high-resolution contact geometry. To enhance the cross-sensor generalizability, we propose reconstructing the sensor's global deformation as a 3D point cloud and using it as the policy input. We further develop a robust, unified 6-DoF bimanual pose acquisition process using Meta Quest controllers, which eliminates the trajectory drift issue in common SLAM-based methods. Comprehensive user studies confirm the efficiency and high usability of ViTaMIn-B among novice and expert operators. Furthermore, experiments on four bimanual manipulation tasks demonstrate its superior task performance relative to existing systems.
>
---
#### [new 047] Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation
- **分类: cs.RO**

- **简介: 该论文提出一种多智能体AI框架，融合多模态大模型与视觉感知，解决传统道路场景检测缺乏语义解释与泛化能力问题，实现C-ITS消息自动生成，实验表明其检测召回率100%，但存在误检与细粒度理解不足。**

- **链接: [http://arxiv.org/pdf/2511.06892v1](http://arxiv.org/pdf/2511.06892v1)**

> **作者:** Kailin Tong; Selim Solmaz; Kenan Mujkic; Gottfried Allmer; Bo Leng
>
> **备注:** submitted to TRA 2026
>
> **摘要:** Conventional road-situation detection methods achieve strong performance in predefined scenarios but fail in unseen cases and lack semantic interpretation, which is crucial for reliable traffic recommendations. This work introduces a multi-agent AI framework that combines multimodal large language models (MLLMs) with vision-based perception for road-situation monitoring. The framework processes camera feeds and coordinates dedicated agents for situation detection, distance estimation, decision-making, and Cooperative Intelligent Transport System (C-ITS) message generation. Evaluation is conducted on a custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\% recall in situation detection and perfect message schema correctness; however, both models suffer from false-positive detections and have reduced performance in terms of number of lanes, driving lane status and cause code. Surprisingly, Gemini-2.5-Flash, though more capable in general tasks, underperforms Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs higher latency (Table II). These findings motivate further work on fine-tuning specialized LLMs or MLLMs tailored for intelligent transportation applications.
>
---
#### [new 048] ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects
- **分类: cs.RO; cs.CV**

- **简介: 论文提出ArtReg方法，利用视觉-触觉点云融合与SE(3)李群卡尔曼滤波，实现对未知刚体与关节物体的无模型位姿追踪与闭环操控，解决机器人无先验知识下复杂对象交互难题。**

- **链接: [http://arxiv.org/pdf/2511.06378v1](http://arxiv.org/pdf/2511.06378v1)**

> **作者:** Prajval Kumar Murali; Mohsen Kaboli
>
> **备注:** Under review
>
> **摘要:** Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints).
>
---
#### [new 049] CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning
- **分类: cs.RO; cs.AI; cs.LG**

- **简介: 论文提出CoFineLLM，首次将共形预测（CP）融入LLM微调，解决语言指令机器人规划中预测集过大、人工干预频繁的问题，通过CP感知微调显著缩小预测集、降低求助率，提升自主性。**

- **链接: [http://arxiv.org/pdf/2511.06575v1](http://arxiv.org/pdf/2511.06575v1)**

> **作者:** Jun Wang; Yevgeniy Vorobeychik; Yiannis Kantaros
>
> **摘要:** Large Language Models (LLMs) have recently emerged as planners for language-instructed agents, generating sequences of actions to accomplish natural language tasks. However, their reliability remains a challenge, especially in long-horizon tasks, since they often produce overconfident yet wrong outputs. Conformal Prediction (CP) has been leveraged to address this issue by wrapping LLM outputs into prediction sets that contain the correct action with a user-defined confidence. When the prediction set is a singleton, the planner executes that action; otherwise, it requests help from a user. This has led to LLM-based planners that can ensure plan correctness with a user-defined probability. However, as LLMs are trained in an uncertainty-agnostic manner, without awareness of prediction sets, they tend to produce unnecessarily large sets, particularly at higher confidence levels, resulting in frequent human interventions limiting autonomous deployment. To address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first CP-aware finetuning framework for LLM-based planners that explicitly reduces prediction-set size and, in turn, the need for user interventions. We evaluate our approach on multiple language-instructed robot planning problems and show consistent improvements over uncertainty-aware and uncertainty-agnostic finetuning baselines in terms of prediction-set size, and help rates. Finally, we demonstrate robustness of our method to out-of-distribution scenarios in hardware experiments.
>
---
#### [new 050] Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots
- **分类: cs.RO**

- **简介: 该论文提出可编程伸缩气动软执行器（PTSPAs），解决软机器人设计维度爆炸问题，通过参数化几何生成与系统实验，建立设计-性能关联，并应用于可部署四足机器人，实现空间自适应形变。**

- **链接: [http://arxiv.org/pdf/2511.06673v1](http://arxiv.org/pdf/2511.06673v1)**

> **作者:** Joel Kemp; Andre Farinha; David Howard; Krishna Manaswi Digumarti; Josh Pinskier
>
> **备注:** 8 pages, 10 figures, Submitted to Robosoft 2026
>
> **摘要:** Soft Robotics presents a rich canvas for free-form and continuum devices capable of exerting forces in any direction and transforming between arbitrary configurations. However, there is no current way to tractably and directly exploit the design freedom due to the curse of dimensionality. Parameterisable sets of designs offer a pathway towards tractable, modular soft robotics that appropriately harness the behavioural freeform of soft structures to create rich embodied behaviours. In this work, we present a parametrised class of soft actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs expand axially on inflation for deployable structures and manipulation in challenging confined spaces. We introduce a parametric geometry generator to customise actuator models from high-level inputs, and explore the new design space through semi-automated experimentation and systematic exploration of key parameters. Using it we characterise the actuators' extension/bending, expansion, and stiffness and reveal clear relationships between key design parameters and performance. Finally we demonstrate the application of the actuators in a deployable soft quadruped whose legs deploy to walk, enabling automatic adaptation to confined spaces. PTSPAs present new design paradigm for deployable and shape morphing structures and wherever large length changes are required.
>
---
#### [new 051] Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics
- **分类: cs.RO**

- **简介: 论文提出Raspi²USBL，一种开源Raspberry Pi驱动的被动倒置USBL水下定位系统，解决GNSS无法用于水下的定位难题，通过低成本硬件与实时信号处理实现高精度测距与测向，验证了其在多种环境下的研究级性能。**

- **链接: [http://arxiv.org/pdf/2511.06998v1](http://arxiv.org/pdf/2511.06998v1)**

> **作者:** Jin Huang; Yingqiang Wang; Ying Chen
>
> **摘要:** Precise underwater positioning remains a fundamental challenge for underwater robotics since global navigation satellite system (GNSS) signals cannot penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source, Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning system designed to provide a low-cost and accessible solution for underwater robotic research. The system comprises a passive acoustic receiver and an active beacon. The receiver adopts a modular hardware architecture that integrates a hydrophone array, a multichannel preamplifier, an oven-controlled crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition (DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an impedance-matching network, a power amplifier, and a transmitting transducer. An open-source C++ software framework provides high-precision clock synchronization and triggering for one-way travel-time (OWTT) messaging, while performing real-time signal processing, including matched filtering, array beamforming, and adaptive gain control, to estimate the time of flight (TOF) and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system was experimentally validated in an anechoic tank, freshwater lake, and open-sea trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing accuracy within 0.1$^\circ$, and stable performance over operational distances up to 1.3 km. These findings confirm that low-cost, reproducible hardware can deliver research-grade underwater positioning accuracy. By releasing both the hardware and software as open-source, Raspi$^2$USBL provides a unified reference platform that lowers the entry barrier for underwater robotics laboratories, fosters reproducibility, and promotes collaborative innovation in underwater acoustic navigation and swarm robotics.
>
---
#### [new 052] Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots
- **分类: cs.RO; cs.AR; cs.CV; cs.SY; eess.SY**

- **简介: 该论文提出Lite VLA，首次在CPU受限的边缘机器人上部署轻量级视觉-语言模型，实现无需云端的实时感知与动作协同，解决资源受限环境下自主决策难题，支持服务、救灾等场景的高效边缘自治。**

- **链接: [http://arxiv.org/pdf/2511.05642v1](http://arxiv.org/pdf/2511.05642v1)**

> **作者:** Justin Williams; Kishor Datta Gupta; Roy George; Mrinmoy Sarkar
>
> **摘要:** The deployment of artificial intelligence models at the edge is increasingly critical for autonomous robots operating in GPS-denied environments where local, resource-efficient reasoning is essential. This work demonstrates the feasibility of deploying small Vision-Language Models (VLMs) on mobile robots to achieve real-time scene understanding and reasoning under strict computational constraints. Unlike prior approaches that separate perception from mobility, the proposed framework enables simultaneous movement and reasoning in dynamic environments using only on-board hardware. The system integrates a compact VLM with multimodal perception to perform contextual interpretation directly on embedded hardware, eliminating reliance on cloud connectivity. Experimental validation highlights the balance between computational efficiency, task accuracy, and system responsiveness. Implementation on a mobile robot confirms one of the first successful deployments of small VLMs for concurrent reasoning and mobility at the edge. This work establishes a foundation for scalable, assured autonomy in applications such as service robotics, disaster response, and defense operations.
>
---
#### [new 053] VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models
- **分类: cs.RO; cs.AI; cs.LG**

- **简介: VLAD-Grasp提出一种无需训练的零样本抓取检测方法，利用视觉语言模型生成目标图像，结合深度与点云对齐恢复可执行抓取位姿，在无需标注数据下实现媲美监督模型的性能。**

- **链接: [http://arxiv.org/pdf/2511.05791v1](http://arxiv.org/pdf/2511.05791v1)**

> **作者:** Manav Kulshrestha; S. Talha Bukhari; Damon Conover; Aniket Bera
>
> **备注:** 8 pages, 4 figures, under review
>
> **摘要:** Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod "impales" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.
>
---
#### [new 054] Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots
- **分类: cs.RO**

- **简介: 该论文提出一种视觉辅助的在线A*路径规划框架，解决传统规划器忽视语义信息的问题。通过轻量语义分割识别任务相关视觉约束，并将其映射为非几何障碍，实现低成本机器人在复杂环境中的安全、上下文感知导航。**

- **链接: [http://arxiv.org/pdf/2511.06801v1](http://arxiv.org/pdf/2511.06801v1)**

> **作者:** Praveen Kumar; Tushar Sandhan
>
> **备注:** 10 pages
>
> **摘要:** The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are seman- tically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a frame- work to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost- effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.
>
---
#### [new 055] Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning
- **分类: cs.RO**

- **简介: 该论文提出自适应人形控制（AHC），解决多行为独立训练导致泛化差的问题。通过多行为知识蒸馏构建基础控制器，再结合在线强化微调提升复杂地形适应性，实现单控制器跨行为、跨地形的自适应运动。**

- **链接: [http://arxiv.org/pdf/2511.06371v1](http://arxiv.org/pdf/2511.06371v1)**

> **作者:** Yingnan Zhao; Xinmiao Wang; Dewei Wang; Xinzhe Liu; Dan Lu; Qilong Han; Peng Liu; Chenjia Bai
>
> **摘要:** Humanoid robots are promising to learn a diverse set of human-like locomotion behaviors, including standing up, walking, running, and jumping. However, existing methods predominantly require training independent policies for each skill, yielding behavior-specific controllers that exhibit limited generalization and brittle performance when deployed on irregular terrains and in diverse situations. To address this challenge, we propose Adaptive Humanoid Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid locomotion controller across different skills and terrains. Specifically, we first train several primary locomotion policies and perform a multi-behavior distillation process to obtain a basic multi-behavior controller, facilitating adaptive behavior switching based on the environment. Then, we perform reinforced fine-tuning by collecting online feedback in performing adaptive behaviors on more diverse terrains, enhancing terrain adaptability for the controller. We conduct experiments in both simulation and real-world experiments in Unitree G1 robots. The results show that our method exhibits strong adaptability across various situations and terrains. Project website: https://ahc-humanoid.github.io.
>
---
#### [new 056] Aerial Image Stitching Using IMU Data from a UAV
- **分类: cs.CV; cs.RO; cs.SY; eess.SY; math.DS**

- **简介: 该论文提出一种融合IMU数据与计算机视觉的无人机航拍图像拼接方法，解决传统特征匹配在大位移、姿态变化下易失败的问题，通过IMU估算位姿、校正畸变并计算单应矩阵，提升拼接精度与鲁棒性。**

- **链接: [http://arxiv.org/pdf/2511.06841v1](http://arxiv.org/pdf/2511.06841v1)**

> **作者:** Selim Ahmet Iz; Mustafa Unel
>
> **摘要:** Unmanned Aerial Vehicles (UAVs) are widely used for aerial photography and remote sensing applications. One of the main challenges is to stitch together multiple images into a single high-resolution image that covers a large area. Featurebased image stitching algorithms are commonly used but can suffer from errors and ambiguities in feature detection and matching. To address this, several approaches have been proposed, including using bundle adjustment techniques or direct image alignment. In this paper, we present a novel method that uses a combination of IMU data and computer vision techniques for stitching images captured by a UAV. Our method involves several steps such as estimating the displacement and rotation of the UAV between consecutive images, correcting for perspective distortion, and computing a homography matrix. We then use a standard image stitching algorithm to align and blend the images together. Our proposed method leverages the additional information provided by the IMU data, corrects for various sources of distortion, and can be easily integrated into existing UAV workflows. Our experiments demonstrate the effectiveness and robustness of our method, outperforming some of the existing feature-based image stitching algorithms in terms of accuracy and reliability, particularly in challenging scenarios such as large displacements, rotations, and variations in camera pose.
>
---
#### [new 057] Towards Human-AI-Robot Collaboration and AI-Agent based Digital Twins for Parkinson's Disease Management: Review and Outlook
- **分类: eess.SP; cs.RO**

- **简介: 该论文提出融合多模态传感与AI代理的闭环人机协作框架，解决帕金森病监测与康复系统割裂问题，构建可进化的数字孪生系统，实现个性化、可解释的智能管理。**

- **链接: [http://arxiv.org/pdf/2511.06036v1](http://arxiv.org/pdf/2511.06036v1)**

> **作者:** Hassan Hizeh; Rim Chighri; Muhammad Mahboob Ur Rahman; Mohamed A. Bahloul; Ali Muqaibel; Tareq Y. Al-Naffouri
>
> **备注:** 20 pages, 5 figures, 4 tables, under review with a journal
>
> **摘要:** The current body of research on Parkinson's disease (PD) screening, monitoring, and management has evolved along two largely independent trajectories. The first research community focuses on multimodal sensing of PD-related biomarkers using noninvasive technologies such as inertial measurement units (IMUs), force/pressure insoles, electromyography (EMG), electroencephalography (EEG), speech and acoustic analysis, and RGB/RGB-D motion capture systems. These studies emphasize data acquisition, feature extraction, and machine learning-based classification for PD screening, diagnosis, and disease progression modeling. In parallel, a second research community has concentrated on robotic intervention and rehabilitation, employing socially assistive robots (SARs), robot-assisted rehabilitation (RAR) systems, and virtual reality (VR)-integrated robotic platforms for improving motor and cognitive function, enhancing social engagement, and supporting caregivers. Despite the complementary goals of these two domains, their methodological and technological integration remains limited, with minimal data- level or decision-level coupling between the two. With the advent of advanced artificial intelligence (AI), including large language models (LLMs), agentic AI systems, a unique opportunity now exists to unify these research streams. We envision a closed-loop sensor-AI-robot framework in which multimodal sensing continuously guides the interaction between the patient, caregiver, humanoid robot (and physician) through AI agents that are powered by a multitude of AI models such as robotic and wearables foundation models, LLM-based reasoning, reinforcement learning, and continual learning. Such closed-loop system enables personalized, explainable, and context-aware intervention, forming the basis for digital twin of the PD patient that can adapt over time to deliver intelligent, patient-centered PD care.
>
---
#### [new 058] Social-Physical Interactions with Virtual Characters: Evaluating the Impact of Physicality through Encountered-Type Haptics
- **分类: cs.HC; cs.RO**

- **简介: 该论文研究虚拟角色社交互动中物理性的影响，提出ETHOS系统通过可交互道具与动态力反馈实现手递物、击拳等触觉交互，验证物理性提升沉浸感、真实感与连接感，属人机交互中的触觉增强任务。**

- **链接: [http://arxiv.org/pdf/2511.05683v1](http://arxiv.org/pdf/2511.05683v1)**

> **作者:** Eric Godden; Jacquie Groenewegen; Michael Wheeler; Matthew K. X. J. Pan
>
> **备注:** 9 pages
>
> **摘要:** This work investigates how robot-mediated physicality influences the perception of social-physical interactions with virtual characters. ETHOS (Encountered-Type Haptics for On-demand Social interaction) is an encountered-type haptic display that integrates a torque-controlled manipulator and interchangeable props with a VR headset to enable three gestures: object handovers, fist bumps, and high fives. We conducted a user study to examine how ETHOS adds physicality to virtual character interactions and how this affects presence, realism, enjoyment, and connection metrics. Each participant experienced one interaction under three conditions: no physicality (NP), static physicality (SP), and dynamic physicality (DP). SP extended the purely virtual baseline (NP) by introducing tangible props for direct contact, while DP further incorporated motion and impact forces to emulate natural touch. Results show presence increased stepwise from NP to SP to DP. Realism, enjoyment, and connection also improved with added physicality, though differences between SP and DP were not significant. Comfort remained consistent across conditions, indicating no added psychological friction. These findings demonstrate the experiential value of ETHOS and motivate the integration of encountered-type haptics into socially meaningful VR experiences.
>
---
#### [new 059] Token Is All You Need: Cognitive Planning through Sparse Intent Alignment
- **分类: cs.CV; cs.AI; cs.LG; cs.RO; 68T40; I.2.9; I.2.6; I.2.10**

- **简介: 该论文面向端到端自动驾驶规划任务，提出“Token Is All You Need”框架，用稀疏语义令牌替代复杂场景建模，仅靠感知信息即可实现高效规划，显著降低轨迹误差，并揭示模型可自适应关注任务相关语义，实现认知式想象规划。**

- **链接: [http://arxiv.org/pdf/2511.05540v1](http://arxiv.org/pdf/2511.05540v1)**

> **作者:** Shiyao Sang
>
> **备注:** 6 pages, 2 figures. Preprint exploring a new cognitive paradigm for autonomous planning
>
> **摘要:** We challenge the long-standing assumption that exhaustive scene modeling is required for high-performance end-to-end autonomous driving (E2EAD). Unlike world-model approaches that rely on computationally intensive future scene generation or vision-language-action (VLA) systems constrained by Markov assumptions, we show that a minimal set of semantically rich tokens is sufficient for effective planning. Experiments on the nuPlan benchmark (720 scenarios, over 11,000 samples) using perception-informed BEV representations yield three key findings: (1) even without future prediction, our sparse representation achieves 0.548 m ADE, comparable to or surpassing prior methods reporting around 0.75 m on nuScenes; (2) conditioning trajectory decoding on predicted future tokens reduces ADE to 0.479 m, a 12.6% improvement over current-state baselines; and (3) explicit reconstruction loss offers no benefit and may degrade performance under reliable perception inputs. Notably, we observe the emergence of temporal fuzziness, where the model adaptively attends to task-relevant semantics rather than aligning rigidly to fixed timestamps, providing a cognitive advantage for planning under uncertainty. Our "token is all you need" principle marks a paradigm shift from reconstructing the world to understanding it, laying a foundation for cognitively inspired systems that plan through imagination rather than reaction.
>
---
#### [new 060] GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets
- **分类: cs.AI; cs.RO**

- **简介: 论文提出GHOST框架，解决图上凸集旅行商问题（GCS-TSP），该问题中边代价依赖轨迹。GHOST结合组合搜索与凸优化，通过高效下界剪枝实现最优解，显著提升复杂轨迹规划效率。**

- **链接: [http://arxiv.org/pdf/2511.06471v1](http://arxiv.org/pdf/2511.06471v1)**

> **作者:** Jingtao Tang; Hang Ma
>
> **备注:** Accepted to AAAI-2026
>
> **摘要:** We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP) defined over a Graph of Convex Sets (GCS) -- a powerful representation for trajectory planning that decomposes the configuration space into convex regions connected by a sparse graph. In this setting, edge costs are not fixed but depend on the specific trajectory selected through each convex region, making classical TSP methods inapplicable. We introduce GHOST, a hierarchical framework that optimally solves the GCS-TSP by combining combinatorial tour search with convex trajectory optimization. GHOST systematically explores tours on a complete graph induced by the GCS, using a novel abstract-path-unfolding algorithm to compute admissible lower bounds that guide best-first search at both the high level (over tours) and the low level (over feasible GCS paths realizing the tour). These bounds provide strong pruning power, enabling efficient search while avoiding unnecessary convex optimization calls. We prove that GHOST guarantees optimality and present a bounded-suboptimal variant for time-critical scenarios. Experiments show that GHOST is orders-of-magnitude faster than unified mixed-integer convex programming baselines for simple cases and uniquely handles complex trajectory planning problems involving high-order continuity constraints and an incomplete GCS.
>
---
#### [new 061] Disentangled Control of Multi-Agent Systems
- **分类: eess.SY; cs.RO; cs.SY**

- **简介: 该论文提出一种通用多智能体控制框架，解决动态耦合导致的去中心化难题，支持多目标与实时应用，实现无近似的时间变密度覆盖、时变编队及密集环境安全导航，具有收敛保证。**

- **链接: [http://arxiv.org/pdf/2511.05900v1](http://arxiv.org/pdf/2511.05900v1)**

> **作者:** Ruoyu Lin; Gennaro Notomista; Magnus Egerstedt
>
> **备注:** This work has been submitted to IEEE Transactions on Control of Network Systems for possible publication
>
> **摘要:** This paper develops a general framework for multi-agent control synthesis, which applies to a wide range of problems with convergence guarantees, regardless of the complexity of the underlying graph topology and the explicit time dependence of the objective function. The proposed framework systematically addresses a particularly challenging problem in multi-agent systems, i.e., decentralization of entangled dynamics among different agents, and it naturally supports multi-objective robotics and real-time implementations. To demonstrate its generality and effectiveness, the framework is implemented across three experiments, namely time-varying leader-follower formation control, decentralized coverage control for time-varying density functions without any approximations, which is a long-standing open problem, and safe formation navigation in dense environments.
>
---
#### [new 062] Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey
- **分类: cs.CV; cs.AI; cs.LG; cs.RO**

- **简介: 该论文综述了深度神经网络感知系统运行时安全监测方法，旨在解决DNN在推理中因泛化错误、OOD输入和对抗攻击导致的安全风险，分类梳理了输入、中间表征与输出三类监测技术，分析优劣并指明未来方向。**

- **链接: [http://arxiv.org/pdf/2511.05982v1](http://arxiv.org/pdf/2511.05982v1)**

> **作者:** Albert Schotschneider; Svetlana Pavlitska; J. Marius Zöllner
>
> **备注:** 6 pages, 1 figure, 2 tables, accepted at IEEE SMC 2025 in Vienna, presented on 8th October 2025
>
> **摘要:** Deep neural networks (DNNs) are widely used in perception systems for safety-critical applications, such as autonomous driving and robotics. However, DNNs remain vulnerable to various safety concerns, including generalization errors, out-of-distribution (OOD) inputs, and adversarial attacks, which can lead to hazardous failures. This survey provides a comprehensive overview of runtime safety monitoring approaches, which operate in parallel to DNNs during inference to detect these safety concerns without modifying the DNN itself. We categorize existing methods into three main groups: Monitoring inputs, internal representations, and outputs. We analyze the state-of-the-art for each category, identify strengths and limitations, and map methods to the safety concerns they address. In addition, we highlight open challenges and future research directions.
>
---
#### [new 063] PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory
- **分类: cs.CV; cs.RO**

- **简介: PanoNav提出一种无地图、仅用RGB的零样本目标导航框架，通过全景场景解析与动态记忆机制，提升多模态大模型的空间理解与长期决策能力，避免局部死锁，显著提升导航成功率。**

- **链接: [http://arxiv.org/pdf/2511.06840v1](http://arxiv.org/pdf/2511.06840v1)**

> **作者:** Qunchao Jin; Yilin Wu; Changhao Chen
>
> **备注:** Accepted as a poster in AAAI 2026
>
> **摘要:** Zero-shot object navigation (ZSON) in unseen environments remains a challenging problem for household robots, requiring strong perceptual understanding and decision-making capabilities. While recent methods leverage metric maps and Large Language Models (LLMs), they often depend on depth sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address this, but they typically make short-sighted decisions, leading to local deadlocks due to a lack of historical context. We propose PanoNav, a fully RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing module to unlock the spatial parsing potential of MLLMs from panoramic RGB inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic Bounded Memory Queue to incorporate exploration history and avoid local deadlocks. Experiments on the public navigation benchmark show that PanoNav significantly outperforms representative baselines in both SR and SPL metrics.
>
---
#### [new 064] On Accurate and Robust Estimation of 3D and 2D Circular Center: Method and Application to Camera-Lidar Calibration
- **分类: cs.CV; cs.RO**

- **简介: 该论文针对LiDAR-相机外参标定中3D-2D圆心匹配不准确的问题，提出基于共形几何代数与弦长方差最小化的双端圆心估计算法，提升标定精度与鲁棒性，支持自然圆形目标。**

- **链接: [http://arxiv.org/pdf/2511.06611v1](http://arxiv.org/pdf/2511.06611v1)**

> **作者:** Jiajun Jiang; Xiao Hu; Wancheng Liu; Wei Jiang
>
> **摘要:** Circular targets are widely used in LiDAR-camera extrinsic calibration due to their geometric consistency and ease of detection. However, achieving accurate 3D-2D circular center correspondence remains challenging. Existing methods often fail due to decoupled 3D fitting and erroneous 2D ellipse-center estimation. To address this, we propose a geometrically principled framework featuring two innovations: (i) a robust 3D circle center estimator based on conformal geometric algebra and RANSAC; and (ii) a chord-length variance minimization method to recover the true 2D projected center, resolving its dual-minima ambi- guity via homography validation or a quasi-RANSAC fallback. Evaluated on synthetic and real-world datasets, our framework significantly outperforms state-of-the-art approaches. It reduces extrinsic estimation error and enables robust calibration across diverse sensors and target types, including natural circular objects. Our code will be publicly released for reproducibility.
>
---
#### [new 065] Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition
- **分类: cs.CV; cs.AI; cs.LG; cs.RO**

- **简介: 该论文面向鲁棒动作识别任务，解决RGB模型依赖统计模式、忽视物理空间姿态的问题。提出融合V-JEPA2动态与CoMotion人体姿态的模型，在高遮挡场景下显著提升性能，强调动作识别需基于空间理解。**

- **链接: [http://arxiv.org/pdf/2511.05622v1](http://arxiv.org/pdf/2511.05622v1)**

> **作者:** Nicholas Babey; Tiffany Gu; Yiheng Li; Cristian Meo; Kevin Zhu
>
> **备注:** Accepted at NeurIPS 2025 SpaVLE, for code see https://github.com/nbabey20/groundactrec , 9 pages, 1 figure
>
> **摘要:** For embodied agents to effectively understand and interact within the world around them, they require a nuanced comprehension of human actions grounded in physical space. Current action recognition models, often relying on RGB video, learn superficial correlations between patterns and action labels, so they struggle to capture underlying physical interaction dynamics and human poses in complex scenes. We propose a model architecture that grounds action recognition in physical space by fusing two powerful, complementary representations: V-JEPA 2's contextual, predictive world dynamics and CoMotion's explicit, occlusion-tolerant human pose data. Our model is validated on both the InHARD and UCF-19-Y-OCC benchmarks for general action recognition and high-occlusion action recognition, respectively. Our model outperforms three other baselines, especially within complex, occlusive scenes. Our findings emphasize a need for action recognition to be supported by spatial understanding instead of statistical pattern recognition.
>
---
#### [new 066] Model-free Adaptive Output Feedback Vibration Suppression in a Cantilever Beam
- **分类: eess.SY; cs.RO; cs.SY**

- **简介: 该论文针对悬臂梁的未知扰动振动问题，提出一种无模型自适应输出反馈控制方法，利用位移与加速度反馈，结合滤波提取关键信息，通过反向成本优化实现高效振动抑制。**

- **链接: [http://arxiv.org/pdf/2511.06084v1](http://arxiv.org/pdf/2511.06084v1)**

> **作者:** Juan Augusto Paredes Salazar; Ankit Goel
>
> **备注:** 16 pages, 14 figures, to be presented at Scitech 2026
>
> **摘要:** This paper presents a model-free adaptive control approach to suppress vibrations in a cantilevered beam excited by an unknown disturbance. The cantilevered beam under harmonic excitation is modeled using a lumped parameter approach. Based on retrospective cost optimization, a sampled-data adaptive controller is developed to suppress vibrations caused by external disturbances. Both displacement and acceleration measurements are considered for feedback. Since acceleration measurements are more sensitive to spillover, which excites higher frequency modes, a filter is developed to extract key displacement information from the acceleration data and enhance suppression performance. The vibration suppression performance is compared using both displacement and acceleration measurements.
>
---
#### [new 067] TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for Embodied AI Research
- **分类: cs.CV; cs.RO**

- **简介: 论文提出TwinOR框架，构建高保真动态手术室数字孪生体，解决实体手术室中AI代理训练受限问题，通过多视角感知融合静态几何与动态行为，实现传感器级真实感仿真，支持Embodied AI的高效训练与评估。**

- **链接: [http://arxiv.org/pdf/2511.07412v1](http://arxiv.org/pdf/2511.07412v1)**

> **作者:** Han Zhang; Yiqing Shen; Roger D. Soberanis-Mukul; Ankita Ghosh; Hao Ding; Lalithkumar Seenivasan; Jose L. Porras; Zhekai Mao; Chenjia Li; Wenjie Xiao; Lonny Yarmus; Angela Christine Argento; Masaru Ishii; Mathias Unberath
>
> **摘要:** Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation. However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings. Digital twins provide high-fidelity, risk-free environments for exploration and training. How we may create photorealistic and dynamic digital representations of ORs that capture relevant spatial, visual, and behavioral complexity remains unclear. We introduce TwinOR, a framework for constructing photorealistic, dynamic digital twins of ORs for embodied AI research. The system reconstructs static geometry from pre-scan videos and continuously models human and equipment motion through multi-view perception of OR activities. The static and dynamic components are fused into an immersive 3D environment that supports controllable simulation and embodied exploration. The proposed framework reconstructs complete OR geometry with centimeter level accuracy while preserving dynamic interaction across surgical workflows, enabling realistic renderings and a virtual playground for embodied AI systems. In our experiments, TwinOR simulates stereo and monocular sensor streams for geometry understanding and visual localization tasks. Models such as FoundationStereo and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their reported accuracy on real indoor datasets, demonstrating that TwinOR provides sensor-level realism sufficient for perception and localization challenges. By establishing a real-to-sim pipeline for constructing dynamic, photorealistic digital twins of OR environments, TwinOR enables the safe, scalable, and data-efficient development and benchmarking of embodied AI, ultimately accelerating the deployment of embodied AI from sim-to-real.
>
---
#### [new 068] In-process 3D Deviation Mapping and Defect Monitoring (3D-DM2) in High Production-rate Robotic Additive Manufacturing
- **分类: cs.CV; cs.RO**

- **简介: 该论文提出实时3D偏差监测系统（3D-DM2），用于高产率机器人增材制造中在线检测形状偏差。通过比对实时构建体与参考模型，实现偏差定位与追踪，支持过程即时补偿，提升成形精度与质量。**

- **链接: [http://arxiv.org/pdf/2511.05604v1](http://arxiv.org/pdf/2511.05604v1)**

> **作者:** Subash Gautam; Alejandro Vargas-Uscategui; Peter King; Hans Lohr; Alireza Bab-Hadiashar; Ivan Cole; Ehsan Asadi
>
> **摘要:** Additive manufacturing (AM) is an emerging digital manufacturing technology to produce complex and freeform objects through a layer-wise deposition. High deposition rate robotic AM (HDRRAM) processes, such as cold spray additive manufacturing (CSAM), offer significantly increased build speeds by delivering large volumes of material per unit time. However, maintaining shape accuracy remains a critical challenge, particularly due to process instabilities in current open-loop systems. Detecting these deviations as they occur is essential to prevent error propagation, ensure part quality, and minimize post-processing requirements. This study presents a real-time monitoring system to acquire and reconstruct the growing part and directly compares it with a near-net reference model to detect the shape deviation during the manufacturing process. The early identification of shape inconsistencies, followed by segmenting and tracking each deviation region, paves the way for timely intervention and compensation to achieve consistent part quality.
>
---
#### [new 069] Multi-Agent Reinforcement Learning for Deadlock Handling among Autonomous Mobile Robots
- **分类: cs.MA; cs.RO; I.2.11; I.6.3**

- **简介: 该论文研究多智能体强化学习（MARL）在自主移动机器人死锁处理中的应用，解决传统规则方法缺乏动态适应性的问题。提出基于CTDE框架的MARL策略，通过仿真验证其在复杂环境中优于传统方法，实现灵活可扩展的死锁规避。**

- **链接: [http://arxiv.org/pdf/2511.07071v1](http://arxiv.org/pdf/2511.07071v1)**

> **作者:** Marcel Müller
>
> **备注:** for associated repositories, see https://github.com/Nerozud/dl_reference_models and https://github.com/Nerozud/FTS_simpel
>
> **摘要:** This dissertation explores the application of multi-agent reinforcement learning (MARL) for handling deadlocks in intralogistics systems that rely on autonomous mobile robots (AMRs). AMRs enhance operational flexibility but also increase the risk of deadlocks, which degrade system throughput and reliability. Existing approaches often neglect deadlock handling in the planning phase and rely on rigid control rules that cannot adapt to dynamic operational conditions. To address these shortcomings, this work develops a structured methodology for integrating MARL into logistics planning and operational control. It introduces reference models that explicitly consider deadlock-capable multi-agent pathfinding (MAPF) problems, enabling systematic evaluation of MARL strategies. Using grid-based environments and an external simulation software, the study compares traditional deadlock handling strategies with MARL-based solutions, focusing on PPO and IMPALA algorithms under different training and execution modes. Findings reveal that MARL-based strategies, particularly when combined with centralized training and decentralized execution (CTDE), outperform rule-based methods in complex, congested environments. In simpler environments or those with ample spatial freedom, rule-based methods remain competitive due to their lower computational demands. These results highlight that MARL provides a flexible and scalable solution for deadlock handling in dynamic intralogistics scenarios, but requires careful tailoring to the operational context.
>
---
#### [new 070] EndoIR: Degradation-Agnostic All-in-One Endoscopic Image Restoration via Noise-Aware Routing Diffusion
- **分类: eess.IV; cs.AI; cs.CV; cs.RO**

- **简介: EndoIR提出一种去降解无关的内窥镜图像恢复框架，通过双域提示、双流扩散与噪声感知路由，单模型联合处理低光、烟雾、出血等复合退化，提升恢复效果与临床实用性。**

- **链接: [http://arxiv.org/pdf/2511.05873v1](http://arxiv.org/pdf/2511.05873v1)**

> **作者:** Tong Chen; Xinyu Ma; Long Bai; Wenyang Wang; Sun Yue; Luping Zhou
>
> **摘要:** Endoscopic images often suffer from diverse and co-occurring degradations such as low lighting, smoke, and bleeding, which obscure critical clinical details. Existing restoration methods are typically task-specific and often require prior knowledge of the degradation type, limiting their robustness in real-world clinical use. We propose EndoIR, an all-in-one, degradation-agnostic diffusion-based framework that restores multiple degradation types using a single model. EndoIR introduces a Dual-Domain Prompter that extracts joint spatial-frequency features, coupled with an adaptive embedding that encodes both shared and task-specific cues as conditioning for denoising. To mitigate feature confusion in conventional concatenation-based conditioning, we design a Dual-Stream Diffusion architecture that processes clean and degraded inputs separately, with a Rectified Fusion Block integrating them in a structured, degradation-aware manner. Furthermore, Noise-Aware Routing Block improves efficiency by dynamically selecting only noise-relevant features during denoising. Experiments on SegSTRONG-C and CEC datasets demonstrate that EndoIR achieves state-of-the-art performance across multiple degradation scenarios while using fewer parameters than strong baselines, and downstream segmentation experiments confirm its clinical utility.
>
---
#### [new 071] Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion
- **分类: cs.CV; cs.AI; cs.RO**

- **简介: 该论文提出FLASH，用于LiDAR超分辨率任务，解决低分辨率传感器感知质量差的问题。通过频域-空域双域融合与自适应多尺度机制，在单次前向传播下实现高精度、实时的3D点云增强，超越现有方法。**

- **链接: [http://arxiv.org/pdf/2511.07377v1](http://arxiv.org/pdf/2511.07377v1)**

> **作者:** June Moh Goo; Zichao Zeng; Jan Boehm
>
> **摘要:** LiDAR super-resolution addresses the challenge of achieving high-quality 3D perception from cost-effective, low-resolution sensors. While recent transformer-based approaches like TULIP show promise, they remain limited to spatial-domain processing with restricted receptive fields. We introduce FLASH (Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a novel framework that overcomes these limitations through dual-domain processing. FLASH integrates two key innovations: (i) Frequency-Aware Window Attention that combines local spatial attention with global frequency-domain analysis via FFT, capturing both fine-grained geometry and periodic scanning patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that replaces conventional skip connections with learned position-specific feature aggregation, enhanced by CBAM attention for dynamic feature selection. Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art performance across all evaluation metrics, surpassing even uncertainty-enhanced baselines that require multiple forward passes. Notably, FLASH outperforms TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which enables real-time deployment. The consistent superiority across all distance ranges validates that our dual-domain approach effectively handles uncertainty through architectural design rather than computationally expensive stochastic inference, making it practical for autonomous systems.
>
---
#### [new 072] Scalable Verification of Neural Control Barrier Functions Using Linear Bound Propagation
- **分类: cs.LG; cs.RO; cs.SY; eess.SY; math.OC**

- **简介: 该论文提出基于线性边界传播的可扩展框架，用于验证神经网络控制屏障函数（CBF）的安全性，解决传统验证方法计算开销大的问题，通过梯度边界与McCormick松弛实现高效、低保守性验证。**

- **链接: [http://arxiv.org/pdf/2511.06341v1](http://arxiv.org/pdf/2511.06341v1)**

> **作者:** Nikolaus Vertovec; Frederik Baymler Mathiesen; Thom Badings; Luca Laurenti; Alessandro Abate
>
> **摘要:** Control barrier functions (CBFs) are a popular tool for safety certification of nonlinear dynamical control systems. Recently, CBFs represented as neural networks have shown great promise due to their expressiveness and applicability to a broad class of dynamics and safety constraints. However, verifying that a trained neural network is indeed a valid CBF is a computational bottleneck that limits the size of the networks that can be used. To overcome this limitation, we present a novel framework for verifying neural CBFs based on piecewise linear upper and lower bounds on the conditions required for a neural network to be a CBF. Our approach is rooted in linear bound propagation (LBP) for neural networks, which we extend to compute bounds on the gradients of the network. Combined with McCormick relaxation, we derive linear upper and lower bounds on the CBF conditions, thereby eliminating the need for computationally expensive verification procedures. Our approach applies to arbitrary control-affine systems and a broad range of nonlinear activation functions. To reduce conservatism, we develop a parallelizable refinement strategy that adaptively refines the regions over which these bounds are computed. Our approach scales to larger neural networks than state-of-the-art verification procedures for CBFs, as demonstrated by our numerical experiments.
>
---
## 更新

#### [replaced 001] DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations
- **分类: cs.CV; cs.AI; cs.RO**

- **链接: [http://arxiv.org/pdf/2505.12310v2](http://arxiv.org/pdf/2505.12310v2)**

> **作者:** Shouyi Lu; Huanyu Zhou; Guirong Zhuo; Xiao Tang
>
> **备注:** 9 pages,5 figures
>
> **摘要:** A novel learning-optimization-combined 4D radar odometry model, named DNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates traditional geometric optimization with end-to-end neural network training, leveraging an innovative differentiable neural-optimization iteration operator. In this framework, point-wise motion flow is first estimated using a neural network, followed by the construction of a cost function based on the relationship between point motion and pose in 3D space. The radar pose is then refined using Gauss-Newton updates. Additionally, we design a dual-stream 4D radar backbone that integrates multi-scale geometric features and clustering-based class-aware features to enhance the representation of sparse 4D radar point clouds. Extensive experiments on the VoD and Snail-Radar datasets demonstrate the superior performance of our model, which outperforms recent classical and learning-based approaches. Notably, our method even achieves results comparable to A-LOAM with mapping optimization using LiDAR point clouds as input. Our models and code will be publicly released.
>
---
#### [replaced 002] Scalable Offline Metrics for Autonomous Driving
- **分类: cs.RO; cs.CV**

- **链接: [http://arxiv.org/pdf/2510.08571v2](http://arxiv.org/pdf/2510.08571v2)**

> **作者:** Animikh Aich; Adwait Kulkarni; Eshed Ohn-Bar
>
> **备注:** Accepted at IROS 2025 (IEEE/RSJ International Conference on Intelligent Robots and Systems); typos corrected
>
> **摘要:** Real-world evaluation of perception-based planning models for robotic systems, such as autonomous vehicles, can be safely and inexpensively conducted offline, i.e. by computing model prediction error over a pre-collected validation dataset with ground-truth annotations. However, extrapolating from offline model performance to online settings remains a challenge. In these settings, seemingly minor errors can compound and result in test-time infractions or collisions. This relationship is understudied, particularly across diverse closed-loop metrics and complex urban maneuvers. In this work, we revisit this undervalued question in policy evaluation through an extensive set of experiments across diverse conditions and metrics. Based on analysis in simulation, we find an even worse correlation between offline and online settings than reported by prior studies, casting doubts on the validity of current evaluation practices and metrics for driving policies. Next, we bridge the gap between offline and online evaluation. We investigate an offline metric based on epistemic uncertainty, which aims to capture events that are likely to cause errors in closed-loop settings. The resulting metric achieves over 13% improvement in correlation compared to previous offline metrics. We further validate the generalization of our findings beyond the simulation environment in real-world settings, where even greater gains are observed.
>
---
#### [replaced 003] MGSO: Monocular Real-time Photometric SLAM with Efficient 3D Gaussian Splatting
- **分类: cs.RO; cs.CV**

- **链接: [http://arxiv.org/pdf/2409.13055v3](http://arxiv.org/pdf/2409.13055v3)**

> **作者:** Yan Song Hu; Nicolas Abboud; Muhammad Qasim Ali; Adam Srebrnjak Yang; Imad Elhajj; Daniel Asmar; Yuhao Chen; John S. Zelek
>
> **备注:** This is the pre-print version of a work that has been published in ICRA 2025 with doi: 10.1109/ICRA55743.2025.11127380. This version may no longer be accessible without notice. Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses. Please cite the official version
>
> **摘要:** Real-time SLAM with dense 3D mapping is computationally challenging, especially on resource-limited devices. The recent development of 3D Gaussian Splatting (3DGS) offers a promising approach for real-time dense 3D reconstruction. However, existing 3DGS-based SLAM systems struggle to balance hardware simplicity, speed, and map quality. Most systems excel in one or two of the aforementioned aspects but rarely achieve all. A key issue is the difficulty of initializing 3D Gaussians while concurrently conducting SLAM. To address these challenges, we present Monocular GSO (MGSO), a novel real-time SLAM system that integrates photometric SLAM with 3DGS. Photometric SLAM provides dense structured point clouds for 3DGS initialization, accelerating optimization and producing more efficient maps with fewer Gaussians. As a result, experiments show that our system generates reconstructions with a balance of quality, memory efficiency, and speed that outperforms the state-of-the-art. Furthermore, our system achieves all results using RGB inputs. We evaluate the Replica, TUM-RGBD, and EuRoC datasets against current live dense reconstruction systems. Not only do we surpass contemporary systems, but experiments also show that we maintain our performance on laptop hardware, making it a practical solution for robotics, A/R, and other real-time applications.
>
---
#### [replaced 004] A Step Toward World Models: A Survey on Robotic Manipulation
- **分类: cs.RO; cs.CV**

- **链接: [http://arxiv.org/pdf/2511.02097v2](http://arxiv.org/pdf/2511.02097v2)**

> **作者:** Peng-Fei Zhang; Ying Cheng; Xiaofan Sun; Shijie Wang; Fengling Li; Lei Zhu; Heng Tao Shen
>
> **备注:** 24 pages, 5 figures
>
> **摘要:** Autonomous agents are increasingly expected to operate in complex, dynamic, and uncertain environments, performing tasks such as manipulation, navigation, and decision-making. Achieving these capabilities requires agents to understand the underlying mechanisms and dynamics of the world, moving beyond reactive control or simple replication of observed states. This motivates the development of world models as internal representations that encode environmental states, capture dynamics, and support prediction, planning, and reasoning. Despite growing interest, the definition, scope, architectures, and essential capabilities of world models remain ambiguous. In this survey, we go beyond prescribing a fixed definition and limiting our scope to methods explicitly labeled as world models. Instead, we examine approaches that exhibit the core capabilities of world models through a review of methods in robotic manipulation. We analyze their roles across perception, prediction, and control, identify key challenges and solutions, and distill the core components, capabilities, and functions that a fully realized world model should possess. Building on this analysis, we aim to motivate further development toward generalizable and practical world models for robotics.
>
---
#### [replaced 005] Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control
- **分类: cs.RO; cs.AI**

- **链接: [http://arxiv.org/pdf/2511.03481v2](http://arxiv.org/pdf/2511.03481v2)**

> **作者:** Jianbo Yuan; Haohua Zhu; Jing Dai; Sheng Yi
>
> **备注:** 8 pages 18 fogures, IEEE RAL accept
>
> **摘要:** The human hand plays a vital role in daily life and industrial applications, yet replicating its multifunctional capabilities-including motion, sensing, and coordinated manipulation with robotic systems remains a formidable challenge. Developing a dexterous robotic hand requires balancing human-like agility with engineering constraints such as complexity, size-to-weight ratio, durability, and force-sensing performance. This letter presents Dex-Hand 021, a high-performance, cable-driven five-finger robotic hand with 12 active and 7 passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight 1 kg design. We propose a proprioceptive force-sensing-based admittance control method to enhance manipulation. Experimental results demonstrate its superior performance: a single-finger load capacity exceeding 10 N, fingertip repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared to PID control, joint torques in multi-object grasping are reduced by 31.19%, significantly improves force-sensing capability while preventing overload during collisions. The hand excels in both power and precision grasps, successfully executing 33 GRASP taxonomy motions and complex manipulation tasks. This work advances the design of lightweight, industrial-grade dexterous hands and enhances proprioceptive control, contributing to robotic manipulation and intelligent manufacturing.
>
---
#### [replaced 006] Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions
- **分类: cs.RO; cs.CV; cs.LG**

- **链接: [http://arxiv.org/pdf/2511.04665v2](http://arxiv.org/pdf/2511.04665v2)**

> **作者:** Kaifeng Zhang; Shuo Sha; Hanxiao Jiang; Matthew Loper; Hyunjong Song; Guangyan Cai; Zhuo Xu; Xiaochen Hu; Changxi Zheng; Yunzhu Li
>
> **备注:** The first two authors contributed equally. Website: https://real2sim-eval.github.io/
>
> **摘要:** Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: https://real2sim-eval.github.io/
>
---
#### [replaced 007] Scaling Cross-Embodiment World Models for Dexterous Manipulation
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2511.01177v2](http://arxiv.org/pdf/2511.01177v2)**

> **作者:** Zihao He; Bo Ai; Tongzhou Mu; Yulin Liu; Weikang Wan; Jiawei Fu; Yilun Du; Henrik I. Christensen; Hao Su
>
> **摘要:** Cross-embodiment learning seeks to build generalist robots that operate across diverse morphologies, but differences in action spaces and kinematics hinder data sharing and policy transfer. This raises a central question: Is there any invariance that allows actions to transfer across embodiments? We conjecture that environment dynamics are embodiment-invariant, and that world models capturing these dynamics can provide a unified interface across embodiments. To learn such a unified world model, the crucial step is to design state and action representations that abstract away embodiment-specific details while preserving control relevance. To this end, we represent different embodiments (e.g., human hands and robot hands) as sets of 3D particles and define actions as particle displacements, creating a shared representation for heterogeneous data and control problems. A graph-based world model is then trained on exploration data from diverse simulated robot hands and real human hands, and integrated with model-based planning for deployment on novel hardware. Experiments on rigid and deformable manipulation tasks reveal three findings: (i) scaling to more training embodiments improves generalization to unseen ones, (ii) co-training on both simulated and real data outperforms training on either alone, and (iii) the learned models enable effective control on robots with varied degrees of freedom. These results establish world models as a promising interface for cross-embodiment dexterous manipulation.
>
---
#### [replaced 008] SceneComplete: Open-World 3D Scene Completion in Cluttered Real World Environments for Robot Manipulation
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2410.23643v5](http://arxiv.org/pdf/2410.23643v5)**

> **作者:** Aditya Agarwal; Gaurav Singh; Bipasha Sen; Tomás Lozano-Pérez; Leslie Pack Kaelbling
>
> **摘要:** Careful robot manipulation in every-day cluttered environments requires an accurate understanding of the 3D scene, in order to grasp and place objects stably and reliably and to avoid colliding with other objects. In general, we must construct such a 3D interpretation of a complex scene based on limited input, such as a single RGB-D image. We describe SceneComplete, a system for constructing a complete, segmented, 3D model of a scene from a single view. SceneComplete is a novel pipeline for composing general-purpose pretrained perception modules (vision-language, segmentation, image-inpainting, image-to-3D, visual-descriptors and pose-estimation) to obtain highly accurate results. We demonstrate its accuracy and effectiveness with respect to ground-truth models in a large benchmark dataset and show that its accurate whole-object reconstruction enables robust grasp proposal generation, including for a dexterous hand. We release the code and additional results on our website.
>
---
#### [replaced 009] DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2506.13922v2](http://arxiv.org/pdf/2506.13922v2)**

> **作者:** Maximilian Du; Shuran Song
>
> **备注:** 9 pages main, 21 pages with appendix and citations. 9 figures. Presented at Neurips 2025
>
> **摘要:** Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io
>
---
#### [replaced 010] Tree-Guided Diffusion Planner
- **分类: cs.AI; cs.RO**

- **链接: [http://arxiv.org/pdf/2508.21800v2](http://arxiv.org/pdf/2508.21800v2)**

> **作者:** Hyeonseong Jeon; Cheolhong Min; Jaesik Park
>
> **备注:** NeurIPS 2025
>
> **摘要:** Planning with pretrained diffusion models has emerged as a promising approach for solving test-time guided control problems. Standard gradient guidance typically performs optimally under convex, differentiable reward landscapes. However, it shows substantially reduced effectiveness in real-world scenarios with non-convex objectives, non-differentiable constraints, and multi-reward structures. Furthermore, recent supervised planning approaches require task-specific training or value estimators, which limits test-time flexibility and zero-shot generalization. We propose a Tree-guided Diffusion Planner (TDP), a zero-shot test-time planning framework that balances exploration and exploitation through structured trajectory generation. We frame test-time planning as a tree search problem using a bi-level sampling process: (1) diverse parent trajectories are produced via training-free particle guidance to encourage broad exploration, and (2) sub-trajectories are refined through fast conditional denoising guided by task objectives. TDP addresses the limitations of gradient guidance by exploring diverse trajectory regions and harnessing gradient information across this expanded solution space using only pretrained models and test-time reward signals. We evaluate TDP on three diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze multi-goal exploration. TDP consistently outperforms state-of-the-art approaches on all tasks. The project page can be found at: https://tree-diffusion-planner.github.io.
>
---
#### [replaced 011] Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2508.08226v2](http://arxiv.org/pdf/2508.08226v2)**

> **作者:** Haiyue Chen; Aniket Datar; Tong Xu; Francesco Cancelliere; Harsh Rangwala; Madhan Balaji Rao; Daeun Song; David Eichinger; Xuesu Xiao
>
> **备注:** 6 pages, accepted by the 2025 IEEE International Symposium on Safety, Security, and Rescue Robotics
>
> **摘要:** Off-road navigation is an important capability for mobile robots deployed in environments that are inaccessible or dangerous to humans, such as disaster response or planetary exploration. Progress is limited due to the lack of a controllable and standardized real-world testbed for systematic data collection and validation. To fill this gap, we introduce Verti-Arena, a reconfigurable indoor facility designed specifically for off-road autonomy. By providing a repeatable benchmark environment, Verti-Arena supports reproducible experiments across a variety of vertically challenging terrains and provides precise ground truth measurements through onboard sensors and a motion capture system. Verti-Arena also supports consistent data collection and comparative evaluation of algorithms in off-road autonomy research. We also develop a web-based interface that enables research groups worldwide to remotely conduct standardized off-road autonomy experiments on Verti-Arena.
>
---
#### [replaced 012] Automated Vehicles at Unsignalized Intersections: Safety and Efficiency Implications of Mixed Human and Automated Traffic
- **分类: cs.RO; cs.AI; stat.AP**

- **链接: [http://arxiv.org/pdf/2410.12538v3](http://arxiv.org/pdf/2410.12538v3)**

> **作者:** Saeed Rahmani; Zhenlin Xu; Simeon C. Calvert; Bart van Arem
>
> **备注:** Published OnlineFirst in Transportation Research Record (TRR), DOI: 10.1177/03611981251370343
>
> **摘要:** The integration of automated vehicles (AVs) into transportation systems presents an unprecedented opportunity to enhance road safety and efficiency. However, understanding the interactions between AVs and human-driven vehicles (HVs) at intersections remains an open research question. This study aims to bridge this gap by examining behavioral differences and adaptations of AVs and HVs at unsignalized intersections by utilizing two large-scale AV datasets from Waymo and Lyft. By using a systematic methodology, the research identifies and analyzes merging and crossing conflicts by calculating key safety and efficiency metrics, including time to collision (TTC), post-encroachment time (PET), maximum required deceleration (MRD), time advantage (TA), and speed and acceleration profiles. Through this approach, the study assesses the safety and efficiency implications of these behavioral differences and adaptations for mixed-autonomy traffic. The findings reveal a paradox: while AVs maintain larger safety margins, their conservative behavior can lead to unexpected situations for human drivers, potentially causing unsafe conditions. From a performance point of view, human drivers tend to exhibit more consistent behavior when interacting with AVs versus other HVs, suggesting AVs may contribute to harmonizing traffic flow patterns. Moreover, notable differences were observed between Waymo and Lyft vehicles, which highlights the importance of considering manufacturer-specific AV behaviors in traffic modeling and management strategies for the safe integration of AVs. The processed dataset, as well as the developed algorithms and scripts, are openly published to foster research on AV-HV interactions.
>
---
#### [replaced 013] IMPACT: Behavioral Intention-aware Multimodal Trajectory Prediction with Adaptive Context Trimming
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2504.09103v3](http://arxiv.org/pdf/2504.09103v3)**

> **作者:** Jiawei Sun; Xibin Yue; Jiahui Li; Tianle Shen; Chengran Yuan; Shuo Sun; Sheng Guo; Quanyun Zhou; Marcelo H Ang Jr
>
> **备注:** accepted by IEEE Robotics and Automation Letters
>
> **摘要:** While most prior research has focused on improving the precision of multimodal trajectory predictions, the explicit modeling of multimodal behavioral intentions (e.g., yielding, overtaking) remains relatively underexplored. This paper proposes a unified framework that jointly predicts both behavioral intentions and trajectories to enhance prediction accuracy, interpretability, and efficiency. Specifically, we employ a shared context encoder for both intention and trajectory predictions, thereby reducing structural redundancy and information loss. Moreover, we address the lack of ground-truth behavioral intention labels in mainstream datasets (Waymo, Argoverse) by auto-labeling these datasets, thus advancing the community's efforts in this direction. We further introduce a vectorized occupancy prediction module that infers the probability of each map polyline being occupied by the target vehicle's future trajectory. By leveraging these intention and occupancy prediction priors, our method conducts dynamic, modality-dependent pruning of irrelevant agents and map polylines in the decoding stage, effectively reducing computational overhead and mitigating noise from non-critical elements. Our approach ranks first among LiDAR-free methods on the Waymo Motion Dataset and achieves first place on the Waymo Interactive Prediction Dataset. Remarkably, even without model ensembling, our single-model framework improves the soft mean average precision (softmAP) by 10 percent compared to the second-best method in the Waymo Interactive Prediction Leaderboard. Furthermore, the proposed framework has been successfully deployed on real vehicles, demonstrating its practical effectiveness in real-world applications.
>
---
#### [replaced 014] Safe On-Orbit Dislodging of Deployable Structures via Robust Adaptive MPC
- **分类: eess.SY; cs.RO; cs.SY**

- **链接: [http://arxiv.org/pdf/2503.16849v2](http://arxiv.org/pdf/2503.16849v2)**

> **作者:** Longsen Gao; Claus Danielson; Andrew Kwas; Rafael Fierro
>
> **备注:** This paper has been resubmitted to IEEE Transactions on Control Systems Technology and is currently under review
>
> **摘要:** This paper proposes a novel robust adaptive model predictive controller for on-orbit dislodging. We study orbit dislodging where a servicing spacecraft uses a robotic arm to free a jammed and unactuated solar panel mounted on a hybrid hinge that acts as a time-varying client on a space station. Our method couples online set-membership identification with a robust adaptive MPC to enforce safety under bounded disturbances. The controller explicitly balances exploration to excite the system and shrink uncertainty and exploitation to improve control performance through a dual-mode cost. The feasibility of the developed robust adaptive MPC method is also examined through dislodging simulations and hardware experiments in freefall and terrestrial laboratory environments, respectively. In addition, the advantages of our method are shown through comparison experiments with several state-of-the-art control schemes for both accuracy of parameter estimation and control performance.
>
---
#### [replaced 015] Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning
- **分类: cs.AI; cs.RO**

- **链接: [http://arxiv.org/pdf/2508.10747v3](http://arxiv.org/pdf/2508.10747v3)**

> **作者:** Sangwoo Jeon; Juchul Shin; Gyeong-Tae Kim; YeonJe Cho; Seongwoo Kim
>
> **备注:** Accepted for publication in International Journal of Control, Automation, and Systems (IJCAS). The Version of Record is available via the publisher
>
> **摘要:** Generalized planning using deep reinforcement learning (RL) combined with graph neural networks (GNNs) has shown promising results in various symbolic planning domains described by PDDL. However, existing approaches typically represent planning states as fully connected graphs, leading to a combinatorial explosion in edge information and substantial sparsity as problem scales grow, especially evident in large grid-based environments. This dense representation results in diluted node-level information, exponentially increases memory requirements, and ultimately makes learning infeasible for larger-scale problems. To address these challenges, we propose a sparse, goal-aware GNN representation that selectively encodes relevant local relationships and explicitly integrates spatial features related to the goal. We validate our approach by designing novel drone mission scenarios based on PDDL within a grid world, effectively simulating realistic mission execution environments. Our experimental results demonstrate that our method scales effectively to larger grid sizes previously infeasible with dense graph representations and substantially improves policy generalization and success rates. Our findings provide a practical foundation for addressing realistic, large-scale generalized planning tasks.
>
---
#### [replaced 016] X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real
- **分类: cs.RO; cs.AI; cs.LG**

- **链接: [http://arxiv.org/pdf/2505.07096v5](http://arxiv.org/pdf/2505.07096v5)**

> **作者:** Prithwish Dan; Kushal Kedia; Angela Chao; Edward Weiyi Duan; Maximus Adrian Pace; Wei-Chiu Ma; Sanjiban Choudhury
>
> **摘要:** Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.
>
---
#### [replaced 017] Simulator Ensembles for Trustworthy Autonomous Driving Testing
- **分类: cs.SE; cs.AI; cs.RO**

- **链接: [http://arxiv.org/pdf/2503.08936v2](http://arxiv.org/pdf/2503.08936v2)**

> **作者:** Lev Sorokin; Matteo Biagiola; Andrea Stocco
>
> **摘要:** Scenario-based testing with driving simulators is extensively used to identify failing conditions of automated driving assistance systems (ADAS). However, existing studies have shown that repeated test execution in the same as well as in distinct simulators can yield different outcomes, which can be attributed to sources of flakiness or different implementations of the physics. In this paper, we present MultiSim, a novel approach to multi-simulation ADAS testing based on a search-based testing approach that leverages an ensemble of simulators to identify failure-inducing, simulator-agnostic test scenarios. During the search, each scenario is evaluated jointly on multiple simulators. Scenarios that produce consistent results across simulators are prioritized for further exploration, while those that fail on only a subset of simulators are given less priority, as they may reflect simulator-specific issues rather than generalizable failures. Our empirical study, which involves testing three lane-keeping ADAS on different pairs of three widely used simulators, demonstrates that MultiSim outperforms single-simulator testing by achieving, on average, a higher rate of simulator-agnostic failures by 66%. Compared to a state-of-the-art multi-simulator approach that combines the outcome of independent test generation campaigns obtained in different simulators, MultiSim identifies, on average, up to 3.4X more simulator-agnostic failing tests and higher failure rates. To avoid the costly execution of test inputs on which simulators disagree, we propose to predict simulator disagreements and bypass test executions. Our results show that utilizing a surrogate model during the search retains the average number of valid failures and also improves efficiency. Our findings indicate that combining an ensemble of simulators is a promising approach for the automated cross-replication in ADAS testing.
>
---
#### [replaced 018] The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards
- **分类: cs.LG; cs.RO**

- **链接: [http://arxiv.org/pdf/2409.15922v5](http://arxiv.org/pdf/2409.15922v5)**

> **作者:** Sukai Huang; Shu-Wei Liu; Nir Lipovetzky; Trevor Cohn
>
> **备注:** accepted by PRL Workshop Series @ ICAPS 2025. 11 main body pages, 21 appendix pages
>
> **摘要:** While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents
>
---
#### [replaced 019] J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities Effectively in Inverse Kinematic Control of Serial Manipulators
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2505.00306v4](http://arxiv.org/pdf/2505.00306v4)**

> **作者:** Shivani Guptasarma; Matthew Strong; Honghao Zhen; Monroe Kennedy III
>
> **备注:** 18 pages, 25 figures. v1: Fig. 1 replaced with faster-loading version. v2: Website at https://jparse-manip.github.io/. v3: Proofs revised and new material added
>
> **摘要:** J-PARSE is an algorithm for smooth first-order inverse kinematic control of a serial manipulator near kinematic singularities. The commanded end-effector velocity is interpreted component-wise, according to the available mobility in each dimension of the task space. First, a substitute "Safety" Jacobian matrix is created, keeping the aspect ratio of the manipulability ellipsoid above a threshold value. The desired motion is then projected onto non-singular and singular directions, and the latter projection scaled down by a factor informed by the threshold value. A right-inverse of the non-singular Safety Jacobian is applied to the modified command. In the absence of joint limits and collisions, this ensures safe transition into and out of low-rank configurations, guaranteeing asymptotic stability for reaching target poses within the workspace, and stability for those outside. Velocity control with J-PARSE is benchmarked against approaches from the literature, and shows high accuracy in reaching and leaving singular target poses. By expanding the available workspace of manipulators, the algorithm finds applications in teleoperation, servoing, and learning. Videos and code are available at https://jparse-manip.github.io/.
>
---
#### [replaced 020] LOG-Nav: Efficient Layout-Aware Object-Goal Navigation with Hierarchical Planning
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2505.06131v2](http://arxiv.org/pdf/2505.06131v2)**

> **作者:** Jiawei Hou; Yuting Xiao; Xiangyang Xue; Taiping Zeng
>
> **摘要:** We introduce LOG-Nav, an efficient layout-aware object-goal navigation approach designed for complex multi-room indoor environments. By planning hierarchically leveraging a global topologigal map with layout information and local imperative approach with detailed scene representation memory, LOG-Nav achieves both efficient and effective navigation. The process is managed by an LLM-powered agent, ensuring seamless effective planning and navigation, without the need for human interaction, complex rewards, or costly training. Our experimental results on the MP3D benchmark achieves 85\% object navigation success rate (SR) and 79\% success rate weighted by path length (SPL) (over 40\% point improvement in SR and 60\% improvement in SPL compared to exsisting methods). Furthermore, we validate the robustness of our approach through virtual agent and real-world robotic deployment, showcasing its capability in practical scenarios.
>
---
#### [replaced 021] ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2507.11000v2](http://arxiv.org/pdf/2507.11000v2)**

> **作者:** Minwoo Cho; Jaehwi Jang; Daehyung Park
>
> **备注:** 8 pages, 6 figures, IEEE Robotics and Automation Letters (RA-L)
>
> **摘要:** We aim to solve the problem of temporal-constraint learning from demonstrations to reproduce demonstration-like logic-constrained behaviors. Learning logic constraints is challenging due to the combinatorially large space of possible specifications and the ill-posed nature of non-Markovian constraints. To figure it out, we introduce a novel temporal-constraint learning method, which we call inverse logic-constraint learning (ILCL). Our method frames ICL as a two-player zero-sum game between 1) a genetic algorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained reinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax trees for parameterized truncated linear temporal logic (TLTL) without predefined templates. Subsequently, Logic-CRL finds a policy that maximizes task rewards under the constructed TLTL constraints via a novel constraint redistribution scheme. Our evaluations show ILCL outperforms state-of-the-art baselines in learning and transferring TL constraints on four temporally constrained tasks. We also demonstrate successful transfer to real-world peg-in-shallow-hole tasks.
>
---
#### [replaced 022] Whole-body motion planning and safety-critical control for aerial manipulation
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2511.02342v2](http://arxiv.org/pdf/2511.02342v2)**

> **作者:** Lin Yang; Jinwoo Lee; Domenico Campolo; H. Jin Kim; Jeonghyun Byun
>
> **备注:** Submitted to 2026 IFAC World Congress with the Journal option (MECHATRONICS)
>
> **摘要:** Aerial manipulation combines the maneuverability of multirotors with the dexterity of robotic arms to perform complex tasks in cluttered spaces. Yet planning safe, dynamically feasible trajectories remains difficult due to whole-body collision avoidance and the conservativeness of common geometric abstractions such as bounding boxes or ellipsoids. We present a whole-body motion planning and safety-critical control framework for aerial manipulators built on superquadrics (SQs). Using an SQ-plus-proxy representation, we model both the vehicle and obstacles with differentiable, geometry-accurate surfaces. Leveraging this representation, we introduce a maximum-clearance planner that fuses Voronoi diagrams with an equilibrium-manifold formulation to generate smooth, collision-aware trajectories. We further design a safety-critical controller that jointly enforces thrust limits and collision avoidance via high-order control barrier functions. In simulation, our approach outperforms sampling-based planners in cluttered environments, producing faster, safer, and smoother trajectories and exceeding ellipsoid-based baselines in geometric fidelity. Actual experiments on a physical aerial-manipulation platform confirm feasibility and robustness, demonstrating consistent performance across simulation and hardware settings. The video can be found at https://youtu.be/hQYKwrWf1Ak.
>
---
#### [replaced 023] ImitDiff: Transferring Foundation-Model Priors for Distraction Robust Visuomotor Policy
- **分类: cs.AI; cs.CV; cs.LG; cs.RO**

- **链接: [http://arxiv.org/pdf/2502.09649v2](http://arxiv.org/pdf/2502.09649v2)**

> **作者:** Yuhang Dong; Haizhou Ge; Yupei Zeng; Jiangning Zhang; Beiwen Tian; Hongrui Zhu; Yufei Jia; Ruixiang Wang; Zhucun Xue; Guyue Zhou; Longhua Ma; Guanzhong Tian
>
> **摘要:** Visuomotor imitation learning policies enable robots to efficiently acquire manipulation skills from visual demonstrations. However, as scene complexity and visual distractions increase, policies that perform well in simple settings often experience substantial performance degradation. To address this challenge, we propose ImitDiff, a diffusion-based imitation learning policy guided by fine-grained semantics within a dual-resolution workflow. Leveraging pretrained priors of vision-language foundation models, our method transforms high-level instructions into pixel-level visual semantic masks. These masks guide a dual-resolution perception pipeline that captures both global context (e.g., overall layout) from low-resolution observation and fine-grained local features (e.g., geometric details) from high-resolution observation, enabling the policy to focus on task-relevant regions. Additionally, we introduce a consistency-driven diffusion transformer action head that bridges visual semantic conditions and real-time action generation. Extensive experiments demonstrate that ImitDiff outperforms state-of-the-art vision-language manipulation frameworks, as well as visuomotor imitation learning policies, particularly under increased scene complexity and visual distractions. Notably, ImitDiff exhibits strong generalization in zero-shot settings involving novel objects and visual distractions. Furthermore, our consistency-driven action head achieves an order-of-magnitude improvement in inference speed while maintaining competitive success rates.
>
---
#### [replaced 024] Multi-cam Multi-map Visual Inertial Localization: System, Validation and Dataset
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2412.04287v2](http://arxiv.org/pdf/2412.04287v2)**

> **作者:** Yufei Wei; Fuzhang Han; Yanmei Jiao; Zhuqing Zhang; Yiyuan Pan; Wenjun Huang; Li Tang; Huan Yin; Xiaqing Ding; Chenxiao Hu; Rong Xiong; Yue Wang
>
> **摘要:** Robot control loops require causal pose estimates that depend only on past and present measurements. At each timestep, controllers compute commands using the current pose without waiting for future refinements. While traditional visual SLAM systems achieve high accuracy through retrospective loop closures, these corrections arrive after control decisions were already executed, violating causality. Visual-inertial odometry maintains causality but accumulates unbounded drift over time. To address the distinct requirements of robot control, we propose a multi-camera multi-map visual-inertial localization system providing real-time, causal pose estimation with bounded localization error through continuous map constraints. Since standard trajectory metrics evaluate post-processed trajectories, we analyze the error composition of map-based localization systems and propose a set of evaluation metrics suitable for measuring causal localization performance. To validate our system, we design a multi-camera IMU hardware setup and collect a challenging long-term campus dataset featuring diverse illumination and seasonal conditions. Experimental results on public benchmarks and on our own collected dataset demonstrate that our system provides significantly higher real-time localization accuracy compared to other methods. To benefit the community, we have made both the system and the dataset open source at https://anonymous.4open.science/r/Multi-cam-Multi-map-VILO-7993.
>
---
#### [replaced 025] Canonical Policy: Learning Canonical 3D Representation for SE(3)-Equivariant Policy
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2505.18474v2](http://arxiv.org/pdf/2505.18474v2)**

> **作者:** Zhiyuan Zhang; Zhengtong Xu; Jai Nanda Lakamsani; Yu She
>
> **摘要:** Visual Imitation learning has achieved remarkable progress in robotic manipulation, yet generalization to unseen objects, scene layouts, and camera viewpoints remains a key challenge. Recent advances address this by using 3D point clouds, which provide geometry-aware, appearance-invariant representations, and by incorporating equivariance into policy architectures to exploit spatial symmetries. However, existing equivariant approaches often lack interpretability and rigor due to unstructured integration of equivariant components. We introduce canonical policy, a principled framework for 3D equivariant imitation learning that unifies 3D point cloud observations under a canonical representation. We first establish a theory of 3D canonical representations, enabling equivariant observation-to-action mappings by grouping both seen and novel point clouds to a canonical representation. We then propose a flexible policy learning pipeline that leverages geometric symmetries from canonical representation and the expressiveness of modern generative models. We validate canonical policy on 12 diverse simulated tasks and 4 real-world manipulation tasks across 16 configurations, involving variations in object color, shape, camera viewpoint, and robot platform. Compared to state-of-the-art imitation learning policies, canonical policy achieves an average improvement of 18.0% in simulation and 39.7% in real-world experiments, demonstrating superior generalization capability and sample efficiency. For more details, please refer to the project website: https://zhangzhiyuanzhang.github.io/cp-website/.
>
---
#### [replaced 026] Online Learning and Coverage of Unknown Fields Using Random-Feature Gaussian Processes
- **分类: cs.RO; cs.SY; eess.SY**

- **链接: [http://arxiv.org/pdf/2509.08117v2](http://arxiv.org/pdf/2509.08117v2)**

> **作者:** Ruijie Du; Ruoyu Lin; Yanning Shen; Magnus Egerstedt
>
> **摘要:** This paper proposes a framework for multi-robot systems to perform simultaneous learning and coverage of a domain of interest characterized by an unknown and potentially time-varying density function. To overcome the limitations of Gaussian Process (GP) regression, we employ Random Feature GP (RFGP) and its online variant (O-RFGP) which enables online and incremental inference. By integrating these with Voronoi-based coverage control and Upper Confidence Bound (UCB) sampling strategy, a team of robots can adaptively focus on important regions while refining the learned spatial field for efficient coverage. The incremental update mechanism of O-RFGP naturally supports time-varying environments, allowing efficient adaptation without retaining historical data. Furthermore, to the best of our knowledge, we provide the first theoretical analysis of online learning and coverage through a regret-based formulation, establishing asymptotic no-regret guarantees in the time-invariant setting. The effectiveness of the proposed framework is demonstrated through simulations with both time-invariant and time-varying density functions, along with a physical experiment with a time-varying density function.
>
---
#### [replaced 027] Pure Vision Language Action (VLA) Models: A Comprehensive Survey
- **分类: cs.RO; cs.AI**

- **链接: [http://arxiv.org/pdf/2509.19012v3](http://arxiv.org/pdf/2509.19012v3)**

> **作者:** Dapeng Zhang; Jing Sun; Chenghui Hu; Xiaoyan Wu; Zhenlong Yuan; Rui Zhou; Fei Shen; Qingguo Zhou
>
> **摘要:** The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.
>
---
#### [replaced 028] A High-Speed Time-Optimal Trajectory Generation Strategy via a Two-layer Planning Model
- **分类: cs.RO; math.OC**

- **链接: [http://arxiv.org/pdf/2503.11072v3](http://arxiv.org/pdf/2503.11072v3)**

> **作者:** Haotian Tan; Yuan-Hua Ni
>
> **摘要:** MPC (Model predictive control)-based motion planning and trajectory generation are essential in applications such as unmanned aerial vehicles, robotic manipulators, and rocket control. However, the real-time implementation of such optimization-based planning faces significant challenges arising from non-convex problem structures and inherent limitations of nonlinear programming -- notably the difficulty in guaranteeing solution quality and the unpredictability of computation time. To improve robustness and computational efficiency, this paper introduces a two-layer motion planning algorithm for intelligent ground vehicles based on convex optimization. The proposed algorithm iteratively constructs discrete optimal control subproblems with small, fixed terminal times, referred to as planning cycles. Each planning cycle is further solved within progressively constructed convex sets generated by utilizing customized search algorithms. The entire solution to the original problem is obtained by incrementally composing the solutions of these subproblems. The proposed algorithm demonstrates enhanced reliability and significantly reduced computation time. We support our approach with theoretical analysis under practical assumptions and numerical experiments. Comparative results with standard sequential convex programming (SCP) methods demonstrate the superiority of our method -- include a significant improved computational speed under dynamic environments while maintain a near optimal final time.
>
---
#### [replaced 029] DiffOG: Differentiable Policy Trajectory Optimization with Generalizability
- **分类: cs.RO**

- **链接: [http://arxiv.org/pdf/2504.13807v5](http://arxiv.org/pdf/2504.13807v5)**

> **作者:** Zhengtong Xu; Zichen Miao; Qiang Qiu; Zhe Zhang; Yu She
>
> **摘要:** Imitation learning-based visuomotor policies excel at manipulation tasks but often produce suboptimal action trajectories compared to model-based methods. Directly mapping camera data to actions via neural networks can result in jerky motions and difficulties in meeting critical constraints, compromising safety and robustness in real-world deployment. For tasks that require high robustness or strict adherence to constraints, ensuring trajectory quality is crucial. However, the lack of interpretability in neural networks makes it challenging to generate constraint-compliant actions in a controlled manner. This paper introduces differentiable policy trajectory optimization with generalizability (DiffOG), a learning-based trajectory optimization framework designed to enhance visuomotor policies. By leveraging the proposed differentiable formulation of trajectory optimization with transformer, DiffOG seamlessly integrates policies with a generalizable optimization layer. DiffOG refines action trajectories to be smoother and more constraint-compliant while maintaining alignment with the original demonstration distribution, thus avoiding degradation in policy performance. We evaluated DiffOG across 11 simulated tasks and 2 real-world tasks. The results demonstrate that DiffOG significantly enhances the trajectory quality of visuomotor policies while having minimal impact on policy performance, outperforming trajectory processing baselines such as greedy constraint clipping and penalty-based trajectory optimization. Furthermore, DiffOG achieves superior performance compared to existing constrained visuomotor policy. For more details, please visit the project website: https://zhengtongxu.github.io/diffog-website/.
>
---
#### [replaced 030] Environment-Driven Online LiDAR-Camera Extrinsic Calibration
- **分类: cs.CV; cs.AI; cs.RO**

- **链接: [http://arxiv.org/pdf/2502.00801v3](http://arxiv.org/pdf/2502.00801v3)**

> **作者:** Zhiwei Huang; Jiaqi Li; Hongbo Zhao; Xiao Ma; Ping Zhong; Xiaohu Zhou; Wei Ye; Rui Fan
>
> **摘要:** LiDAR-camera extrinsic calibration (LCEC) is crucial for multi-modal data fusion in autonomous robotic systems. Existing methods, whether target-based or target-free, typically rely on customized calibration targets or fixed scene types, which limit their applicability in real-world scenarios. To address these challenges, we present EdO-LCEC, the first environment-driven online calibration approach. Unlike traditional target-free methods, EdO-LCEC employs a generalizable scene discriminator to estimate the feature density of the application environment. Guided by this feature density, EdO-LCEC extracts LiDAR intensity and depth features from varying perspectives to achieve higher calibration accuracy. To overcome the challenges of cross-modal feature matching between LiDAR and camera, we introduce dual-path correspondence matching (DPCM), which leverages both structural and textural consistency for reliable 3D-2D correspondences. Furthermore, we formulate the calibration process as a joint optimization problem that integrates global constraints across multiple views and scenes, thereby enhancing overall accuracy. Extensive experiments on real-world datasets demonstrate that EdO-LCEC outperforms state-of-the-art methods, particularly in scenarios involving sparse point clouds or partially overlapping sensor views.
>
---
#### [replaced 031] Real-time Multi-view Omnidirectional Depth Estimation for Real Scenarios based on Teacher-Student Learning with Unlabeled Data
- **分类: cs.CV; cs.RO**

- **链接: [http://arxiv.org/pdf/2409.07843v2](http://arxiv.org/pdf/2409.07843v2)**

> **作者:** Ming Li; Xiong Yang; Chaofan Wu; Jiaheng Li; Pinzhi Wang; Xuejiao Hu; Sidan Du; Yang Li
>
> **摘要:** Omnidirectional depth estimation enables efficient 3D perception over a full 360-degree range. However, in real-world applications such as autonomous driving and robotics, achieving real-time performance and robust cross-scene generalization remains a significant challenge for existing algorithms. In this paper, we propose a real-time omnidirectional depth estimation method for edge computing platforms named Rt-OmniMVS, which introduces the Combined Spherical Sweeping method and implements the lightweight network structure to achieve real-time performance on edge computing platforms. To achieve high accuracy, robustness, and generalization in real-world environments, we introduce a teacher-student learning strategy. We leverage the high-precision stereo matching method as the teacher model to predict pseudo labels for unlabeled real-world data, and utilize data and model augmentation techniques for training to enhance performance of the student model Rt-OmniMVS. We also propose HexaMODE, an omnidirectional depth sensing system based on multi-view fisheye cameras and edge computation device. A large-scale hybrid dataset contains both unlabeled real-world data and synthetic data is collected for model training. Experiments on public datasets demonstrate that proposed method achieves results comparable to state-of-the-art approaches while consuming significantly less resource. The proposed system and algorithm also demonstrate high accuracy in various complex real-world scenarios, both indoors and outdoors, achieving an inference speed of 15 frames per second on edge computing platforms.
>
---
#### [replaced 032] SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where
- **分类: cs.GR; cs.MM; cs.RO**

- **链接: [http://arxiv.org/pdf/2509.23852v4](http://arxiv.org/pdf/2509.23852v4)**

> **作者:** Yiheng Huang; Junran Peng; Silei Shen; Jingwei Yang; ZeJi Wei; ChenCheng Bai; Yonghao He; Wei Sui; Muyi Sun; Yan Liu; Xu-Cheng Yin; Man Zhang; Zhaoxiang Zhang; Chuanchen Luo
>
> **摘要:** The accompanying actions and gestures in dialogue are often closely linked to interactions with the environment, such as looking toward the interlocutor or using gestures to point to the described target at appropriate moments. Speech and semantics guide the production of gestures by determining their timing (WHEN) and style (HOW), while the spatial locations of interactive objects dictate their directional execution (WHERE). Existing approaches either rely solely on descriptive language to generate motions or utilize audio to produce non-interactive gestures, thereby lacking the characterization of interactive timing and spatial intent. This significantly limits the applicability of conversational gesture generation, whether in robotics or in the fields of game and animation production. To address this gap, we present a full-stack solution. We first established a unique data collection method to simultaneously capture high-precision human motion and spatial intent. We then developed a generation model driven by audio, language, and spatial data, alongside dedicated metrics for evaluating interaction timing and spatial accuracy. Finally, we deployed the solution on a humanoid robot, enabling rich, context-aware physical interactions.
>
---
#### [replaced 033] A Learning-Based Control Barrier Function for Car-Like Robots: Toward Less Conservative Collision Avoidance
- **分类: cs.RO; cs.MA; cs.SY; eess.SY**

- **链接: [http://arxiv.org/pdf/2411.08999v2](http://arxiv.org/pdf/2411.08999v2)**

> **作者:** Jianye Xu; Bassam Alrifaee
>
> **备注:** 8 pages, 8 figures
>
> **摘要:** We propose a learning-based Control Barrier Function (CBF) to reduce conservatism in collision avoidance for car-like robots. Traditional CBFs often use the Euclidean distance between robots' centers as a safety margin, which neglects their headings and approximates their geometries as circles. Although this simplification meets the smoothness and differentiability requirements of CBFs, it may result in overly conservative behavior in dense environments. We address this by designing a safety margin that considers both the robot's heading and actual shape, thereby enabling a more precise estimation of safe regions. Because this safety margin is non-differentiable, we approximate it with a neural network to ensure differentiability. In addition, we propose a notion of relative dynamics that makes the learning process tractable. In a case study, we establish the theoretical foundation for applying this notion to a nonlinear kinematic bicycle model. Numerical experiments in overtaking and bypassing scenarios show that our approach reduces conservatism (e.g., requiring 33.5% less lateral space for bypassing) without incurring significant extra computation time. Code: https://github.com/bassamlab/sigmarl
>
---
#### [replaced 034] Toward an Agricultural Operational Design Domain: A Framework
- **分类: cs.RO; cs.SE; cs.SY; eess.SY; I.2.9; I.1.4; J.7**

- **链接: [http://arxiv.org/pdf/2511.02937v2](http://arxiv.org/pdf/2511.02937v2)**

> **作者:** Mirco Felske; Jannik Redenius; Georg Happich; Julius Schöning
>
> **备注:** 18 pages, 7 figures, 2 tables
>
> **摘要:** The agricultural sector increasingly relies on autonomous systems that operate in complex and variable environments. Unlike on-road applications, agricultural automation integrates driving and working processes, each of which imposes distinct operational constraints. Handling this complexity and ensuring consistency throughout the development and validation processes requires a structured, transparent, and verified description of the environment. However, existing Operational Design Domain (ODD) concepts do not yet address the unique challenges of agricultural applications. Therefore, this work introduces the Agricultural ODD (Ag-ODD) Framework, which can be used to describe and verify the operational boundaries of autonomous agricultural systems. The Ag-ODD Framework consists of three core elements. First, the Ag-ODD description concept, which provides a structured method for unambiguously defining environmental and operational parameters using concepts from ASAM Open ODD and CityGML. Second, the 7-Layer Model derived from the PEGASUS 6-Layer Model, has been extended to include a process layer to capture dynamic agricultural operations. Third, the iterative verification process verifies the Ag-ODD against its corresponding logical scenarios, derived from the 7-Layer Model, to ensure the Ag-ODD's completeness and consistency. Together, these elements provide a consistent approach for creating unambiguous and verifiable Ag-ODD. Demonstrative use cases show how the Ag-ODD Framework can support the standardization and scalability of environmental descriptions for autonomous agricultural systems.
>
---
